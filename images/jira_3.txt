Summary: Spark job relying over Hudi are blocked after one or zero commit
Issue key: SPARK-47842
Issue id: 13575730
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Blocker
Resolution: 
Assignee: 
Reporter: pontisa
Creator: pontisa
Created: 13/Apr/24 14:00
Updated: 26/Apr/24 00:22
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark, Structured Streaming
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Hello, we are facing the fact that some pyspark job that rely on Hudi seems to be blocked, in fact if we go over the spark console we can see the situation in the attachment
we can see that we have 71 completed jobs but those are CDC process that should read from Kafka topic continuously. We verified yet that there are messages queued over the kafka topic. If you kill the application and then restart in some cases the job will act normally and other times the job still remain stacked.

Our deploy condition are the following:
We read INSERT, UPDATE and DELETE operation from a Kafka topic and we replicate them in a target hudi table stored on Hive via a pyspark job running 24/7

 

PYSPARK WRITE
df_source.writeStream.foreachBatch(foreach_batch_write_function)
 {{ FOR EACH BATCH FUNCTION:
#management of delete messages
batchDF_deletes.write.format('hudi') \
.option('hoodie.datasource.write.operation', 'delete') \
.options(**hudiOptions_table) \
.mode('append') \
.save(S3_OUTPUT_PATH)

#management of update and insert messages
batchDF_upserts.write.format('org.apache.hudi') \
.option('hoodie.datasource.write.operation', 'upsert') \
.options(**hudiOptions_table) \
.mode('append') \
.save(S3_OUTPUT_PATH)}}
 
SPARK SUBMIT
spark-submit --master yarn --deploy-mode cluster --num-executors 1 --executor-memory 1G --executor-cores 2 --conf spark.dynamicAllocation.enabled=false --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.sql.hive.convertMetastoreParquet=false --jars /usr/lib/hudi/hudi-spark-bundle.jar <path_to_script>
Environment: Hudi version : 0.12.1-amzn-0
Spark version : 3.3.0
Hive version : 3.1.3
Hadoop version : 3.3.3 amz
Storage (HDFS/S3/GCS..) : S3
Running on Docker? (yes/no) : no (EMR 6.9.0)
Additional context
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 13/Apr/24 14:03;pontisa;console_spark.png;https://issues.apache.org/jira/secure/attachment/13068176/console_spark.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): English, italian
Custom field (Last public comment date): 2024-04-13 14:00:32.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1om8o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Structured Streaming
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Optimize hive patition filter when the comparision dataType not match
Issue key: SPARK-45387
Issue id: 13552522
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: tianyima
Creator: tianyima
Created: 30/Sep/23 09:47
Updated: 16/Apr/24 07:44
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.3.0, 3.4.0, 3.5.0, 3.5.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Suppose we have a partitioned table `table_pt` with partition colum `dt` which is StringType and the table metadata is managed by Hive Metastore, if we filter partition by dt = '123', this filter can be pushed down to data source directly, but if the filter condition is number, e.g. dt = 123, Spark will not known which partition should be pushed down. Thus in the process of physical plan optimization, Spark will pull all of that table's partition meta data to client side, to decide which partition filter should be push down to the data source. This is poor of performance if the table has thousands of partitions and increasing the risk of hive metastore oom.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 13/Oct/23 15:40;tianyima;PruneFileSourcePartitions.diff;https://issues.apache.org/jira/secure/attachment/13063546/PruneFileSourcePartitions.diff
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 05 09:57:20 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1kobk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Feb/24 03:43;doki;I can't reproduce it in spark 3.5.0.

I try to create a partitioned csv table on hdfs like follow:
{code:java}
create external table noaa (column0 string, column1 int, column2 string, column3 int, column4 string, column5 string, column6 string, column7 string) PARTITIONED BY (year string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' LOCATION '/tmp/noaa'; 

alter table noaa add partition (year = '2019') LOCATION '/tmp/noaa/year=2019';

alter table noaa add partition (year = '2020') LOCATION '/tmp/noaa/year=2020';{code}
and the spark plan is 
{code:java}
scala> spark.sql("select * from noaa where year=2019 limit 10").explain(true)
== Parsed Logical Plan ==
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project [*]
      +- 'Filter ('year = 2019)
         +- 'UnresolvedRelation [noaa], [], false== Analyzed Logical Plan ==
column0: string, column1: string, column2: string, column3: string, column4: string, column5: string, column6: string, column7: string, year: string
GlobalLimit 10
+- LocalLimit 10
   +- Project [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62, year#63]
      +- Filter (cast(year#63 as int) = 2019)
         +- SubqueryAlias spark_catalog.default.noaa
            +- HiveTableRelation [`spark_catalog`.`default`.`noaa`, org.apache.hadoop.hive.serde2.OpenCSVSerde, Data Cols: [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62], Partition Cols: [year#63]]== Optimized Logical Plan ==
GlobalLimit 10
+- LocalLimit 10
   +- Filter (isnotnull(year#63) AND (cast(year#63 as int) = 2019))
      +- HiveTableRelation [`spark_catalog`.`default`.`noaa`, org.apache.hadoop.hive.serde2.OpenCSVSerde, Data Cols: [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62], Partition Cols: [year#63], Pruned Partitions: [(year=2019)]]== Physical Plan ==
CollectLimit 10
+- Scan hive spark_catalog.default.noaa [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62, year#63], HiveTableRelation [`spark_catalog`.`default`.`noaa`, org.apache.hadoop.hive.serde2.OpenCSVSerde, Data Cols: [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62], Partition Cols: [year#63], Pruned Partitions: [(year=2019)]], [isnotnull(year#63), (cast(year#63 as int) = 2019)]{code}
The filter has been pushed down.;;;, 05/Mar/24 09:57;tianyima;[~doki] the output execution plan is the final result, but the problem lies in the optimize process.

In your example, the partition key is stringType, but was cast to int to filter partitions. The driver will get all the partition to do this filter. If you have a hive table with thousands of partitions, this process will very slow and costly.;;;
Affects Version/s.1: 3.1.2
Component/s.1: 
Comment.1: 05/Mar/24 09:57;tianyima;[~doki] the output execution plan is the final result, but the problem lies in the optimize process.

In your example, the partition key is stringType, but was cast to int to filter partitions. The driver will get all the partition to do this filter. If you have a hive table with thousands of partitions, this process will very slow and costly.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0, Unknown

Summary: spark-xml misplaces string tag content
Issue key: SPARK-45414
Issue id: 13552891
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: gcera21
Creator: gcera21
Created: 04/Oct/23 13:31
Updated: 07/Feb/24 06:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: h1. Intro

Hi all! Please expect some degree of incompleteness in this issue as this is the very first one I post, and feel free to edit it as you like - I welcome your feedback.

My goal is to provide you with as many details and indications as I can on this issue that I am currently facing with a Client of mine on its Production environment (we use Azure Databricks DBR 11.3 LTS).

I was told by Sean Owen [[srowen (Sean Owen) (github.com)|https://github.com/srowen]], who maintains the spark-xml maven repository on GitHub [[https://github.com/srowen/spark-xml]] to post an issue here because "This code has been ported to Apache Spark now anyway so won't be updated here" (refer to his comment [here|#issuecomment-1744792958]).
h1. Issue

When I write a DataFrame into xml format via the spark-xml library either (1) I get an error if empty string columns are in between non-string nested ones or (2) if I put all string columns at the end then I get a wrong xml where the content of string tags are misplaced into the following ones.
h1. Code to reproduce the issue

Please find below the end-to-end code snippet that results into the error
h2. CASE (1): ERROR

When empty strings are in between non-string nested ones, the write fails with the following error.

_Caused by: java.lang.IllegalArgumentException: Failed to convert value MyDescription (class of class java.lang.String) in type ArrayType(StructType(StructField(_ID,StringType,true),StructField(_Level,StringType,true)),true) to XML._

Please find attached the full trace of the error.
{code:python}
fake_file_df = spark \
    .sql(
        """SELECT
            CAST(STRUCT('ItemId' AS `_Type`, '123' AS `_VALUE`) AS STRUCT<_Type: STRING, _VALUE: STRING>) AS ItemID,
            CAST(STRUCT('UPC' AS `_Type`, '123' AS `_VALUE`) AS STRUCT<_Type: STRING, _VALUE: STRING>) AS UPC,
            CAST('' AS STRING) AS _SerialNumberFlag,
            CAST('MyDescription' AS STRING) AS Description,
            CAST(ARRAY(STRUCT(NULL AS `_ID`, NULL AS `_Level`)) AS ARRAY<STRUCT<_ID: STRING, _Level: STRING>>) AS MerchandiseHierarchy,
            CAST(ARRAY(STRUCT(NULL AS `_ValueTypeCode`, NULL AS `_VALUE`)) AS ARRAY<STRUCT<_ValueTypeCode: STRING, _Value: STRING>>) AS ItemPrice,
            CAST('' AS STRING) AS Color,
            CAST('' AS STRING) AS IntendedIndustry,
            CAST(STRUCT(NULL AS `Name`) AS STRUCT<Name: STRING>) AS Manufacturer,
            CAST(STRUCT(NULL AS `Season`) AS STRUCT<Season: STRING>) AS Marketing,
            CAST(STRUCT(NULL AS `_Name`) AS STRUCT<_Name: STRING>) AS BrandOwner,
            CAST(ARRAY(STRUCT('Attribute1' AS `_Name`, 'Value1' AS `_VALUE`)) AS ARRAY<STRUCT<_Name: STRING, AttributeValue: STRING>>) AS ItemAttribute_culinary,
            CAST(ARRAY(STRUCT(NULL AS `_Name`, ARRAY(ARRAY(STRUCT(NULL AS `AttributeCode`, NULL AS `AttributeValue`))) AS `_VALUE`)) AS ARRAY<STRUCT<_Name: STRING, _VALUE: ARRAY<ARRAY<STRUCT<AttributeCode: STRING, AttributeValue: STRING>>>>>) AS ItemAttribute_noculinary,
            CAST(STRUCT(STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Depth`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Height`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Width`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Diameter`) AS STRUCT<Depth: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Height: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Width: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Diameter: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>>) AS ItemMeasurements,
            CAST(STRUCT('GroupA' AS `TaxGroupID`, 'CodeA' AS `TaxExemptCode`, '1' AS `TaxAmount`) AS STRUCT<TaxGroupID: STRING, TaxExemptCode: STRING, TaxAmount: STRING>) AS TaxInformation,
            CAST('' AS STRING) AS ItemImageUrl,
            CAST(ARRAY(ARRAY(STRUCT(NULL AS `_action`, NULL AS `_franchiseeId`, NULL AS `_franchiseeName`))) AS ARRAY<ARRAY<STRUCT<_action: STRING, _franchiseeId: STRING, _franchiseeName: STRING>>>) AS ItemFranchisees,
            CAST('Add' AS STRING) AS _Action
        ;"""
    )

# fake_file_df.display()
fake_file_df \
    .coalesce(1) \
    .write \
    .format('com.databricks.spark.xml') \
    .option('declaration', 'version="1.0" encoding="UTF-8"') \
    .option("nullValue", "") \
    .option('rootTag', "root_tag") \
    .option('rowTag', "row_tag") \
    .mode('overwrite') \
    .save(xml_folder_path) {code}
I noticed that it works if I try to write all columns up to "Color" (excluded), namely:
{code:python}
fake_file_df \
    .select(
        "ItemID",
        "UPC",
        "_SerialNumberFlag",
        "Description",
        "MerchandiseHierarchy",
        "ItemPrice"
    ) \
    .coalesce(1) \
    .write \
    .format('com.databricks.spark.xml') \
    .option('declaration', 'version="1.0" encoding="UTF-8"') \
    .option("nullValue", "") \
    .option('rootTag', "root_tag") \
    .option('rowTag', "row_tag") \
    .mode('overwrite') \
    .save(xml_folder_path){code}
h2. CASE (2): MISPLACED XML

When I put all string columns at the end of the 1-row DataFrame it mistakenly writes the content of one column into the tag right after it.
{code:python}
fake_file_df = spark \
    .sql(
        """SELECT
            CAST(STRUCT('ItemId' AS `_Type`, '123' AS `_VALUE`) AS STRUCT<_Type: STRING, _VALUE: STRING>) AS ItemID,
            CAST(STRUCT('UPC' AS `_Type`, '123' AS `_VALUE`) AS STRUCT<_Type: STRING, _VALUE: STRING>) AS UPC,
            CAST(ARRAY(STRUCT(NULL AS `_ID`, NULL AS `_Level`)) AS ARRAY<STRUCT<_ID: STRING, _Level: STRING>>) AS MerchandiseHierarchy,
            CAST(ARRAY(STRUCT(NULL AS `_ValueTypeCode`, NULL AS `_VALUE`)) AS ARRAY<STRUCT<_ValueTypeCode: STRING, _Value: STRING>>) AS ItemPrice,
            CAST(STRUCT(NULL AS `Name`) AS STRUCT<Name: STRING>) AS Manufacturer,
            CAST(STRUCT(NULL AS `Season`) AS STRUCT<Season: STRING>) AS Marketing,
            CAST(STRUCT(NULL AS `_Name`) AS STRUCT<_Name: STRING>) AS BrandOwner,
            CAST(ARRAY(STRUCT('Attribute1' AS `_Name`, 'Value1' AS `_VALUE`)) AS ARRAY<STRUCT<_Name: STRING, AttributeValue: STRING>>) AS ItemAttribute_culinary,
            CAST(ARRAY(STRUCT(NULL AS `_Name`, ARRAY(ARRAY(STRUCT(NULL AS `AttributeCode`, NULL AS `AttributeValue`))) AS `_VALUE`)) AS ARRAY<STRUCT<_Name: STRING, _VALUE: ARRAY<ARRAY<STRUCT<AttributeCode: STRING, AttributeValue: STRING>>>>>) AS ItemAttribute_noculinary,
            CAST(STRUCT(STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Depth`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Height`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Width`, STRUCT(NULL AS `_UnitOfMeasure`, NULL AS `_VALUE`) AS `Diameter`) AS STRUCT<Depth: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Height: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Width: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>, Diameter: STRUCT<_UnitOfMeasure: STRING, _VALUE: STRING>>) AS ItemMeasurements,
            CAST(STRUCT('GroupA' AS `TaxGroupID`, 'CodeA' AS `TaxExemptCode`, '1' AS `TaxAmount`) AS STRUCT<TaxGroupID: STRING, TaxExemptCode: STRING, TaxAmount: STRING>) AS TaxInformation,
            CAST(ARRAY(ARRAY(STRUCT(NULL AS `_action`, NULL AS `_franchiseeId`, NULL AS `_franchiseeName`))) AS ARRAY<ARRAY<STRUCT<_action: STRING, _franchiseeId: STRING, _franchiseeName: STRING>>>) AS ItemFranchisees,
            CAST('' AS STRING) AS _SerialNumberFlag,
            CAST('MyDescription' AS STRING) AS Description,
            CAST('' AS STRING) AS Color,
            CAST('' AS STRING) AS IntendedIndustry,
            CAST('' AS STRING) AS ItemImageUrl,
            CAST('Add' AS STRING) AS _Action
        ;"""
    )

fake_file_df \
    .coalesce(1) \
    .write \
    .format('com.databricks.spark.xml') \
    .option('declaration', 'version="1.0" encoding="UTF-8"') \
    .option("nullValue", "") \
    .option('rootTag', "root_tag") \
    .option('rowTag', "row_tag") \
    .mode('overwrite') \
    .save(xml_folder_path) {code}
The output is a wrong xml where "MyDescription" is written inside the "Color" tag instead of the "Description" tag (but if you display the "fake_file_df" DataFrame it looks good as "MyDescription" is under the "Description" column).
{code:xml}
<?xml version="1.0" encoding="UTF-8"?>
<root_tag>
    <row_tag SerialNumberFlag="" Action="Add">
        <ItemID Type="ItemId">123</ItemID>
        <UPC Type="UPC">123</UPC>
        <MerchandiseHierarchy ID="" Level=""/>
        <ItemPrice ValueTypeCode="" Value=""/>
        <Manufacturer>
            <Name></Name>
        </Manufacturer>
        <Marketing>
            <Season></Season>
        </Marketing>
        <BrandOwner Name=""/>
        <ItemAttribute_culinary Name="Attribute1">
            <AttributeValue>Value1</AttributeValue>
        </ItemAttribute_culinary>
        <ItemAttribute_noculinary Name="">
            <item>
                <AttributeCode></AttributeCode>
                <AttributeValue></AttributeValue>
            </item>
        </ItemAttribute_noculinary>
        <ItemMeasurements>
            <Depth UnitOfMeasure=""></Depth>
            <Height UnitOfMeasure=""></Height>
            <Width UnitOfMeasure=""></Width>
            <Diameter UnitOfMeasure=""></Diameter>
        </ItemMeasurements>
        <TaxInformation>
            <TaxGroupID>GroupA</TaxGroupID>
            <TaxExemptCode>CodeA</TaxExemptCode>
            <TaxAmount>1</TaxAmount>
        </TaxInformation>
        <ItemFranchisees>
            <item action="" franchiseeId="" franchiseeName=""/>
        </ItemFranchisees>
        <Description></Description>
        <Color>MyDescription</Color>
        <IntendedIndustry></IntendedIndustry>
        <ItemImageUrl></ItemImageUrl>
    </row_tag>
</root_tag> {code}
h1. Current workaround I put into Production

As it looks like spark-xml is having a hard time when non-empty and empty string columns are separated by non-string ones (e.g., a nested struct or array column) I programmatically move all string columns at the end of the DataFrame right before the write command executes.

Not only that, I add a "fake" string column before each and every string column ("Col1 AS FAKE_Col1") as it also looks like spark-xml is misplacing ahead of 1 tag the content of string columns when writing the xml. And, of course, I have to read back the xml file and get rid of all these "fake" tags before I can feed it into the downward process.

 

Thanks!

~Giuseppe Ceravolo

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 04/Oct/23 13:32;gcera21;IllegalArgumentException.txt;https://issues.apache.org/jira/secure/attachment/13063356/IllegalArgumentException.txt, 02/Feb/24 04:01;ritikam;Screen Shot 2024-02-01 at 7.46.29 PM.png;https://issues.apache.org/jira/secure/attachment/13066420/Screen+Shot+2024-02-01+at+7.46.29+PM.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Feb 07 06:17:46 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1kqkg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Nov/23 23:16;ritikam;Change _SerialNumberFlag to. SerialNumberFlag and _Action to Action. Then the query runs without error.

***************************
<root_tag>
    <row_tag>
        <ItemID Type="ItemId">123</ItemID>
        <UPC Type="UPC">123</UPC>
        <SerialNumberFlag></SerialNumberFlag>
        <Description>MyDescription</Description>
        <MerchandiseHierarchy ID="" Level=""/>
        <ItemPrice ValueTypeCode="" Value=""/>
        <Color></Color>
        <IntendedIndustry></IntendedIndustry>
        <Manufacturer>
            <Name></Name>
        </Manufacturer>
        <Marketing>
            <Season></Season>
        </Marketing>
        <BrandOwner Name=""/>
        <ItemAttribute_culinary Name="Attribute1">
            <AttributeValue>Value1</AttributeValue>
        </ItemAttribute_culinary>
        <ItemAttribute_noculinary Name="">
            <item>
                <AttributeCode></AttributeCode>
                <AttributeValue></AttributeValue>
            </item>
        </ItemAttribute_noculinary>
        <ItemMeasurements>
            <Depth UnitOfMeasure=""></Depth>
            <Height UnitOfMeasure=""></Height>
            <Width UnitOfMeasure=""></Width>
            <Diameter UnitOfMeasure=""></Diameter>
        </ItemMeasurements>
        <TaxInformation>
            <TaxGroupID>GroupA</TaxGroupID>
            <TaxExemptCode>CodeA</TaxExemptCode>
            <TaxAmount>1</TaxAmount>
        </TaxInformation>
        <ItemImageUrl></ItemImageUrl>
        <ItemFranchisees>
            <item action="" franchiseeId="" franchiseeName=""/>
        </ItemFranchisees>
        <Action>Add</Action>
    </row_tag>

</root_tag>
;;;, 07/Nov/23 23:55;gcera21;[~ritikam] thanks but no because I need those 2 fields to be written as row_tag's attributes (not as its tags);;;, 08/Nov/23 00:26;ritikam;In that case have both of these in the end like this
CAST(ARRAY(ARRAY(STRUCT(NULL AS `_action`, NULL AS `_franchiseeId`, NULL AS `_franchiseeName`))) AS 
 ARRAY<ARRAY<STRUCT<_action: STRING, _franchiseeId: STRING, _franchiseeName: STRING>>>) AS ItemFranchisees,
CAST('Add' AS STRING) AS _Action,
CAST('' AS STRING) AS _SerialNumberFlag
*************************************<root_tag>
    <row_tag Action="Add" SerialNumberFlag="">
        <ItemID Type="ItemId">123</ItemID>
        <UPC Type="UPC">123</UPC>
        <Description>MyDescription</Description>
        <MerchandiseHierarchy ID="" Level=""/>
        <ItemPrice ValueTypeCode="" Value=""/>
        <Color></Color>
        <IntendedIndustry></IntendedIndustry>
        <Manufacturer>
            <Name></Name>
        </Manufacturer>
        <Marketing>
            <Season></Season>
        </Marketing>
        <BrandOwner Name=""/>
        <ItemAttribute_culinary Name="Attribute1">
            <AttributeValue>Value1</AttributeValue>
        </ItemAttribute_culinary>
        <ItemAttribute_noculinary Name="">
            <item>
                <AttributeCode></AttributeCode>
                <AttributeValue></AttributeValue>
            </item>
        </ItemAttribute_noculinary>
        <ItemMeasurements>
            <Depth UnitOfMeasure=""></Depth>
            <Height UnitOfMeasure=""></Height>
            <Width UnitOfMeasure=""></Width>
            <Diameter UnitOfMeasure=""></Diameter>
        </ItemMeasurements>
        <TaxInformation>
            <TaxGroupID>GroupA</TaxGroupID>
            <TaxExemptCode>CodeA</TaxExemptCode>
            <TaxAmount>1</TaxAmount>
        </TaxInformation>
        <ItemImageUrl></ItemImageUrl>
        <ItemFranchisees>
            <item action="" franchiseeId="" franchiseeName=""/>
        </ItemFranchisees>
    </row_tag>

</root_tag>
;;;, 16/Nov/23 10:44;gcera21;[~ritikam] I appreciate your support, but I do not want to have to manually/programmatically move up or down one or more fields... I am looking for an automatic fix of this error
by the way, I have already put in place (in production) the workaround you are suggesting by programmatically moving down all string columns, and adding a new "fake" column for each one of them, writing the file like that and then reading it back to remove the "fake" tags and re-writing it... not the best solution I guess :);;;, 07/Feb/24 06:16;ritikam;@Sean Owen 

What is causing this issue is this piece of code 

 
{code:java}
com.databricks.spark.xml.parsers.StaxXMLGenerator  
def writeElement {....
val (names, values) = elements.unzip
val elementSchema = StructType(schema.filter(names.contains))
val elementRow = Row.fromSeq(row.toSeq.filter(values.contains))


{code}
 

In this code elements contains only elements, since attributes have already been handled before but row contains values for both attributes and elements. In this sql the attribute  "SerialNumberFlag" has value ''. But other elements like Color also has value ''. Therefore the line of code in red will also get the value for "SerialNumberFlag" where as the elementSchema will not have the schema for that attribute but instead would have schema for MerchandiseHierarchy. Hence MerchandiseHierarchy gets the value '' causing the issue.

Here are the values we get for elementSchema and elementRow which show the mismatch

 
{code:java}
24/02/06 22:05:38 INFO StaxXmlGenerator$: In apply elementSchema  is StructField(ItemID,StructType(StructField(_Type,StringType,true),StructField(_VALUE,StringType,true)),false),StructField(UPC,StructType(StructField(_Type,StringType,true),StructField(_VALUE,StringType,true)),false),StructField(Description,StringType,false),StructField(MerchandiseHierarchy,ArrayType(StructType(StructField(_ID,StringType,true),StructField(_Level,StringType,true)),true),false),StructField(ItemPrice,ArrayType(StructType(StructField(_ValueTypeCode,StringType,true),StructField(_Value,StringType,true)),true),false),StructField(Color,StringType,false),StructField(IntendedIndustry,StringType,false),StructField(Manufacturer,StructType(StructField(Name,StringType,true)),false),StructField(Marketing,StructType(StructField(Season,StringType,true)),false),StructField(BrandOwner,StructType(StructField(_Name,StringType,true)),false),StructField(ItemAttribute_culinary,ArrayType(StructType(StructField(_Name,StringType,true),StructField(AttributeValue,StringType,true)),true),false),StructField(ItemAttribute_noculinary,ArrayType(StructType(StructField(_Name,StringType,true),StructField(_VALUE,ArrayType(ArrayType(StructType(StructField(AttributeCode,StringType,true),StructField(AttributeValue,StringType,true)),true),true),true)),true),false),StructField(ItemMeasurements,StructType(StructField(Depth,StructType(StructField(_UnitOfMeasure,StringType,true),StructField(_VALUE,StringType,true)),true),StructField(Height,StructType(StructField(_UnitOfMeasure,StringType,true),StructField(_VALUE,StringType,true)),true),StructField(Width,StructType(StructField(_UnitOfMeasure,StringType,true),StructField(_VALUE,StringType,true)),true),StructField(Diameter,StructType(StructField(_UnitOfMeasure,StringType,true),StructField(_VALUE,StringType,true)),true)),false),StructField(TaxInformation,StructType(StructField(TaxGroupID,StringType,true),StructField(TaxExemptCode,StringType,true),StructField(TaxAmount,StringType,true)),false),StructField(ItemImageUrl,StringType,false),StructField(ItemFranchisees,ArrayType(ArrayType(StructType(StructField(_action,StringType,true),StructField(_franchiseeId,StringType,true),StructField(_franchiseeName,StringType,true)),true),true),false)

24/02/06 22:05:38 INFO StaxXmlGenerator$:  In apply the elementRow is [ItemId,123],[UPC,123],MyDescription,,WrappedArray([null,null]),WrappedArray([null,null]),,,[null],[null],[null],WrappedArray([Attribute1,Value......
 


{code};;;, 07/Feb/24 06:17;ritikam;So basically the fix is to replace 

val elementRow = Row.fromSeq(row.toSeq.filter(values.contains))

with

val elementRow = Row.fromSeq(values.toSeq)

 ;;;
Affects Version/s.1: 
Component/s.1: Spark Core
Comment.1: 07/Nov/23 23:55;gcera21;[~ritikam] thanks but no because I need those 2 fields to be written as row_tag's attributes (not as its tags);;;
Comment.2: 08/Nov/23 00:26;ritikam;In that case have both of these in the end like this
CAST(ARRAY(ARRAY(STRUCT(NULL AS `_action`, NULL AS `_franchiseeId`, NULL AS `_franchiseeName`))) AS 
 ARRAY<ARRAY<STRUCT<_action: STRING, _franchiseeId: STRING, _franchiseeName: STRING>>>) AS ItemFranchisees,
CAST('Add' AS STRING) AS _Action,
CAST('' AS STRING) AS _SerialNumberFlag
*************************************<root_tag>
    <row_tag Action="Add" SerialNumberFlag="">
        <ItemID Type="ItemId">123</ItemID>
        <UPC Type="UPC">123</UPC>
        <Description>MyDescription</Description>
        <MerchandiseHierarchy ID="" Level=""/>
        <ItemPrice ValueTypeCode="" Value=""/>
        <Color></Color>
        <IntendedIndustry></IntendedIndustry>
        <Manufacturer>
            <Name></Name>
        </Manufacturer>
        <Marketing>
            <Season></Season>
        </Marketing>
        <BrandOwner Name=""/>
        <ItemAttribute_culinary Name="Attribute1">
            <AttributeValue>Value1</AttributeValue>
        </ItemAttribute_culinary>
        <ItemAttribute_noculinary Name="">
            <item>
                <AttributeCode></AttributeCode>
                <AttributeValue></AttributeValue>
            </item>
        </ItemAttribute_noculinary>
        <ItemMeasurements>
            <Depth UnitOfMeasure=""></Depth>
            <Height UnitOfMeasure=""></Height>
            <Width UnitOfMeasure=""></Width>
            <Diameter UnitOfMeasure=""></Diameter>
        </ItemMeasurements>
        <TaxInformation>
            <TaxGroupID>GroupA</TaxGroupID>
            <TaxExemptCode>CodeA</TaxExemptCode>
            <TaxAmount>1</TaxAmount>
        </TaxInformation>
        <ItemImageUrl></ItemImageUrl>
        <ItemFranchisees>
            <item action="" franchiseeId="" franchiseeName=""/>
        </ItemFranchisees>
    </row_tag>

</root_tag>
;;;
Comment.3: 16/Nov/23 10:44;gcera21;[~ritikam] I appreciate your support, but I do not want to have to manually/programmatically move up or down one or more fields... I am looking for an automatic fix of this error
by the way, I have already put in place (in production) the workaround you are suggesting by programmatically moving down all string columns, and adding a new "fake" column for each one of them, writing the file like that and then reading it back to remove the "fake" tags and re-writing it... not the best solution I guess :);;;
Comment.4: 07/Feb/24 06:16;ritikam;@Sean Owen 

What is causing this issue is this piece of code 

 
{code:java}
com.databricks.spark.xml.parsers.StaxXMLGenerator  
def writeElement {....
val (names, values) = elements.unzip
val elementSchema = StructType(schema.filter(names.contains))
val elementRow = Row.fromSeq(row.toSeq.filter(values.contains))


{code}
 

In this code elements contains only elements, since attributes have already been handled before but row contains values for both attributes and elements. In this sql the attribute  "SerialNumberFlag" has value ''. But other elements like Color also has value ''. Therefore the line of code in red will also get the value for "SerialNumberFlag" where as the elementSchema will not have the schema for that attribute but instead would have schema for MerchandiseHierarchy. Hence MerchandiseHierarchy gets the value '' causing the issue.

Here are the values we get for elementSchema and elementRow which show the mismatch

 
{code:java}
24/02/06 22:05:38 INFO StaxXmlGenerator$: In apply elementSchema  is StructField(ItemID,StructType(StructField(_Type,StringType,true),StructField(_VALUE,StringType,true)),false),StructField(UPC,StructType(StructField(_Type,StringType,true),StructField(_VALUE,StringType,true)),false),StructField(Description,StringType,false),StructField(MerchandiseHierarchy,ArrayType(StructType(StructField(_ID,StringType,true),StructField(_Level,StringType,true)),true),false),StructField(ItemPrice,ArrayType(StructType(StructField(_ValueTypeCode,StringType,true),StructField(_Value,StringType,true)),true),false),StructField(Color,StringType,false),StructField(IntendedIndustry,StringType,false),StructField(Manufacturer,StructType(StructField(Name,StringType,true)),false),StructField(Marketing,StructType(StructField(Season,StringType,true)),false),StructField(BrandOwner,StructType(StructField(_Name,StringType,true)),false),StructField(ItemAttribute_culinary,ArrayType(StructType(StructField(_Name,StringType,true),StructField(AttributeValue,StringType,true)),true),false),StructField(ItemAttribute_noculinary,ArrayType(StructType(StructField(_Name,StringType,true),StructField(_VALUE,ArrayType(ArrayType(StructType(StructField(AttributeCode,StringType,true),StructField(AttributeValue,StringType,true)),true),true),true)),true),false),StructField(ItemMeasurements,StructType(StructField(Depth,StructType(StructField(_UnitOfMeasure,StringType,true),StructField(_VALUE,StringType,true)),true),StructField(Height,StructType(StructField(_UnitOfMeasure,StringType,true),StructField(_VALUE,StringType,true)),true),StructField(Width,StructType(StructField(_UnitOfMeasure,StringType,true),StructField(_VALUE,StringType,true)),true),StructField(Diameter,StructType(StructField(_UnitOfMeasure,StringType,true),StructField(_VALUE,StringType,true)),true)),false),StructField(TaxInformation,StructType(StructField(TaxGroupID,StringType,true),StructField(TaxExemptCode,StringType,true),StructField(TaxAmount,StringType,true)),false),StructField(ItemImageUrl,StringType,false),StructField(ItemFranchisees,ArrayType(ArrayType(StructType(StructField(_action,StringType,true),StructField(_franchiseeId,StringType,true),StructField(_franchiseeName,StringType,true)),true),true),false)

24/02/06 22:05:38 INFO StaxXmlGenerator$:  In apply the elementRow is [ItemId,123],[UPC,123],MyDescription,,WrappedArray([null,null]),WrappedArray([null,null]),,,[null],[null],[null],WrappedArray([Attribute1,Value......
 


{code};;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Data duplication may occur when fallback to origin shuffle block
Issue key: SPARK-45134
Issue id: 13550347
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: gaoyajun02
Creator: gaoyajun02
Created: 12/Sep/23 10:19
Updated: 11/Jan/24 00:19
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0, 3.4.0, 3.5.0
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: pull-request-available
Description: One possible situation that has been found is that, during the process of requesting mergedBlockMeta, when the channel is closed, it may trigger two callback callbacks and result in duplicate data for the original shuffle blocks.
 # The first time is when the channel is inactivated, the responseHandler will execute the callback for all outstandingRpcs.
 # The second time is when the listener corresponding to shuffleClient.writeAndFlush executes the callback after the channel is closed.

Some Error Logs:
{code:java}
23/09/08 09:22:21 ERROR shuffle-client-7-1 TransportResponseHandler: Still have 1 requests outstanding when connection from host/ip:prot is closed
23/09/08 09:22:21 ERROR shuffle-client-7-1 PushBasedFetchHelper: Failed to get the meta of push-merged block for (3, 54) from host:port
java.io.IOException: Connection from host:port closed
        at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:147)
        at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
        at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
        at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
        at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
        at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
        at io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818)
        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:745)
 
23/09/08 09:22:21 ERROR shuffle-client-7-1 PushBasedFetchHelper: Failed to get the meta of push-merged block for (3, 54) from host:port
java.io.IOException: Failed to send RPC RPC 8079698359363123411 to host/ip:port: java.nio.channels.ClosedChannelException
        at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:433)
        at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:409)
        at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:577)
        at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:551)
        at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:490)
        at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:615)
        at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:608)
        at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:993)
        at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
        at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
        at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:790)
        at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:767)
        at io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:790)
        at io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758)
        at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:767)
        at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)
        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.nio.channels.ClosedChannelException
        at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
        ... 18 more {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 20 03:29:41 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1kaw8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Sep/23 10:43;gaoyajun02;Hi, [~csingh] [~vsowrirajan] [~mshen] , Can you take a look? hope some suggestions.;;;, 20/Sep/23 03:29;snoot;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/43004;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 20/Sep/23 03:29;snoot;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/43004;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0

Summary: pyspark.ml.stat.Correlation - Spearman Correlation method giving incorrect and inconsistent results for the same DataFrame if it has huge amount of Ties.
Issue key: SPARK-42905
Issue id: 13529702
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: dronzer
Creator: dronzer
Created: 23/Mar/23 05:20
Updated: 24/Nov/23 00:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: ML
Due Date: 
Votes: 0
Labels: correctness, pull-request-available
Description: pyspark.ml.stat.Correlation

Following is the Scenario where the Correlation function fails for giving correct Spearman Coefficient Results.

Tested E.g -> Spark DataFrame has 2 columns A and B.

!image-2023-03-23-10-55-26-879.png|width=562,height=162!

Column A has 3 Distinct Values and total of 108Million rows

Column B has 4 Distinct Values and total of 108Million rows

If I Calculate the correlation for this DataFrame in Python Pandas DF.corr, it gives the correct answer even if i run the same code multiple times the same answer is produced. (Each column has only 3-4 distinct values)

!image-2023-03-23-10-53-37-461.png|width=468,height=287!

 

Coming to Spark and using Spearman Correlation produces a *different results* for the *same dataframe* on multiple runs. (see below) (each column in this df has only 3-4 distinct values)

!image-2023-03-23-10-52-49-392.png|width=516,height=322!

 

Basically in python Pandas Df.corr it gives same results on same dataframe on multiple runs which is expected behaviour. However, in Spark using the same data it gives different result, moreover running the same cell with same data multiple times produces different results meaning the output is inconsistent.

Coming to data the only observation I could conclude is Ties in data. (Only 3-4 Distinct values over 108M Rows.) This scenario is not handled in Spark Correlation method as the same data when used in python using df.corr produces consistent results.

The only Workaround we could find to get consistent and the same output as from python in Spark is by using Pandas UDF as shown below:

!image-2023-03-23-10-52-11-481.png|width=518,height=111!

!image-2023-03-23-10-51-28-420.png|width=509,height=270!

 

We also tried pyspark.pandas.DataFrame .corr method and it produces incorrect and inconsistent results for this case too.

Only PandasUDF seems to provide consistent results.

 

Another point to note is : If i add some random noise to the data, which will inturn increase the distinct values in the data. It again gives consistent results for any runs. Which makes me believe that the Python version handles ties correctly and gives consistent results no matter how many ties exist. However, pyspark method is somehow not able to handle many ties in data.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 23/Mar/23 05:21;dronzer;image-2023-03-23-10-51-28-420.png;https://issues.apache.org/jira/secure/attachment/13056595/image-2023-03-23-10-51-28-420.png, 23/Mar/23 05:22;dronzer;image-2023-03-23-10-52-11-481.png;https://issues.apache.org/jira/secure/attachment/13056596/image-2023-03-23-10-52-11-481.png, 23/Mar/23 05:22;dronzer;image-2023-03-23-10-52-49-392.png;https://issues.apache.org/jira/secure/attachment/13056597/image-2023-03-23-10-52-49-392.png, 23/Mar/23 05:23;dronzer;image-2023-03-23-10-53-37-461.png;https://issues.apache.org/jira/secure/attachment/13056598/image-2023-03-23-10-53-37-461.png, 23/Mar/23 05:25;dronzer;image-2023-03-23-10-55-26-879.png;https://issues.apache.org/jira/secure/attachment/13056599/image-2023-03-23-10-55-26-879.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 5.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 23 07:31:23 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1gsco:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Aug/23 07:31;zhenhaozhang;minimal reproducible example. the result is incorrect and inconsistent when tied value size > 10_000_000

 
{code:java}
import org.apache.spark.ml.linalg.{Matrix, Vectors, Vector}
import org.apache.spark.ml.stat.Correlation
import org.apache.spark.sql.Row

val N = 10000002
val x = sc.range(0, N).map(i => if (i < N - 1) 1.0 else 2.0)
val y = sc.range(0, N).map(i => if (i < N - 1) 2.0 else 1.0)
//val s1 = Statistics.corr(x, y, "spearman")
val df = x.zip(y)
  .map{case (x, y) => Vectors.dense(x, y)}
  .map(Tuple1.apply)
  .repartition(1) 
  .toDF("features")
  
val Row(coeff1: Matrix) = Correlation.corr(df, "features", "spearman").head
val r = coeff1(0, 1)
println(s"spearman correlation in spark: $r")
// spearman correlation in spark: -9.999990476024495E-8 {code}
 

 

the correct result is -1.0;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: spark3-shell errors org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table <hive_table_name>. Permission denied: user [AD user] does not have [SELECT] privilege on [<database>/<hive table>] when reads hive view 
Issue key: SPARK-44339
Issue id: 13542861
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: amarrocks85
Creator: amarrocks85
Created: 08/Jul/23 01:39
Updated: 09/Jul/23 02:22
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Shell, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: *Problem statement* 

A hive view is created using beeline to restrict the users from accessing the original hive table since the data contains sensitive information. 

For illustration purpose, let's consider a sensitive table as emp_db.employee with columns id, name, salary created through beeline by user '{*}userA{*}'

 
{code:java}
create external table emp_db.employee (id int, name string, salary double) location '<hdfs_path>'{code}
 

A view is created using beeline by the same user '{*}userA{*}'

 
{code:java}
ate view empview_db.emp_v  as select id,name from emp_db.employee' {code}
 

From Ranger UI, we define a policy under Hadoop SQL Policies that will let '{*}userB{*}' to access database - empview_db  and table - emp_v with SELECT permission.

 

*Steps to replicate* 
 # ssh to edge node where beeline is available using *userB*
 # Try executing following queries
 ## select * from emp_db.employee  *;*
 ## desc formatted empview_db.emp_v;
 ## Above queries works fine without any issues.
 # Now, try using spark3-shell using *userB* 
{code:java}
# spark3-shell --deploy-mode client
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/07/08 01:24:09 WARN HiveConf: HiveConf of name hive.masking.algo does not exist
Spark context Web UI available at http://xxxxxxx:4040
Spark context available as 'sc' (master = yarn, app id = application_xxx_xxx).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0.3.3.7180.0-274
      /_/
         
Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)
Type in expressions to have them evaluated.
Type :help for more information.scala> spark.table("empview_db.emp_v").schema
23/07/08 01:24:30 WARN HiveClientImpl: Detected HiveConf hive.execution.engine is 'tez' and will be reset to 'mr' to disable useless hive logic
Hive Session ID = b1e3c813-aea9-40da-9012-949e82d4205e
org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table employee. Permission denied: user [userB] does not have [SELECT] privilege on [emp_db/employee]
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:110)
  at org.apache.spark.sql.hive.HiveExternalCatalog.tableExists(HiveExternalCatalog.scala:877)
  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.tableExists(ExternalCatalogWithListener.scala:146)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.tableExists(SessionCatalog.scala:488)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireTableExists(SessionCatalog.scala:224)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:514)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:500)
  at org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:66)
  at org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:311)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1206)
  at scala.Option.orElse(Option.scala:447)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1205)
  at scala.Option.orElse(Option.scala:447)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1197)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1068)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)
  at org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:208)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)
  at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)
  at org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:208)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  at scala.collection.immutable.List.foldLeft(List.scala:91)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
  at scala.collection.immutable.List.foreach(List.scala:431)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveViews$2(Analyzer.scala:1012)
  at org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$resolveViews$1(Analyzer.scala:1012)
  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withAnalysisContext(Analyzer.scala:166)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1004)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$resolveViews(Analyzer.scala:1020)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.$anonfun$applyOrElse$47(Analyzer.scala:1068)
  at scala.Option.map(Option.scala:230)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1068)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1032)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1032)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:991)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  at scala.collection.immutable.List.foldLeft(List.scala:91)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
  at scala.collection.immutable.List.foreach(List.scala:431)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)
  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:186)
  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:511)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:186)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:185)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
  at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:607)
  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:600)
  ... 47 elided
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch table employee. Permission denied: user [userB] does not have [SELECT] privilege on [emp_db/employee]
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1462)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1411)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1391)
  at org.apache.spark.sql.hive.client.Shim_v0_12.getTable(HiveShim.scala:639)
  at org.apache.spark.sql.hive.client.HiveClientImpl.getRawTableOption(HiveClientImpl.scala:429)
  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$tableExists$1(HiveClientImpl.scala:444)
  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
  at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:321)
  at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:248)
  at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:247)
  at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:301)
  at org.apache.spark.sql.hive.client.HiveClientImpl.tableExists(HiveClientImpl.scala:444)
  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$tableExists$1(HiveExternalCatalog.scala:877)
  at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:101)
  ... 151 more
Caused by: org.apache.hadoop.hive.metastore.api.MetaException: Permission denied: user [userB] does not have [SELECT] privilege on [emp_db/employee]
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result$get_table_req_resultStandardScheme.read(ThriftHiveMetastore.java)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result$get_table_req_resultStandardScheme.read(ThriftHiveMetastore.java)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_req_result.read(ThriftHiveMetastore.java)
  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:88)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table_req(ThriftHiveMetastore.java:2378)
  at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table_req(ThriftHiveMetastore.java:2365)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:2047)
  at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTable(SessionHiveMetaStoreClient.java:206)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:213)
  at com.sun.proxy.$Proxy48.getTable(Unknown Source)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:3514)
  at com.sun.proxy.$Proxy48.getTable(Unknown Source)
  at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1453)
  ... 165 more

{code}

*Expected behavior* - we want spark to behave just like beeline where SELECT * from <view-name> and DESC formatted <view-name> on view works fine without any errors. 

The CDP 7.1.7 documentation link [https://docs.cloudera.com/cdp-private-cloud-base/7.1.7/developing-spark-applications/topics/spark-interaction-with-hive-views.html?]  describes 'Interacting Hive Views'. However, the explanation doesn't fit well with the behavior we see from spark3-shell for hive views.

Looking forward for feedback and inputs that may unblock my use case. Please let me know if  you need any further information. 

 
Environment: CDP 7.1.7 Ranger, kerberized and hadoop impersonation enabled.
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jul 09 02:22:00 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1j13k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Jul/23 02:22;yumwang;It seems it's cloudera spark issue.;;;
Affects Version/s.1: 
Component/s.1: Spark Submit
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Catalyst 'ColumnPruning' Optimizer does not play well with sql function 'explode'
Issue key: SPARK-39854
Issue id: 13473128
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: jiajiwu
Creator: jiajiwu
Created: 24/Jul/22 15:35
Updated: 30/May/23 22:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.2.1, 3.2.2, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The *ColumnPruning* optimizer batch does not always work with *explode* sql function.
 * Here's a code snippet to repro the issue:

 
{code:java}
import spark.implicits._

val testJson =
  """{
    | "b": {
    |  "id": "id00",
    |  "data": [{
    |   "b1": "vb1",
    |   "b2": 101,
    |   "ex2": [
    |    { "fb1": false, "fb2": 11, "fb3": "t1" },
    |    { "fb1": true, "fb2": 12, "fb3": "t2" }
    |   ]}, {
    |   "b1": "vb2",
    |   "b2": 102,
    |   "ex2": [
    |    { "fb1": false, "fb2": 13, "fb3": "t3" },
    |    { "fb1": true, "fb2": 14, "fb3": "t4" }
    |   ]}
    |  ],
    |  "fa": "tes",
    |  "v": "1.5"
    | }
    |}
    |""".stripMargin
val df = spark.read.json((testJson :: Nil).toDS())
  .withColumn("ex_b", explode($"b.data.ex2"))
  .withColumn("ex_b2", explode($"ex_b"))
val df1 = df
  .withColumn("rt", struct(
    $"b.fa".alias("rt_fa"),
    $"b.v".alias("rt_v")
  ))
  .drop("b", "ex_b")
df1.show(false){code}
 * the result exception:

{code:java}
Exception in thread "main" java.lang.IllegalStateException: Couldn't find _extract_v#35 in [_extract_fa#36,ex_b2#13]
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableLike.map(TraversableLike.scala:286)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
    at scala.collection.AbstractTraversable.map(Traversable.scala:108)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:698)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
    at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1196)
    at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1195)
    at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:513)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
    at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1196)
    at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1195)
    at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:513)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:528)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)
    at scala.collection.immutable.List.map(List.scala:297)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)
    at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:69)
    at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:196)
    at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:151)
    at org.apache.spark.sql.execution.GenerateExec.consume(GenerateExec.scala:58)
    at org.apache.spark.sql.execution.GenerateExec.codeGenCollection(GenerateExec.scala:232)
    at org.apache.spark.sql.execution.GenerateExec.doConsume(GenerateExec.scala:145)
    at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:223)
    at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)
    at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:151)
    at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:216)
    at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:265)
    at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:196)
    at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:151)
    at org.apache.spark.sql.execution.GenerateExec.consume(GenerateExec.scala:58)
    at org.apache.spark.sql.execution.GenerateExec.codeGenCollection(GenerateExec.scala:232)
    at org.apache.spark.sql.execution.GenerateExec.doConsume(GenerateExec.scala:145)
    at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:223)
    at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)
    at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:151)
    at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:42)
    at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:89)
    at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:223)
    at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)
    at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:151)
    at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:216)
    at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:265)
    at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:196)
    at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:151)
    at org.apache.spark.sql.execution.RDDScanExec.consume(ExistingRDD.scala:153)
    at org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:485)
    at org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:458)
    at org.apache.spark.sql.execution.RDDScanExec.doProduce(ExistingRDD.scala:153)
    at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
    at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.RDDScanExec.produce(ExistingRDD.scala:153)
    at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:242)
    at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
    at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:216)
    at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:55)
    at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
    at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:42)
    at org.apache.spark.sql.execution.GenerateExec.doProduce(GenerateExec.scala:134)
    at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
    at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.GenerateExec.produce(GenerateExec.scala:58)
    at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:242)
    at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
    at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:216)
    at org.apache.spark.sql.execution.GenerateExec.doProduce(GenerateExec.scala:134)
    at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
    at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.GenerateExec.produce(GenerateExec.scala:58)
    at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:55)
    at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
    at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:42)
    at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:660)
    at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:723)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)
    at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:340)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:473)
    at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)
    at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
    at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3960)
    at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2955)
    at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3950)
    at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512)
    at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3948)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:111)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:171)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3948)
    at org.apache.spark.sql.Dataset.head(Dataset.scala:2955)
    at org.apache.spark.sql.Dataset.take(Dataset.scala:3176)
    at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)
    at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)
    at org.apache.spark.sql.Dataset.show(Dataset.scala:834)
    at org.apache.spark.sql.Dataset.show(Dataset.scala:811)
    at org.apache.spark.opticloud.replaceWithAliases_Issue$.main(replaceWithAliases_Issue.scala:70)
    at org.apache.spark.opticloud.replaceWithAliases_Issue.main(replaceWithAliases_Issue.scala) {code}
 

 

Note: this issue is initially reported to the [spark-xml|https://github.com/databricks/spark-xml/issues/580] repo. However, it turns out be an issue within catalyst optimizer (the snippet above does not has dependency on *spark-xml* ).
Environment: Spark version: the latest (3.4.0-SNAPSHOT)

OS: Ubuntu 20.04

JDK: Amazon corretto-11.0.14.1
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): SPARK-35194
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): scala
Custom field (Last public comment date): Tue May 30 22:46:25 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z174ls:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Jul/22 18:47;jiajiwu;One workaround is to exclude *ColumnPruning* by set spark config:

{color:#54b33e}"spark.sql.optimizer.excludedRules" {color}-> {color:#54b33e}"org.apache.spark.sql.catalyst.optimizer.ColumnPruning"{color};;;, 24/Jul/22 19:49;apachespark;User 'jiaji-wu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37269;;;, 24/Jul/22 19:49;apachespark;User 'jiaji-wu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37269;;;, 30/Jul/22 18:08;apachespark;User 'jiaji-wu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37348;;;, 17/Sep/22 00:03;dongjoon;Thank you, [~jiajiwu] .

The recommended workaround would be to disable nested schema pruning only instead of disabling all ColumnPruning rule.
{code:java}
spark.sql.optimizer.expression.nestedPruning.enabled=false 
spark.sql.optimizer.nestedSchemaPruning.enabled=false{code}
 

For example, 
{code:java}
$ bin/spark-shell -c spark.sql.optimizer.expression.nestedPruning.enabled=false -c spark.sql.optimizer.nestedSchemaPruning.enabled=false
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/09/16 17:02:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1663372921582).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.16)
Type in expressions to have them evaluated.
Type :help for more information.scala> :paste
// Entering paste mode (ctrl-D to finish)import spark.implicits._val testJson =
  """{
    | "b": {
    |  "id": "id00",
    |  "data": [{
    |   "b1": "vb1",
    |   "b2": 101,
    |   "ex2": [
    |    { "fb1": false, "fb2": 11, "fb3": "t1" },
    |    { "fb1": true, "fb2": 12, "fb3": "t2" }
    |   ]}, {
    |   "b1": "vb2",
    |   "b2": 102,
    |   "ex2": [
    |    { "fb1": false, "fb2": 13, "fb3": "t3" },
    |    { "fb1": true, "fb2": 14, "fb3": "t4" }
    |   ]}
    |  ],
    |  "fa": "tes",
    |  "v": "1.5"
    | }
    |}
    |""".stripMargin
val df = spark.read.json((testJson :: Nil).toDS())
  .withColumn("ex_b", explode($"b.data.ex2"))
  .withColumn("ex_b2", explode($"ex_b"))
val df1 = df
  .withColumn("rt", struct(
    $"b.fa".alias("rt_fa"),
    $"b.v".alias("rt_v")
  ))
  .drop("b", "ex_b")
df1.show(false)// Exiting paste mode, now interpreting.+---------------+----------+
|ex_b2          |rt        |
+---------------+----------+
|{false, 11, t1}|{tes, 1.5}|
|{true, 12, t2} |{tes, 1.5}|
|{false, 13, t3}|{tes, 1.5}|
|{true, 14, t4} |{tes, 1.5}|
+---------------+----------+import spark.implicits._
testJson: String =
"{
 "b": {
  "id": "id00",
  "data": [{
   "b1": "vb1",
   "b2": 101,
   "ex2": [
    { "fb1": false, "fb2": 11, "fb3": "t1" },
    { "fb1": true, "fb2": 12, "fb3": "t2" }
   ]}, {
   "b1": "vb2",
   "b2": 102,
   "ex2": [
    { "fb1": false, "fb2": 13, "fb3": "t3" },
    { "fb1": true, "fb2": 14, "fb3": "t4" }
   ]}
  ],
  "fa": "tes",
  "v": "1.5"
 }
}
"
df: org.apache.spark.sql.DataFrame = [b: struct<data: array<struct<b1:string,b2:bigint,ex2:array<struct<fb1:boolean,fb2:bigint,fb3:string>>>>, fa: string ... 2 more fields>, ex_b: array<struct<fb1:boolean,fb2:bigint,fb3:string>> ... 1 more field]
df1: org.apache.spark.sql.DataFrame = [ex_b2: struct<fb1: boolean, fb... {code};;;, 17/Sep/22 00:38;dongjoon;It turns out SPARK-35194 caused this regression. I tested at the commit of SPARK-35194 and its parent commit and verified that this regression happens after SPARK-35194.;;;, 30/May/23 22:46;dongjoon;I verified with 3.4.0 and added `3.4.0` to the Affected Version because we still have this bug.;;;
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 24/Jul/22 19:49;apachespark;User 'jiaji-wu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37269;;;
Comment.2: 24/Jul/22 19:49;apachespark;User 'jiaji-wu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37269;;;
Comment.3: 30/Jul/22 18:08;apachespark;User 'jiaji-wu' has created a pull request for this issue:
https://github.com/apache/spark/pull/37348;;;
Comment.4: 17/Sep/22 00:03;dongjoon;Thank you, [~jiajiwu] .

The recommended workaround would be to disable nested schema pruning only instead of disabling all ColumnPruning rule.
{code:java}
spark.sql.optimizer.expression.nestedPruning.enabled=false 
spark.sql.optimizer.nestedSchemaPruning.enabled=false{code}
 

For example, 
{code:java}
$ bin/spark-shell -c spark.sql.optimizer.expression.nestedPruning.enabled=false -c spark.sql.optimizer.nestedSchemaPruning.enabled=false
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/09/16 17:02:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1663372921582).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.16)
Type in expressions to have them evaluated.
Type :help for more information.scala> :paste
// Entering paste mode (ctrl-D to finish)import spark.implicits._val testJson =
  """{
    | "b": {
    |  "id": "id00",
    |  "data": [{
    |   "b1": "vb1",
    |   "b2": 101,
    |   "ex2": [
    |    { "fb1": false, "fb2": 11, "fb3": "t1" },
    |    { "fb1": true, "fb2": 12, "fb3": "t2" }
    |   ]}, {
    |   "b1": "vb2",
    |   "b2": 102,
    |   "ex2": [
    |    { "fb1": false, "fb2": 13, "fb3": "t3" },
    |    { "fb1": true, "fb2": 14, "fb3": "t4" }
    |   ]}
    |  ],
    |  "fa": "tes",
    |  "v": "1.5"
    | }
    |}
    |""".stripMargin
val df = spark.read.json((testJson :: Nil).toDS())
  .withColumn("ex_b", explode($"b.data.ex2"))
  .withColumn("ex_b2", explode($"ex_b"))
val df1 = df
  .withColumn("rt", struct(
    $"b.fa".alias("rt_fa"),
    $"b.v".alias("rt_v")
  ))
  .drop("b", "ex_b")
df1.show(false)// Exiting paste mode, now interpreting.+---------------+----------+
|ex_b2          |rt        |
+---------------+----------+
|{false, 11, t1}|{tes, 1.5}|
|{true, 12, t2} |{tes, 1.5}|
|{false, 13, t3}|{tes, 1.5}|
|{true, 14, t4} |{tes, 1.5}|
+---------------+----------+import spark.implicits._
testJson: String =
"{
 "b": {
  "id": "id00",
  "data": [{
   "b1": "vb1",
   "b2": 101,
   "ex2": [
    { "fb1": false, "fb2": 11, "fb3": "t1" },
    { "fb1": true, "fb2": 12, "fb3": "t2" }
   ]}, {
   "b1": "vb2",
   "b2": 102,
   "ex2": [
    { "fb1": false, "fb2": 13, "fb3": "t3" },
    { "fb1": true, "fb2": 14, "fb3": "t4" }
   ]}
  ],
  "fa": "tes",
  "v": "1.5"
 }
}
"
df: org.apache.spark.sql.DataFrame = [b: struct<data: array<struct<b1:string,b2:bigint,ex2:array<struct<fb1:boolean,fb2:bigint,fb3:string>>>>, fa: string ... 2 more fields>, ex_b: array<struct<fb1:boolean,fb2:bigint,fb3:string>> ... 1 more field]
df1: org.apache.spark.sql.DataFrame = [ex_b2: struct<fb1: boolean, fb... {code};;;
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: ignoreCorruptFiles results data loss
Issue key: SPARK-40591
Issue id: 13483559
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: Qin Yao
Creator: Qin Yao
Created: 28/Sep/22 01:20
Updated: 28/Sep/22 02:15
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.0, 3.2.2, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: correctness
Description: Let's take a look at the case below, the left and the right are visiting the same table and its partitions, and both of them are ignoreCorruptFiles=true. The right side shows that a task skips partial of data it reads because of encountering 'corrupt data', while the left read this file correctly. As ignoreCorruptFiles coarsely works with RuntimeException and IOException, it can not always represent data corruption.

!image-2022-09-28-09-20-21-693.png!

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 28/Sep/22 01:20;Qin Yao;image-2022-09-28-09-20-21-693.png;https://issues.apache.org/jira/secure/attachment/13049840/image-2022-09-28-09-20-21-693.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 28 01:55:58 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18whc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Sep/22 01:55;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/38024;;;
Affects Version/s.1: 3.2.2
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: The filter operator gets wrong results in char type
Issue key: SPARK-37051
Issue id: 13407195
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: frankli
Creator: frankli
Created: 19/Oct/21 03:16
Updated: 02/Nov/21 03:43
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: When I try the following sample SQL on  the TPCDS data, the filter operator returns an empty row set (shown in web ui).

_select * from item where i_category = 'Music' limit 100;_

The table is in ORC format, and i_category is char(50) type. 

Data is inserted by hive, and queried by Spark.

I guest that the char(50) type will remains redundant blanks after the actual word.

It will affect the boolean value of  "x.equals(Y)", and results in wrong results.

Luckily, the varchar type is OK. 

 

This bug can be reproduced by a few steps.

>>> desc t2_orc;
 +------------+-----------++------------+
|col_name|data_type|comment|

+------------+-----------++------------+
|a|string      |NULL|
|b|char(50)  |NULL|
|c|int            |NULL|

+------------+-----------++----------–+

>>> select * from t2_orc where a='a';
 +-----+---++------+
|a|b|c|

+-----+---++------+
|a|b|1|
|a|b|2|
|a|b|3|
|a|b|4|
|a|b|5|

+-----+---++----–+

>>> select * from t2_orc where b='b';
 +-----+---++------+
|a|b|c|

+-----+---++------+
 +-----+---++------+

 

By the way, Spark's tests should add more cases on the char type.

 

== Physical Plan ==
 CollectLimit (3)
 +- Filter (2)
 +- Scan orc tpcds_bin_partitioned_orc_2.item (1)

(1) Scan orc tpcds_bin_partitioned_orc_2.item
 Output [22]: [i_item_sk#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21|#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21]
 Batched: false
 Location: InMemoryFileIndex [hdfs://tpcds_bin_partitioned_orc_2.db/item]
 PushedFilters: [IsNotNull(i_category), +EqualTo(i_category,+Music         )]++++
 ReadSchema: struct<i_item_sk:bigint,i_item_id:string,i_rec_start_date:date,i_rec_end_date:date,i_item_desc:string,i_current_price:decimal(7,2),i_wholesale_cost:decimal(7,2),i_brand_id:int,i_brand:string,i_class_id:int,i_class:string,i_category_id:int,i_category:string,i_manufact_id:int,i_manufact:string,i_size:string,i_formulation:string,i_color:string,i_units:string,i_container:string,i_manager_id:int,i_product_name:string>

(2) Filter
 Input [22]: [i_item_sk#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21|#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21]
 Condition : (isnotnull(i_category#12) AND +(i_category#12 = Music         ))+

(3) CollectLimit
 Input [22]: [i_item_sk#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21|#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21]
 Arguments: 100

 
Environment: Spark 3.1.2

Scala 2.12 / Java 1.8
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Nov 02 03:43:44 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vyco:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Oct/21 03:32;frankli;[~dongjoon] Can I trouble you to take a look. Thanks a lot.;;;, 19/Oct/21 09:58;frankli;It seems to be affected by the right padding.

[SPARK-34192][SQL] [https://github.com/apache/spark/commit/d1177b52304217f4cb86506fd1887ec98879ed16]

[~yaoqiang];;;, 26/Oct/21 03:05;wangzhun;We have the same problem, tcpds-q7 does not work. The cd_education_status field of the customer_demographics table in parquet format is char(20).
{code:java}
select count(*) from tpcds.customer_demographics where cd_education_status ='College'{code}
spark3.1.2  returns 0 rows
spark2.3.4/hive/presto returns 27w rows;;;, 26/Oct/21 08:37;dongjoon;Does Parquet work in those scenario, [~frankli] and [~wangzhun]?;;;, 26/Oct/21 08:48;frankli;This scenario also occur on Parquet. [~dongjoon]

Spark3.1 will do padding for both writer and reader side.

So, Spark 3.1 cannot read Hive data without padding, while Spark 2.4 works well.;;;, 26/Oct/21 09:10;dongjoon;Got it. If that happens on Parquet, we had better drop `ORC` from the JIRA title. I removed it first.
> This scenario also occur on Parquet.;;;, 02/Nov/21 03:37;LuciferYang;Can you test
{code:java}
select * from t2_orc where b='b0000000000000000000000000000000000000000000000000';
{code}
1 B and 49 zeros

 

 ;;;, 02/Nov/21 03:43;frankli;I know this SQL can work, but this behavior is different from MYSQL and PostgreSQL.;;;
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 19/Oct/21 09:58;frankli;It seems to be affected by the right padding.

[SPARK-34192][SQL] [https://github.com/apache/spark/commit/d1177b52304217f4cb86506fd1887ec98879ed16]

[~yaoqiang];;;
Comment.2: 26/Oct/21 03:05;wangzhun;We have the same problem, tcpds-q7 does not work. The cd_education_status field of the customer_demographics table in parquet format is char(20).
{code:java}
select count(*) from tpcds.customer_demographics where cd_education_status ='College'{code}
spark3.1.2  returns 0 rows
spark2.3.4/hive/presto returns 27w rows;;;
Comment.3: 26/Oct/21 08:37;dongjoon;Does Parquet work in those scenario, [~frankli] and [~wangzhun]?;;;
Comment.4: 26/Oct/21 08:48;frankli;This scenario also occur on Parquet. [~dongjoon]

Spark3.1 will do padding for both writer and reader side.

So, Spark 3.1 cannot read Hive data without padding, while Spark 2.4 works well.;;;
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark doesn't create SUCCESS file in Spark 3.3.0+ when partitionOverwriteMode is dynamic
Issue key: SPARK-44884
Issue id: 13547988
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dipayandev
Creator: dipayandev
Created: 20/Aug/23 12:34
Updated: 22/Jul/24 12:33
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: pull-request-available
Description: The issue is not happening in Spark 2.x (I am using 2.4.0), but only in 3.3.0 (tested with 3.4.1 as well)

Code to reproduce the issue

 
{code:java}
scala> spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic") 
scala> val DF = Seq(("test1", 123)).toDF("name", "num")
scala> DF.write.option("path", "gs://test_bucket/table").mode("overwrite").partitionBy("num").format("orc").saveAsTable("test_schema.test_tb1") {code}
 

The above code succeeds and creates external Hive table, but {*}there is no SUCCESS file generated{*}.

Adding the content of the bucket after table creation

!image-2023-08-25-13-01-42-137.png|width=500,height=130!

 The same code when running with spark 2.4.0 (with or without external path), generates the SUCCESS file.
{code:java}
scala> DF.write.mode(SaveMode.Overwrite).partitionBy("num").format("orc").saveAsTable("test_schema.test_tb1"){code}
!image-2023-08-20-18-46-53-342.png|width=465,height=166!

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 20/Aug/23 13:16;dipayandev;image-2023-08-20-18-46-53-342.png;https://issues.apache.org/jira/secure/attachment/13062293/image-2023-08-20-18-46-53-342.png, 25/Aug/23 07:31;dipayandev;image-2023-08-25-13-01-42-137.png;https://issues.apache.org/jira/secure/attachment/13062437/image-2023-08-25-13-01-42-137.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jul 22 12:33:01 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1jwcg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Aug/23 17:38;stevel@apache.org;this is created in the committer; for hadoop-mapreduce ones  "mapreduce.fileoutputcommitter.marksuccessfuljobs"; is the flag to enable this; if it is not being created then it'll be down to how saveAsTable commits work;;;, 21/Aug/23 18:10;dipayandev;[~stevel@apache.org] , have set that also, but still no _SUCCESS file when we pass an external path. I am not using any custom committer. Its the default Hadoop-mapreduce one. Can you please point me to the code? 
{code:java}
spark.conf.set("spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs", true) {code};;;, 21/Aug/23 18:13;dipayandev;There is no reason to disable this feature in Spark 3.3.0. There can be lots of downstream applications that are dependent on the _SUCCESS file and this feature change wasn't mention anywhere in the release. Any workaround for this? [~stevel@apache.org] ;;;, 22/Aug/23 16:00;stevel@apache.org;[~dipayandev] i don't think think anyone has disabled the option; doesn't surface in my test setup (manifest and s3a committers).

Afraid you are going to have to debug it yourself, as it is your env which has the problem.

does everything work if you use .saveAs() rather than .saveAsTable()?;;;, 22/Aug/23 16:09;dipayandev;[~stevel@apache.org] , I am running on DataProc but I am able to replicate the same from my local machine as well.

_SUCCESS file is created
 * Spark 2.x (2.4.0 I am using) with .saveAsTable() with or without external path.

_SUCCESS file is not created
 * Spark 3.3.0 with .saveAsTable() with or without external path.

As mentioned, I have set the following config, but no help.
spark.conf.set("spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs", true) 
Are you able to replicate the issue with the snippet I have shared or the _SUCCESS file is generating at your end when an external path is passed?;;;, 23/Aug/23 17:23;stevel@apache.org;i'm not trying to replicate it; i have too many other things to do. in open source, sadly, everyone gets to fend for themselves, and i'm not actually a spark developer. i'd suggest looking at what changed in .saveAsTable to see what possible changes may be to blame...;;;, 25/Aug/23 08:24;dipayandev;I am able to narrow down the problem. This issue is happening when we're setting  *_partitionOverwriteMode to dynamic._  __* This config is required to do incremental updates.
{code:java}
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic").{code}
When I am setting it to 'static' , __SUCCESS_ file is generated.;;;, 25/Aug/23 14:11;stevel@apache.org;so using insert overwrite. yes, what happens there is that the entire job is written to a temporary subdir, and on job complete the partition directories are updated, but not any files on the root path created by job committer itself;;;, 25/Aug/23 14:19;dipayandev;Right, the behaviour is same in Spark 2 and 3. However, in Spark 2.x after renaming the temporary subdir, it writes the _SUCCESS file on the root path but NOT in Spark 3.x when that param is passed. 

I see this part of the code ([Hadoop Committer|https://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java#L433[]]) is not changed in the latest hadoop-mapreduce, but somewhere probably _partitionOverwriteMode_ {color:#172b4d}option is broken when passed from latest Spark Dataframewriter. {color}

In Spark 2.x, the _SUCCESS file gets updated everytime you do insert overwrite. 

 ;;;, 31/Aug/23 14:27;medb;Is this a duplicate of the https://issues.apache.org/jira/browse/SPARK-35279?;;;, 22/Jul/24 08:57;anikaKelhanka;*Issue:*
* This issue happens specifically when  {{partitionOverwriteMode = dynamic}} (Insert Overwrite - [SPARK-20236|https://issues.apache.org/jira/browse/SPARK-20236]).
* "_SUCCESS" file is created for spark version <= 3.0.2, given:  {{"spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs"=”true”}}.
* "_SUCCESS" file is not created for spark version > 3.0.2 even when {{"spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs"=”true”}}.

*Analysis (RCA):*

* In the Spark versions prior to 3.0.2, the SUCCESS Marker file is created on the root path when spark job is successful. This is expected behavior.
* What changed: After the change for [SPARK-29302|https://issues.apache.org/jira/browse/SPARK-29302] (dynamic partition overwrite with speculation enabled) got merged, the SUCCESS marker file stopped getting created at the root location when the Spark job writes in dynamic partition override mode. 
* The change [SPARK-29302|https://issues.apache.org/jira/browse/SPARK-29302] (dynamic partition overwrite with speculation enabled) sets the {{committerOutputPath=${stagingDir}}} which previously stored root dir path, in [this codeblock|https://github.com/apache/spark/pull/29000/files#diff-15b529afe19e971b138fc604909bcab2e42484babdcea937f41d18cb22d9401dR167-R175]. 
* The {{committerOutputPath}} parameter is passed on to the hadoop committer, which creates the SUCCESS marker file at the path specified in {{committerOutputPath}} parameter. Thus, the SUCESS marker is now created inside the stagingDir.
* Once Hadoop committer has finished writing, The Spark Commit Protocol logic copies all the data files to root path, [but NOT the SUCCESS marker] before deleting the ${stagingDir}.  
* The stagingDir is then deleted along with SUCCESS Marker file. 

*Proposed Fix:*

The gap in this logic can be mended by adding a step to copy _SUCCESS file as well to the final location before deleting the stagingDir. 
Also, ensure that when {{"spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs"=”false”}}, the _SUCCESS marker file will not be created by the Hadoop output committers in stagingDir itself.

I am working on a fix for same. ;;;, 22/Jul/24 12:33;stevel@apache.org;FWIW The new manifest committer, written for performance on abfs and correctness + performance on gcs generates the exact same JSON file as the s3a committers, and can be executed against local file:// URLs as well as hdfs. If the base hadoop version spark uses includes this committer (MAPREDUCE-7341} then you could write a test to verify the copied file is JSON; the class org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData will actually load the manifest and let you access and print its internals

bq. Also, ensure that when "spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs"=”false”, the _SUCCESS marker file will not be created by the Hadoop output committers in stagingDir itself.


that's in the hadoop mapreduce codebase, should all be good there -but tests are welcome. ;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 21/Aug/23 18:10;dipayandev;[~stevel@apache.org] , have set that also, but still no _SUCCESS file when we pass an external path. I am not using any custom committer. Its the default Hadoop-mapreduce one. Can you please point me to the code? 
{code:java}
spark.conf.set("spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs", true) {code};;;, 22/Jul/24 08:57;anikaKelhanka;*Issue:*
* This issue happens specifically when  {{partitionOverwriteMode = dynamic}} (Insert Overwrite - [SPARK-20236|https://issues.apache.org/jira/browse/SPARK-20236]).
* "_SUCCESS" file is created for spark version <= 3.0.2, given:  {{"spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs"=”true”}}.
* "_SUCCESS" file is not created for spark version > 3.0.2 even when {{"spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs"=”true”}}.

*Analysis (RCA):*

* In the Spark versions prior to 3.0.2, the SUCCESS Marker file is created on the root path when spark job is successful. This is expected behavior.
* What changed: After the change for [SPARK-29302|https://issues.apache.org/jira/browse/SPARK-29302] (dynamic partition overwrite with speculation enabled) got merged, the SUCCESS marker file stopped getting created at the root location when the Spark job writes in dynamic partition override mode. 
* The change [SPARK-29302|https://issues.apache.org/jira/browse/SPARK-29302] (dynamic partition overwrite with speculation enabled) sets the {{committerOutputPath=${stagingDir}}} which previously stored root dir path, in [this codeblock|https://github.com/apache/spark/pull/29000/files#diff-15b529afe19e971b138fc604909bcab2e42484babdcea937f41d18cb22d9401dR167-R175]. 
* The {{committerOutputPath}} parameter is passed on to the hadoop committer, which creates the SUCCESS marker file at the path specified in {{committerOutputPath}} parameter. Thus, the SUCESS marker is now created inside the stagingDir.
* Once Hadoop committer has finished writing, The Spark Commit Protocol logic copies all the data files to root path, [but NOT the SUCCESS marker] before deleting the ${stagingDir}.  
* The stagingDir is then deleted along with SUCCESS Marker file. 

*Proposed Fix:*

The gap in this logic can be mended by adding a step to copy _SUCCESS file as well to the final location before deleting the stagingDir. 
Also, ensure that when {{"spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs"=”false”}}, the _SUCCESS marker file will not be created by the Hadoop output committers in stagingDir itself.

I am working on a fix for same. ;;;, 22/Jul/24 12:33;stevel@apache.org;FWIW The new manifest committer, written for performance on abfs and correctness + performance on gcs generates the exact same JSON file as the s3a committers, and can be executed against local file:// URLs as well as hdfs. If the base hadoop version spark uses includes this committer (MAPREDUCE-7341} then you could write a test to verify the copied file is JSON; the class org.apache.hadoop.mapreduce.lib.output.committer.manifest.files.ManifestSuccessData will actually load the manifest and let you access and print its internals

bq. Also, ensure that when "spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs"=”false”, the _SUCCESS marker file will not be created by the Hadoop output committers in stagingDir itself.


that's in the hadoop mapreduce codebase, should all be good there -but tests are welcome. ;;;
Comment.2: 21/Aug/23 18:13;dipayandev;There is no reason to disable this feature in Spark 3.3.0. There can be lots of downstream applications that are dependent on the _SUCCESS file and this feature change wasn't mention anywhere in the release. Any workaround for this? [~stevel@apache.org] ;;;
Comment.3: 22/Aug/23 16:00;stevel@apache.org;[~dipayandev] i don't think think anyone has disabled the option; doesn't surface in my test setup (manifest and s3a committers).

Afraid you are going to have to debug it yourself, as it is your env which has the problem.

does everything work if you use .saveAs() rather than .saveAsTable()?;;;
Comment.4: 22/Aug/23 16:09;dipayandev;[~stevel@apache.org] , I am running on DataProc but I am able to replicate the same from my local machine as well.

_SUCCESS file is created
 * Spark 2.x (2.4.0 I am using) with .saveAsTable() with or without external path.

_SUCCESS file is not created
 * Spark 3.3.0 with .saveAsTable() with or without external path.

As mentioned, I have set the following config, but no help.
spark.conf.set("spark.hadoop.mapreduce.fileoutputcommitter.marksuccessfuljobs", true) 
Are you able to replicate the issue with the snippet I have shared or the _SUCCESS file is generating at your end when an external path is passed?;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: The column pruning is not working as expected for nested struct in an array
Issue key: SPARK-47230
Issue id: 13570339
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mathfool
Creator: mathfool
Created: 29/Feb/24 15:31
Updated: 21/Jul/24 00:23
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.4.0, 3.5.0, 3.5.1
Fix Version/s: 
Component/s: Optimizer, Spark Core, Spark Shell
Due Date: 
Votes: 1
Labels: pull-request-available
Description: See the code snippet below, when explode an array of struct and select one field in the struct,  some unexpected behaviour observed:
 * If the field in the struct is in the select clause, not in the where clause, the column pruning works as expected.
 * If the field in the struct is in the select clause and in the where clause, the column pruning not working.
 * If the field in the struct is not even selected, the column pruning not working

{code:java}
from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType, StructField, StructType, ArrayType
import random

spark = SparkSession.builder.appName("example").getOrCreate()# Create an RDD with an array of structs, each array having a random size between 5 to 10
rdd = spark.range(1000).rdd.map(lambda x: (x.id + 3, [(x.id + i, x.id - i) for i in range(1, random.randint(5, 11))]))

# Define a new schema

schema = StructType([
    StructField("a", IntegerType(), True),
    StructField("b", ArrayType(StructType([
        StructField("x", IntegerType(), True),
        StructField("y", IntegerType(), True)
    ])), True)
])

# Create a DataFrame with the new schema
df = spark.createDataFrame(rdd, schema=schema)

# Write the DataFrame to a parquet file
df.repartition(1).write.mode('overwrite').parquet('test.parquet')

# Read the parquet file back into a DataFrame
df = spark.read.parquet('test.parquet') 
df.createOrReplaceTempView("df_view")
spark.conf.set('spark.sql.optimizer.nestedSchemaPruning.enabled', 'true')

# case 1, as expected
sql_query = """
SELECT a, EXPLODE(b.x) AS bb
FROM df_view
"""
spark.sql(sql_query).explain()

# ReadSchema: struct<a:int,b:array<struct<x:int>>>

# case 2, as expected
sql_query = """
SELECT a, bb.x 
FROM df_view 
lateral view explode(b) as bb
"""
spark.sql(sql_query).explain()

# ReadSchema: struct<a:int,b:array<struct<x:int>>>

# case 3, bug: should only read b.x
sql_query = """
SELECT a, bb.x 
FROM df_view 
lateral view explode(b) as bb
where bb.x is not null
"""
spark.sql(sql_query).explain()

#ReadSchema: struct<a:int,b:array<struct<x:int,y:int>>>

#case 4, bug? seems no need to read both a and b
sql_query = """
SELECT a
FROM df_view 
lateral view explode(b) as bb
"""
spark.sql(sql_query).explain()

#ReadSchema: struct<a:int,b:array<struct<x:int,y:int>>>{code}
 
 
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2024-02-29 15:31:45.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1npds:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.4.0
Component/s.1: Spark Core
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0, Unknown

Summary: Incorrect AnalysisException thrown using when() and mixed data types
Issue key: SPARK-48868
Issue id: 13585414
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: abuqutaita
Creator: abuqutaita
Created: 11/Jul/24 06:50
Updated: 11/Jul/24 06:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.4.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Observe the following sample code, where I'm using when() based on typeof():
{code:java}
from pyspark.sql.types import (
    ArrayType,
    IntegerType,
    StringType,
    StructField,
    StructType,
)

schema = StructType(
    [
        StructField("ID", IntegerType(), nullable=False),
        StructField("name", StringType(), nullable=False),
        StructField("colors", ArrayType(StringType()), nullable=False),
    ]
)
data = [
    (1, "John", ["red", "blue", "green"]),
    (2, "Jane", ["yellow", "orange", "purple"]),
    (3, "Bob", ["black", "white"]),
    (4, "Alice", ["pink"]),
    (5, "Tom", ["brown", "gray"]),
]
df = spark.createDataFrame(data, schema)
col = "name"
df = df.withColumn(
    col,
    F.when(F.expr(f"typeof({col}) == 'string'"), F.trim(col))
    .when(
        F.expr(f"typeof({col}) LIKE 'array%'"),
        F.array_join(col, ","),
    )
    .otherwise(F.lit(None)),
)
{code}
 
Here's the exception I'm seeing:
{noformat}
pyspark.sql.utils.AnalysisException: cannot resolve 'array_join(name, ',')' due to data type mismatch: argument 1 requires array<string> type, however, 'name' is of string type.;
'Project [ID#0, CASE WHEN (typeof(name#1) = string) THEN trim(name#1, None) WHEN typeof(name#1) LIKE array% THEN array_join(name#1, ,, None) ELSE null END AS name#6, colors#2]
+- LogicalRDD [ID#0, name#1, colors#2], false
{noformat}
 

If I change col to "colors", I get this similar exception:
{noformat}
pyspark.sql.utils.AnalysisException: cannot resolve 'trim(colors)' due to data type mismatch: argument 1 requires string type, however, 'colors' is of array<string> type.;
'Project [ID#0, name#1, CASE WHEN (typeof(colors#2) = string) THEN trim(colors#2, None) WHEN typeof(colors#2) LIKE array% THEN array_join(colors#2, ,, None) ELSE null END AS colors#6]
+- LogicalRDD [ID#0, name#1, colors#2], false
{noformat}
 

It seems to try to evaluate all possible paths of code for type checking, even if that code path won't be hit for the current query. I was able to repro this on 3.3.0 and 3.4.0.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2024-07-11 06:50:13.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1q9u8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.4.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Kafka custom partition location assignment in Spark Structured Streaming (rack awareness)
Issue key: SPARK-46798
Issue id: 13565729
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: rschwagercharter
Creator: rschwagercharter
Created: 22/Jan/24 23:53
Updated: 02/Jul/24 16:56
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.0, 3.2.0, 3.3.0, 3.4.0, 3.5.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: pull-request-available
Description: I'd like to propose, and implement if approved, support for custom partition location assignment. [Please find the design doc for SPARK-46798 describing the change here.|https://docs.google.com/document/d/1RoEk_mt8AUh9sTQZ1NfzIuuYKf1zx6BP1K3IlJ2b8iM/edit#heading=h.pbt6pdb2jt5c]

SPARK-15406 Added Kafka consumer support to Spark Structured Streaming, but it did not add custom partition location assignment as a feature. The Structured Streaming Kafka consumer as it exists today evenly allocates Kafka topic partitions to executors without regard to Kafka broker rack information or executor location. This behavior can drive large cross-AZ networking costs in large deployments.

[The design doc for SPARK-15406|https://docs.google.com/document/d/19t2rWe51x7tq2e5AOfrsM9qb8_m7BRuv9fel9i0PqR8/edit#heading=h.k36c6oyz89xw] described the ability to assign Kafka partitions to particular executors (a feature which would enable rack awareness), but it seems that feature was never implemented.

For DStreams users, there does seem to be a way to assign Kafka partitions to Spark executors in a custom fashion with [LocationStrategies.PreferFixed|https://github.com/apache/spark/blob/master/connector/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/LocationStrategy.scala#L69], so this sort of functionality has a precedent.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2024-01-22 23:53:59.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1mxfc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0, Unknown

Summary: The Dynamic Partition Pruning (DPP) feature in Spark can cause the generated Directed Acyclic Graph (DAG) to expand.
Issue key: SPARK-48486
Issue id: 13581137
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: binsoo.chen
Creator: binsoo.chen
Created: 31/May/24 08:25
Updated: 27/Jun/24 08:24
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: The Dynamic Partition Pruning (DPP) feature in Spark can cause the generated Directed Acyclic Graph (DAG) to expand.

for example: The partition field of table A or B is part_dt.  

select a.part_dt,{*}b.part_dt{*}
from a left join b on b.part_dt = a.part_dt 
where a.part_dt in ('2024-05-30','2024-05-31')

During the generation of Dynamic Partition Pruning (DPP), the tree will be traversed recursively more times. 

24/05/31 16:14:14 INFO DataSourceStrategy: Pruning directories with: part_dt#219 IN (2024-05-30,2024-05-31)
24/05/31 16:14:14 INFO DataSourceStrategy: Pruning directories with: part_dt#223 IN (2024-05-30,2024-05-31),isnotnull(part_dt#223),dynamicpruning#234 [part_dt#223|#223]
24/05/31 16:14:14 INFO DataSourceStrategy: Pruning directories with: part_dt#219 IN (2024-05-30,2024-05-31)

The last one is extra：24/05/31 16:14:14 INFO DataSourceStrategy: Pruning directories with: part_dt#219 IN (2024-05-30,2024-05-31)

When more partitions meet the condition, it will increase by 2 to the power of n, then divided by 2.

for example:

select a.part_dt{*},b.part_dt{*},c.part_dt
from a 
left join b on b.part_dt = a.part_dt 
left join c on c.part_dt = a.part_dt
where a.part_dt in ('2024-05-30','2024-05-31')

result:

24/05/31 16:29:19 INFO ExecuteStatement: Execute in full collect mode
24/05/31 16:29:19 INFO DataSourceStrategy: Pruning directories with: part_dt#249 IN (2024-05-30,2024-05-31)
24/05/31 16:29:19 INFO DataSourceStrategy: Pruning directories with: part_dt#253 IN (2024-05-30,2024-05-31),isnotnull(part_dt#253),dynamicpruning#261 [part_dt#253|#253]
24/05/31 16:29:19 INFO DataSourceStrategy: Pruning directories with: part_dt#257 IN (2024-05-30,2024-05-31),isnotnull(part_dt#257),dynamicpruning#262 [part_dt#257|#257]
24/05/31 16:29:19 INFO DataSourceStrategy: Pruning directories with: part_dt#249 IN (2024-05-30,2024-05-31)
24/05/31 16:29:19 INFO DataSourceStrategy: Pruning directories with: part_dt#249 IN (2024-05-30,2024-05-31)
24/05/31 16:29:19 INFO DataSourceStrategy: Pruning directories with: part_dt#253 IN (2024-05-30,2024-05-31),isnotnull(part_dt#253),dynamicpruning#261 [part_dt#253|#253]
24/05/31 16:29:19 INFO DataSourceStrategy: Pruning directories with: part_dt#249 IN (2024-05-30,2024-05-31)

The last four lines are all superfluous.

When more than 10 conditions are met, it will cause a memory overflow when generating DPP, and the generated plan will be about 400 times larger.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 31/May/24 08:41;binsoo.chen;create_table.sql;https://issues.apache.org/jira/secure/attachment/13069226/create_table.sql
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 27 08:21:49 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1pjhk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/May/24 08:56;binsoo.chen;The reason for the problem starting from Spark 3.3 is that the method getFilterableTableScan in org.apache.spark.sql.execution.dynamicpruning.PartitionPruning has added support for HiveTableRelation. Then, after the traversal of plan transformUp in the prune method, amplification occurred during the recursion of insertPredicate in splitConjunctivePredicates(condition).foreach.;;;, 27/Jun/24 08:21;binsoo.chen;SELECT *
FROM fact_sk a
LEFT JOIN fact_sk_1 b on b.store_id = a.store_id
LEFT JOIN fact_sk_2 c on c.store_id = a.store_id
WHERE a.store_id in ('4','5')
By analyzing the generated Optimized Logical Plan.

== Optimized Logical Plan ==
Join LeftOuter, (store_id#5490 = store_id#5482)
:- Join LeftOuter, (store_id#5486 = store_id#5482)
:  :- Filter cast(store_id#5482 as string) IN (4,5)
:  :  +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet
:  +- Filter ((cast(store_id#5486 as string) IN (4,5) AND isnotnull(store_id#5486)) AND dynamicpruning#5506 [store_id#5486|#5486])
:     :  +- Filter cast(store_id#5482 as string) IN (4,5)
:     :     +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet
:     +- Relation default.fact_sk_1[date_id#5483,product_id#5484,units_sold#5485,store_id#5486|#5483,product_id#5484,units_sold#5485,store_id#5486] parquet
+- Filter ((cast(store_id#5490 as string) IN (4,5) AND isnotnull(store_id#5490)) AND dynamicpruning#5507 [store_id#5490|#5490])
   :  +- Join LeftOuter, (store_id#5486 = store_id#5482)
   :     :- Filter cast(store_id#5482 as string) IN (4,5)
   :     :  +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet
   :     +- Filter dynamicpruning#5506 [store_id#5486|#5486]
   :        :  +- Filter cast(store_id#5482 as string) IN (4,5)
   :        :     +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet
   :        +- Filter (cast(store_id#5486 as string) IN (4,5) AND isnotnull(store_id#5486))
   :           +- Relation default.fact_sk_1[date_id#5483,product_id#5484,units_sold#5485,store_id#5486|#5483,product_id#5484,units_sold#5485,store_id#5486] parquet
   +- Relation default.fact_sk_2[date_id#5487,product_id#5488,units_sold#5489,store_id#5490|#5487,product_id#5488,units_sold#5489,store_id#5490] parquet

 

In the previous join, during the DPP transformation, the pushdown is converted into a subquery, forming the following:

:- Join LeftOuter, (store_id#5486 = store_id#5482) 
: :- Filter cast(store_id#5482 as string) IN (4,5) 
: : +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet 
: +- Filter ((cast(store_id#5486 as string) IN (4,5) AND isnotnull(store_id#5486)) AND dynamicpruning#5506 [store_id#5486|#5486]) 
: : +- Filter cast(store_id#5482 as string) IN (4,5) 
: : +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet 
: +- Relation default.fact_sk_1[date_id#5483,product_id#5484,units_sold#5485,store_id#5486|#5483,product_id#5484,units_sold#5485,store_id#5486] parquet

This is then treated as a whole, and then passed down. On this basis, a subquery is added.

+- Filter (cast(store_id#5490 as string) IN (4,5) AND isnotnull(store_id#5490)) 
+- Relation default.fact_sk_2[date_id#5487,product_id#5488,units_sold#5489,store_id#5490|#5487,product_id#5488,units_sold#5489,store_id#5490] parquet The above will be converted to:

+- Filter ((cast(store_id#5490 as string) IN (4,5) AND isnotnull(store_id#5490)) AND dynamicpruning#5507 [store_id#5490|#5490]) 
: +- Join LeftOuter, (store_id#5486 = store_id#5482) 
: :- Filter cast(store_id#5482 as string) IN (4,5) 
: : +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet 
: +- Filter dynamicpruning#5506 [store_id#5486|#5486] 
: : +- Filter cast(store_id#5482 as string) IN (4,5) : : +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet 
: +- Filter (cast(store_id#5486 as string) IN (4,5) AND isnotnull(store_id#5486)) 
: +- Relation default.fact_sk_1[date_id#5483,product_id#5484,units_sold#5485,store_id#5486|#5483,product_id#5484,units_sold#5485,store_id#5486] parquet 
+- Relation default.fact_sk_2[date_id#5487,product_id#5488,units_sold#5489,store_id#5490|#5487,product_id#5488,units_sold#5489,store_id#5490] parquet

In this case, when encountering multiple nested subquery pushdowns, accumulation occurs. If it's 5 times, it's 2 to the power of 5; if it's 10 times, it's 2 to the power of 10. Here, a strategy needs to be added to limit the nesting within a certain range.

If it is changed to traverse by transformDown, it means giving up nesting. When each DPP is processed, only the last layer of subquery will be added.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 27/Jun/24 08:21;binsoo.chen;SELECT *
FROM fact_sk a
LEFT JOIN fact_sk_1 b on b.store_id = a.store_id
LEFT JOIN fact_sk_2 c on c.store_id = a.store_id
WHERE a.store_id in ('4','5')
By analyzing the generated Optimized Logical Plan.

== Optimized Logical Plan ==
Join LeftOuter, (store_id#5490 = store_id#5482)
:- Join LeftOuter, (store_id#5486 = store_id#5482)
:  :- Filter cast(store_id#5482 as string) IN (4,5)
:  :  +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet
:  +- Filter ((cast(store_id#5486 as string) IN (4,5) AND isnotnull(store_id#5486)) AND dynamicpruning#5506 [store_id#5486|#5486])
:     :  +- Filter cast(store_id#5482 as string) IN (4,5)
:     :     +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet
:     +- Relation default.fact_sk_1[date_id#5483,product_id#5484,units_sold#5485,store_id#5486|#5483,product_id#5484,units_sold#5485,store_id#5486] parquet
+- Filter ((cast(store_id#5490 as string) IN (4,5) AND isnotnull(store_id#5490)) AND dynamicpruning#5507 [store_id#5490|#5490])
   :  +- Join LeftOuter, (store_id#5486 = store_id#5482)
   :     :- Filter cast(store_id#5482 as string) IN (4,5)
   :     :  +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet
   :     +- Filter dynamicpruning#5506 [store_id#5486|#5486]
   :        :  +- Filter cast(store_id#5482 as string) IN (4,5)
   :        :     +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet
   :        +- Filter (cast(store_id#5486 as string) IN (4,5) AND isnotnull(store_id#5486))
   :           +- Relation default.fact_sk_1[date_id#5483,product_id#5484,units_sold#5485,store_id#5486|#5483,product_id#5484,units_sold#5485,store_id#5486] parquet
   +- Relation default.fact_sk_2[date_id#5487,product_id#5488,units_sold#5489,store_id#5490|#5487,product_id#5488,units_sold#5489,store_id#5490] parquet

 

In the previous join, during the DPP transformation, the pushdown is converted into a subquery, forming the following:

:- Join LeftOuter, (store_id#5486 = store_id#5482) 
: :- Filter cast(store_id#5482 as string) IN (4,5) 
: : +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet 
: +- Filter ((cast(store_id#5486 as string) IN (4,5) AND isnotnull(store_id#5486)) AND dynamicpruning#5506 [store_id#5486|#5486]) 
: : +- Filter cast(store_id#5482 as string) IN (4,5) 
: : +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet 
: +- Relation default.fact_sk_1[date_id#5483,product_id#5484,units_sold#5485,store_id#5486|#5483,product_id#5484,units_sold#5485,store_id#5486] parquet

This is then treated as a whole, and then passed down. On this basis, a subquery is added.

+- Filter (cast(store_id#5490 as string) IN (4,5) AND isnotnull(store_id#5490)) 
+- Relation default.fact_sk_2[date_id#5487,product_id#5488,units_sold#5489,store_id#5490|#5487,product_id#5488,units_sold#5489,store_id#5490] parquet The above will be converted to:

+- Filter ((cast(store_id#5490 as string) IN (4,5) AND isnotnull(store_id#5490)) AND dynamicpruning#5507 [store_id#5490|#5490]) 
: +- Join LeftOuter, (store_id#5486 = store_id#5482) 
: :- Filter cast(store_id#5482 as string) IN (4,5) 
: : +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet 
: +- Filter dynamicpruning#5506 [store_id#5486|#5486] 
: : +- Filter cast(store_id#5482 as string) IN (4,5) : : +- Relation default.fact_sk[date_id#5479,product_id#5480,units_sold#5481,store_id#5482|#5479,product_id#5480,units_sold#5481,store_id#5482] parquet 
: +- Filter (cast(store_id#5486 as string) IN (4,5) AND isnotnull(store_id#5486)) 
: +- Relation default.fact_sk_1[date_id#5483,product_id#5484,units_sold#5485,store_id#5486|#5483,product_id#5484,units_sold#5485,store_id#5486] parquet 
+- Relation default.fact_sk_2[date_id#5487,product_id#5488,units_sold#5489,store_id#5490|#5487,product_id#5488,units_sold#5489,store_id#5490] parquet

In this case, when encountering multiple nested subquery pushdowns, accumulation occurs. If it's 5 times, it's 2 to the power of 5; if it's 10 times, it's 2 to the power of 10. Here, a strategy needs to be added to limit the nesting within a certain range.

If it is changed to traverse by transformDown, it means giving up nesting. When each DPP is processed, only the last layer of subquery will be added.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add consistency check and fallback for mapIds in push-merged block meta
Issue key: SPARK-48580
Issue id: 13582208
Parent id: 13337114.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gaoyajun02
Creator: gaoyajun02
Created: 11/Jun/24 02:15
Updated: 18/Jun/24 14:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0, 3.4.0, 3.5.0
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: pull-request-available
Description: When push-based shuffle enabled, 0.03% of the spark application in our cluster experienced shuffle data loss. The metrics of Exchange as follows:

!image-2024-06-11-10-19-57-227.png|width=405,height=170!

We eventually found some WARN logs on the shuffle server:
 
{code:java}
WARN shuffle-server-8-216 org.apache.spark.network.shuffle.RemoteBlockPushResolver: Application application_xxxx shuffleId 0 shuffleMergeId 0 reduceId 133 update to index/meta failed{code}
 

And analyzed the cause from the code：

The merge metadata obtained by the reduce side from the driver comes from the {{mapTracker}} in the server's memory, while the actual reading of chunk data is based on the records in the shuffle server's {{{}metaFile{}}}. There is no consistency check between the two.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 11/Jun/24 02:19;gaoyajun02;image-2024-06-11-10-19-57-227.png;https://issues.apache.org/jira/secure/attachment/13069422/image-2024-06-11-10-19-57-227.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2024-06-11 02:15:59.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1pq3c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0

Summary: Support returning a table or set of rows in CREATE FUNCTION
Issue key: SPARK-39247
Issue id: 13446161
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Erik Jansen
Creator: Erik Jansen
Created: 21/May/22 10:25
Updated: 08/Jun/24 17:54
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: The Databricks version supports create a function which returns a result set:
_"Creates a SQL scalar or table function that takes a set of arguments and returns a scalar value or a set of rows."_
Such functions are very useful as the user can pass a list of parameters which can be used in the function to create a dynamic result set (a query which will use the passed parameters). 

See the example from databricks:
{_}"{_}{_}-- Produce all weekdays between two dates > CREATE FUNCTION weekdays(start DATE, end DATE) RETURNS TABLE(day_of_week STRING, day DATE) RETURN SELECT extract(DAYOFWEEK_ISO FROM day), day FROM (SELECT sequence(weekdays.start, weekdays.end)) AS T(days) LATERAL VIEW explode(days) AS day WHERE extract(DAYOFWEEK_ISO FROM day) BETWEEN 1 AND 5;"{_}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-43797
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Jun 08 17:51:56 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12khs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Jun/24 17:51;jboarman;Being able to create a *UDTF* (user-defined table function), also known as a tabular UDF, table UDF, UDF table function, or a TVF (user-defined table-value function), are incredibly useful and very powerful, especially in combination with the LATERAL invocations.

Other platforms, such as SQL Server has offered this feature for more than 20 years back, which means this has been around long enough to develop deep and frequent use cases in more mature data teams.

 
{panel:title=Documentation from a few major data platforms covering their implementation usage: }
*_Databricks -- Create a SQL table function_*
https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.html#create-a-sql-table-function

*_Snowflake -- Creating Tabular SQL UDFs (UDTFs)_*
https://docs.snowflake.com/en/developer-guide/udf/sql/udf-sql-tabular-functions

*_BigQuery -- Creating table-valued functions (TVF)_*
https://cloud.google.com/bigquery/docs/table-functions

*_Postgres -- SQL Functions Returning TABLE_*
https://www.postgresql.org/docs/current/xfunc-sql.html#XFUNC-SQL-FUNCTIONS-RETURNING-TABLE

*_SAP HANA -- Table user-defined functions (TUDF)_*
https://help.sap.com/docs/SAP_HANA_PLATFORM/de2486ee947e43e684d39702027f8a94/2fc6d7beebd14c579457092e91519082.html?q=create%20function

*_IBM DB2 -- CREATE FUNCTION (user-defined external table)_*
https://www.ibm.com/docs/en/db2/11.5?topic=statements-create-function-external-table

*_Oracle -- Pipelined Table Functions_*
https://docs.oracle.com/en/database/oracle/oracle-database/19/lnpls/PIPELINED-clause.html

*_MSSQL -- Inline and Multi-statement table-valued functions (TVF/MSTVF)_*
https://learn.microsoft.com/en-us/sql/relational-databases/user-defined-functions/create-user-defined-functions-database-engine?view=sql-server-ver16#inline-table-valued-function-tvf
{panel}
 ;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Push partial aggregation through join
Issue key: SPARK-38506
Issue id: 13433147
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 10/Mar/22 15:09
Updated: 31/May/24 07:08
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: pull-request-available
Description: Please see https://docs.teradata.com/r/Enterprise_IntelliFlex_VMware/SQL-Request-and-Transaction-Processing/Join-Planning-and-Optimization/Partial-GROUP-BY-Block-Optimization for more details.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): SPARK-38505
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue May 24 14:18:14 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10dfk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Mar/22 14:45;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36005;;;, 14/May/22 13:04;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36552;;;, 14/May/22 13:05;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36552;;;, 24/May/22 14:18;yumwang;Benchmark result after [this commit|https://github.com/apache/spark/pull/36552/commits/d029bb2c7c003dff28e3af940fb06cd0b14fc6cb]:
|SQL|Before(ms)|With Partial Aggregation Optimization(ms)|
|v1.4 q4|72478|61261|
|v1.4 q5|23235|20971|
|v1.4 q10|13406|8422|
|v1.4 q11|37975|24236|
|v1.4 q14a|154484|52502|
|v1.4 q14b|128712|57363|
|v1.4 q23a|153233|58932|
|v1.4 q23b|159162|78401|
|v1.4 q24a|392441|84826|
|v1.4 q24b|408129|76384|
|v1.4 q31|14696|13766|
|v1.4 q35|29005|17662|
|v1.4 q37|20076|9218|
|v1.4 q47|36560|31299|
|v1.4 q54|12283|11306|
|v1.4 q57|38530|35303|
|v1.4 q69|15839|11968|
|v1.4 q82|24498|13451|
|v1.4 q95|69196|42653|
|v2.7 q6|9129|10527|
|v2.7 q10a|11778|9909|
|v2.7 q11|40113|29130|
|v2.7 q14|159807|38052|
|v2.7 q14a|238149|128097|
|v2.7 q22a|9344|5269|
|v2.7 q35|36910|14705|
|v2.7 q35a|32793|13303|
|v2.7 q47|49689|27308|
|v2.7 q57|26016|28775|
|v2.7 q74|42607|19340|
|modifiedQueries q10|11675|8628|
|modifiedQueries q98|6877|5219|;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 14/May/22 13:04;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36552;;;
Comment.2: 14/May/22 13:05;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36552;;;
Comment.3: 24/May/22 14:18;yumwang;Benchmark result after [this commit|https://github.com/apache/spark/pull/36552/commits/d029bb2c7c003dff28e3af940fb06cd0b14fc6cb]:
|SQL|Before(ms)|With Partial Aggregation Optimization(ms)|
|v1.4 q4|72478|61261|
|v1.4 q5|23235|20971|
|v1.4 q10|13406|8422|
|v1.4 q11|37975|24236|
|v1.4 q14a|154484|52502|
|v1.4 q14b|128712|57363|
|v1.4 q23a|153233|58932|
|v1.4 q23b|159162|78401|
|v1.4 q24a|392441|84826|
|v1.4 q24b|408129|76384|
|v1.4 q31|14696|13766|
|v1.4 q35|29005|17662|
|v1.4 q37|20076|9218|
|v1.4 q47|36560|31299|
|v1.4 q54|12283|11306|
|v1.4 q57|38530|35303|
|v1.4 q69|15839|11968|
|v1.4 q82|24498|13451|
|v1.4 q95|69196|42653|
|v2.7 q6|9129|10527|
|v2.7 q10a|11778|9909|
|v2.7 q11|40113|29130|
|v2.7 q14|159807|38052|
|v2.7 q14a|238149|128097|
|v2.7 q22a|9344|5269|
|v2.7 q35|36910|14705|
|v2.7 q35a|32793|13303|
|v2.7 q47|49689|27308|
|v2.7 q57|26016|28775|
|v2.7 q74|42607|19340|
|modifiedQueries q10|11675|8628|
|modifiedQueries q98|6877|5219|;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Invalid previous reader checks in Vectorized DELTA_BYTE_ARRAY parquet decoder
Issue key: SPARK-48234
Issue id: 13578869
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yutsareva
Creator: yutsareva
Created: 10/May/24 12:31
Updated: 10/May/24 14:57
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0, 3.4.1, 3.4.2, 3.4.3, 3.5.0, 3.5.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: pull-request-available
Description: The vectorized DELTA_BYTE_ARRAY Parquet decoder can cause read failures when reading columns with varying page encodings and if some pages are encoded using DELTA_BYTE_ARRAY.

Same bug existed in parquet-mr reader but was fixed 3 months ago. There is no separate bug fix commit, it was silently fixed along with other changes. https://github.com/apache/parquet-mr/blob/c241170d9bc2cd8415b04e06ecea40ed3d80f64d/parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnReaderBase.java#L732
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2024-05-10 12:31:28.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1p5iw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.1, 3.4.0, 3.4.1, 3.4.2, 3.4.3, 3.5.0, 3.5.1
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.12.0, EMR-6.13.0, EMR-6.14.0, EMR-6.15.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0, Unknown

Summary: EquivalentExpressions throw IllegalStateException
Issue key: SPARK-46632
Issue id: 13563989
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zhenhaozhang
Creator: zhenhaozhang
Created: 09/Jan/24 04:07
Updated: 10/May/24 02:15
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.4.0, 3.5.0
Fix Version/s: 
Component/s: Optimizer, Spark Core, SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: EquivalentExpressions throw IllegalStateException with some IF expresssion

```scala
import org.apache.spark.sql.catalyst.dsl.expressions.DslExpression
import org.apache.spark.sql.catalyst.expressions.\{EquivalentExpressions, If, Literal}
import org.apache.spark.sql.functions.col

val one = Literal(1.0)
val y = col("y").expr
val e1 = If(
  Literal(true),
  y * one * one + one * one * y,
  y * one * one + one * one * y
)
(new EquivalentExpressions).addExprTree(e1)
```

 

result is 

```

java.lang.IllegalStateException: Cannot update expression: (1.0 * 1.0) in map: Map(ExpressionEquals(('y * 1.0)) -> ExpressionStats(('y * 1.0))) with use count: -1
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.updateExprInMap(EquivalentExpressions.scala:85)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.updateExprTree(EquivalentExpressions.scala:198)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.$anonfun$updateExprTree$1(EquivalentExpressions.scala:200)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.$anonfun$updateExprTree$1$adapted(EquivalentExpressions.scala:200)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.updateExprTree(EquivalentExpressions.scala:200)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.$anonfun$updateExprTree$1(EquivalentExpressions.scala:200)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.$anonfun$updateExprTree$1$adapted(EquivalentExpressions.scala:200)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.updateExprTree(EquivalentExpressions.scala:200)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.updateCommonExprs(EquivalentExpressions.scala:128)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.$anonfun$updateExprTree$3(EquivalentExpressions.scala:201)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.$anonfun$updateExprTree$3$adapted(EquivalentExpressions.scala:201)
  at scala.collection.immutable.List.foreach(List.scala:431)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.updateExprTree(EquivalentExpressions.scala:201)
  at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:188)
  ... 49 elided

```
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2024-01-09 04:07:17.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1mmow:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.4.0
Component/s.1: Spark Core
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0

Summary: InsertIntoHadoopFsRelationCommand unnecessarily fetches details of partitions in most cases
Issue key: SPARK-38230
Issue id: 13428770
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: coalchan
Creator: coalchan
Created: 16/Feb/22 10:23
Updated: 08/May/24 12:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.0.2, 3.3.0, 3.4.0, 3.5.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: In `org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand`, `sparkSession.sessionState.catalog.listPartitions` will call method `org.apache.hadoop.hive.metastore.listPartitionsPsWithAuth` of hive metastore client, this method will produce multiple queries per partition on hive metastore db. So when you insert into a table which has too many partitions(ie: 10k), it will produce too many queries on hive metastore db(ie: n * 10k = 10nk), it puts a lot of strain on the database.

In fact, it calls method `listPartitions` in order to get locations of partitions and get `customPartitionLocations`. But in most cases, we do not have custom partitions, we can just get partition names, so we can call method listPartitionNames.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 22 09:22:39 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zmkg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Feb/22 09:00;apachespark;User 'coalchan' has created a pull request for this issue:
https://github.com/apache/spark/pull/35549;;;, 17/Feb/22 09:01;apachespark;User 'coalchan' has created a pull request for this issue:
https://github.com/apache/spark/pull/35549;;;, 22/Nov/22 09:10;roczei;Hi [~coalchan],

[Your pull request|https://github.com/apache/spark/pull/35549] has been automatically closed by the github action, I would like to create a new pull request based on yours and continue to work on this if you agree.
 ;;;, 16/Jan/23 05:44;apachespark;User 'czxm' has created a pull request for this issue:
https://github.com/apache/spark/pull/39595;;;, 16/Jan/23 05:54;ximz;Hello [~coalchan] Thanks for working on this.  I created PR based on your work with some improvements as per [~Jackey Lee]'s comment. 
[~roczei] Can you please review the PR and let me know if I missed anything? Thank you.;;;, 17/Jan/23 09:08;roczei;Hi [~ximz],

> [~roczei] Can you please review the PR and let me know if I missed anything? Thank you.

I will try to allocate some time for this next week. ;;;, 21/Jun/23 03:28;jeanlyn;We found Hive metastore crash frequently after upgrade Spark from 2.4.7 to 3.3.2. After investigation, I found `InsertIntoHadoopFsRelationCommand` will pull all partitions when using dynamicPartitionOverwrite, and i find this issue after solves the problem by using generate paths to get partitions to get partitions in our environment. So, I have submitted a new pull request, hoping to help you.;;;, 22/Jun/23 09:22;githubbot;User 'jeanlyn' has created a pull request for this issue:
https://github.com/apache/spark/pull/41628;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 17/Feb/22 09:01;apachespark;User 'coalchan' has created a pull request for this issue:
https://github.com/apache/spark/pull/35549;;;
Comment.2: 22/Nov/22 09:10;roczei;Hi [~coalchan],

[Your pull request|https://github.com/apache/spark/pull/35549] has been automatically closed by the github action, I would like to create a new pull request based on yours and continue to work on this if you agree.
 ;;;
Comment.3: 16/Jan/23 05:44;apachespark;User 'czxm' has created a pull request for this issue:
https://github.com/apache/spark/pull/39595;;;
Comment.4: 16/Jan/23 05:54;ximz;Hello [~coalchan] Thanks for working on this.  I created PR based on your work with some improvements as per [~Jackey Lee]'s comment. 
[~roczei] Can you please review the PR and let me know if I missed anything? Thank you.;;;
EMR Versions: EMR-6.12.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0, Unknown

Summary: DataSourceV2: View support
Issue key: SPARK-39800
Issue id: 13471933
Parent id: 13296394.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jzhuge
Creator: jzhuge
Created: 17/Jul/22 16:45
Updated: 13/Apr/24 00:16
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Support Data source V2 views.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jan 30 07:23:05 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16x8o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Jan/23 07:22;apachespark;User 'jzhuge' has created a pull request for this issue:
https://github.com/apache/spark/pull/39796;;;, 30/Jan/23 07:23;apachespark;User 'jzhuge' has created a pull request for this issue:
https://github.com/apache/spark/pull/39796;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 30/Jan/23 07:23;apachespark;User 'jzhuge' has created a pull request for this issue:
https://github.com/apache/spark/pull/39796;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in org.apache.spark.scheduler
Issue key: SPARK-38473
Issue id: 13432766
Parent id: 13423097.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bozhang
Creator: bozhang
Created: 09/Mar/22 05:19
Updated: 16/Mar/24 00:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: pull-request-available
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Nov 29 18:34:48 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10b34:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/Oct/23 19:08;hannahkamundson;Hello everyone (and [~bozhang]),

I am a graduate student at the University of Texas (Computer Science). I have a project in my Distributed Systems course to contribute to an open source distributed project. Would it be okay if I worked on this ticket?

 

Thanks for your help,

Hannah;;;, 01/Nov/23 16:58;hannahkamundson;I am working on this ticket now!;;;, 29/Nov/23 18:34;hannahkamundson;Hi [~maxgekk]  [~bozhang]! This one is ready for review when you get a chance :) Let us know your thoughts https://github.com/apache/spark/pull/44080;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 01/Nov/23 16:58;hannahkamundson;I am working on this ticket now!;;;
Comment.2: 29/Nov/23 18:34;hannahkamundson;Hi [~maxgekk]  [~bozhang]! This one is ready for review when you get a chance :) Let us know your thoughts https://github.com/apache/spark/pull/44080;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Fix performance regression in JDK 17 caused from RocksDB logging
Issue key: SPARK-47369
Issue id: 13571674
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: neilramaswamy
Creator: neilramaswamy
Created: 12/Mar/24 21:04
Updated: 13/Mar/24 17:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0, 3.4.1, 3.4.2, 3.5.0, 3.5.1
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: JDK 17 has a performance regression in the JNI's AttachCurrentThread and DetachCurrentThread calls, as reported here: [https://bugs.openjdk.org/browse/JDK-8314859]. You can find a minimal reproduction of the JDK issue in that bug report. I have marked as affected versions 3.3.0^ since that is when JDK 17 started being offered in Spark.

For context, every time RocksDB logs, it currently [attaches itself to the JVM|https://github.com/facebook/rocksdb/blob/main/java/rocksjni/loggerjnicallback.cc#L140], invokes the RocksDB [logging callback that we specify|https://github.com/apache/spark/blob/8fcef1657a02189f91d5485eabb5b165706cdce9/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala#L839], and then [detaches itself from the JVM|https://github.com/facebook/rocksdb/blob/main/java/rocksjni/loggerjnicallback.cc#L170]. These attach/detach calls regressed, causing JDK 17 SS queries to run up to 10-15% slower than their respective JDK 8 queries.

For example, a 100K record/second dropDuplicates had a p95 latency regression of 12%. A regression of 12% and 21% (at the p95) was observed for a query with 1M record/second, 100K keys, 10 second windows, and 0 second watermark.

Because the Hotspot folks marked this as "Won't fix," one way to fix this is to avoid the JNI entirely and write the RocksDB to stderr. RocksDB [8.11.3 natively supports this|https://github.com/facebook/rocksdb/wiki/Logging-in-RocksJava#configuring-a-native-logger] (I implemented that feature in RocksJava). We can configure our RocksDB logger to do its logging this way.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 13 17:48:32 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1nxm8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Mar/24 21:05;neilramaswamy;cc: [~dongjoon] and [~LuciferYang], who were involved on the JDK 17 upgrade to 8.11.3. Maybe you would be interested in discussing this issue and the proposed fix.;;;, 12/Mar/24 22:00;dongjoon;Thank you for reporting, [~neilramaswamy]  Could you provide a reproducible Spark example for the further discussion?;;;, 13/Mar/24 07:40;LuciferYang;From the current Spark code, it appears that the {{Logger}} is only set for the {{RocksDB}} instance built for external shuffle db(inRocksDBProvider), and not for other parts. However, it seems that the Spark code does not actively print RocksDB-related logs (perhaps my confirmation method is incorrect, could you provide a way to confirm it? [~neilramaswamy] );;;, 13/Mar/24 17:48;neilramaswamy;[~dongjoon], I will create a minimal Spark repro and paste it in here shortly.

[~LuciferYang], I believe that you're looking at the `DBProvider`, which invokes the`RocksDBProvider`. Indeed, this is used for the external shuffle. However, Structured Streaming uses the RocksDB class, which [_always_ creates a logger invoked over the JNI|https://github.com/apache/spark/blob/b75325ccefa67b0c2daee317264808c67d76854f/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala#L132]. It's possible that the External shuffle service's use of RocksDB logging also suffers from a JDK 17 JNI attach/detach performance regression but I have never observed this in practice. If you are able to create a regression repro for the ESS, we should address it in a separate ticket.;;;
Affects Version/s.1: 3.3.1
Component/s.1: 
Comment.1: 12/Mar/24 22:00;dongjoon;Thank you for reporting, [~neilramaswamy]  Could you provide a reproducible Spark example for the further discussion?;;;
Comment.2: 13/Mar/24 07:40;LuciferYang;From the current Spark code, it appears that the {{Logger}} is only set for the {{RocksDB}} instance built for external shuffle db(inRocksDBProvider), and not for other parts. However, it seems that the Spark code does not actively print RocksDB-related logs (perhaps my confirmation method is incorrect, could you provide a way to confirm it? [~neilramaswamy] );;;
Comment.3: 13/Mar/24 17:48;neilramaswamy;[~dongjoon], I will create a minimal Spark repro and paste it in here shortly.

[~LuciferYang], I believe that you're looking at the `DBProvider`, which invokes the`RocksDBProvider`. Indeed, this is used for the external shuffle. However, Structured Streaming uses the RocksDB class, which [_always_ creates a logger invoked over the JNI|https://github.com/apache/spark/blob/b75325ccefa67b0c2daee317264808c67d76854f/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala#L132]. It's possible that the External shuffle service's use of RocksDB logging also suffers from a JDK 17 JNI attach/detach performance regression but I have never observed this in practice. If you are able to create a regression repro for the ESS, we should address it in a separate ticket.;;;
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.12.0, EMR-6.13.0, EMR-6.14.0, EMR-6.15.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0, Unknown

Summary: LATERAL regresses correlation name resolution
Issue key: SPARK-47308
Issue id: 13571051
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: srielau
Creator: srielau
Created: 06/Mar/24 19:53
Updated: 06/Mar/24 19:53
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.5.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: {code:java}
CREATE TABLE persons(name STRING);
INSERT INTO persons VALUES('Table: persons');
CREATE OR REPLACE TABLE women(name STRING);
INSERT INTO women VALUES('Table: women');

-- This works:
SELECT (SELECT max(folk.id) 
          FROM persons AS men(id),
       (SELECT name) AS folk(id))
  FROM women;
Table: women

-- This does not:
SELECT (SELECT max(folk.id) 
          FROM persons AS men(id), 
        LATERAL (SELECT name) AS folk(id))
  FROM women;
[UNRESOLVED_COLUMN.WITHOUT_SUGGESTION] A column, variable, or function parameter with name `name` cannot be resolved.  SQLSTATE: 42703;{code}

This is weird. LATERAL should be strictly additive to name resolution rules.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2024-03-06 19:53:39.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1nts0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): cloud_fan
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.5.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0

Summary: log4j race condition during shutdown
Issue key: SPARK-47220
Issue id: 13570249
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: holden
Reporter: holden
Creator: holden
Created: 29/Feb/24 00:46
Updated: 29/Feb/24 00:46
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: There is a race condition during shutdown which can result in a few different errors:
 * ERROR Attempted to append to non-started appender
 *  ERROR Unable to write to stream

Since I've only seen it during stop() triggered within a shutdown hook I believe this is caused by the parallel execution of shutdown hooks (see [https://stackoverflow.com/questions/17400136/how-to-log-within-shutdown-hooks-with-log4j2] )
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2024-02-29 00:46:37.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1nots:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Decimal multiply can produce the wrong answer because it rounds twice
Issue key: SPARK-40129
Issue id: 13477305
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: revans2
Creator: revans2
Created: 17/Aug/22 21:06
Updated: 23/Feb/24 00:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: This looks like it has been around for a long time, but I have reproduced it in 3.2.0+

The example here is multiplying Decimal(38, 10) by another Decimal(38, 10), but I think it can be reproduced with other number combinations, and possibly with divide too.
{code:java}
Seq("9173594185998001607642838421.5479932913").toDF.selectExpr("CAST(value as DECIMAL(38,10)) as a").selectExpr("a * CAST(-12 as DECIMAL(38,10))").show(truncate=false)
{code}
This produces an answer in Spark of {{-110083130231976019291714061058.575920}} But if I do the calculation in regular java BigDecimal I get {{-110083130231976019291714061058.575919}}
{code:java}
BigDecimal l = new BigDecimal("9173594185998001607642838421.5479932913");
BigDecimal r = new BigDecimal("-12.0000000000");
BigDecimal prod = l.multiply(r);
BigDecimal rounded_prod = prod.setScale(6, RoundingMode.HALF_UP);
{code}
Spark does essentially all of the same operations, but it used Decimal to do it instead of java's BigDecimal directly. Spark, by way of Decimal, will set a MathContext for the multiply operation that has a max precision of 38 and will do half up rounding. That means that the result of the multiply operation in Spark is {{{}-110083130231976019291714061058.57591950{}}}, but for the java BigDecimal code the result is {{{}-110083130231976019291714061058.57591949560000000000{}}}. Then in CheckOverflow for 3.2.0 and 3.3.0 or in just the regular Multiply expression in 3.4.0 the setScale is called (as a part of Decimal.setPrecision). At that point the already rounded number is rounded yet again resulting in what is arguably a wrong answer by Spark.

I have not fully tested this, but it looks like we could just remove the MathContext entirely in Decimal, or set it to UNLIMITED. All of the decimal operations appear to have their own overflow and rounding anyways. If we want to potentially reduce the total memory usage, we could also set the max precision to 39 and truncate (round down) the result in the math context instead.  That would then let us round the result correctly in setPrecision afterwards.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Nov 27 14:39:55 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17u9k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/May/23 16:13;fanjia;https://github.com/apache/spark/pull/41156;;;, 12/May/23 16:18;hudson;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41156;;;, 27/Nov/23 14:39;tgraves;this looks like a dup of https://issues.apache.org/jira/browse/SPARK-45786;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 12/May/23 16:18;hudson;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41156;;;
Comment.2: 27/Nov/23 14:39;tgraves;this looks like a dup of https://issues.apache.org/jira/browse/SPARK-45786;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Reuse CachedDNSToSwitchMapping for yarn  container requests
Issue key: SPARK-36964
Issue id: 13405742
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gaoyajun02
Creator: gaoyajun02
Created: 09/Oct/21 07:03
Updated: 15/Feb/24 10:07
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.0.3, 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Spark Core, YARN
Due Date: 
Votes: 1
Labels: pull-request-available
Description: Similar to SPARK-13704​, In some cases, YarnAllocator add container requests with locality preference can be expensive, it may call the topology script for rack awareness.

When submit a very large job in a very large Yarn cluster, the topology script may take signifiant time to run. And this blocks receiving YarnSchedulerBackend's RequestExecutors rpc calls, This request comes from spark dynamic executor allocation thread, which may blocks the ExecutorAllocationListener, and then result in executorManagement queue backlog.

 

Some logs:
{code:java}
21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76) at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:839) at org.apache.spark.ExecutorAllocationManager.addExecutors(ExecutorAllocationManager.scala:411) at org.apache.spark.ExecutorAllocationManager.updateAndSyncNumExecutorsTarget(ExecutorAllocationManager.scala:361) at org.apache.spark.ExecutorAllocationManager.org$apache$spark$ExecutorAllocationManager$$schedule(ExecutorAllocationManager.scala:316) at org.apache.spark.ExecutorAllocationManager$$anon$1.run(ExecutorAllocationManager.scala:227) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263) at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:294) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) ... 12 more21/09/29 12:04:35 WARN spark-dynamic-executor-allocation ExecutorAllocationManager: Unable to reach the cluster manager to request 1922 total executors!

21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76) at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:839) at org.apache.spark.ExecutorAllocationManager.addExecutors(ExecutorAllocationManager.scala:411) at org.apache.spark.ExecutorAllocationManager.updateAndSyncNumExecutorsTarget(ExecutorAllocationManager.scala:361) at org.apache.spark.ExecutorAllocationManager.org$apache$spark$ExecutorAllocationManager$$schedule(ExecutorAllocationManager.scala:316) at org.apache.spark.ExecutorAllocationManager$$anon$1.run(ExecutorAllocationManager.scala:227) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263) at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:294) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) ... 12 more21/09/29 12:04:35 WARN spark-dynamic-executor-allocation ExecutorAllocationManager: Unable to reach the cluster manager to request 1922 total executors!


21/09/29 12:02:49 ERROR dag-scheduler-event-loop AsyncEventQueue: Dropping event from queue executorManagement. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
21/09/29 12:02:49 WARN dag-scheduler-event-loop AsyncEventQueue: Dropped 1 events from executorManagement since the application started.
21/09/29 12:02:55 INFO spark-listener-group-eventLog AsyncEventQueue: Process of event SparkListenerExecutorAdded(1632888172920,543,org.apache.spark.scheduler.cluster.ExecutorData@8cfab8f5,None) by listener EventLoggingListener took 3.037686034s.
21/09/29 12:03:03 INFO spark-listener-group-eventLog AsyncEventQueue: Process of event SparkListenerBlockManagerAdded(1632888181779,BlockManagerId(1359, --, 57233, None),2704696934,Some(2704696934),Some(0)) by listener EventLoggingListener took 1.462598355s.
21/09/29 12:03:49 WARN dispatcher-BlockManagerMaster AsyncEventQueue: Dropped 74388 events from executorManagement since Wed Sep 29 12:02:49 CST 2021.
21/09/29 12:04:35 INFO spark-listener-group-executorManagement AsyncEventQueue: Process of event SparkListenerStageSubmitted(org.apache.spark.scheduler.StageInfo@52f810ad,{...}) by listener ExecutorAllocationListener took 116.526408932s.
21/09/29 12:04:49 WARN heartbeat-receiver-event-loop-thread AsyncEventQueue: Dropped 18892 events from executorManagement since Wed Sep 29 12:03:49 CST 2021.
21/09/29 12:05:49 WARN dispatcher-BlockManagerMaster AsyncEventQueue: Dropped 19397 events from executorManagement since Wed Sep 29 12:04:49 CST 2021.
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): YARN-9394
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Nov 10 21:38:07 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vpew:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Oct/21 07:43;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/34231;;;, 09/Oct/21 07:43;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/34231;;;, 10/Nov/22 21:38;geoff_langenderfer;environment:

{code:bash}

Release label:emr-6.8.0
Hadoop distribution:Amazon 3.2.1
Applications:Spark 3.3.0

{code}

here's another stacktrace:

 
{code:bash}

22/11/10 19:05:20 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
22/11/10 19:15:12 ERROR AsyncEventQueue: Dropping event from queue executorManagement. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
22/11/10 19:15:20 ERROR AsyncEventQueue: Dropping event from queue eventLog. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
22/11/10 19:16:20 ERROR TransportChannelHandler: Connection to /10.0.0.107:47700 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.rpc.io.connectionTimeout if this is wrong.
22/11/10 19:16:20 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /10.0.0.107:47700 is closed
22/11/10 19:16:20 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(Profile: id = 0, executor resources: cores -> name: cores, amount: 32, script: , vendor: ,memory -> name: memory, amount: 218880, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0 -> 6721),Map(0 -> 212218),Map(0 -> Map(* -> 212218)),Set()) to AM was unsuccessful
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /10.0.0.107:47700 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
    at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38) ~[scala-library-2.12.15.jar:?]
    at scala.util.Failure.recover(Try.scala:234) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Future.$anonfun$recover$1(Future.scala:395) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.complete(Promise.scala:53) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.complete$(Promise.scala:52) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82) ~[scala-library-2.12.15.jar:?]
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.tryFailure(Promise.scala:112) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.tryFailure$(Promise.scala:112) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187) ~[scala-library-2.12.15.jar:?]
    at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
    at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from /10.0.0.107:47700 in 120 seconds
    at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    ... 6 more
22/11/10 19:16:20 ERROR TransportClient: Failed to send RPC RPC 6522570887499332644 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
io.netty.channel.StacklessClosedChannelException: null
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
22/11/10 19:16:20 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(Profile: id = 0, executor resources: cores -> name: cores, amount: 32, script: , vendor: ,memory -> name: memory, amount: 218880, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0 -> 6634),Map(0 -> 212218),Map(0 -> Map(* -> 212218)),Set()) to AM was unsuccessful
java.io.IOException: Failed to send RPC RPC 6522570887499332644 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
    at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: io.netty.channel.StacklessClosedChannelException
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
22/11/10 19:16:20 ERROR TransportClient: Failed to send RPC RPC 4915998445035541588 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
io.netty.channel.StacklessClosedChannelException: null
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
22/11/10 19:16:20 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(Profile: id = 0, executor resources: cores -> name: cores, amount: 32, script: , vendor: ,memory -> name: memory, amount: 218880, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0 -> 6568),Map(0 -> 212218),Map(0 -> Map(* -> 212218)),Set()) to AM was unsuccessful
java.io.IOException: Failed to send RPC RPC 4915998445035541588 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
    at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: io.netty.channel.StacklessClosedChannelException

{code}
;;;
Affects Version/s.1: 3.1.2
Component/s.1: YARN
Comment.1: 09/Oct/21 07:43;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/34231;;;
Comment.2: 10/Nov/22 21:38;geoff_langenderfer;environment:

{code:bash}

Release label:emr-6.8.0
Hadoop distribution:Amazon 3.2.1
Applications:Spark 3.3.0

{code}

here's another stacktrace:

 
{code:bash}

22/11/10 19:05:20 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
22/11/10 19:15:12 ERROR AsyncEventQueue: Dropping event from queue executorManagement. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
22/11/10 19:15:20 ERROR AsyncEventQueue: Dropping event from queue eventLog. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
22/11/10 19:16:20 ERROR TransportChannelHandler: Connection to /10.0.0.107:47700 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.rpc.io.connectionTimeout if this is wrong.
22/11/10 19:16:20 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /10.0.0.107:47700 is closed
22/11/10 19:16:20 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(Profile: id = 0, executor resources: cores -> name: cores, amount: 32, script: , vendor: ,memory -> name: memory, amount: 218880, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0 -> 6721),Map(0 -> 212218),Map(0 -> Map(* -> 212218)),Set()) to AM was unsuccessful
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /10.0.0.107:47700 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
    at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38) ~[scala-library-2.12.15.jar:?]
    at scala.util.Failure.recover(Try.scala:234) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Future.$anonfun$recover$1(Future.scala:395) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.complete(Promise.scala:53) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.complete$(Promise.scala:52) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82) ~[scala-library-2.12.15.jar:?]
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.tryFailure(Promise.scala:112) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.tryFailure$(Promise.scala:112) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187) ~[scala-library-2.12.15.jar:?]
    at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
    at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from /10.0.0.107:47700 in 120 seconds
    at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    ... 6 more
22/11/10 19:16:20 ERROR TransportClient: Failed to send RPC RPC 6522570887499332644 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
io.netty.channel.StacklessClosedChannelException: null
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
22/11/10 19:16:20 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(Profile: id = 0, executor resources: cores -> name: cores, amount: 32, script: , vendor: ,memory -> name: memory, amount: 218880, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0 -> 6634),Map(0 -> 212218),Map(0 -> Map(* -> 212218)),Set()) to AM was unsuccessful
java.io.IOException: Failed to send RPC RPC 6522570887499332644 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
    at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: io.netty.channel.StacklessClosedChannelException
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
22/11/10 19:16:20 ERROR TransportClient: Failed to send RPC RPC 4915998445035541588 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
io.netty.channel.StacklessClosedChannelException: null
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
22/11/10 19:16:20 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(Profile: id = 0, executor resources: cores -> name: cores, amount: 32, script: , vendor: ,memory -> name: memory, amount: 218880, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0 -> 6568),Map(0 -> 212218),Map(0 -> Map(* -> 212218)),Set()) to AM was unsuccessful
java.io.IOException: Failed to send RPC RPC 4915998445035541588 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
    at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: io.netty.channel.StacklessClosedChannelException

{code}
;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: PYSPARK: Observation computes the wrong results when using `corr` function 
Issue key: SPARK-40549
Issue id: 13483067
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: canimus
Creator: canimus
Created: 25/Sep/22 12:10
Updated: 02/Feb/24 17:52
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: correctness
Description: Minimalistic description of the odd computation results.

When creating a new `Observation` object and computing a simple correlation function between 2 columns, the results appear to be non-deterministic.
{code:java}
# Init
from pyspark.sql import SparkSession, Observation
import pyspark.sql.functions as F

df = spark.createDataFrame([(float(i), float(i*10),) for i in range(10)], schema="id double, id2 double")

for i in range(10):
    o = Observation(f"test_{i}")
    df_o = df.observe(o, F.corr("id", "id2").eqNullSafe(1.0))
    df_o.count()
    print(o.get)

# Results
{'(corr(id, id2) <=> 1.0)': False}
{'(corr(id, id2) <=> 1.0)': False}
{'(corr(id, id2) <=> 1.0)': False}
{'(corr(id, id2) <=> 1.0)': True}
{'(corr(id, id2) <=> 1.0)': True}
{'(corr(id, id2) <=> 1.0)': True}
{'(corr(id, id2) <=> 1.0)': True}
{'(corr(id, id2) <=> 1.0)': True}
{'(corr(id, id2) <=> 1.0)': True}
{'(corr(id, id2) <=> 1.0)': False}{code}
 
Environment: {code:java}
// lsb_release -a
No LSB modules are available.
Distributor ID: Ubuntu
Description:    Ubuntu 22.04.1 LTS
Release:        22.04
Codename:       jammy {code}
{code:java}
 // python -V
python 3.10.4
{code}
{code:java}
 // lshw -class cpu
*-cpu                             
description: CPU        product: AMD Ryzen 9 3900X 12-Core Processor        vendor: Advanced Micro Devices [AMD]        physical id: f        bus info: cpu@0        version: 23.113.0        serial: Unknown        slot: AM4        size: 2194MHz        capacity: 4672MHz        width: 64 bits        clock: 100MHz        capabilities: lm fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp x86-64 constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es cpufreq        configuration: cores=12 enabledcores=12 microcode=141561875 threads=24
{code}
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python
Custom field (Last public comment date): Fri Feb 02 17:52:18 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18ths:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Feb/24 17:52;nchammas;I think this is just a consequence of floating point arithmetic being imprecise.
{code:python}
>>> for i in range(10):
...     o = Observation(f"test_{i}")
...     df_o = df.observe(o, F.corr("id", "id2"))
...     df_o.count()
...     print(o.get)
... 
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0000000000000002}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0000000000000002}
{'corr(id, id2)': 0.9999999999999999}
{'corr(id, id2)': 1.0} {code}
Unfortunately, {{corr}} seems to convert to float internally, so even if you give it decimals you will get a similar result:
{code:python}
>>> from decimal import Decimal
>>> import pyspark.sql.functions as F
>>> 
>>> df = spark.createDataFrame(
...     [(Decimal(i), Decimal(i * 10)) for i in range(10)],
...     schema="id decimal, id2 decimal",
... )for i in range(10):
    o = Observation(f"test_{i}")
    df_o = df.observe(o, F.corr("id", "id2"))
    df_o.count()
    print(o.get)
>>> 
>>> for i in range(10):
...     o = Observation(f"test_{i}")
...     df_o = df.observe(o, F.corr("id", "id2"))
...     df_o.count()
...     print(o.get)
... 
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 0.9999999999999999}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0000000000000002}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0}
{'corr(id, id2)': 1.0} {code}

I don't think there is anything that can be done here.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: StructType.toDDL does not pick up on non-nullability of column in nested struct
Issue key: SPARK-43341
Issue id: 13534700
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bramboog
Creator: bramboog
Created: 02/May/23 14:56
Updated: 18/Jan/24 14:28
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.3.1, 3.3.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: h2. The problem

When converting a StructType instance containing a nested StructType column which in turn contains a column for which {{nullable = false}} to a DDL string using {{{}.toDDL{}}}, the resulting DDL string does not include this non-nullability. For example:
{code:java}
val testschema = StructType(List(
  StructField("key", IntegerType, false),
  StructField("value", StringType, true),
  StructField("nestedCols", StructType(List(
    StructField("nestedKey", IntegerType, false),
    StructField("nestedValue", StringType, true)
  )), false)
))

println(testschema.toDDL)
println(StructType.fromDDL(testschema.toDDL)){code}
gives:
{code:java}
key INT NOT NULL,value STRING,nestedCols STRUCT<nestedKey: INT, nestedValue: STRING> NOT NULL

StructType(
  StructField(key,IntegerType,false),
  StructField(value,StringType,true),
  StructField(nestedCols,StructType(
    StructField(nestedKey,IntegerType,true),
    StructField(nestedValue,StringType,true)
  ),false)
){code}
 

This is due to the fact that {{StructType.toDDL}} calls {{StructField.toDDL}} for its fields, which in turn calls {{.sql}} for its {{{}dataType{}}}. If {{dataType}} is a {{{}StructType{}}}, the call to {{.sql}} in turn calls {{.sql}} for all the nested fields, and this last method does not include the nullability of the field in its output.
h2. Proposed solution

{{StructField.toDDL}} should call {{dataType.toDDL}} for a {{{}StructType{}}}, since this will include information about nullability of nested columns.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): Converting a StructType with nested non-nullable columns to a DDL string using .toDDL will now actually include the non-nullability in said DDL string.
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): scala
Custom field (Last public comment date): Wed May 03 09:11:57 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1hn2g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/May/23 09:11;githubbot;User 'BramBoog' has created a pull request for this issue:
https://github.com/apache/spark/pull/41016;;;
Affects Version/s.1: 3.3.1
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Executor obtained error information 
Issue key: SPARK-43221
Issue id: 13533426
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yorksity
Creator: yorksity
Created: 20/Apr/23 15:56
Updated: 17/Jan/24 00:19
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.1, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Block Manager
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Spark on Yarn Cluster

When multiple executors exist on a node, and the same block exists on both executors, with some in memory and some on disk.

Probabilistically, the executor failed to obtain the block,throw Exception:

java.lang.ArrayIndexOutofBoundsException: 0

    at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:183)

 

Next, I will replay the process of the problem occurring: 

step 1:

The executor requests the driver to obtain block information(locationsAndStatusOption). The input parameters are BlockId and the host of its own node. Please note that it does not carry port information

line:1092

!image-2023-04-21-00-24-22-059.png!

step 2:

On the driver side, the driver obtains all blockManagers holding the block based on the BlockId. For non remote shuffle scenarios, the driver will retrieve the first one with the blockId and blockManager from the locations

Assuming that there are two BlockManagers holding the BlockId on this node, BM-1 holds the Block and stores it in memory, and BM-2 holds the Block and stores it in disk

Assuming the returned status is of type memory and its disksize is 0

line: 852, 856

!image-2023-04-21-00-30-41-851.png!

step 3:

This method will return a BlockLocationsAndStatus object. If there are BMs using disk, the disk's path information will be stored in localDirs

!image-2023-04-21-00-50-10-918.png!

step 4:

When the executor obtains locationsAndStatusOption, localDirs is not empty, but status.diskSize is 0

line: 1102

!image-2023-04-21-00-54-11-968.png!

step 5:

The readDiskBlockFromSameHostExecutor only determines whether the Block file exists, and then directly uses the incoming blocksize to read the byte array. If the blocksize is 0, it returns an empty byte array

Only checked if the file exists

line: 1234, 1240

!image-2023-04-21-00-57-29-140.png!

Taking values from an empty array, causing an out of bounds problem
Environment: 
Original Estimate: 86400.0
Remaining Estimate: 86400.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 86400.0
Σ Remaining Estimate: 86400.0
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 20/Apr/23 16:20;yorksity;image-2023-04-21-00-19-58-021.png;https://issues.apache.org/jira/secure/attachment/13057452/image-2023-04-21-00-19-58-021.png, 20/Apr/23 16:24;yorksity;image-2023-04-21-00-24-22-059.png;https://issues.apache.org/jira/secure/attachment/13057453/image-2023-04-21-00-24-22-059.png, 20/Apr/23 16:30;yorksity;image-2023-04-21-00-30-41-851.png;https://issues.apache.org/jira/secure/attachment/13057454/image-2023-04-21-00-30-41-851.png, 20/Apr/23 16:50;yorksity;image-2023-04-21-00-50-10-918.png;https://issues.apache.org/jira/secure/attachment/13057455/image-2023-04-21-00-50-10-918.png, 20/Apr/23 16:53;yorksity;image-2023-04-21-00-53-20-720.png;https://issues.apache.org/jira/secure/attachment/13057456/image-2023-04-21-00-53-20-720.png, 20/Apr/23 16:54;yorksity;image-2023-04-21-00-54-11-968.png;https://issues.apache.org/jira/secure/attachment/13057457/image-2023-04-21-00-54-11-968.png, 20/Apr/23 16:57;yorksity;image-2023-04-21-00-57-29-140.png;https://issues.apache.org/jira/secure/attachment/13057458/image-2023-04-21-00-57-29-140.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 7.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-04-20 15:56:59.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1hf9c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Garbage collection doesn't include finalization run
Issue key: SPARK-44459
Issue id: 13543792
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: vmakarevich
Creator: vmakarevich
Created: 17/Jul/23 13:03
Updated: 30/Dec/23 00:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming, Web UI
Due Date: 
Votes: 0
Labels: gzip, memory-bug, memory-control, MemoryLeak, pull-request-available, structured-streaming
Description:  
{panel:title=Problem description}
Full text with figures available [here(4 min read)|https://medium.com/@vitaliy.makarevich.work/spark-structured-streaming-and-java-util-zip-and-finalize-method-83181c6bc86f] , I can post it here as well, but will post a shorter version.
When running a relatively big application(dozens of streams in parallel), Spark driver is growing in memory up to 110GBs(at this moment I was stopping the test). When I check the heapdump/JMX finalization queue size, I see it's struggling with accumulating java.lang.ref.Finalizer and underlying objects in them. Most of the objects in finalize queue are from java.util.zip package.
{panel}
 
{panel:title=Underlying Java implementation}
In a nutshell, there is a Java 8 finalizer method in Object. If it's not empty when the object is garbage collected, it's not removed once found unused but put in the Finalizer queue. Then JVM runs the Finalizer thread which takes each object from the queue and runs `finalize`. The problem is for big applications, the finalizer queue grows incomparably with finalization frequency/thread priority.
Very frequently, zip package instances are referring to C memory(since it's implemented in Native way), so even native memory is not being cleaned until `finalize` is called.
{panel}
 
{panel:title=Application}
As for the application I caught it - it runs 90 streaming queries in parallel with a batch frequency of about 1 hour. The application is reading data from [Apache Hudi|https://hudi.apache.org/] and writes output to another path in Apache Hudi(0.12.1 version). It's running on AWS EMR 6.9 on Java 8.
Spark UI/Event log is enabled with settings 
{code:java}
    "spark.ui.enabled" = "true"
    ## How many jobs the Spark UI and status APIs remember before garbage collecting.
    ## This is a target maximum, and fewer elements may be retained in some circumstances.
    ## Default value: 1000
    "spark.ui.retainedJobs" = "100"    ## How many stages the Spark UI and status APIs remember before garbage collecting.
    ## This is a target maximum, and fewer elements may be retained in some circumstances.
    ## Default value: 1000
    "spark.ui.retainedStages" = "50"    ## How many tasks in one stage the Spark UI and status APIs remember before garbage collecting.
    ## This is a target maximum, and fewer elements may be retained in some circumstances.
    ## Default value: 100000
    "spark.ui.retainedTasks" = "50"    ## How many DAG graph nodes the Spark UI and status APIs remember before garbage collecting.
    ## Default value: Int.MaxValue (2^31) - Here we use 2^15 instead.
    "spark.ui.dagGraph.retainedRootRDDs" = "32768"  "spark.worker.ui.retainedExecutors" = "10"
    "spark.worker.ui.retainedDrivers" = "10"
    "spark.sql.ui.retainedExecutions" = "10"
    "spark.streaming.ui.retainedBatches" = "10"    "spark.eventLog.enabled": "true"
    "spark.eventLog.rotation.enabled" : "true",
    "spark.eventLog.rotation.interval" : "3600",
    "spark.eventLog.rotation.minFileSize" : "1024m",
    "spark.eventLog.rotation.maxFilesToRetain" : "5" {code}
 

Spark UI is a crucial part since without it(with disabled), memory consumption is fine. I've played around with it, unfortunately even with very conservative settings it doesn't work well.
{panel}
 
{panel:title=What is the inner source of the issue}
I'm not sure about the source of the issue, but it looks like the Driver is heavily using zip package for small data. I assume it's coming from some networking where traffic is compressed(I saw some java.util.zip instances coming from Netty, but once they are GCed I could not track back the source since it's referenced only be FinalizerQueue).
{panel}
 
{panel:title=Proposed solution}
As a workaround, I've added a background service that runs `System.runFinalization()` with the same frequency as `spark.cleaner.periodicGC.interval` and it works well, memory consumption stays stable at an acceptable level(from indefinite growth to >100GB it stays at 60-70 GB total heap(~40GB used) which I consider ok for such intensive application).
So proposed solution is to add [here|https://github.com/apache/spark/blob/85d8d62216d3b830cc5af3dec05422a9cda4cea0/core/src/main/scala/org/apache/spark/ContextCleaner.scala#L131] a `System.runFinalization()` call. I don't think there is any drawback related to it(like reduced performance or so). But it may be added as a separate service like the current `System.gc` or under a feature flag for compatibility as well.
I'll be able to create a patchset for it once someone confirms it's acceptable.
{panel}
 

 

 
Environment: AWS EMR 6.9

Hudi 0.12.1
Spark 3.3.0
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-07-17 13:03:14.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1j6ts:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Web UI
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: literal function is giving wrong value for minimum datetime
Issue key: SPARK-46489
Issue id: 13562763
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ashutosh060
Creator: ashutosh060
Created: 22/Dec/23 18:56
Updated: 22/Dec/23 18:56
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: While using PySpark functions f.lit to read datetime.min value, it is giving wrong value. 

Below is the screenshot for the same,

!image-2023-12-22-10-56-30-473.png!

Expectation is to get same value as datetime.min in case of f.lit(datetime.min).

 
Environment: 
Original Estimate: 604800.0
Remaining Estimate: 604800.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 604800.0
Σ Remaining Estimate: 604800.0
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 22/Dec/23 18:56;ashutosh060;image-2023-12-22-10-56-30-473.png;https://issues.apache.org/jira/secure/attachment/13065567/image-2023-12-22-10-56-30-473.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python
Custom field (Last public comment date): 2023-12-22 18:56:17.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1mf4o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Replace all Scala `require` with `SparkException.require`
Issue key: SPARK-46458
Issue id: 13562353
Parent id: 13423097.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: hannahkamundson
Creator: hannahkamundson
Created: 19/Dec/23 17:18
Updated: 19/Dec/23 20:54
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Block Manager, Connect, Connect Contrib, Deploy, DStreams, Graph, GraphX, Input/Output, Kubernetes, Mesos, MLlib, Optimizer, Pandas API on Spark, Project Infra, PS
Due Date: 
Votes: 0
Labels: 
Description: Replace all the Scala precondition `require`s in the codebase with the `SparkException.require` in this PR https://github.com/apache/spark/pull/44336
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Dec 19 20:54:13 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1mcls:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Dec/23 17:19;hannahkamundson;I will work on this one as soon as the related PR is complete;;;, 19/Dec/23 20:54;hannahkamundson;[~maxgekk] I am adding this but wanted to make sure you are okay with this since you seem to be owning the process!;;;
Affects Version/s.1: 
Component/s.1: Connect, MLlib, Optimizer, Pandas API on Spark, Project Infra, PS
Comment.1: 19/Dec/23 20:54;hannahkamundson;[~maxgekk] I am adding this but wanted to make sure you are okay with this since you seem to be owning the process!;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Migrate onto error classes
Issue key: SPARK-37935
Issue id: 13423097
Parent id: 
Issue Type: Umbrella
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: maxgekk
Reporter: maxgekk
Creator: maxgekk
Created: 17/Jan/22 16:28
Updated: 19/Dec/23 20:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core, SQL
Due Date: 
Votes: 0
Labels: 
Description: The PR https://github.com/apache/spark/pull/32850 introduced error classes as a part of the error messages framework (https://issues.apache.org/jira/browse/SPARK-33539). Need to migrate all exceptions from QueryExecutionErrors, QueryCompilationErrors and QueryParsingErrors on the error classes using instances of SparkThrowable, and carefully test every error class by writing tests in dedicated test suites:
*  QueryExecutionErrorsSuite for the errors that are occurred during query execution
* QueryCompilationErrorsSuite ... query compilation or eagerly executing commands
* QueryParsingErrorsSuite ... parsing errors

Here is an example https://github.com/apache/spark/pull/35157 of how an existing Java exception can be replaced, and testing of related error classes.At the end, we should migrate all exceptions from the files Query.*Errors.scala and cover all error classes from the error-classes.json file by tests.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): SPARK-33539, SPARK-34920
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-44111, SPARK-41288
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): SPARK-38781
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 01 11:12:24 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ynqw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): maxgekk
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Oct/22 13:19;maxgekk;[~panbingkun] [~ivoson] [~xleesf] [~YActs] [~lvshaokang] [~kaifeiYi] [~LuciferYang] I have opened a few sub-tasks to improve the error framework. Please, feel free to take some of them if you are interested in this. also cc [~wenchen] [~itholic] For instance:
* Migrate type check failures on error classes. One of the tasks is https://issues.apache.org/jira/browse/SPARK-40369
* Use checkError() in test suites, see https://issues.apache.org/jira/browse/SPARK-40784;;;, 26/Oct/22 10:17;maxgekk;[~panbingkun] [~ivoson] [~xleesf] [~YActs] [~lvshaokang] [~kaifeiYi] [~LuciferYang] There are still some error classes that are not covered by tests. If you would like to write a test, please, open a sub-tasks in the umbrella JIRA and ping me. also cc [~itholic] [~wenchen] Here is the list:
- CANNOT_INFER_DATE
- + CONNECT.INTERCEPTOR_CTOR_MISSING, see SPARK-41004
- + CONNECT.INTERCEPTOR_RUNTIME_ERROR, see SPARK-41004
- + DATATYPE_MISMATCH.BINARY_ARRAY_DIFF_TYPES, see SPARK-41021
- + DATATYPE_MISMATCH.CANNOT_CONVERT_TO_JSON, see SPARK-41021
- + DATATYPE_MISMATCH.FRAME_LESS_OFFSET_WITHOUT_FOLDABLE, see SPARK-41021
- + DATATYPE_MISMATCH.MAP_FROM_ENTRIES_WRONG_TYPE, see SPARK-41021
- + DATATYPE_MISMATCH.NON_STRING_TYPE, see SPARK-41021
- + DATATYPE_MISMATCH.NULL_TYPE, see SPARK-41021
- + DATATYPE_MISMATCH.SPECIFIED_WINDOW_FRAME_DIFF_TYPES, see SPARK-41021
- + DATATYPE_MISMATCH.SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE, see SPARK-41021
- + DATATYPE_MISMATCH.SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE, see SPARK-41021
- + DATATYPE_MISMATCH.UNSPECIFIED_FRAME, see SPARK-41021
- + DEFAULT_DATABASE_NOT_EXISTS, see SPARK-41022
- + INDEX_ALREADY_EXISTS, see SPARK-41022
- + INDEX_NOT_FOUND, see SPARK-41022
- + ROUTINE_NOT_FOUND, see SPARK-41022
- TOO_MANY_ARRAY_ELEMENTS
- UNRESOLVED_FIELD.WITHOUT_SUGGESTION
- UNRESOLVED_MAP_KEY.WITHOUT_SUGGESTION
- UNSUPPORTED_FEATURE.DISTRIBUTE_BY
- UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.CORRELATED_COLUMN_NOT_FOUND
- UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.UNSUPPORTED_CORRELATED_REFERENCE_DATA_TYPE
- UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.UNSUPPORTED_CORRELATED_SCALAR_SUBQUERY
- UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.UNSUPPORTED_IN_EXISTS_SUBQUERY;;;, 29/May/23 16:27;srowen;There are 420 sub-tasks in this JIRA. Is this really necessary? it's a ton of noise when browsing open isuses. Why can't these be batched?;;;, 29/May/23 17:53;maxgekk;[~panbingkun] [~beliefer] Could you batch error classes when you open JIRAs when it is possible, please.;;;, 30/May/23 01:23;panbingkun;Okay，I will try to batch them as much as possible, but there may be a problem, which is that when there are many changes to a PR, it may make it a bit difficult for reviewers. [~srowen] [~maxgekk] ;;;, 30/May/23 07:55;maxgekk;>  it may make it a bit difficult for reviewers. 

[~panbingkun] I think 4-7 *related* error classes per PR is enough. Reviewers could ask to split your PR when it is difficult to review.;;;, 30/May/23 11:34;panbingkun;> 4-7 *related* error classes per PR is enough

Ok, Let's continue.;;;, 01/Jun/23 11:12;beliefer;[~maxgekk]I agree 3+ error classes per PR. I will create a single issue for these error classes.;;;
Affects Version/s.1: 
Component/s.1: SQL
Comment.1: 26/Oct/22 10:17;maxgekk;[~panbingkun] [~ivoson] [~xleesf] [~YActs] [~lvshaokang] [~kaifeiYi] [~LuciferYang] There are still some error classes that are not covered by tests. If you would like to write a test, please, open a sub-tasks in the umbrella JIRA and ping me. also cc [~itholic] [~wenchen] Here is the list:
- CANNOT_INFER_DATE
- + CONNECT.INTERCEPTOR_CTOR_MISSING, see SPARK-41004
- + CONNECT.INTERCEPTOR_RUNTIME_ERROR, see SPARK-41004
- + DATATYPE_MISMATCH.BINARY_ARRAY_DIFF_TYPES, see SPARK-41021
- + DATATYPE_MISMATCH.CANNOT_CONVERT_TO_JSON, see SPARK-41021
- + DATATYPE_MISMATCH.FRAME_LESS_OFFSET_WITHOUT_FOLDABLE, see SPARK-41021
- + DATATYPE_MISMATCH.MAP_FROM_ENTRIES_WRONG_TYPE, see SPARK-41021
- + DATATYPE_MISMATCH.NON_STRING_TYPE, see SPARK-41021
- + DATATYPE_MISMATCH.NULL_TYPE, see SPARK-41021
- + DATATYPE_MISMATCH.SPECIFIED_WINDOW_FRAME_DIFF_TYPES, see SPARK-41021
- + DATATYPE_MISMATCH.SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE, see SPARK-41021
- + DATATYPE_MISMATCH.SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE, see SPARK-41021
- + DATATYPE_MISMATCH.UNSPECIFIED_FRAME, see SPARK-41021
- + DEFAULT_DATABASE_NOT_EXISTS, see SPARK-41022
- + INDEX_ALREADY_EXISTS, see SPARK-41022
- + INDEX_NOT_FOUND, see SPARK-41022
- + ROUTINE_NOT_FOUND, see SPARK-41022
- TOO_MANY_ARRAY_ELEMENTS
- UNRESOLVED_FIELD.WITHOUT_SUGGESTION
- UNRESOLVED_MAP_KEY.WITHOUT_SUGGESTION
- UNSUPPORTED_FEATURE.DISTRIBUTE_BY
- UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.CORRELATED_COLUMN_NOT_FOUND
- UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.UNSUPPORTED_CORRELATED_REFERENCE_DATA_TYPE
- UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.UNSUPPORTED_CORRELATED_SCALAR_SUBQUERY
- UNSUPPORTED_SUBQUERY_EXPRESSION_CATEGORY.UNSUPPORTED_IN_EXISTS_SUBQUERY;;;
Comment.2: 29/May/23 16:27;srowen;There are 420 sub-tasks in this JIRA. Is this really necessary? it's a ton of noise when browsing open isuses. Why can't these be batched?;;;
Comment.3: 29/May/23 17:53;maxgekk;[~panbingkun] [~beliefer] Could you batch error classes when you open JIRAs when it is possible, please.;;;
Comment.4: 30/May/23 01:23;panbingkun;Okay，I will try to batch them as much as possible, but there may be a problem, which is that when there are many changes to a PR, it may make it a bit difficult for reviewers. [~srowen] [~maxgekk] ;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in org.apache.spark.launcher
Issue key: SPARK-38465
Issue id: 13432758
Parent id: 13423097.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bozhang
Creator: bozhang
Created: 09/Mar/22 05:18
Updated: 13/Dec/23 19:28
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Oct 01 12:47:31 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10b1c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Oct/22 12:47;khalidmammadov9@gmail.com;Hi [~bozhang] [~maxgekk], I would like to look into this if there are no objections?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Cached DataFrame keeps growing
Issue key: SPARK-44900
Issue id: 13548114
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: varun2807
Creator: varun2807
Created: 21/Aug/23 23:26
Updated: 10/Dec/23 11:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 1
Labels: 
Description: Scenario :

We have a kafka streaming application where the data lookups are happening by joining  another DF which is cached, and the caching strategy is MEMORY_AND_DISK.

However the size of the cached DataFrame keeps on growing for every micro batch the streaming application process and that's being visible under storage tab.

A similar stack overflow thread was already raised.

https://stackoverflow.com/questions/55601779/spark-dataframe-cache-keeps-growing
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Dec 10 11:00:39 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1jx4g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Aug/23 17:14;varun2807;[~yao] , is there a way I could prioritize this issue as it's causing us production impact ?;;;, 24/Aug/23 02:34;yao;How about releasing the cached rdds if you never touch it again;;;, 24/Aug/23 02:39;varun2807;[~yao]  Thanks for the comment. However we can't release as cached RDD's are being used in every micro batch that's why could not unpersist.;;;, 24/Aug/23 06:01;yao;Please note that there is a storage limit in place. Adding items without removing any may lead to cache evictions and recomputations. In such cases, caching may not be as effective as direct computation, as extra write paths are introduced. Probably, you should optimize your program.

 

 ;;;, 24/Aug/23 17:45;varun2807;We do not keep on adding , they are existing (RDD ID's also wont change) cached RDD's which keeps on growing, tested with spark3 as well, seeing same behavior and also changed the persisting strategy to just MEMORY instead of MEMORY_AND_DISK, but no luck.

 ;;;, 25/Aug/23 07:54;yao;Set spark.cleaner.periodicGC.interval to a smaller value(3min) might help;;;, 25/Aug/23 18:37;yaud;[~yao] we tried spark.cleaner.periodicGC.interval=1min but it didn't help. 

Here are my observations:
 * this happens even in a very simple scenario - see the example to reproduce below
 * it happens after join
 * uncontrollable growth of disk usage starts only when any portion of RDD got spilled to disk
 * if cached RDD remains 100% in memory this issue doesn't happen
 * when an executor dies then "Size on Disk" on Storage tab gets reduced by the amount of storage blocks held by that dead executor (makes sense)

It looks like some storage blocks (shuffle blocks?) are being tracked under that cached RDD and never (or at least not in a reasonable time) released until the executor dies.

Our worry is whether it is just disk size usage tracking bug or those blocks are actually kept on the disk because our production job disk usage (per Spark UI) grew by 6TB in a span of 10 hours.

Here's the code to reproduce:
{code:java}
val conf = new SparkConf().set("spark.master", "yarn")
val spark = SparkSession.builder().config(conf).getOrCreate()

import spark.implicits._

val sc = spark.sparkContext
val ssc = new StreamingContext(sc, Seconds(10))
// create a pseudo stream
val rddQueue = new mutable.Queue[RDD[Long]]()
val stream = ssc.queueStream(rddQueue, oneAtATime = true)
// create a simple lookup table
val lookup = sc.range(start = 0, end = 50000000, numSlices = 10)
    .toDF("id")
    .withColumn("value", md5(rand().cast(StringType)))
    .cache()
// for every micro-batch perform value lookup via join
stream.foreachRDD { rdd =>
  val df = rdd.toDF("id")
  df.join(lookup, Seq("id"), "leftouter").count()
}
// run the streaming
ssc.start()
for (_ <- 1 to 1000000) {
  rddQueue.synchronized {
    val firstId = Random.nextInt(50000000)
    rddQueue += sc.range(start = firstId, end = firstId + 10000, numSlices = 4)
  }
  Thread.sleep(10)
}
ssc.stop() {code}
Submit parameters (selected to create storage memory deficit and cause cache to be spilled):
{code:java}
--executor-cores 2 --num-executors 5 --executor-memory 1250m --driver-memory 1g \
--conf spark.dynamicAllocation.enabled=false --conf spark.sql.shuffle.partitions=10 {code}
When executed, disk usage of that cached lookup DF grows really fast.

Same thing happens in Spark 2.4 and 3.3;;;, 28/Aug/23 19:20;varun2807;[~yao] hope you got a chance to look into what [~yaud] mentioned.;;;, 29/Aug/23 04:57;yxzhang;Hi [~varun2807]  [~yaud]  did you check the actual cached file size on disk, on the yarn node manager local filesystem?  Is it really ever growing?;;;, 29/Aug/23 05:09;yaud;[~yxzhang] looks like it is just disk usage tracking issue as disk space is not used as much.

However it affects effectiveness of cached data since Spark spills it to disk as it believes it doesn't fit memory anymore so eventually it becomes 100% stored on disk.;;;, 31/Aug/23 03:20;varun2807;[~yxzhang] / [~yao] any update for us ?;;;, 04/Dec/23 14:21;wfanming;I have analyzed the program execution details in the logs and it seems that there is an issue with the 'org.apache.spark.status.AppStatusListener#updateRDDBlock' method.The method directly calculates the usage of {{rdd.memoryUsed}} and {{{}rdd.diskUsed{}}}, but it does not pay sufficient attention to the {{{}storageLevel{}}}.;;;, 08/Dec/23 17:44;dongjoon;Could you try this with Apache Spark 3.5.0, please?;;;, 10/Dec/23 11:00;wfanming;Of course, the following is my execution in the spark3.5.1 environment. I only extracted the relevant logs related to the production, storage, and reuse of rdd_1538 partition 1 under the Storage tab of the WEB UI.
{quote}23/12/10 18:05:23 INFO MemoryStore: Block rdd_1538_1 stored as values in memory (estimated size 176.7 MiB, free 310.2 MiB)
[rdd_1538_1]
23/12/10 18:05:24 INFO BlockManager: Found block rdd_1538_1 locally
23/12/10 18:05:27 INFO BlockManager: Dropping block rdd_1538_1 from memory
23/12/10 18:05:27 INFO BlockManager: Writing block rdd_1538_1 to disk
23/12/10 18:05:34 INFO MemoryStore: Block rdd_1538_1 stored as values in memory (estimated size 176.7 MiB, free 133.5 MiB)
23/12/10 18:05:34 INFO BlockManager: Found block rdd_1538_1 locally
23/12/10 18:05:40 INFO BlockManager: Found block rdd_1538_1 locally
23/12/10 18:05:42 INFO BlockManager: Dropping block rdd_1538_1 from memory
23/12/10 18:05:46 INFO MemoryStore: Block rdd_1538_1 stored as values in memory (estimated size 176.7 MiB, free 133.5 MiB)
{quote}
Through analysis of the above logs and combined with the UI display situation, the "Size on Disk" displayed on the Storage interface is incomprehensible.

If the partitions of this RDD are cached normally, shouldn't the size under "Size on Disk" label change?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 24/Aug/23 02:34;yao;How about releasing the cached rdds if you never touch it again;;;, 31/Aug/23 03:20;varun2807;[~yxzhang] / [~yao] any update for us ?;;;, 04/Dec/23 14:21;wfanming;I have analyzed the program execution details in the logs and it seems that there is an issue with the 'org.apache.spark.status.AppStatusListener#updateRDDBlock' method.The method directly calculates the usage of {{rdd.memoryUsed}} and {{{}rdd.diskUsed{}}}, but it does not pay sufficient attention to the {{{}storageLevel{}}}.;;;, 08/Dec/23 17:44;dongjoon;Could you try this with Apache Spark 3.5.0, please?;;;, 10/Dec/23 11:00;wfanming;Of course, the following is my execution in the spark3.5.1 environment. I only extracted the relevant logs related to the production, storage, and reuse of rdd_1538 partition 1 under the Storage tab of the WEB UI.
{quote}23/12/10 18:05:23 INFO MemoryStore: Block rdd_1538_1 stored as values in memory (estimated size 176.7 MiB, free 310.2 MiB)
[rdd_1538_1]
23/12/10 18:05:24 INFO BlockManager: Found block rdd_1538_1 locally
23/12/10 18:05:27 INFO BlockManager: Dropping block rdd_1538_1 from memory
23/12/10 18:05:27 INFO BlockManager: Writing block rdd_1538_1 to disk
23/12/10 18:05:34 INFO MemoryStore: Block rdd_1538_1 stored as values in memory (estimated size 176.7 MiB, free 133.5 MiB)
23/12/10 18:05:34 INFO BlockManager: Found block rdd_1538_1 locally
23/12/10 18:05:40 INFO BlockManager: Found block rdd_1538_1 locally
23/12/10 18:05:42 INFO BlockManager: Dropping block rdd_1538_1 from memory
23/12/10 18:05:46 INFO MemoryStore: Block rdd_1538_1 stored as values in memory (estimated size 176.7 MiB, free 133.5 MiB)
{quote}
Through analysis of the above logs and combined with the UI display situation, the "Size on Disk" displayed on the Storage interface is incomprehensible.

If the partitions of this RDD are cached normally, shouldn't the size under "Size on Disk" label change?;;;
Comment.2: 24/Aug/23 02:39;varun2807;[~yao]  Thanks for the comment. However we can't release as cached RDD's are being used in every micro batch that's why could not unpersist.;;;
Comment.3: 24/Aug/23 06:01;yao;Please note that there is a storage limit in place. Adding items without removing any may lead to cache evictions and recomputations. In such cases, caching may not be as effective as direct computation, as extra write paths are introduced. Probably, you should optimize your program.

 

 ;;;
Comment.4: 24/Aug/23 17:45;varun2807;We do not keep on adding , they are existing (RDD ID's also wont change) cached RDD's which keeps on growing, tested with spark3 as well, seeing same behavior and also changed the persisting strategy to just MEMORY instead of MEMORY_AND_DISK, but no luck.

 ;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: decouple amplab jenkins from spark website, builds and tests
Issue key: SPARK-37571
Issue id: 13415931
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: shaneknapp
Reporter: shaneknapp
Creator: shaneknapp
Created: 08/Dec/21 01:15
Updated: 05/Dec/23 20:15
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 23/Dec/21 00:00
Votes: 0
Labels: 
Description: we will be turning off jenkins on dec 23rd, and we need to decouple the build infra from jenkins, as well as remove any amplab jenkins-specific docs on the website, scripts and infra setup.

i'll be creating > 1 PRs for this.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 08/Dec/21 01:41;shaneknapp;audit.txt;https://issues.apache.org/jira/secure/attachment/13037103/audit.txt, 08/Dec/21 01:25;shaneknapp;spark-repo-to-be-audited.txt;https://issues.apache.org/jira/secure/attachment/13037102/spark-repo-to-be-audited.txt
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Dec 05 20:15:34 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xg2w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Dec/21 01:42;shaneknapp;this is gonna take a while...  nearly a decade later, jenkins' reach is pretty deep:  [^audit.txt];;;, 05/Dec/23 16:17;nchammas;Since we've [retired|https://lists.apache.org/thread/5n59fs22rtytflbz4sz1pz32ozzfbkrx] the venerable Jenkins infrastructure, I suppose we can close this issue.;;;, 05/Dec/23 20:15;shaneknapp;a blast from the past!  XD

sounds good to me...  there are still some vestiges of amplab and jenkins in some bits of the repo but nothing really to write home about:

[https://github.com/search?q=repo%3Aapache%2Fspark+amplab&type=code]

[https://github.com/search?q=repo%3Aapache%2Fspark+jenkins&type=code]

the 'jenkins' entries *mostly* look to be setting bits for tests' setup and my quick perusal didn't raise any flags.  it might be a decent idea to audit this stuff at some point and pull it out.  :shrug:;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 05/Dec/23 16:17;nchammas;Since we've [retired|https://lists.apache.org/thread/5n59fs22rtytflbz4sz1pz32ozzfbkrx] the venerable Jenkins infrastructure, I suppose we can close this issue.;;;
Comment.2: 05/Dec/23 20:15;shaneknapp;a blast from the past!  XD

sounds good to me...  there are still some vestiges of amplab and jenkins in some bits of the repo but nothing really to write home about:

[https://github.com/search?q=repo%3Aapache%2Fspark+amplab&type=code]

[https://github.com/search?q=repo%3Aapache%2Fspark+jenkins&type=code]

the 'jenkins' entries *mostly* look to be setting bits for tests' setup and my quick perusal didn't raise any flags.  it might be a decent idea to audit this stuff at some point and pull it out.  :shrug:;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: inject an early scan pushdown rule
Issue key: SPARK-37518
Issue id: 13415012
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: beliefer
Creator: beliefer
Created: 02/Dec/21 10:24
Updated: 04/Dec/23 06:46
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Currently, Spark supports push down filters, aggregates and limit. All the job is completed by V2ScanRelationPushDown.
But, V2ScanRelationPushDown have a lot limit.
Users want apply custom rule for push down after V2ScanRelationPushDown failed.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Dec 02 11:08:11 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xafc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Dec/21 11:08;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/34779;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: first operator should respect the nullability of child expression as well as ignoreNulls option
Issue key: SPARK-44517
Issue id: 13544575
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: codingcat
Creator: codingcat
Created: 24/Jul/23 02:14
Updated: 04/Nov/23 00:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: I found the following problem when using Spark recently:

 
{code:java}
// code placeholder
import spark.implicits._

val s = Seq((1.2, "s", 2.2)).toDF("v1", "v2", "v3")

val schema = StructType(Seq(StructField("v1", DoubleType, nullable = false),StructField("v2", StringType, nullable = true),StructField("v3", DoubleType, nullable = false)))

val df = spark.createDataFrame(s.rdd, schema)val inputDF = 

val inputDF = df.dropDuplicates("v3")

spark.sql("CREATE TABLE local.db.table (\n v1 DOUBLE NOT NULL,\n v2 STRING, v3 DOUBLE NOT NULL)")

inputDF.write.mode("overwrite").format("iceberg").save("local.db.table") {code}
 

 

when I use the above code to write to iceberg (i guess Delta Lake will have the same problem) , I got very confusing exception


{code:java}
Exception in thread "main" java.lang.IllegalArgumentException: Cannot write incompatible dataset to table with schema:

table 

{  1: v1: required double  2: v2: optional string  3: v3: required double}

Provided schema:

table {  1: v1: optional double  2: v2: optional string  3: v3: required double} {code}

basically it complains that we have v1 as the nullable column in our `inputDF` above which is not allowed since we created table with the v1 as not nullable. The confusion comes from that,  if we check the schema with printSchema() of inputDF, v1 is not nullable
{noformat}
root 
|-- v1: double (nullable = false) 
|-- v2: string (nullable = true) 
|-- v3: double (nullable = false){noformat}
Clearly, something changed the v1's nullability unexpectedly!

 

After some debugging I found that the key is that dropDuplicates("v3"). In optimization phase, we have ReplaceDeduplicateWithAggregate to replace the Deduplicate with aggregate on v3 and run first() over all other columns. However, first() operator has hard coded nullable as always "true" which is the source of changed nullability of v1

 

this is a very confusing behavior of Spark, and probably no one really noticed as we do not care too much without the new table formats like delta lake and iceberg which can make nullability check correctly. Nowadays, we users adopt them more and more, this is surfaced up

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Aug 05 16:51:43 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1jbns:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Aug/23 16:51;hiveqa;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/42117;;;
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.12.0, EMR-6.13.0, EMR-6.14.0, EMR-6.15.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Support to extract partial filters of datasource v2 table and push them down
Issue key: SPARK-44419
Issue id: 13543501
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 14/Jul/23 05:57
Updated: 03/Nov/23 00:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description:  

Run the following sql, and the date predicate in the where clause is not pushed down and it would cause a full table scan.

 
{code:java}
SELECT
id,
data,
date
FROM
testcat.db.table
where
(date = 20221110 and udfStrLen(data) = 8)
or
(date = 20221111 and udfStrLen(data) = 8)  {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 14 09:32:06 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1j514:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Jul/23 09:32;hudson;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/42000;;;
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: optimize adaptive skew join for ExistenceJoin
Issue key: SPARK-44426
Issue id: 13543538
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 14/Jul/23 10:24
Updated: 03/Nov/23 00:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: For this query,  InSubQuery would be cast to ExistenceJoin and now ExistenceJoin does not support automatic data skew for the left table.
{code:java}
SELECT * FROM skewData1
where
(key1 in (select key2 from skewData2)
or value1 in (select value2 from skewData2){code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-07-14 10:24:16.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1j59c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Fixed matching check for CharType/VarcharType
Issue key: SPARK-44414
Issue id: 13543489
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 14/Jul/23 02:22
Updated: 28/Oct/23 00:16
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Running the following code throws an exception
{code:java}
val analyzer = getAnalyzer
// check varchar type
val json1 = "{\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(80)\"}"
val metadata1 = new MetadataBuilder().withMetadata(Metadata.fromJson(json1)).build()

val query1 = TestRelation(StructType(Seq(
StructField("x", StringType, metadata = metadata1),
StructField("y", StringType, metadata = metadata1))).toAttributes)

val table1 = TestRelation(StructType(Seq(
StructField("x", StringType, metadata = metadata1),
StructField("y", StringType, metadata = metadata1))).toAttributes)

val parsedPlanByName1 = byName(table1, query1)
analyzer.executeAndCheck(parsedPlanByName1, new QueryPlanningTracker()) {code}
 

Exception details are as follows
{code:java}
org.apache.spark.sql.AnalysisException: unresolved operator 'AppendData TestRelation [x#8, y#9], true;
'AppendData TestRelation [x#8, y#9], true
+- TestRelation [x#6, y#7]    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:52)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:51)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:156)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$47(CheckAnalysis.scala:704)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$47$adapted(CheckAnalysis.scala:702)
    at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:186)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:702)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:92)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:156)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:177)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:174)
    at org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite.$anonfun$new$36(DataSourceV2AnalysisSuite.scala:691) {code}
 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-07-14 02:22:16.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1j4yg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: DataSourceV2 cannot report KeyGroupedPartitioning with multiple keys per partition
Issue key: SPARK-42716
Issue id: 13527620
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: enricomi
Creator: enricomi
Created: 08/Mar/23 11:14
Updated: 10/Oct/23 00:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: From Spark 3.0.0 until 3.2.3, a DataSourceV2 could report its partitioning as {{KeyGroupedPartitioning}} via {{SupportsReportPartitioning}}, even if multiple keys belong to a partition.

With SPARK-37377, only if all partitions implement {{HasPartitionKey}}, the partition information reported through {{SupportsReportPartitioning}} is considered by catalyst. But this limits the number of keys per partition to 1.

Spark should continue to support the more general situation of {{KeyGroupedPartitioning}} with multiple keys per partition, like {{HashPartitioning}}.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 08 11:23:06 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1gfi8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Mar/23 11:22;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40334;;;, 08/Mar/23 11:23;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40334;;;
Affects Version/s.1: 3.3.1
Component/s.1: 
Comment.1: 08/Mar/23 11:23;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40334;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.12.0, EMR-6.13.0, EMR-6.14.0, EMR-6.15.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: approx_percentile produces wrong results for large decimals.
Issue key: SPARK-42775
Issue id: 13528301
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mashplant
Creator: mashplant
Created: 13/Mar/23 20:53
Updated: 08/Oct/23 00:19
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 2.1.0, 2.2.0, 2.3.0, 2.4.0, 3.0.0, 3.1.0, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: In the {{approx_percentile}} expression, Spark casts decimal to double to update the aggregation state ([ApproximatePercentile.scala#L181|https://github.com/apache/spark/blob/933dc0c42f0caf74aaa077fd4f2c2e7208452b9b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproximatePercentile.scala#L181]) and casts the result double back to decimal ([ApproximatePercentile.scala#L206|https://github.com/apache/spark/blob/933dc0c42f0caf74aaa077fd4f2c2e7208452b9b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/ApproximatePercentile.scala#L206]). The precision loss in the casts can make the result decimal out of its precision range. This can lead to the following counter-intuitive results:
{code:sql}
spark-sql> select approx_percentile(col, 0.5) from values (9999999999999999999) as tab(col);
NULL
spark-sql> select approx_percentile(col, 0.5) is null from values (9999999999999999999) as tab(col);
false
spark-sql> select cast(approx_percentile(col, 0.5) as string) from values (9999999999999999999) as tab(col);
10000000000000000000
spark-sql> desc select approx_percentile(col, 0.5) from values (9999999999999999999) as tab(col);
approx_percentile(col, 0.5, 10000)	decimal(19,0) 
{code}
The result is actually not null, so the second query returns false. The first query returns null because the result cannot fit into {{{}decimal(19, 0){}}}.

A suggested fix is to use {{Decimal.changePrecision}} here to ensure the result fits, and really returns a null or throws an exception when the result doesn't fit.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 15 01:21:44 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1gjpk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Mar/23 01:21;apachespark;User 'chenhao-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/40429;;;
Affects Version/s.1: 2.2.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Decimal precision exceeds max precision error when using unary minus on min Decimal values on Scala 2.13 Spark
Issue key: SPARK-45438
Issue id: 13553173
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: navkumar
Creator: navkumar
Created: 06/Oct/23 17:46
Updated: 06/Oct/23 17:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.4.0, 3.4.1, 3.5.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: scala
Description: When submitting an application to Spark built with Scala 2.13, there are issues with Decimal overflow that show up when using unary minus (and also {{abs()}} which uses unary minus under the hood.

Here is an example PySpark reproduce use case:

{code}
from decimal import Decimal

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, DecimalType

spark = SparkSession.builder \
      .master("local[*]") \
      .appName("decimal_precision") \
      .config("spark.rapids.sql.explain", "ALL") \
      .config("spark.sql.ansi.enabled", "true") \
      .config("spark.sql.legacy.allowNegativeScaleOfDecimal", 'true') \
      .getOrCreate()  

precision = 38
scale = 0
DECIMAL_MIN = Decimal('-' + ('9' * precision) + 'e' + str(-scale))

data = [[DECIMAL_MIN]]

schema = StructType([
    StructField("a", DecimalType(precision, scale), True)])
df = spark.createDataFrame(data=data, schema=schema)

df.selectExpr("a", "-a").show()
{code}

This particular example will run successfully on Spark built with Scala 2.12, but throw a java.math.ArithmeticException on Spark built with Scala 2.13. 

If you change the value of {{DECIMAL_MIN}} in the previous code to something just ahead of the original DECIMAL_MIN, you will not get an exception thrown, but instead you will get an incorrect answer (possibly due to overflow):

{code}
...
DECIMAL_MIN = Decimal('-8' + ('9' * (precision-1)) + 'e' + str(-scale))
...
{code} 

Output:
{code}
+--------------------+--------------------+
|                   a|               (- a)|
+--------------------+--------------------+
|-8999999999999999...|90000000000000000...|
+--------------------+--------------------+
{code}

It looks like the code in {{Decimal.scala}} uses {{scala.math.BigDecimal}}. See https://github.com/scala/bug/issues/11590 with updates on how Scala 2.13 handles BigDecimal. It looks like there is {{java.math.MathContext}} missing when performing these operations. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-10-06 17:46:16.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ksb4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.1, 3.4.1, 3.5.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.12.0, EMR-6.13.0, EMR-6.14.0, EMR-6.15.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0, Unknown

Summary: [SQL] Spark JDBC Savemode Supports Upsert
Issue key: SPARK-38200
Issue id: 13428260
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: melin
Creator: melin
Created: 14/Feb/22 08:02
Updated: 04/Oct/23 16:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: upsert sql for different databases， Most databases support merge sql：

sqlserver merge into sql : [https://github.com/apache/incubator-seatunnel/blob/dev/seatunnel-connectors-v2/connector-jdbc/src/main/java/org/apache/seatunnel/connectors/seatunnel/jdbc/internal/dialect/sqlserver/SqlServerDialect.java]

mysql: [https://github.com/apache/incubator-seatunnel/blob/dev/seatunnel-connectors-v2/connector-jdbc/src/main/java/org/apache/seatunnel/connectors/seatunnel/jdbc/internal/dialect/mysql/MysqlDialect.java]

oracle merge into sql : [https://github.com/apache/incubator-seatunnel/blob/dev/seatunnel-connectors-v2/connector-jdbc/src/main/java/org/apache/seatunnel/connectors/seatunnel/jdbc/internal/dialect/oracle/OracleDialect.java]

postgres: [https://github.com/apache/incubator-seatunnel/blob/dev/seatunnel-connectors-v2/connector-jdbc/src/main/java/org/apache/seatunnel/connectors/seatunnel/jdbc/internal/dialect/psql/PostgresDialect.java]

postgres merg into sql : [https://www.postgresql.org/docs/current/sql-merge.html]

db2 merge into sql : [https://www.ibm.com/docs/en/db2-for-zos/12?topic=statements-merge]

derby merge into sql: [https://db.apache.org/derby/docs/10.14/ref/rrefsqljmerge.html]

he merg into sql : [https://www.tutorialspoint.com/h2_database/h2_database_merge.htm]

 

[~yao] 

 

https://github.com/melin/datatunnel/tree/master/plugins/jdbc/src/main/scala/com/superior/datatunnel/plugin/jdbc/support/dialect

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 04 16:48:26 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zjfk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Feb/22 08:10;melin;[~beliefer] ;;;, 17/Feb/22 02:37;beliefer;[~melin] Thank you for the ping. Does the syntax is an ANSI standard?;;;, 17/Feb/22 04:50;melin;[~beliefer]  Upsert SQL is not ANSI standard, Mysql and PG are very widely used, and many data integration systems support UPSERT. There is a strong need to synchronize data in real business. I hope it can be implemented for mysql and PG.;;;, 21/Feb/22 03:29;beliefer;[~melin] OK. Does other SQL could finish the same work as Upsert SQL?;;;, 21/Feb/22 05:14;melin;[~beliefer]  

oracle: [https://docs.oracle.com/en/database/other-databases/nosql-database/21.1/sqlfornosql/adding-table-rows-using-insert-and-upsert-statements.html]

db2 or sqlserver
{code:java}
MERGE INTO mytable AS mt USING (
    SELECT * FROM TABLE (
        VALUES 
            (123, 'text')
    )
) AS vt(id, val) ON (mt.id = vt.id)
WHEN MATCHED THEN
    UPDATE SET val = vt.val
WHEN NOT MATCHED THEN
    INSERT (id, val) VALUES (vt.id, vt.val)
; {code};;;, 01/Mar/22 09:57;beliefer;[~melin] It seems not consistent.;;;, 03/Aug/22 02:27;melin;Yes, not standard syntax, can it be supported?;;;, 07/Apr/23 05:10;melin;[~beliefer] 

MERGE INTO is a standard sql: https://en.wikipedia.org/wiki/Merge_%28SQL%29， 

mysql doesn't implement it, most databases do;;;, 12/Jun/23 08:51;enricomi;Sadly, MERGE is shown to perform worse than UPDATE+INSERT in some databases:
http://www.dba-oracle.com/t_merge_upsert_performance.htm
https://michalmolka.medium.com/sql-server-merge-vs-upsert-877702d23674;;;, 12/Jun/23 08:52;enricomi;Related: SPARK-19335;;;, 15/Jun/23 09:10;enricomi;Created pull request for this: https://github.com/apache/spark/pull/41611;;;, 19/Sep/23 14:25;yairo1987;[~EnricoMi] any news on when this important feature going to be merged?;;;, 19/Sep/23 17:00;enricomi;Sadly, still no feedback from reviewers.;;;, 04/Oct/23 16:48;hudson;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/41518;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 17/Feb/22 02:37;beliefer;[~melin] Thank you for the ping. Does the syntax is an ANSI standard?;;;, 15/Jun/23 09:10;enricomi;Created pull request for this: https://github.com/apache/spark/pull/41611;;;, 19/Sep/23 14:25;yairo1987;[~EnricoMi] any news on when this important feature going to be merged?;;;, 19/Sep/23 17:00;enricomi;Sadly, still no feedback from reviewers.;;;, 04/Oct/23 16:48;hudson;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/41518;;;
Comment.2: 17/Feb/22 04:50;melin;[~beliefer]  Upsert SQL is not ANSI standard, Mysql and PG are very widely used, and many data integration systems support UPSERT. There is a strong need to synchronize data in real business. I hope it can be implemented for mysql and PG.;;;
Comment.3: 21/Feb/22 03:29;beliefer;[~melin] OK. Does other SQL could finish the same work as Upsert SQL?;;;
Comment.4: 21/Feb/22 05:14;melin;[~beliefer]  

oracle: [https://docs.oracle.com/en/database/other-databases/nosql-database/21.1/sqlfornosql/adding-table-rows-using-insert-and-upsert-statements.html]

db2 or sqlserver
{code:java}
MERGE INTO mytable AS mt USING (
    SELECT * FROM TABLE (
        VALUES 
            (123, 'text')
    )
) AS vt(id, val) ON (mt.id = vt.id)
WHEN MATCHED THEN
    UPDATE SET val = vt.val
WHEN NOT MATCHED THEN
    INSERT (id, val) VALUES (vt.id, vt.val)
; {code};;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Consolidate whole stage and non-whole stage subexpression elimination
Issue key: SPARK-37467
Issue id: 13413922
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kimahriman
Creator: kimahriman
Created: 26/Nov/21 00:49
Updated: 03/Oct/23 12:56
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Currently there are separate subexpression elimination paths for whole stage and non-whole stage codegen. Consolidating these into a single code path would make it simpler to add further enhancements, such as supporting lambda functions  (https://issues.apache.org/jira/browse/SPARK-37466).
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Nov 27 15:28:40 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0x3pk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Nov/21 15:27;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/34727;;;, 27/Nov/21 15:28;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/34727;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 27/Nov/21 15:28;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/34727;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Why isCheckpointed returns false while as checkpointing is eager by default?
Issue key: SPARK-45249
Issue id: 13551508
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kondziolka9ld
Creator: kondziolka9ld
Created: 21/Sep/23 09:28
Updated: 21/Sep/23 09:33
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Hi!

I consider why `isCheckpointed` method return `false` - could someone explain it? I would expect `true`.
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/
         
Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.13)
Type in expressions to have them evaluated.
Type :help for more information.
scala> sc.setCheckpointDir("file:///tmp/")
scala> val df1 = Seq(1,2,3,4).toDF
df1: org.apache.spark.sql.DataFrame = [value: int]
scala> val df2 = df1.checkpoint()
df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: int]
scala> df2.rdd.isCheckpointed
res2: Boolean = false  // why false?{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-09-21 09:28:29.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ki28:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: RADIX sort is not stable and can produce different results for first/collect_list aggs
Issue key: SPARK-45243
Issue id: 13551449
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: revans2
Creator: revans2
Created: 20/Sep/23 17:53
Updated: 20/Sep/23 17:53
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I just set the version as 3.3.0. I think it is all versions, but I have not tested it to be sure.

I simple query like
{code:java}
spark.read.parquet("/tmp/TEST").groupBy("a").agg(first(col("b"))).show()
{code}
can produce different results depending on if RADIX sort is enabled or not. In this case "a" is a LONG and "b" is a STRING. The STRING forces the aggregation to be a sort based aggregation and b being a long causes the sort to default to a RADIX sort.

In practice first, last, and collect_list are not deterministic because the order that a shuffle arrives to a task is a race so the order that the rows arrive after the second stage of the aggregation is totally up in the air. first and last might just be called pick_one. But in this case the data is small enough that there is a single partition so it should be deterministic. But it is not.
{code:java}
scala> spark.read.parquet("/tmp/TEST").show(100, false)
+--------------------+----+
|a                   |b   |
+--------------------+----+
|4080731634774120135 |HH  |
|-7996385019137306797|AA  |
|4891386765580059730 |BI  |
|-2578026341565473682|DE  |
|-7264635988756013877|CH  |
|5656737394922367923 |AG  |
|-6183011807271780569|BG  |
|109827782918242415  |CD  |
|-4058328039203991995|FA  |
|null                |FG  |
|4080731634774120135 |ID  |
|-7996385019137306797|GG  |
|4891386765580059730 |AC  |
|-2578026341565473682|null|
|-7264635988756013877|HF  |
|5656737394922367923 |II  |
|-6183011807271780569|FC  |
|109827782918242415  |DI  |
|-4058328039203991995|IH  |
|null                |FE  |
|4080731634774120135 |HA  |
|-7996385019137306797|ID  |
|4891386765580059730 |GI  |
|-2578026341565473682|GB  |
|-7264635988756013877|EC  |
|5656737394922367923 |DA  |
|-6183011807271780569|BB  |
|109827782918242415  |AE  |
|-4058328039203991995|FE  |
|null                |AE  |
|4080731634774120135 |BC  |
|-7996385019137306797|HF  |
+--------------------+----+

scala> spark.read.parquet("/tmp/TEST").groupBy("a").agg(first(col("b"))).show()
+--------------------+--------+
|                   a|first(b)|
+--------------------+--------+
|                null|      FG|
|-7996385019137306797|      GG|
|-7264635988756013877|      CH|
|-6183011807271780569|      BG|
|-4058328039203991995|      FA|
|-2578026341565473682|      DE|
|  109827782918242415|      CD|
| 4080731634774120135|      HH|
| 4891386765580059730|      AC|
| 5656737394922367923|      AG|
+--------------------+--------+

scala> spark.conf.set("spark.sql.sort.enableRadixSort", false)

scala> spark.read.parquet("/tmp/TEST").groupBy("a").agg(first(col("b"))).show()
+--------------------+--------+
|                   a|first(b)|
+--------------------+--------+
|                null|      FG|
|-7996385019137306797|      AA|
|-7264635988756013877|      CH|
|-6183011807271780569|      BG|
|-4058328039203991995|      FA|
|-2578026341565473682|      DE|
|  109827782918242415|      CD|
| 4080731634774120135|      HH|
| 4891386765580059730|      BI|
| 5656737394922367923|      AG|
+--------------------+--------+
{code}
Here the values for -7996385019137306797 changed from GG with radix sort on to AA with it off.  AA is technially correct, because it appears on line 2 of the input where as GG shows up on line 12. 

 

I honestly don't know if Spark expects the sort to be stable or not. Looking at the code SortExec and UnsafeExternalSorter do not make any claims about being stable and https://issues.apache.org/jira/browse/SPARK-23973 indicates that sort is not stable, so this might just works as designed.  I just find it odd that in most cases it is stable, so I guess that was just by accident.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-09-20 17:53:20.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1khp4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Better Data Source V2 operator pushdown framework
Issue key: SPARK-38852
Issue id: 13438855
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: beliefer
Creator: beliefer
Created: 11/Apr/22 06:43
Updated: 15/Sep/23 17:19
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, Spark supports push down Filters and Aggregates to data source.
However, the Data Source V2 operator pushdown framework has the following shortcomings:

# Only simple filter and aggregate are supported, which makes it impossible to apply in most scenarios
# The incompatibility of SQL syntax makes it impossible to apply in most scenarios
# Aggregate push down does not support multiple partitions of data sources
# Spark's additional aggregate will cause some overhead
# Limit push down is not supported
# Top n push down is not supported
# Aggregate push down does not support group by expressions
# Aggregate push down does not support not use aggregate functions
# Offset push down is not supported
# Paging push down is not supported
# UDF/UDAF push down is not supported
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): SPARK-36695
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): SPARK-38788
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 13 16:22:18 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11c0w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Apr/22 22:38;xkrogen;What's the relationship between this and SPARK-38788? Seems like they are laying out the same goal?;;;, 13/Apr/22 16:22;maxgekk;SPARK-38788 was created specifically for the release note. Also it includes features/bug fixes that were made out of this umbrella, for example https://issues.apache.org/jira/browse/SPARK-36644;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 13/Apr/22 16:22;maxgekk;SPARK-38788 was created specifically for the release note. Also it includes features/bug fixes that were made out of this umbrella, for example https://issues.apache.org/jira/browse/SPARK-36644;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: JDBCTable support properties
Issue key: SPARK-39136
Issue id: 13444131
Parent id: 
Issue Type: Task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 10/May/22 08:37
Updated: 15/Sep/23 04:25
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
{code:java}
         >
         > desc formatted jdbc.test.people;
NAME	string
ID	int

# Partitioning
Not partitioned

# Detailed Table Information
Name	test.people
Table Properties	[]
Time taken: 0.048 seconds, Fetched 9 row(s)
{code}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue May 10 09:05:14 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1280w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/May/22 08:37;angerszhuuu;Raise a ticket soon;;;, 10/May/22 09:04;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36495;;;, 10/May/22 09:05;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36495;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 10/May/22 09:04;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36495;;;
Comment.2: 10/May/22 09:05;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36495;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: CollectMetrics is executed twice if it is followed by a sort
Issue key: SPARK-37487
Issue id: 13414304
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tanelk
Creator: tanelk
Created: 29/Nov/21 13:24
Updated: 15/Sep/23 01:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: correctness
Description: It is best examplified by this new UT in DataFrameCallbackSuite:
{code}
  test("SPARK-37487: get observable metrics with sort by callback") {
    val df = spark.range(100)
      .observe(
        name = "my_event",
        min($"id").as("min_val"),
        max($"id").as("max_val"),
        // Test unresolved alias
        sum($"id"),
        count(when($"id" % 2 === 0, 1)).as("num_even"))
      .observe(
        name = "other_event",
        avg($"id").cast("int").as("avg_val"))
      .sort($"id".desc)

    validateObservedMetrics(df)
  }
{code}

The count and sum aggregate report twice the number of rows:
{code}
[info] - SPARK-37487: get observable metrics with sort by callback *** FAILED *** (169 milliseconds)
[info]   [0,99,9900,100] did not equal [0,99,4950,50] (DataFrameCallbackSuite.scala:342)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
[info]   at org.apache.spark.sql.util.DataFrameCallbackSuite.checkMetrics$1(DataFrameCallbackSuite.scala:342)
[info]   at org.apache.spark.sql.util.DataFrameCallbackSuite.validateObservedMetrics(DataFrameCallbackSuite.scala:350)
[info]   at org.apache.spark.sql.util.DataFrameCallbackSuite.$anonfun$new$21(DataFrameCallbackSuite.scala:324)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
{code}

I could not figure out how this happes. Hopefully the UT can help with debugging
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Dec 01 06:27:23 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0x62g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Nov/21 13:28;tanelk;[~cloud_fan] and [~sarutak], you helped with the last CollectMetrics bug. Perhaps you have some idea, why this is happening.
;;;, 30/Nov/21 22:03;sarutak;[~tanelk] Thank you for pinging me.
I think a sampling job for the global sort performs the extra CollectMetrics (operations before the sort are performed twice).
Please let me look into more.;;;, 01/Dec/21 03:01;gurwls223;cc [~beliefer] FYI;;;, 01/Dec/21 06:27;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34765;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 30/Nov/21 22:03;sarutak;[~tanelk] Thank you for pinging me.
I think a sampling job for the global sort performs the extra CollectMetrics (operations before the sort are performed twice).
Please let me look into more.;;;
Comment.2: 01/Dec/21 03:01;gurwls223;cc [~beliefer] FYI;;;
Comment.3: 01/Dec/21 06:27;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34765;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: MetadataFetchFailedException due to decommission block migrations
Issue key: SPARK-38101
Issue id: 13426562
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: eejbyfeldt
Creator: eejbyfeldt
Created: 04/Feb/22 07:19
Updated: 04/Sep/23 13:16
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.1.3, 3.2.1, 3.2.2, 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 2
Labels: 
Description: As noted in SPARK-34939 there is race when using broadcast for map output status. Explanation from SPARK-34939

> After map statuses are broadcasted and the executors obtain serialized broadcasted map statuses. If any fetch failure happens after, Spark scheduler invalidates cached map statuses and destroy broadcasted value of the map statuses. Then any executor trying to deserialize serialized broadcasted map statuses and access broadcasted value, IOException will be thrown. Currently we don't catch it in MapOutputTrackerWorker and above exception will fail the application.

But if running with `spark.decommission.enabled=true` and `spark.storage.decommission.shuffleBlocks.enabled=true` there is another way to hit this race, when a node is decommissioning and the shuffle blocks are migrated. After a block has been migrated an update will be sent to the driver for each block and the map output caches will be invalidated.

Here are a driver when we hit the race condition running with spark 3.2.0:
{code:java}
2022-01-28 03:20:12,409 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 5.5 MiB, free 11.0 GiB)
2022-01-28 03:20:12,410 INFO spark.ShuffleStatus: Updating map output for 192108 to BlockManagerId(760, ip-10-231-63-204.ec2.internal, 34707, None)
2022-01-28 03:20:12,410 INFO spark.ShuffleStatus: Updating map output for 179529 to BlockManagerId(743, ip-10-231-34-160.ec2.internal, 44225, None)
2022-01-28 03:20:12,414 INFO spark.ShuffleStatus: Updating map output for 187194 to BlockManagerId(761, ip-10-231-43-219.ec2.internal, 39943, None)
2022-01-28 03:20:12,415 INFO spark.ShuffleStatus: Updating map output for 190303 to BlockManagerId(270, ip-10-231-33-206.ec2.internal, 38965, None)
2022-01-28 03:20:12,416 INFO spark.ShuffleStatus: Updating map output for 192220 to BlockManagerId(270, ip-10-231-33-206.ec2.internal, 38965, None)
2022-01-28 03:20:12,416 INFO spark.ShuffleStatus: Updating map output for 182306 to BlockManagerId(688, ip-10-231-43-41.ec2.internal, 35967, None)
2022-01-28 03:20:12,417 INFO spark.ShuffleStatus: Updating map output for 190387 to BlockManagerId(772, ip-10-231-55-173.ec2.internal, 35523, None)
2022-01-28 03:20:12,417 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.0 MiB, free 10.9 GiB)
2022-01-28 03:20:12,417 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on ip-10-231-63-1.ec2.internal:34761 (size: 4.0 MiB, free: 11.0 GiB)
2022-01-28 03:20:12,418 INFO memory.MemoryStore: Block broadcast_27_piece1 stored as bytes in memory (estimated size 1520.4 KiB, free 10.9 GiB)
2022-01-28 03:20:12,418 INFO storage.BlockManagerInfo: Added broadcast_27_piece1 in memory on ip-10-231-63-1.ec2.internal:34761 (size: 1520.4 KiB, free: 11.0 GiB)
2022-01-28 03:20:12,418 INFO spark.MapOutputTracker: Broadcast outputstatuses size = 416, actual size = 5747443
2022-01-28 03:20:12,419 INFO spark.ShuffleStatus: Updating map output for 153389 to BlockManagerId(154, ip-10-231-42-104.ec2.internal, 44717, None)
2022-01-28 03:20:12,419 INFO broadcast.TorrentBroadcast: Destroying Broadcast(27) (from updateMapOutput at BlockManagerMasterEndpoint.scala:594)
2022-01-28 03:20:12,427 INFO storage.BlockManagerInfo: Added rdd_65_20310 on disk on ip-10-231-32-25.ec2.internal:40657 (size: 77.6 MiB)
2022-01-28 03:20:12,427 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on ip-10-231-63-1.ec2.internal:34761 in memory (size: 4.0 MiB, free: 11.0 GiB)
{code}
While the Broadcast is being constructed we have updates coming in and the broadcast is destroyed almost immediately. On this particular job we ended up hitting the race condition a lot of times and it caused ~18 task failures and stage retries within 20 seconds causing us to hit our stage retry limit and the job to fail.

As far I understand this was the expected behavior for handling this case after SPARK-34939. But it seems like when combined with decommissioning hitting the race is a bit too common.

We have observed this behavior running 3.2.0 and 3.2.1, but I think other current versions are all so affected.

The executor will usually fail with errors like
{code:java}
org.apache.spark.shuffle.MetadataFetchFailedException: Unable to deserialize broadcasted map statuses for shuffle xx: java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_171_piece0 of broadcast_171{{}} {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-34939
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Mar 10 06:37:08 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z90w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Feb/22 18:52;dongjoon;Thank you for filing a JIRA, [~eejbyfeldt].;;;, 04/Feb/22 19:21;viirya;Thanks for reporting this issue, [~eejbyfeldt].

;;;, 09/Mar/22 18:23;medb;Is there a workaround for this issue?;;;, 10/Mar/22 06:37;eejbyfeldt;The race condition only exists when broadcast is used for the MapOutputStatuses. So one workaround might be to increase the value of `spark.shuffle.mapOutput.minSizeForBroadcast`: https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/internal/config/package.scala#L1457-L1462 so that broadcast is not used. But I suspect that if you make it to large you might run into performance problems.;;;
Affects Version/s.1: 3.1.3
Component/s.1: 
Comment.1: 04/Feb/22 19:21;viirya;Thanks for reporting this issue, [~eejbyfeldt].

;;;
Comment.2: 09/Mar/22 18:23;medb;Is there a workaround for this issue?;;;
Comment.3: 10/Mar/22 06:37;eejbyfeldt;The race condition only exists when broadcast is used for the MapOutputStatuses. So one workaround might be to increase the value of `spark.shuffle.mapOutput.minSizeForBroadcast`: https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/internal/config/package.scala#L1457-L1462 so that broadcast is not used. But I suspect that if you make it to large you might run into performance problems.;;;
Comment.4: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Spark Sql  insert into hive table dynamic partitions slow
Issue key: SPARK-42988
Issue id: 13530885
Parent id: 
Issue Type: Question
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: thomasgx
Creator: thomasgx
Created: 31/Mar/23 03:14
Updated: 31/Aug/23 09:45
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 31/Aug/23 09:42;wujunzhe;image-2023-08-31-17-42-31-348.png;https://issues.apache.org/jira/secure/attachment/13062625/image-2023-08-31-17-42-31-348.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 31 09:44:03 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1gzmw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/Mar/23 03:18;thomasgx;Use the same sql to execute on hive and spark, insert hive multi-level partition table, and find that in the stage of inserting partitions, hive is much faster than spark, and spark processes one file at a time during the processing process. What is the optimization in this place? What is the way, why not use the same move method as hive in spark;;;, 31/Mar/23 04:59;thomasgx;saprk using  hive api

externalCatalog.loadDynamicPartitions(
db = table.database,
table = table.identifier.table,
tmpLocation.toString,
partitionSpec,
overwrite,
numDynamicPartitions) 

 

this method add part one by one ,should we modify it 

 ;;;, 10/Apr/23 03:21;gurwls223;Would be great if there's a reproducer and some analysis made based on observation.;;;, 31/Aug/23 09:44;wujunzhe;h3. I have the same question. Here's the question I posed on stackflow [link title|[apache spark - Why are spark3 dynamic partitions slow to write to hive - Stack Overflow|https://stackoverflow.com/questions/76997680/why-are-spark3-dynamic-partitions-slow-to-write-to-hive]]

This is one of my tasks, the code is almost the same adjusted part of the log printing, respectively, using {{spark2}} (left) and {{spark3}} run (right), in the case of the same parameters, {{spark3}} each job running speed are significantly better than {{{}spark3{}}}, but the total running time {{spark3}} spent 1.2h, while {{spark2}} only spent 44 min. Why this phenomenon occurs? What is this extra time used for?[The top part is the eventTime forspark2 r, and the bottom part is for a spark3]

!image-2023-08-31-17-42-31-348.png!;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 31/Mar/23 04:59;thomasgx;saprk using  hive api

externalCatalog.loadDynamicPartitions(
db = table.database,
table = table.identifier.table,
tmpLocation.toString,
partitionSpec,
overwrite,
numDynamicPartitions) 

 

this method add part one by one ,should we modify it 

 ;;;
Comment.2: 10/Apr/23 03:21;gurwls223;Would be great if there's a reproducer and some analysis made based on observation.;;;
Comment.3: 31/Aug/23 09:44;wujunzhe;h3. I have the same question. Here's the question I posed on stackflow [link title|[apache spark - Why are spark3 dynamic partitions slow to write to hive - Stack Overflow|https://stackoverflow.com/questions/76997680/why-are-spark3-dynamic-partitions-slow-to-write-to-hive]]

This is one of my tasks, the code is almost the same adjusted part of the log printing, respectively, using {{spark2}} (left) and {{spark3}} run (right), in the case of the same parameters, {{spark3}} each job running speed are significantly better than {{{}spark3{}}}, but the total running time {{spark3}} spent 1.2h, while {{spark2}} only spent 44 min. Why this phenomenon occurs? What is this extra time used for?[The top part is the eventTime forspark2 r, and the bottom part is for a spark3]

!image-2023-08-31-17-42-31-348.png!;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark structured streaming performance regression in latency times reading/writing to kafka since 3.0.2
Issue key: SPARK-44933
Issue id: 13548431
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ebaggott
Creator: ebaggott
Created: 23/Aug/23 19:49
Updated: 23/Aug/23 20:33
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 2.4.8, 3.0.2, 3.1.0, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description:  During a migration from spark 2.4.4 to spark 3.4.0 I have noticed slower latency times in spark structured streaming when reading and writing to kafka. I have tested using both CONTINUOUS and MICROBATCH.

In simple read and write to kafka using CONTINUOUS mode in spark 2.4.4 I usually see latency times of ~5ms in our appllication. When moving to spark 3.4.0 this increased to ~15ms.

I stripped it back to a very simple test where I send 2 data fields in csv format to a kafka topic using a simple producer. Then I have a simple consumer which reads from the input topic and writes to an output topic. The 2 fields are an ID and an amount value. I read from both topics and retrieve the kafka timestamp value for all rows. I then subtract the input timestamp from the output timestamp to get the latency. To keep things as simple as possible I am using 1 kafka partition and I am using local[1] as the spark master.

Version    latency (ms)    Trigger
2.4.4    3.25    CONTINUOUS
3.4.0    7.23    CONTINUOUS
2.4.4    640    MICROBATCH
3.4.0    693    MICROBATCH
I have tried all versions of spark 3.x and I believe this issue was introduced in 3.0.2. I also tried different versions of spark 2.4.x and I see the same behaviour when going from 2.4.7 to 2.4.8.

In the simple test I only use a few jars. One of these is spark-sql-kafka-0-10_2.12 When running on spark 3.0.2 using the 3.0.2 version of this jar I see the slower times. When I run again on spark 3.0.2 and use the 3.0.1 version of this jar I see the faster times.

The same thing happens between 2.4.7 version and the 2.4.8 version. The 2.4.8 version has the slower times.

Has anyone else observed a slow down in latency in structured streaming when reading from kafka ?

Are there any settings I need to change when moving to these versions ?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-08-23 19:49:51.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1jz2o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.0.2
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Couldn't submit Spark application to Kubenetes in versions v1.27.3
Issue key: SPARK-44847
Issue id: 13547707
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: siddaraju_gc
Creator: siddaraju_gc
Created: 17/Aug/23 11:03
Updated: 17/Aug/23 11:03
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: Spark-submit ( cluster mode on Kubernetes ) results error *io.fabric8.kubernetes.client.KubernetesClientException* on my 3 nodes k8s cluster.

Steps followed:
 * using IBM cloud, created 3 Instances
 * 1st Instance act as master node and another two acts as worker nodes

 
{noformat}
root@vsi-spark-master:/opt# kubectl get nodes
NAME                 STATUS   ROLES                  AGE   VERSION
vsi-spark-master     Ready    control-plane,master   2d    v1.27.3+k3s1
vsi-spark-worker-1   Ready    <none>                 47h   v1.27.3+k3s1
vsi-spark-worker-2   Ready    <none>                 47h   v1.27.3+k3s1{noformat}
 * Copy spark-3.4.1-bin-hadoop3.tgz in to /opt/spark folder 
 * Ran spark by using below command

 
{noformat}
root@vsi-spark-master:/opt# /opt/spark/bin/spark-submit --master k8s://http://<master_node_IP>:6443 --conf spark.kubernetes.authenticate.submission.oauthToken=$TOKEN --deploy-mode cluster --name spark-pi --class org.apache.spark.examples.SparkPi --conf spark.executor.instances=5 --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark  --conf spark.kubernetes.container.image=sushmakorati/testrepo:pyrandomGB local:///opt/spark/examples/jars/spark-examples_2.12-3.4.1.jar{noformat}
 * And getting below error message.

{noformat}
3/07/27 12:56:26 WARN Utils: Kubernetes master URL uses HTTP instead of HTTPS.
23/07/27 12:56:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/07/27 12:56:26 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
23/07/27 12:56:26 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
23/07/27 12:56:27 ERROR Client: Please check "kubectl auth can-i create pod" first. It should be yes.
Exception in thread "main" io.fabric8.kubernetes.client.KubernetesClientException: An error has occurred.
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:129)
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:122)
    at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:44)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:1113)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:93)
    at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:153)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5(KubernetesClientApplication.scala:250)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5$adapted(KubernetesClientApplication.scala:244)
    at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2786)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:244)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:216)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1020)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: Connection reset
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:535)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:558)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleCreate(OperationSupport.java:349)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:711)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:93)
    at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)
    ... 15 more
Caused by: java.net.SocketException: Connection reset
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)
    at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)
    at okio.Okio$2.read(Okio.java:140)
    at okio.AsyncTimeout$2.read(AsyncTimeout.java:237)
    at okio.RealBufferedSource.read(RealBufferedSource.java:47)
    at okhttp3.internal.http1.Http1Codec$AbstractSource.read(Http1Codec.java:363)
    at okhttp3.internal.http1.Http1Codec$UnknownLengthSource.read(Http1Codec.java:507)
    at okio.RealBufferedSource.exhausted(RealBufferedSource.java:57)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientImpl$OkHttpAsyncBody.doConsume(OkHttpClientImpl.java:127)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
23/07/27 12:56:27 INFO ShutdownHookManager: Shutdown hook called
23/07/27 12:56:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-70ee50ef-d9e9-4220-91f4-15a282031095{noformat}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-08-17 11:03:12.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1jum0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Spark Submit
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: When multiple SQLs are concurrent, the driver subquery thread is permanently locked
Issue key: SPARK-41129
Issue id: 13502068
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: roychen
Creator: roychen
Created: 14/Nov/22 05:40
Updated: 14/Aug/23 11:40
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When a sql has only a small amount of concurrency (10), the sql will generate 11 jobs, and can be executed smoothly (Please refer to attached picture).

But when I increased the number of concurrency to 20, each sql only executed the first job and stopped (Please refer to attached picture),

And look at the driver thread dump and find that the subquery threads (20 threads) is locked, detail below

 
{code:java}
Monitor(org.apache.spark.sql.execution.aggregate.HashAggregateExec@1335537910}), Lock(java.util.concurrent.ThreadPoolExecutor$Worker@1502413281}), Monitor(org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec@890810300}), Monitor(java.lang.Object@603970601}), Monitor(org.apache.spark.sql.execution.exchange.ShuffleExchangeExec@2042514973}) {code}
{code:java}
sun.misc.Unsafe.park(Native Method) java.util.concurrent.locks.LockSupport.park(LockSupport.java:175) java.util.concurrent.FutureTask.awaitDone(FutureTask.java:429) java.util.concurrent.FutureTask.get(FutureTask.java:191) org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310) org.apache.spark.sql.execution.SubqueryExec.executeCollect(basicPhysicalOperators.scala:861) org.apache.spark.sql.execution.ScalarSubquery.updateResult(subquery.scala:80) org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1(SparkPlan.scala:262) org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1$adapted(SparkPlan.scala:261) org.apache.spark.sql.execution.SparkPlan$$Lambda$3650/586819338.apply(Unknown Source) scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) org.apache.spark.sql.execution.SparkPlan.waitForSubqueries(SparkPlan.scala:261) => holding Monitor(org.apache.spark.sql.execution.aggregate.HashAggregateExec@1335537910}) org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:231) org.apache.spark.sql.execution.SparkPlan$$Lambda$3645/1297667696.apply(Unknown Source) org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229) org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92) org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92) org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47) org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:660) org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:723) org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194) org.apache.spark.sql.execution.SparkPlan$$Lambda$3644/556844527.apply(Unknown Source) org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232) org.apache.spark.sql.execution.SparkPlan$$Lambda$3645/1297667696.apply(Unknown Source) org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229) org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190) org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135) => holding Monitor(org.apache.spark.sql.execution.exchange.ShuffleExchangeExec@2042514973}) org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135) org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:140) => holding Monitor(org.apache.spark.sql.execution.exchange.ShuffleExchangeExec@2042514973}) org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139) org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68) org.apache.spark.sql.execution.exchange.ShuffleExchangeLike$$Lambda$3671/828122256.apply(Unknown Source) org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232) org.apache.spark.sql.execution.SparkPlan$$Lambda$3645/1297667696.apply(Unknown Source) org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229) org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68) org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67) org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115) org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:174) => holding Monitor(org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec@890810300}) org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:174) org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:176) org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82) org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:258) org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:256) org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3669/118271447.apply(Unknown Source) scala.collection.Iterator.foreach(Iterator.scala:943) scala.collection.Iterator.foreach$(Iterator.scala:943) scala.collection.AbstractIterator.foreach(Iterator.scala:1431) scala.collection.IterableLike.foreach(IterableLike.scala:74) scala.collection.IterableLike.foreach$(IterableLike.scala:73) scala.collection.AbstractIterable.foreach(Iterable.scala:56) org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:256) org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$3616/1041219151.apply(Unknown Source) org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:228) => holding Monitor(java.lang.Object@603970601}) org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:367) org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeTake(AdaptiveSparkPlanExec.scala:344) org.apache.spark.sql.execution.SubqueryExec.$anonfun$relationFuture$2(basicPhysicalOperators.scala:834) org.apache.spark.sql.execution.SubqueryExec$$Lambda$3652/1074832567.apply(Unknown Source) org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionId$1(SQLExecution.scala:145) org.apache.spark.sql.execution.SQLExecution$$$Lambda$3653/1322734277.apply(Unknown Source) org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169) org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:143) org.apache.spark.sql.execution.SubqueryExec.$anonfun$relationFuture$1(basicPhysicalOperators.scala:830) org.apache.spark.sql.execution.SubqueryExec$$Lambda$3648/502350376.apply(Unknown Source) org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:191) org.apache.spark.sql.execution.SQLExecution$$$Lambda$3649/2139778019.call(Unknown Source) java.util.concurrent.FutureTask.run(FutureTask.java:266) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) java.lang.Thread.run(Thread.java:750)
{code}
Not sure what's causing it, please let me know if you need any info, thanks!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 14/Nov/22 05:43;roychen;locksqlPNG.PNG;https://issues.apache.org/jira/secure/attachment/13052161/locksqlPNG.PNG, 14/Nov/22 05:42;roychen;normaljobs.PNG;https://issues.apache.org/jira/secure/attachment/13052160/normaljobs.PNG
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 14 11:40:09 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1c2f4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Aug/23 11:40;JacobZheng;I'm experiencing the exact same thing, not sure if it has anything to do with concurrency. Did you find a cause or solution to the problem?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Do not combine multiple Generate operators in the same WholeStageCodeGen node because it can  easily cause OOM failures if arrays are relatively large
Issue key: SPARK-44759
Issue id: 13546803
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tafranky@gmail.com
Creator: tafranky@gmail.com
Created: 10/Aug/23 09:23
Updated: 13/Aug/23 06:03
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1
Fix Version/s: 
Component/s: Deploy, Optimizer, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: This is an issue since the WSCG  implementation of the generate node. 

Because WSCG compute rows in batches , the combination of WSCG and the explode operation consume a lot of the dedicated executor memory. This is even more true when the WSCG node contains multiple explode nodes. This is the case when flattening a nested array.

The generate node used to flatten array generally  produces an amount of output rows that is significantly higher than the input rows.

the number of output rows generated is even drastically higher when flattening a nested array .

When we combine more that 1 generate node in the same WholeStageCodeGen  node, we run  a high risk of running out of memory for multiple reasons. 

1- As you can see from snapshots added in the comments ,  the rows created in the nested loop are saved in a writer buffer.  In this case because the rows were big , the job failed with an Out Of Memory Exception error .

2_ The generated WholeStageCodeGen result in a nested loop that for each row  , will explode the parent array and then explode the inner array.  The rows are accumulated in the writer buffer without accounting for the row size.

Please view the attached Spark Gui and Spark Dag 

In my case the wholestagecodegen includes 2 explode nodes. 

Because the array elements are large , we end up with an Out Of Memory error. 

 

I recommend that we do not merge  multiple explode nodes in the same whole stage code gen node . Doing so leads to potential memory issues.

In our case , the job execution failed with an  OOM error because the the WSCG executed  into a nested for loop . 

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 10/Aug/23 16:27;tafranky@gmail.com;image-2023-08-10-09-27-24-124.png;https://issues.apache.org/jira/secure/attachment/13062036/image-2023-08-10-09-27-24-124.png, 10/Aug/23 16:29;tafranky@gmail.com;image-2023-08-10-09-29-24-804.png;https://issues.apache.org/jira/secure/attachment/13062037/image-2023-08-10-09-29-24-804.png, 10/Aug/23 16:32;tafranky@gmail.com;image-2023-08-10-09-32-46-163.png;https://issues.apache.org/jira/secure/attachment/13062038/image-2023-08-10-09-32-46-163.png, 10/Aug/23 16:33;tafranky@gmail.com;image-2023-08-10-09-33-47-788.png;https://issues.apache.org/jira/secure/attachment/13062039/image-2023-08-10-09-33-47-788.png, 10/Aug/23 09:25;tafranky@gmail.com;wholestagecodegen_wc1_debug_wholecodegen_passed;https://issues.apache.org/jira/secure/attachment/13062030/wholestagecodegen_wc1_debug_wholecodegen_passed
Custom field (Affects version (Component)): 
Custom field (Attachment count): 5.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 10 16:34:45 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1jpe8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Aug/23 16:27;tafranky@gmail.com;WSCG  generated code that calls  generate_doConsume_0

!image-2023-08-10-09-27-24-124.png!;;;, 10/Aug/23 16:29;tafranky@gmail.com;WSCG  generated code for first Generate node 

!image-2023-08-10-09-29-24-804.png!;;;, 10/Aug/23 16:33;tafranky@gmail.com;WSCG  generated code for second Generate node 

!image-2023-08-10-09-32-46-163.png!;;;, 10/Aug/23 16:34;tafranky@gmail.com;Spark Dag for the use case . The failure is from the execution of WholeStageCodeGen(2)

!image-2023-08-10-09-33-47-788.png!;;;
Affects Version/s.1: 3.0.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1
Component/s.1: Optimizer
Comment.1: 10/Aug/23 16:29;tafranky@gmail.com;WSCG  generated code for first Generate node 

!image-2023-08-10-09-29-24-804.png!;;;
Comment.2: 10/Aug/23 16:33;tafranky@gmail.com;WSCG  generated code for second Generate node 

!image-2023-08-10-09-32-46-163.png!;;;
Comment.3: 10/Aug/23 16:34;tafranky@gmail.com;Spark Dag for the use case . The failure is from the execution of WholeStageCodeGen(2)

!image-2023-08-10-09-33-47-788.png!;;;
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.12.0, EMR-6.13.0, EMR-6.14.0, EMR-6.15.0, EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: New functions in Spark SQL 3.3
Issue key: SPARK-38783
Issue id: 13437572
Parent id: 
Issue Type: Epic
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: maxgekk
Reporter: maxgekk
Creator: maxgekk
Created: 04/Apr/22 09:16
Updated: 11/Aug/23 17:02
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Get together all functions and extensions of existing functions introduced in Spark SQL 3.3.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): SPARK-16280, SPARK-36754, SPARK-28137
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): ghx-label-11
Custom field (Epic Link): 
Custom field (Epic Name): New functions in Spark SQL 3.3
Custom field (Epic Status): To Do
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-04 09:16:49.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1147c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Conflicting attribute during join two times the same table (AQE is disabled)
Issue key: SPARK-44739
Issue id: 13546646
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kondziolka9ld
Creator: kondziolka9ld
Created: 09/Aug/23 08:58
Updated: 09/Aug/23 09:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: h2. Issue

I come across a something that seems to be bug in *pyspark* (when I disable adaptive queries). It is about joining two times the same dataframe (please look at reproduction steps below). 
----
h2. Reproduction steps
{code:java}
pyspark --conf spark.sql.adaptive.enabled=false
Python 3.8.10 (default, Nov 14 2022, 12:59:47) 
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
23/08/09 10:18:54 WARN Utils: Your hostname, kondziolka-dd-laptop resolves to a loopback address: 127.0.1.1; using 192.168.0.18 instead (on interface wlp0s20f3)
23/08/09 10:18:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/08/09 10:18:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/Using Python version 3.8.10 (default, Nov 14 2022 12:59:47)
Spark context Web UI available at http://192.168.0.18:4040
Spark context available as 'sc' (master = local[*], app id = local-1691569137130).
SparkSession available as 'spark'.

>>> sc.setCheckpointDir("file:///tmp")
>>> df1=spark.createDataFrame([(1, 42)], ["id", "fval"])
>>> df2=spark.createDataFrame([(1, 0, "jeden")], ["id", "target", "aux"]) 
>>> df2.explain()
== Physical Plan ==
*(1) Scan ExistingRDD[id#4L,target#5L,aux#6]
>>> j1=df1.join(df2, ["id"]).select("fval", "aux").checkpoint()
>>> j1.explain()
== Physical Plan ==
*(1) Scan ExistingRDD[fval#1L,aux#6]
>>> # we see that both j1 and df2 refers to the same attribute aux#6
>>> # let's join df2 to j1. Both of them has aux column.
>>> j1.join(df2, "aux")                                                      
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/kondziolkadd/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py", line 1539, in join
    jdf = self._jdf.join(other._jdf, on, how)
  File "/home/kondziolkadd/.local/lib/python3.8/site-packages/pyspark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/home/kondziolkadd/.local/lib/python3.8/site-packages/pyspark/sql/utils.py", line 196, in deco
    raise converted from None
pyspark.sql.utils.AnalysisException: 
Failure when resolving conflicting references in Join:
'Join Inner
:- LogicalRDD [fval#1L, aux#6], false
+- LogicalRDD [id#4L, target#5L, aux#6], false

Conflicting attributes: aux#6
;
'Join Inner
:- LogicalRDD [fval#1L, aux#6], false
+- LogicalRDD [id#4L, target#5L, aux#6], false
{code}
 
----
h2. Workaround

The workaround is about renaming columns twice times - I mean identity rename `X -> X' -> X`. It looks like it forces rewrite of metadata (change attribute id) and in this way it avoids conflict.
{code:java}
>>> sc.setCheckpointDir("file:///tmp")
>>> df1=spark.createDataFrame([(1, 42)], ["id", "fval"])
>>> df2=spark.createDataFrame([(1, 0, "jeden")], ["id", "target", "aux"])
>>> df2.explain()
== Physical Plan ==
*(1) Scan ExistingRDD[id#4L,target#5L,aux#6]
>>> j1=df1.join(df2, ["id"]).select("fval", "aux").withColumnRenamed("aux", "_aux").withColumnRenamed("_aux", "aux").checkpoint()
>>> j1.explain()                                                                
== Physical Plan ==
*(1) Scan ExistingRDD[fval#1L,aux#19]
>>> j1.join(df2, "aux")
>>>
{code}
----
h2. Others
 * Repartition before checkpoint is workaround as well (it does not change id of attribute)

{code:java}
>>> j1=df1.join(df2, ["id"]).select("fval", "aux").repartition(100).checkpoint() 
>>> j1.join(df2, "aux") {code}
 * Without `checkpoint` issue does not occur (although id is the same)

{code:java}
>>> j1=df1.join(df2, ["id"]).select("fval", "aux")
>>> j1.join(df2, "aux") {code}
 * Without disabling `AQE` it does not occur
 * I was not able to reproduce it on spark -  by saying that I mean that I reproduced it only in `pyspark`.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): pyspark, python
Custom field (Last public comment date): 2023-08-09 08:58:28.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1jofc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Memory issue with Structured streaming
Issue key: SPARK-40927
Issue id: 13492129
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mihirkelkar
Creator: mihirkelkar
Created: 26/Oct/22 21:56
Updated: 03/Aug/23 09:22
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.2, 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 1
Labels: 
Description: In Pyspark Structured streaming with Kafka as source and sink, the driver as well as the executors seem to get OOM killed after a long period of time (few days). Not able to pinpoint to any specific thing.

But 8-12 hrs long runs also show the slow memory creep in Prometheus metrics values -
 # JVM Off-heap memory of both driver and executors keep on increasing over time (12-24hrs observation time) [I have NOT enabled off-heap usage]
 # JVM heap memory of executors also keeps on bumping up in slow steps.
 # JVM RSS of executors and driver keeps increasing but python RSS does not increase

-Basic operation of counting rows from within sdf.forEachBatch() is being done to debug ( -Original business logic has Some dropDuplicates, aggregations , windowing are being done within the forEachBatch.

-watermarking on a custom timestamp column is being done. 

 

Heap Dump analysis shows large no. of duplicate strings (which look like generated code). Further large no. of byte[], char[] and UTF8String objects.. Does this point to any potential memory leak in Tungsten optimizer related code?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 03 09:22:05 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ad5c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Aug/23 09:22;iainm;In our case I found the following settings greatly improved our streaming applications, currently running for over 2 weeks without OOM killed (previously lasted a day or two)

1. Use RocksDB state store provider improved executor memory usage

        "spark.sql.streaming.stateStore.providerClass" -> "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider"

        Not sure if there is a leak in the default HDFS state store implementation or not.

2. Store UI on disk instead of in memory in the driver

        "spark.ui.store.path" -> "some path"

 

Old issue but I hope this helps someone;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: parse_url treats key as regular expression
Issue key: SPARK-44500
Issue id: 13544306
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: revans2
Creator: revans2
Created: 20/Jul/23 14:36
Updated: 24/Jul/23 23:41
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0, 3.4.0, 3.4.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: To be clear I am not 100% sure that this is a bug. It might be a feature, but I don't see anywhere that it is used as a feature. If it is a feature it really should be documented, because there are pitfalls. If it is a bug it should be fixed because it is really confusing and it is simple to shoot yourself in the foot.

```scala
> val urls = Seq("http://foo/bar?abc=BAD&a.c=GOOD", "http://foo/bar?a.c=GOOD&abc=BAD").toDF
> urls.selectExpr("parse_url(value, 'QUERY', 'a.c')").show(false)

+----------------------------+
|parse_url(value, QUERY, a.c)|
+----------------------------+
|BAD                         |
|GOOD                        |
+----------------------------+

> urls.selectExpr("parse_url(value, 'QUERY', 'a[c')").show(false)
java.util.regex.PatternSyntaxException: Unclosed character class near index 15
(&|^)a[c=([^&]*)
               ^
  at java.util.regex.Pattern.error(Pattern.java:1969)
  at java.util.regex.Pattern.clazz(Pattern.java:2562)
  at java.util.regex.Pattern.sequence(Pattern.java:2077)
  at java.util.regex.Pattern.expr(Pattern.java:2010)
  at java.util.regex.Pattern.compile(Pattern.java:1702)
  at java.util.regex.Pattern.<init>(Pattern.java:1352)
  at java.util.regex.Pattern.compile(Pattern.java:1028)

```

The simple fix is to quote the key when making the pattern.

```scala
  private def getPattern(key: UTF8String): Pattern = {
    Pattern.compile(REGEXPREFIX + Pattern.quote(key.toString) + REGEXSUBFIX)
  }
```
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jul 24 23:41:37 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ja00:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Jul/23 23:41;planga82;[~jan.chou.wu@gmail.com] What do you think?;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.13.0, EMR-6.14.0, EMR-6.15.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: when using df.astype("str") on pyspark dataframe. None are converted "None"
Issue key: SPARK-39568
Issue id: 13462979
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gokulyc
Creator: gokulyc
Created: 23/Jun/22 12:51
Updated: 03/Jul/23 05:20
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Pandas API on Spark
Due Date: 
Votes: 0
Labels: 
Description: when using df.astype("str") on pyspark dataframe. None are converted "None".

 
 - Not able to keep Null values as None. instead it is converted to string "None" for whole column.

 

https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/9431837223032/3368288770184753/3794760114602748/latest.html
Environment: Tried on azure databricks.
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 23/Jun/22 13:06;gokulyc;chrome_M3JLsVzzb2.png;https://issues.apache.org/jira/secure/attachment/13045511/chrome_M3JLsVzzb2.png, 23/Jun/22 13:03;gokulyc;image-2022-06-23-18-33-42-324.png;https://issues.apache.org/jira/secure/attachment/13045510/image-2022-06-23-18-33-42-324.png, 23/Jun/22 13:03;gokulyc;loan200 - Copy.csv;https://issues.apache.org/jira/secure/attachment/13045509/loan200+-+Copy.csv
Custom field (Affects version (Component)): 
Custom field (Attachment count): 3.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jul 03 05:20:41 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z15fdc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Jun/22 13:08;gokulyc;Used sample data to explain issue, Please provide if there is any better solution. 

I need to insert values in database with None as Null and all columns should be string for my use case.;;;, 23/Jun/22 13:18;gokulyc;Using applymap() as workaround to update them back as Null, but it is not performant as logic needs to traverse whole dataframe.;;;, 05/Jul/22 02:45;gurwls223;[~itholic] [~XinrongM] [~podongfeng] FYI;;;, 03/Jul/23 05:20;kmrnag;Thank you for reporting [~gokulyc]. Adding an example to recreate the issue without the need for loading the file.

 
{code:java}
import pyspark.pandas as ps
import pandas as pd
from pyspark.sql import SparkSession

d = { 'col' : [ "1", "2", None] }
df = ps.DataFrame(data=d, columns=['col'])

int_df = df.astype("int")
str_df = df.astype("str")

int_df.isnull().sum()
#output is
#col    1
#dtype: int64

str_df.isnull().sum()
#output is
#col    0
#dtype: int64{code};;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 23/Jun/22 13:18;gokulyc;Using applymap() as workaround to update them back as Null, but it is not performant as logic needs to traverse whole dataframe.;;;
Comment.2: 05/Jul/22 02:45;gurwls223;[~itholic] [~XinrongM] [~podongfeng] FYI;;;
Comment.3: 03/Jul/23 05:20;kmrnag;Thank you for reporting [~gokulyc]. Adding an example to recreate the issue without the need for loading the file.

 
{code:java}
import pyspark.pandas as ps
import pandas as pd
from pyspark.sql import SparkSession

d = { 'col' : [ "1", "2", None] }
df = ps.DataFrame(data=d, columns=['col'])

int_df = df.astype("int")
str_df = df.astype("str")

int_df.isnull().sum()
#output is
#col    1
#dtype: int64

str_df.isnull().sum()
#output is
#col    0
#dtype: int64{code};;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Enhance collapse window optimization to work in case partition or order by keys are expressions
Issue key: SPARK-40176
Issue id: 13477949
Parent id: 
Issue Type: Task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ayaga
Creator: ayaga
Created: 22/Aug/22 08:06
Updated: 29/Jun/23 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: In window operator with multiple window functions, if any expression is present in partition by or sort order columns, windows are not collapsed even if partition and order by expression is same for all those window functions.

E.g. query:

val w = Window.{_}partitionBy{_}("key").orderBy({_}lower{_}({_}col{_}("value")))

df.select({_}lead{_}("key", 1).over(w), {_}lead{_}("value", 1).over(w))

Current Plan:

-Window(lead(value,1), key, _w1) -------------- W1

- Sort (key, _w1)

-Project (lower(“value”) as _w1) --------- P1

-Window(lead(key,1), key, _w0) ---------------- W2

-Sort(key, _w0)

-Exchange(key)

-Project (lower(“value”) as _w0) ---- P2

-Scan

 

W1 and W2 can be merged in single window
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 29 18:01:41 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17y8g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Aug/22 08:08;ayaga;Working on this ticket;;;, 20/Oct/22 13:10;dovijoel;Is this related to this issue? https://issues.apache.org/jira/browse/SPARK-30552 ;;;, 29/Jun/23 18:01;ayaga;This solves this issue partially https://issues.apache.org/jira/browse/SPARK-41805. Remaining  cases are being solved in https://issues.apache.org/jira/browse/SPARK-42588;;;
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 20/Oct/22 13:10;dovijoel;Is this related to this issue? https://issues.apache.org/jira/browse/SPARK-30552 ;;;
Comment.2: 29/Jun/23 18:01;ayaga;This solves this issue partially https://issues.apache.org/jira/browse/SPARK-41805. Remaining  cases are being solved in https://issues.apache.org/jira/browse/SPARK-42588;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: SPIP: Row-level operations in Data Source V2
Issue key: SPARK-35801
Issue id: 13384421
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: aokolnychyi
Creator: aokolnychyi
Created: 17/Jun/21 20:21
Updated: 22/Jun/23 20:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: SPIP
Description: Row-level operations such as UPDATE, DELETE, MERGE are becoming more and more important for modern Big Data workflows. Use cases include but are not limited to deleting a set of records for regulatory compliance, updating a set of records to fix an issue in the ingestion pipeline, applying changes in a transaction log to a fact table. Row-level operations allow users to easily express their use cases that would otherwise require much more SQL. Common patterns for updating partitions are to read, union, and overwrite or read, diff, and append. Using commands like MERGE, these operations are easier to express and can be more efficient to run.

Hive supports [MERGE|https://blog.cloudera.com/update-hive-tables-easy-way/] and Spark should implement similar support.

SPIP: https://docs.google.com/document/d/12Ywmc47j3l2WF4anG5vL4qlrhT2OKigb7_EbIKhxg60

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): SPARK-38085
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-44111
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Jan 21 17:48:36 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s1yg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): viirya
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Jun/21 01:42;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33008;;;, 22/Jun/21 01:43;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33008;;;, 01/Feb/22 21:51;aokolnychyi;[~viirya], shall we keep this one open until the implementation is done or can we close it now? The community has already voted on this SPIP.;;;, 01/Feb/22 22:08;viirya;I think we can leave this open and put sub-tasks under this, like https://issues.apache.org/jira/browse/SPARK-34849.;;;, 16/Jan/23 09:01;dongjoon;Hi, [~viirya]and [~aokolnychyi]. Are we going to open this in Apache Spark 3.4.0 as `Unresolved`?;;;, 21/Jan/23 17:48;aokolnychyi;We should probably keep it open even beyond 3.4 as the item is not complete. I will add sub-tasks as we go.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 22/Jun/21 01:43;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33008;;;
Comment.2: 01/Feb/22 21:51;aokolnychyi;[~viirya], shall we keep this one open until the implementation is done or can we close it now? The community has already voted on this SPIP.;;;
Comment.3: 01/Feb/22 22:08;viirya;I think we can leave this open and put sub-tasks under this, like https://issues.apache.org/jira/browse/SPARK-34849.;;;
Comment.4: 16/Jan/23 09:01;dongjoon;Hi, [~viirya]and [~aokolnychyi]. Are we going to open this in Apache Spark 3.4.0 as `Unresolved`?;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Cannot parse Type from german "umlaut"
Issue key: SPARK-44108
Issue id: 13540776
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jomach
Creator: jomach
Created: 20/Jun/23 14:52
Updated: 20/Jun/23 18:25
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Hello all, 

 

I have a client that has a column named : bfzgtäeil

Spark cannot handle this. My test: 

 
{code:java}
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
import org.scalatest.funsuite.AnyFunSuite

class HiveTest extends AnyFunSuite {

  test("test that Spark does not cut columns with ä") {
    val data = "bfzugtäeil:string"
    CatalystSqlParser.parseDataType(data)
  }

} {code}
I debugged it and I'm deep on the  org.antlr.v4.runtime.Lexer class. 

Any ideas ? 

 
{code:java}
== SQL ==bfzugtäeil:string------^^^
	at org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:306)	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:143)	at org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseDataType(ParseDriver.scala:41)	at com.deutschebahn.zod.fvdl.commons.spark.app.captured.HiveTest2.$anonfun$new$1(HiveTest2.scala:13) {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-06-20 14:52:50.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1iob4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Unable to create spark session when master URL set to k8s in spark 3.3.0
Issue key: SPARK-44102
Issue id: 13540691
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: shamim_er123
Creator: shamim_er123
Created: 20/Jun/23 05:56
Updated: 20/Jun/23 06:05
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Getting below error while creating Spark Session using JAVA 

 

Error in executing job: 
org.apache.spark.SparkException: Could not parse Master URL: 'k8s://[https://kubernetes.default.svc.cluster.local:443|https://kubernetes.default.svc.cluster.local/]'
    at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2982)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:563)
    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2704)
    at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953)
    at scala.Option.getOrElse(Option.scala:189)

 

This was working fine with spark version 3.0.1, after upgrading spark version to 3.3.0. its starting throwing above error. 

 

Below are the Dependency version we used.

Spark Core 3.3.0

Spark Kubernetes Jar 3.3.0

io.fabrics dependency with version 5.12.2

 

We are not using spark-submit rather submitting spark application using java code  
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-06-20 05:56:52.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ins8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in org.apache.spark.rpc
Issue key: SPARK-38472
Issue id: 13432765
Parent id: 13423097.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bozhang
Creator: bozhang
Created: 09/Mar/22 05:19
Updated: 25/May/23 02:20
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu May 25 02:20:24 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10b2w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/May/23 02:20;panbingkun;I work on it.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support IO encryption for push-based shuffle
Issue key: SPARK-36744
Issue id: 13400837
Parent id: 13337114.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: minyang
Creator: minyang
Created: 13/Sep/21 17:54
Updated: 22/May/23 20:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: As a follow up to SPARK-36705, push-based shuffle is not compatible with IO encryption. We need to support IO encryption for push-based shuffle.
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-43583
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-09-13 17:54:55.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uv5s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in org.apache.spark.rdd
Issue key: SPARK-38471
Issue id: 13432764
Parent id: 13423097.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bozhang
Creator: bozhang
Created: 09/Mar/22 05:19
Updated: 08/May/23 17:56
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon May 08 17:56:02 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10b2o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Apr/22 03:16;ivoson;I am working on this.;;;, 16/Apr/23 03:13;blindcat;Since the last attempt to work on it was over a year ago. I am interested in taking over this ticket and working towards its completion.;;;, 07/May/23 19:21;blindcat;Hey [~bozhang] I'm currently working on this ticket. Could you assign this ticket to me? Thanks.
Additionaly I have question for scenario like [this,|https://github.com/xchen1189/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala#L295] for those kind of simple IO exception, should it be migrated to error classes?;;;, 08/May/23 03:27;bozhang;Hi [~blindcat] , thanks for working on this!

I don't have permission to assign this ticket to you, maybe we need to ask a committer for that.

For the code you posted in the link, I think we might not need to migrate that to error class, since that's a catch and re-throw, instead of constructing a new exception.;;;, 08/May/23 17:51;blindcat;awesome, thanks. ;;;, 08/May/23 17:56;blindcat;Hey [~maxgekk], I'm currently working on this ticket. Could you assign this ticket to me? Thanks;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 16/Apr/23 03:13;blindcat;Since the last attempt to work on it was over a year ago. I am interested in taking over this ticket and working towards its completion.;;;
Comment.2: 07/May/23 19:21;blindcat;Hey [~bozhang] I'm currently working on this ticket. Could you assign this ticket to me? Thanks.
Additionaly I have question for scenario like [this,|https://github.com/xchen1189/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala#L295] for those kind of simple IO exception, should it be migrated to error classes?;;;
Comment.3: 08/May/23 03:27;bozhang;Hi [~blindcat] , thanks for working on this!

I don't have permission to assign this ticket to you, maybe we need to ask a committer for that.

For the code you posted in the link, I think we might not need to migrate that to error class, since that's a catch and re-throw, instead of constructing a new exception.;;;
Comment.4: 08/May/23 17:51;blindcat;awesome, thanks. ;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Exception in thread "main" java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;
Issue key: SPARK-43278
Issue id: 13533964
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jiangjiguang0719
Creator: jiangjiguang0719
Created: 25/Apr/23 09:16
Updated: 26/Apr/23 17:07
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Java API
Due Date: 
Votes: 0
Labels: 
Description: Java version: 1.8.0_331, Apache Maven 3.8.4

I run next steps:
 # git clone [https://github.com/apache/spark.git]
 # git checkout -b v3.3.0 3.3.0
 #  mvn clean install -DskipTests
 # copy hive-site.xml to examples/src/main/resources/
 # execute TPC-H Q6 

 
{code:java}
public static void main(String[] args) throws InterruptedException {
        SparkConf sparkConf = new SparkConf()
                .setAppName("demo")
                .setMaster("local[1]")
                ;        
         SparkSession sparkSession = SparkSession.builder()
                .config(sparkConf)
                .enableHiveSupport()
                .getOrCreate();
        sparkSession.sql("use local_tpch_sf10_uncompressed_etl");        
        sparkSession.sql(TPCH.SQL6).show();
} {code}
 

 

get the error info:

Exception in thread "main" java.lang.NoSuchMethodError: java.nio.ByteBuffer.flip()Ljava/nio/ByteBuffer;
    at org.apache.spark.util.io.ChunkedByteBufferOutputStream.toChunkedByteBuffer(ChunkedByteBufferOutputStream.scala:115)
    at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:325)
    at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:140)
    at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:95)
    at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
    at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:75)
    at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1529)
    at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:235)
    at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:457)
    at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:448)
    at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:547)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:221)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
    at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:217)
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 26 17:07:32 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1hijs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Apr/23 17:07;gurwls223;Seems like your JRE is not 8?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Broadcast joins should pushdown join constraints as Filter to the larger relation
Issue key: SPARK-39753
Issue id: 13471181
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: devict
Creator: devict
Created: 12/Jul/22 09:21
Updated: 25/Apr/23 13:40
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 2
Labels: 
Description: SPARK-19609 was bulk-closed a while ago, but not fixed. I've decided to re-open it here for more visibility, since I believe this bug has a major impact and that fixing it could drastically improve the performance of many pipelines.

Allow me to paste the initial description again here:

_For broadcast inner-joins, where the smaller relation is known to be small enough to materialize on a worker, the set of values for all join columns is known and fits in memory. Spark should translate these values into a {{Filter}} pushed down to the datasource. The common join condition of equality, i.e. {{{}lhs.a == rhs.a{}}}, can be written as an {{a in ...}} clause. An example of pushing such filters is already present in the form of {{IsNotNull}} filters via_ [~sameerag]{_}'s work on SPARK-12957 subtasks.{_}

_This optimization could even work when the smaller relation does not fit entirely in memory. This could be done by partitioning the smaller relation into N pieces, applying this predicate pushdown for each piece, and unioning the results._

 

Essentially, when doing a Broadcast join, the smaller side can be used to filter down the bigger side before performing the join. As of today, the join will read all partitions of the bigger side, without pruning partitions
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): SPARK-19609
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Apr 25 13:40:30 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16sm0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Jul/22 09:22;devict;cc [~ndimiduk] since you created the original ticket ;;;, 12/Jul/22 09:59;yumwang;[~devict] I do not think {{lhs.a == rhs.a}} can be written as an a in ... clause. For example:
{code:sql}
create table t1(id int, name string) using parquet;
create table t2(id int, name string) using parquet;

insert into t1 values (1, 'a'),(2, 'b'),(3, 'c'),(4, 'd'),(5, 'e'), (1, 'a'),(2, 'b'),(3, 'c'),(4, 'd'),(5, 'e');
insert into t2 values (1, 'a'),(2, 'b'),(1, 'a'),(2, 'b');

select t1.* from t1 join t2 on t1.id = t2.id;
select t1.* from t1 where t1.id in (1, 2);
{code}
The result do not match:
{code:}
spark-sql> select t1.* from t1 join t2 on t1.id = t2.id;
1	a
1	a
1	a
1	a
2	b
2	b
2	b
2	b
spark-sql> select t1.* from t1 where t1.id in (1, 2);
1	a
1	a
2	b
2	b
{code}

;;;, 12/Jul/22 12:36;devict;[~yumwang] 

The idea would be to filter down t1 before joining it to t2, such that:

{code:sql}
select t1.* from t1 join t2 on t1.id = t2.id
{code}

would become:

{code:sql}
select t1.* from (
    select * from t1 where t1.id IN (1, 2)
) as filtered_t1
join t2 on filtered_t1.id = t2.id
{code}

This allows the t1 table to be filtered down before doing the join, including pruning partitions and files to be read.;;;, 12/Jul/22 12:50;devict;We currently work around this by doing the following (using your example and Dataframes):
{code:scala}
val t2Materialized = t2.select(col("id")).collectAsList().map(_.getAs(0))
val filteredT1 = t1.filter(col("id").isInCollection(t2Materialized))
val joined = filteredT1.join(t2.hint("broadcast"), "id") 
{code};;;, 13/Jul/22 03:28;yumwang;[~devict] So the build side needs to be executed twice. ;;;, 13/Jul/22 11:34;devict;[~yumwang] I'm not too familiar with build side and Spark SQL internals, unfortunately :). Do you think this change is a good idea? 

Let me know if there's anything I can do to help move this forward!;;;, 08/Aug/22 08:49;ndimiduk;Linking to the original issue.;;;, 25/Apr/23 13:40;lefjhq01;Any developement on this ticket?;;;
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 12/Jul/22 09:59;yumwang;[~devict] I do not think {{lhs.a == rhs.a}} can be written as an a in ... clause. For example:
{code:sql}
create table t1(id int, name string) using parquet;
create table t2(id int, name string) using parquet;

insert into t1 values (1, 'a'),(2, 'b'),(3, 'c'),(4, 'd'),(5, 'e'), (1, 'a'),(2, 'b'),(3, 'c'),(4, 'd'),(5, 'e');
insert into t2 values (1, 'a'),(2, 'b'),(1, 'a'),(2, 'b');

select t1.* from t1 join t2 on t1.id = t2.id;
select t1.* from t1 where t1.id in (1, 2);
{code}
The result do not match:
{code:}
spark-sql> select t1.* from t1 join t2 on t1.id = t2.id;
1	a
1	a
1	a
1	a
2	b
2	b
2	b
2	b
spark-sql> select t1.* from t1 where t1.id in (1, 2);
1	a
1	a
2	b
2	b
{code}

;;;
Comment.2: 12/Jul/22 12:36;devict;[~yumwang] 

The idea would be to filter down t1 before joining it to t2, such that:

{code:sql}
select t1.* from t1 join t2 on t1.id = t2.id
{code}

would become:

{code:sql}
select t1.* from (
    select * from t1 where t1.id IN (1, 2)
) as filtered_t1
join t2 on filtered_t1.id = t2.id
{code}

This allows the t1 table to be filtered down before doing the join, including pruning partitions and files to be read.;;;
Comment.3: 12/Jul/22 12:50;devict;We currently work around this by doing the following (using your example and Dataframes):
{code:scala}
val t2Materialized = t2.select(col("id")).collectAsList().map(_.getAs(0))
val filteredT1 = t1.filter(col("id").isInCollection(t2Materialized))
val joined = filteredT1.join(t2.hint("broadcast"), "id") 
{code};;;
Comment.4: 13/Jul/22 03:28;yumwang;[~devict] So the build side needs to be executed twice. ;;;
EMR Versions: EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark build fails in Windows
Issue key: SPARK-38114
Issue id: 13426695
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zauberer
Creator: zauberer
Created: 04/Feb/22 19:28
Updated: 21/Apr/23 21:52
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 1
Labels: 
Description: java.lang.NoSuchMethodError: org.fusesource.jansi.AnsiConsole.wrapOutputStream(Ljava/io/OutputStream;)Ljava/io/OutputStream;
jline.AnsiWindowsTerminal.detectAnsiSupport(AnsiWindowsTerminal.java:57)
jline.AnsiWindowsTerminal.<init>(AnsiWindowsTerminal.java:27)

 

A similar issue is being faced by the quarkus project with latest Maven. 

[https://github.com/quarkusio/quarkus/issues/19491]

 

Upgrading the scala-maven-plugin seems to resolve the issue but this ticket can be a blocker

https://issues.apache.org/jira/browse/SPARK-36547
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Apr 21 21:52:13 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z9u8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Jan/23 10:48;Penglei Shi;I  had the same problem. I think we should fix it.;;;, 21/Apr/23 21:52;felipepessoto;Hi, this seems a big issue. Anybody found a workaround?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 21/Apr/23 21:52;felipepessoto;Hi, this seems a big issue. Anybody found a workaround?;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Fix remote table location based on database location
Issue key: SPARK-39203
Issue id: 13445307
Parent id: 
Issue Type: Bug
Status: Reopened
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 17/May/22 03:25
Updated: 21/Apr/23 01:30
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 2.2.0, 2.3.0, 2.4.0, 3.0.0, 3.1.0, 3.1.1, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: We have HDFS and Hive on cluster A. We have Spark on cluster B and need to read data from cluster A. The table location is incorrect:
{noformat}
spark-sql> desc formatted  default.test_table;
fas_acct_id         	decimal(18,0)
fas_acct_cd         	string
cmpny_cd            	string
entity_id           	string
cre_date            	date
cre_user            	string
upd_date            	timestamp
upd_user            	string

# Detailed Table Information
Database             default
Table               	test_table
Type                	EXTERNAL
Provider            	parquet
Statistics          	25310025737 bytes
Location            	/user/hive/warehouse/test_table
Serde Library       	org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
InputFormat         	org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
OutputFormat        	org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
Storage Properties  	[compression=snappy]

spark-sql> desc database default;
Namespace Name	default
Comment
Location	viewfs://clusterA/user/hive/warehouse/
Owner     hive_dba
{noformat}

The correct table location should be viewfs://clusterA/user/hive/warehouse/test_table.
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Apr 21 01:30:10 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12f88:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/May/22 10:23;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36625;;;, 31/May/22 10:09;yumwang;Issue resolved by pull request 36625
[https://github.com/apache/spark/pull/36625];;;, 20/Oct/22 12:48;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/38321;;;, 20/Apr/23 09:11;githubbot;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40871;;;, 21/Apr/23 01:29;gurwls223;Reverted in https://github.com/apache/spark/pull/40871;;;, 21/Apr/23 01:30;gurwls223;But to be clear, this change exists in Spark 3.4.0.
It was taken out from 3.4.1 and 3.5.0.;;;
Affects Version/s.1: 2.3.0
Component/s.1: 
Comment.1: 31/May/22 10:09;yumwang;Issue resolved by pull request 36625
[https://github.com/apache/spark/pull/36625];;;
Comment.2: 20/Oct/22 12:48;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/38321;;;
Comment.3: 20/Apr/23 09:11;githubbot;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40871;;;
Comment.4: 21/Apr/23 01:29;gurwls223;Reverted in https://github.com/apache/spark/pull/40871;;;
EMR Versions: EMR-6.12.0, EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Support "ESCAPE BY" in SparkScriptTransformationExec
Issue key: SPARK-43218
Issue id: 13533398
Parent id: 
Issue Type: Wish
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jiang13021
Creator: jiang13021
Created: 20/Apr/23 13:23
Updated: 20/Apr/23 13:23
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: If I don't `set spark.sql.catalogImplementation=hive`, I can't use "SELECT TRANSFORM" with "ESCAPE BY". Although HiveScriptTransform also doesn't implement ESCAPE BY, I can use RowFormatSerde to achieve this ability.

 

In fact, HiveScriptTransform doesn't need to connect to Hive Metastore. I can use reflection to forcibly call HiveScriptTransformationExec without connecting to Hive Metastore, and it can work properly. Maybe HiveScriptTransform can be more generic.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-04-20 13:23:32.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1hf34:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: decom.sh can cause an UnsupportedOperationException
Issue key: SPARK-43175
Issue id: 13533049
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: icardnell
Creator: icardnell
Created: 18/Apr/23 13:12
Updated: 18/Apr/23 13:16
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: decom.sh can cause an UnsupportedOperationException which then causes the Executor to die with a SparkUncaughtException and does not complete the decommission properly.

 
*Problem:*
SignalUtils.scala line 124:
 
{code:java}
if (escalate) {
   prevHandler.handle(sig)
}{code}
 
 
*Logs:*
 
{noformat}
failed - error: command '/opt/decom.sh' exited with 137: + echo 'Asked to decommission' + date + tee -a ++ ps -o pid -C java ++ awk '{ sub(/^[ \t]+/, """"); print }' ++ tail -n 1 + WORKER_PID=17 + echo 'Using worker pid 17' + kill -s SIGPWR 17 + echo 'Waiting for worker pid to exit' + timeout 60 tail --pid=17 -f /dev/null , message: ""Asked to decommission\nMon Apr 17 23:44:35 UTC 2023\nUsing worker pid 17\nWaiting for worker pid to exit\n+ echo 'Asked to decommission'\n+ date\n+ tee -a\n++ ps -o pid -C java\n++ awk '{ sub(/^[ \\t]+/, \""\""); print }'\n++ tail -n 1\n+ WORKER_PID=17\n+ echo 'Using worker pid 17'\n+ kill -s SIGPWR 17\n+ echo 'Waiting for worker pid to exit'\n+ timeout 60 tail --pid=17 -f /dev/null\n""",2023-04-17T23:44:39Z,
"java.lang.UnsupportedOperationException: invoking native signal handle not supported
 at java.base/jdk.internal.misc.Signal$NativeHandler.handle(Unknown Source)
 at jdk.unsupported/sun.misc.Signal$SunMiscHandler.handle(Unknown Source)
 at org.apache.spark.util.SignalUtils$ActionHandler.handle(SignalUtils.scala:124)
 at jdk.unsupported/sun.misc.Signal$InternalMiscHandler.handle(Unknown Source)
 at java.base/jdk.internal.misc.Signal$1.run(Unknown Source) at java.base/java.lang.Thread.run(Unknown Source)",2023-04-17T23:44:35.407488217Z "2023-04-17 23:44:35
[SIGPWR handler] ERROR org.apache.spark.util.SparkUncaughtExceptionHandler - Uncaught exception in thread Thread[SIGPWR handler,9,system] - {}",2023-04-17T23:44:35.407457859Z
 " ... 1 more",2023-04-17T23:44:35.405548994Z "
 at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)",2023-04-17T23:44:35.405542621Z "
 at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)",2023-04-17T23:44:35.405536674Z "
 at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)",2023-04-17T23:44:35.405516396Z "
 at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)",2023-04-17T23:44:35.405416352Z "
 at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)",2023-04-17T23:44:35.405410491Z "
...
 at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)",2023-04-17T23:44:35.405262304Z "
 at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)",2023-04-17T23:44:35.405256591Z "
 at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:209)",2023-04-17T23:44:35.405250814Z{noformat}
 

In this case prevHandler is the NativeHandler (See [https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/19fb8f93c59dfd791f62d41f332db9e306bc1422/src/java.base/share/classes/jdk/internal/misc/Signal.java#L280]) and it throws the exception.

*Possible Solutions:*

 * Check if prevHandler is an instance of NativeHandler and do not call it in that case.
 * try catch around the invoke of the handler and log a warning/error on exceptions.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-04-18 13:12:38.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1hcxk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in org.apache.spark.ui
Issue key: SPARK-38478
Issue id: 13432771
Parent id: 13423097.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bozhang
Creator: bozhang
Created: 09/Mar/22 05:20
Updated: 02/Apr/23 03:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Apr 02 03:50:01 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10b48:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Apr/23 08:37;Wencong Liu;I'd like to take this ticket, would you like to assign it to me? [~bozhang] ;;;, 02/Apr/23 03:50;bozhang;Thanks! [~Wencong Liu] please feel free to submit a PR for this.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 02/Apr/23 03:50;bozhang;Thanks! [~Wencong Liu] please feel free to submit a PR for this.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Union does not propagate Metadata output
Issue key: SPARK-41498
Issue id: 13513183
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: fred-db
Creator: fred-db
Created: 12/Dec/22 12:41
Updated: 14/Mar/23 15:39
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.3.0, 3.3.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, the Union operator does not propagate any metadata output. This makes it impossible to access any metadata if a Union operator is used, even though the children have the exact same metadata output.
Example:

 
{code:java}
val df1 = spark.read.load(path1)
val df2 = spark.read.load(path2)
df1.union(df2).select("_metadata.file_path"). // <-- fails{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 14 15:39:03 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1dyy8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Dec/22 12:47;apachespark;User 'fred-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/38941;;;, 23/Dec/22 11:39;cloud_fan;Issue resolved by pull request 38941
[https://github.com/apache/spark/pull/38941];;;, 10/Mar/23 16:09;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40371;;;, 10/Mar/23 18:31;dongjoon;This is reverted via https://github.com/apache/spark/commit/164db5ba3c39614017f5ef6428194a442d79b425;;;, 14/Mar/23 15:39;dongjoon;I removed the `Target Version`.;;;
Affects Version/s.1: 3.1.3
Component/s.1: 
Comment.1: 23/Dec/22 11:39;cloud_fan;Issue resolved by pull request 38941
[https://github.com/apache/spark/pull/38941];;;
Comment.2: 10/Mar/23 16:09;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40371;;;
Comment.3: 10/Mar/23 18:31;dongjoon;This is reverted via https://github.com/apache/spark/commit/164db5ba3c39614017f5ef6428194a442d79b425;;;
Comment.4: 14/Mar/23 15:39;dongjoon;I removed the `Target Version`.;;;
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: current_user() is blocked from VALUES, but current_timestamp() is not
Issue key: SPARK-42638
Issue id: 13526747
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: srielau
Creator: srielau
Created: 01/Mar/23 20:30
Updated: 04/Mar/23 06:26
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: VALUES(current_user());
returns:

cannot evaluate expression current_user() in inline table definition.; line 1 pos 8

 

The same with current_timestamp() works.

It appears current_user() is recognized as non-deterministic. But it is constant within the statement, just like current_timestanmp().

PS: It's not clear why we block non-deterministic functions to begin with....
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Mar 04 06:26:01 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ga4o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Mar/23 06:26;Zing;Maybe we can use `insert as select` to achieve the same effect?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Aggregate should be group only after collapse project to aggregate
Issue key: SPARK-40159
Issue id: 13477792
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wankun
Creator: wankun
Created: 21/Aug/22 03:26
Updated: 01/Mar/23 21:33
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: CollapseProject rule will merge project expressions into AggregateExpressions in aggregate, which will make the *aggregate.groupOnly* to false.

{code}
val df = testData.distinct().select('key + 1, ('key + 1).cast("long"))
df.queryExecution.optimizedPlan.collect {
  case a: Aggregate => a
}.foreach(agg => assert(agg.groupOnly === true)) 
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 01 21:33:03 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17x9s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Aug/22 03:39;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/37594;;;, 21/Aug/22 03:39;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/37594;;;, 01/Mar/23 21:33;ritikam;This issue seems to have been resolved by SPARK-38489;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 21/Aug/22 03:39;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/37594;;;
Comment.2: 01/Mar/23 21:33;ritikam;This issue seems to have been resolved by SPARK-38489;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support fetching shuffle blocks in batch with i/o encryption
Issue key: SPARK-34827
Issue id: 13366816
Parent id: 13407393.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dongjoon
Creator: dongjoon
Created: 22/Mar/21 20:34
Updated: 01/Mar/23 17:36
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-34790
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 01 17:36:19 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0p26o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Aug/21 04:41;Gengliang.Wang;[~dongjoon] I plan to cut 3.2 RC1 next week. What is the status of this one?  Do we need to have it in Spark 3.2?;;;, 10/Aug/21 18:30;dongjoon;AFAIK, this is not resolved yet. SPARK-34790 simply disabled fetching shuffle blocks in batch when io encryption is enabled .
I guess we can mention this as a known issue of Apache Spark 3.2.0 release note and move the `Target Version` of this to `3.3.0`.

What do you think about that, [~Gengliang.Wang] and [~cloud_fan]?;;;, 10/Aug/21 18:31;dongjoon;Also, cc [~Mridul] and [~tgraves] and [~viirya], too.;;;, 11/Aug/21 03:18;Gengliang.Wang;+1, Thank you [~dongjoon];;;, 11/Aug/21 06:55;cloud_fan;Marking as a known issue LGTM. This is not a blocker to me as it only affects performance, not a correctness issue.;;;, 11/Aug/21 19:12;dongjoon;Thank you. I moved the Target Version to 3.3.0.;;;, 01/Mar/23 15:31;apachespark;User 'tomvanbussel' has created a pull request for this issue:
https://github.com/apache/spark/pull/40234;;;, 01/Mar/23 17:35;dongjoon;I removed `Target Version` field value, `3.3.0`, because it's outdated and invalid. We will ship Apache Spark 3.4.0 without this.;;;, 01/Mar/23 17:36;dongjoon;Also, I lower the priority from `Blocker` to `Major` because this is only a perf issue.;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 10/Aug/21 18:30;dongjoon;AFAIK, this is not resolved yet. SPARK-34790 simply disabled fetching shuffle blocks in batch when io encryption is enabled .
I guess we can mention this as a known issue of Apache Spark 3.2.0 release note and move the `Target Version` of this to `3.3.0`.

What do you think about that, [~Gengliang.Wang] and [~cloud_fan]?;;;
Comment.2: 10/Aug/21 18:31;dongjoon;Also, cc [~Mridul] and [~tgraves] and [~viirya], too.;;;
Comment.3: 11/Aug/21 03:18;Gengliang.Wang;+1, Thank you [~dongjoon];;;
Comment.4: 11/Aug/21 06:55;cloud_fan;Marking as a known issue LGTM. This is not a blocker to me as it only affects performance, not a correctness issue.;;;
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: add named_struct to spark scala api for parity to spark sql
Issue key: SPARK-40679
Issue id: 13484765
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: douglas.moore@databricks.com
Creator: douglas.moore@databricks.com
Created: 06/Oct/22 13:24
Updated: 28/Feb/23 22:13
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core, SQL
Due Date: 
Votes: 0
Labels: 
Description: To facilitate migration from cast(struct(.... to `named_struct` where comparison of structures is needed.

To maintain parity between APIs. Understand that using `expr` would work, but it wouldn't be typed.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Feb 28 22:13:06 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z193w0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Feb/23 22:13;planga82;I think the original development supports the creation of NamedStructs in the dataframe interface.

[https://github.com/apache/spark/pull/6874/files]

 
{code:java}
/**
   * Creates a new struct column.
   * If the input column is a column in a [[DataFrame]], or a derived column expression
   * that is named (i.e. aliased), its name would be remained as the StructField's name,
   * otherwise, the newly generated StructField's name would be auto generated as col${index + 1},
   * i.e. col1, col2, col3, ...
*/
def struct(cols: Column*): Column = {
...{code}
 
{code:java}
test("struct with column expression to be automatically named") {
    val df = Seq((1, "str")).toDF("a", "b")
    val result = df.select(struct((col("a") * 2), col("b")))
    val expectedType = StructType(Seq(
      StructField("col1", IntegerType, nullable = false),
      StructField("b", StringType)
    ))
    assert(result.first.schema(0).dataType === expectedType)
    checkAnswer(result, Row(Row(2, "str")))
  } 
{code}
 

 

 ;;;
Affects Version/s.1: 
Component/s.1: SQL
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Protobuf: Generate descriptor files at build time
Issue key: SPARK-40848
Issue id: 13487158
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: rangadi
Creator: rangadi
Created: 19/Oct/22 19:43
Updated: 27/Feb/23 18:15
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Protobuf
Due Date: 
Votes: 0
Labels: 
Description: Generate descriptor files during the build rather than pre-creating them. 

[~rangadi] will do this. 

cc: [~sanysandish@gmail.com] 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): SPARK-42606
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-10-19 19:43:53.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19iio:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark MasterWebUI and WorkerWebUI fail to start when NSSDB used as keystore, getting java.security.KeyStoreException: PKCS11 not found.
Issue key: SPARK-42511
Issue id: 13525562
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: sshukla05
Creator: sshukla05
Created: 21/Feb/23 13:13
Updated: 21/Feb/23 13:24
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.3.1, 3.3.2
Fix Version/s: 
Component/s: Spark Core, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: When we are running Spark by setting up below SSL configurations, Spark masterwebui and workerwebui is fail to start.
{quote}
        "spark.ssl.enabled":"true",
        "spark.ssl.keyStore":"/opt/ibm/jdk/conf/security/nss.fips.cfg",
        "spark.ssl.keyStorePassword":"<keystore passwd>",
        "spark.ssl.keyStoreType":"PKCS11"
{quote}

*Errors :*

{quote}23/02/21 12:29:43 INFO Master: Running Spark version 3.3.1
23/02/21 12:29:43 ERROR MasterWebUI: Failed to bind MasterWebUI
java.security.KeyStoreException: PKCS11 not found
	at java.base/java.security.KeyStore.getInstance(KeyStore.java:878)
	at org.sparkproject.jetty.util.security.CertificateUtils.getKeyStore(CertificateUtils.java:46)
	at org.sparkproject.jetty.util.ssl.SslContextFactory.loadKeyStore(SslContextFactory.java:1203)
	at org.sparkproject.jetty.util.ssl.SslContextFactory.load(SslContextFactory.java:322)
	at org.sparkproject.jetty.util.ssl.SslContextFactory.doStart(SslContextFactory.java:244)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
	at org.sparkproject.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)
	at org.sparkproject.jetty.server.SslConnectionFactory.doStart(SslConnectionFactory.java:97)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.sparkproject.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)
	at org.sparkproject.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117)
	at org.sparkproject.jetty.server.AbstractConnector.doStart(AbstractConnector.java:323)
	at org.sparkproject.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)
	at org.sparkproject.jetty.server.ServerConnector.doStart(ServerConnector.java:234)
	at org.sparkproject.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.spark.ui.JettyUtils$.newConnector$1(JettyUtils.scala:303)
	at org.apache.spark.ui.JettyUtils$.sslConnect$1(JettyUtils.scala:322)
	at org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$4(JettyUtils.scala:326)
	at org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$4$adapted(JettyUtils.scala:326)
	at org.apache.spark.util.Utils$.$anonfun$startServiceOnPort$2(Utils.scala:2401)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:2393)
	at org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$2(JettyUtils.scala:326)
	at org.apache.spark.ui.JettyUtils$.$anonfun$startJettyServer$2$adapted(JettyUtils.scala:315)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:315)
	at org.apache.spark.ui.WebUI.initServer(WebUI.scala:144)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:153)
	at org.apache.spark.deploy.master.Master.onStart(Master.scala:138)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:120)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:839)
Caused by: java.security.NoSuchAlgorithmException: PKCS11 KeyStore not available
	at java.base/sun.security.jca.GetInstance.getInstance(GetInstance.java:159)
	at java.base/java.security.Security.getImpl(Security.java:719)
	at java.base/java.security.KeyStore.getInstance(KeyStore.java:875)
	... 37 more
{quote}

content of nss fips config file.

{quote}name = NSS-FIPS
nssLibraryDirectory = /usr/lib64
nssSecmodDirectory = /etc/pki/nssdb
nssDbMode = readOnly
nssModule = fips

attributes(*,CKO_SECRET_KEY,CKK_GENERIC_SECRET)={ CKA_SIGN=true }{quote}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Feb 21 13:19:26 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1g2u0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Feb/23 13:19;sshukla05;setting up below options as well in spark submit command.
{quote}--conf spark.driver.extraJavaOptions= -Dderby.system.home=/.local/share/jupyter/runtime/kernel-21f4c67c-bcf6-4600-96dc-e59510692c6a-20230221_122939 -Dlog4j.logFile=/home/spark/shared/logs/kernel-python3.10-python3.10-20230221_122939.log -Dlog4j.configuration=file:/opt/ibm/jkg/log4j/log4j.properties -Dsemeru.fips=true -Djavax.net.ssl.keyStore=/opt/ibm/jdk/conf/security/nss.fips.cfg -Djavax.net.ssl.keyStorePassword=changeit -Djavax.net.ssl.keyStoreType=PKCS11 -Dfile.encoding=UTF-8

--conf spark.executor.extraJavaOptions= -Dderby.system.home=/.local/share/jupyter/runtime/kernel-21f4c67c-bcf6-4600-96dc-e59510692c6a-20230221_122939 -Dlog4j.logFile=/home/spark/shared/logs/kernel-python3.10-python3.10-20230221_122939.log -Dlog4j.configuration=file:/opt/ibm/jkg/log4j/log4j.properties -Dsemeru.fips=true -Djavax.net.ssl.keyStore=/opt/ibm/jdk/conf/security/nss.fips.cfg -Djavax.net.ssl.keyStorePassword=changeit -Djavax.net.ssl.keyStoreType=PKCS11 -Dfile.encoding=UTF-8
{quote};;;
Affects Version/s.1: 3.3.1
Component/s.1: Spark Submit
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: dataset.where() omit quotes if where IN clause has more than 10 operands
Issue key: SPARK-42450
Issue id: 13524873
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ebious
Creator: ebious
Created: 15/Feb/23 15:43
Updated: 16/Feb/23 11:45
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: dataset.where()/filter() omit string quotes if where IN clause has more than 10 operands. With datasourceV1 works as expected. 
Attached files: java-code.txt, stacktrace.txt, sql.txt
 - Spark verison 3.3.0
 - Scala version 2.12
 - DatasourceV2
 - Postgres
 - Postrgres JDBC Driver: 42+
 - Java8
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 15/Feb/23 15:43;ebious;java-code.txt;https://issues.apache.org/jira/secure/attachment/13055475/java-code.txt, 15/Feb/23 15:49;ebious;sql.txt;https://issues.apache.org/jira/secure/attachment/13055477/sql.txt, 15/Feb/23 15:43;ebious;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/13055476/stacktrace.txt
Custom field (Affects version (Component)): 
Custom field (Attachment count): 3.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-02-15 15:43:10.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1fyl4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Data Source V2: Remove read specific distributions
Issue key: SPARK-33807
Issue id: 13346333
Parent id: 13112874.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: aokolnychyi
Creator: aokolnychyi
Created: 16/Dec/20 10:33
Updated: 08/Feb/23 04:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: We should remove the read-specific distributions for DS V2 as discussed [here|https://github.com/apache/spark/pull/30706#discussion_r543059827].
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): SPARK-37377
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Feb 08 04:05:49 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0lkfk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Aug/21 05:00;Gengliang.Wang;[~aokolnychyi] I plan to cut 3.2 RC1 next week. What is the status of this one? Do we need to have it in Spark 3.2?;;;, 12/Aug/21 06:58;aokolnychyi;I don't think it should be a blocker. We created this issue during the review but it is up to Wenchen to decide here. I'd say it is not a blocker and we better do it in 3.3 as there is an ongoing design discussion for bucketed joins that may affect this ticket too.;;;, 12/Aug/21 06:58;aokolnychyi;Sorry for the delay commenting!;;;, 16/Aug/21 14:53;Gengliang.Wang;[~cloud_fan]what do you think?;;;, 16/Aug/21 14:56;cloud_fan;Sounds fine to me;;;, 08/Feb/23 03:29;dongjoon;According to the discussion, I lowered the `Priority` from `Blocker` to `Major`.;;;, 08/Feb/23 04:05;csun;This is actually already resolved as part of SPARK-37377.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 12/Aug/21 06:58;aokolnychyi;I don't think it should be a blocker. We created this issue during the review but it is up to Wenchen to decide here. I'd say it is not a blocker and we better do it in 3.3 as there is an ongoing design discussion for bucketed joins that may affect this ticket too.;;;
Comment.2: 12/Aug/21 06:58;aokolnychyi;Sorry for the delay commenting!;;;
Comment.3: 16/Aug/21 14:53;Gengliang.Wang;[~cloud_fan]what do you think?;;;
Comment.4: 16/Aug/21 14:56;cloud_fan;Sounds fine to me;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use Hive and Spark SQL to modify table field comment, the modified results of Hive cannot be queried using Spark SQL
Issue key: SPARK-41241
Issue id: 13505572
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: haoweiliang
Creator: haoweiliang
Created: 23/Nov/22 16:00
Updated: 01/Feb/23 11:36
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.0.0, 3.1.0, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: -- Hive

> create table table_test(id int);

> alter table table_test change column id id int comment "hive comment";

> desc formatted table_test;
{code:java}
+-------------------------------+----------------------------------------------------+----------------------------------------------------+
|           col_name            |                     data_type                      |                      comment                       |
+-------------------------------+----------------------------------------------------+----------------------------------------------------+
| # col_name                    | data_type                                          | comment                                            |
| id                            | int                                                | hive comment                                        |
|                               | NULL                                               | NULL                                               |
| # Detailed Table Information  | NULL                                               | NULL                                               |
| Database:                     | default                                            | NULL                                               |
| OwnerType:                    | USER                                               | NULL                                               |
| Owner:                        | anonymous                                          | NULL                                               |
| CreateTime:                   | Wed Nov 23 23:06:41 CST 2022                       | NULL                                               |
| LastAccessTime:               | UNKNOWN                                            | NULL                                               |
| Retention:                    | 0                                                  | NULL                                               |
| Location:                     | hdfs://localhost:8020/warehouse/tablespace/managed/hive/table_test | NULL                                               |
| Table Type:                   | MANAGED_TABLE                                      | NULL                                               |
| Table Parameters:             | NULL                                               | NULL                                               |
|                               | COLUMN_STATS_ACCURATE                              | {\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"id\":\"true\"}} |
|                               | bucketing_version                                  | 2                                                  |
|                               | last_modified_by                                   | anonymous                                          |
|                               | last_modified_time                                 | 1669216665                                         |
|                               | numFiles                                           | 0                                                  |
|                               | numRows                                            | 0                                                  |
|                               | rawDataSize                                        | 0                                                  |
|                               | totalSize                                          | 0                                                  |
|                               | transactional                                      | true                                               |
|                               | transactional_properties                           | default                                            |
|                               | transient_lastDdlTime                              | 1669216665                                         |
|                               | NULL                                               | NULL                                               |
| # Storage Information         | NULL                                               | NULL                                               |
| SerDe Library:                | org.apache.hadoop.hive.ql.io.orc.OrcSerde          | NULL                                               |
| InputFormat:                  | org.apache.hadoop.hive.ql.io.orc.OrcInputFormat    | NULL                                               |
| OutputFormat:                 | org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat   | NULL                                               |
| Compressed:                   | No                                                 | NULL                                               |
| Num Buckets:                  | -1                                                 | NULL                                               |
| Bucket Columns:               | []                                                 | NULL                                               |
| Sort Columns:                 | []                                                 | NULL                                               |
| Storage Desc Params:          | NULL                                               | NULL                                               |
|                               | serialization.format                               | 1                                                  |
+-------------------------------+----------------------------------------------------+----------------------------------------------------+ {code}
-- Spark SQL

> alter table table_test change column id id int comment "spark comment";

> desc formatted table_test;
{code:java}
+-------------------------------+----------------------------------------------------+--------------+
|           col_name            |                     data_type                      |   comment    |
+-------------------------------+----------------------------------------------------+--------------+
| id                            | int                                                | spark comment  |
|                               |                                                    |              |
| # Detailed Table Information  |                                                    |              |
| Catalog                       | spark_catalog                                      |              |
| Database                      | default                                            |              |
| Table                         | table_test                                         |              |
| Owner                         | anonymous                                          |              |
| Created Time                  | Wed Nov 23 23:06:41 CST 2022                       |              |
| Last Access                   | UNKNOWN                                            |              |
| Created By                    | Spark 2.2 or prior                                 |              |
| Type                          | MANAGED                                            |              |
| Provider                      | hive                                               |              |
| Table Properties              | [bucketing_version=2, last_modified_by=anonymous, last_modified_time=1669216665, transactional=true, transactional_properties=default, transient_lastDdlTime=1669216711] |              |
| Location                      | hdfs://localhost:8020/warehouse/tablespace/managed/hive/table_test |              |
| Serde Library                 | org.apache.hadoop.hive.ql.io.orc.OrcSerde          |              |
| InputFormat                   | org.apache.hadoop.hive.ql.io.orc.OrcInputFormat    |              |
| OutputFormat                  | org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat   |              |
| Storage Properties            | [serialization.format=1]                           |              |
| Partition Provider            | Catalog                                            |              | {code}
-- Hive

> alter table table_test change column id id int comment "hive new comment";

 

-- Spark SQL

> desc formatted table_test;
{code:java}
+-------------------------------+----------------------------------------------------+--------------+
|           col_name            |                     data_type                      |   comment    |
+-------------------------------+----------------------------------------------------+--------------+
| id                            | int                                                | spark comment  |
|                               |                                                    |              |
| # Detailed Table Information  |                                                    |              |
| Catalog                       | spark_catalog                                      |              |
| Database                      | default                                            |              |
| Table                         | table_test                                         |              |
| Owner                         | anonymous                                          |              |
| Created Time                  | Wed Nov 23 23:06:41 CST 2022                       |              |
| Last Access                   | UNKNOWN                                            |              |
| Created By                    | Spark 2.2 or prior                                 |              |
| Type                          | MANAGED                                            |              |
| Provider                      | hive                                               |              |
| Table Properties              | [bucketing_version=2, last_modified_by=anonymous, last_modified_time=1669216736, transactional=true, transactional_properties=default, transient_lastDdlTime=1669216736] |              |
| Location                      | hdfs://localhost:8020/warehouse/tablespace/managed/hive/table_test |              |
| Serde Library                 | org.apache.hadoop.hive.ql.io.orc.OrcSerde          |              |
| InputFormat                   | org.apache.hadoop.hive.ql.io.orc.OrcInputFormat    |              |
| OutputFormat                  | org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat   |              |
| Storage Properties            | [serialization.format=1]                           |              |
| Partition Provider            | Catalog                                            |              |
+-------------------------------+----------------------------------------------------+--------------+ {code}
 

Alternately modify table field comments with hive and spark，the modified results of hive cannot be queried using spark sql

 

 

 
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Dec 05 07:22:19 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1co08:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Nov/22 16:01;haoweiliang;I will fix it;;;, 29/Nov/22 00:21;xkrogen;I think, as a general rule, it's not safe to modify Spark datasource tables using Hive. Spark stores a bunch of extra (Spark-specific) metadata in the properties of the table, and Hive will not modify this metadata since it is not aware of it. So I would consider this behavior to be expected?;;;, 05/Dec/22 07:22;haoweiliang;[~xkrogen] The problem is that Spark modifies the Hive table field comment, and then uses Hive to modify, Spark cannot find the latest comment. I think Spark should be compatible with Hive, and there should be no data inconsistency when using the Spark or Hive engine to query.;;;
Affects Version/s.1: 3.1.0
Component/s.1: 
Comment.1: 29/Nov/22 00:21;xkrogen;I think, as a general rule, it's not safe to modify Spark datasource tables using Hive. Spark stores a bunch of extra (Spark-specific) metadata in the properties of the table, and Hive will not modify this metadata since it is not aware of it. So I would consider this behavior to be expected?;;;
Comment.2: 05/Dec/22 07:22;haoweiliang;[~xkrogen] The problem is that Spark modifies the Hive table field comment, and then uses Hive to modify, Spark cannot find the latest comment. I think Spark should be compatible with Hive, and there should be no data inconsistency when using the Spark or Hive engine to query.;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Implement lazy materialization for the vectorized Parquet reader
Issue key: SPARK-36527
Issue id: 13395525
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: csun
Creator: csun
Created: 16/Aug/21 18:17
Updated: 31/Jan/23 19:22
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: At the moment the Parquet vectorized reader will eagerly decode all the columns that are in the read schema, before any filter has been applied to them. This is costly. Instead it's better to only materialize these column vectors when the data are actually needed.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-35743
Outward issue link (Reference): SPARK-25643, SPARK-42256
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-16 18:17:40.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tye0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Update embedded vis-timeline javascript resources
Issue key: SPARK-40684
Issue id: 13484873
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: apurtell
Creator: apurtell
Created: 06/Oct/22 20:55
Updated: 30/Jan/23 20:49
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark 3.3 currently ships with embedded vis-timeline javascript resources subject to CVE-2020-28487, detected as a minor problem by several static vulnerability assessment tools.

https://nvd.nist.gov/vuln/detail/CVE-2020-28487: 
bq. This affects the package vis-timeline before 7.4.4. An attacker with the ability to control the items of a Timeline element can inject additional script code into the generated application.

This issue is not meant to imply a security problem in Spark itself. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jan 30 20:49:58 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z194k0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Jan/23 20:49;ess-truveta;I think I filed [SPARK-39740] vis-timeline @ 4.2.1 vulnerable to XSS attacks - ASF JIRA (apache.org) for the same issue, but haven't seen any updates on that ticket either.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Make "SPARK-34674: Close SparkContext after the Main method has finished" configurable
Issue key: SPARK-42219
Issue id: 13521805
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: attilapiros
Reporter: attilapiros
Creator: attilapiros
Created: 27/Jan/23 19:59
Updated: 27/Jan/23 21:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.3.0, 3.3.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: We run into an error after an upgrade from Spark 3.1 to Spark 3.2 cased by SPARK-34674 which closed the SparkContext right after the application start. This application was a spark job server built on top of springboot so all the job submits were outside of the main method.


Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jan 27 21:48:24 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ffrc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Jan/23 20:18;attilapiros;I will open a PR regarding this;;;, 27/Jan/23 21:47;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/39775;;;, 27/Jan/23 21:48;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/39775;;;
Affects Version/s.1: 3.1.3
Component/s.1: 
Comment.1: 27/Jan/23 21:47;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/39775;;;
Comment.2: 27/Jan/23 21:48;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/39775;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Spark 3.3.0 binary breaking change missing from release notes
Issue key: SPARK-42107
Issue id: 13520022
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: rozza
Creator: rozza
Created: 18/Jan/23 10:28
Updated: 25/Jan/23 01:01
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Java API
Due Date: 
Votes: 0
Labels: 
Description: SPARK-37929 contains a binary breaking change in the SupportsNamespaces API


See: [https://github.com/apache/spark/pull/35246/files#r792289685]

 

There is no mention in the [release notes|https://spark.apache.org/releases/spark-release-3-3-0.html]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jan 25 01:01:05 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1f4tk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Jan/23 01:01;gurwls223;cc [~cloud_fan] and [~dchvn]. I think we should at least add them into release notes.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Files added to the spark-submit command with master K8s and deploy mode cluster, end up in a non deterministic location inside the driver.
Issue key: SPARK-42170
Issue id: 13521128
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: santosh.pingale
Creator: santosh.pingale
Created: 24/Jan/23 14:24
Updated: 24/Jan/23 14:25
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.2, 3.3.0
Fix Version/s: 
Component/s: Kubernetes, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: Files added to the spark-submit command with master K8s and deploy mode cluster, end up in a non deterministic location inside the driver.

eg:

{{spark-submit --files myfile --master k8s.. --deploy-mode cluster` will upload the files to /tmp/spark-uuid/myfile}}

The issue happens because [Utils.createTempDir()|https://github.com/apache/spark/blob/v3.3.1/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L344] creates a directory with a uuid in the directory name. This issue does not affect the --archives option, because we `unarchive` the archives into the destination directory which is relative to the working dir. This bug affects file access pre & post app creation. For example if we distribute python dependencies with pex, we need to use --files to attach the pex file and change the spark.pyspark.python to point to this file. But the file location can not be determined before submitting the app. On the other hand, after the app is created, referencing the files without using `SparkFiles.get` also does not work
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-01-24 14:24:42.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1fblk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.3.0
Component/s.1: Spark Submit
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: ALTER TABLE .. REPLACE COLUMNS works incorrectly for v2 tables in tests
Issue key: SPARK-37303
Issue id: 13411462
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: maxgekk
Creator: maxgekk
Created: 12/Nov/21 10:51
Updated: 07/Jan/23 15:01
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The test demonstrates the issue:

{code:scala}
  test("Replace columns in v2 table") {
    withNamespaceAndTable("ns", "tbl") { t =>
      sql(s"CREATE TABLE $t (i INT, data STRING) $defaultUsing")
      sql(s"INSERT INTO $t SELECT 0, 'abc'")
      sql(
        s"""
           |ALTER TABLE $t REPLACE COLUMNS (
           | new_i STRING,
           | new_data LONG)""".stripMargin)
      sql(s"INSERT INTO $t SELECT 'def', 1000")

      checkAnswer(
        sql(s"SELECT new_i, new_data FROM $t"),
        Seq(Row("def", 1000)))
    }
  }
{code}

The test fails:
{code:java}
== Results ==
!== Correct Answer - 1 ==   == Spark Answer - 2 ==
!struct<>                   struct<new_i:string,new_data:bigint>
![def,1000]                 [,103079215107]
!                           [def,1000]
{code}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Jan 07 15:01:52 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wojk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Jan/23 15:01;apachespark;User 'smallzhongfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/39447;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Decouple CPU with IO work in vectorized Parquet reader
Issue key: SPARK-36529
Issue id: 13395531
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: csun
Creator: csun
Created: 16/Aug/21 18:26
Updated: 06/Jan/23 22:27
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently it seems the vectorized Parquet reader does almost everything in a sequential manner:
1. read the row group using file system API (perhaps from remote storage like S3)
2. allocate buffers and store those row group bytes into them
3. decompress the data pages
4. in Spark, decode all the read columns one by one
5. read the next row group and repeat from 1.

A lot of improvements can be done to decouple the IO and CPU intensive work. In addition, we could parallelize the row group loading and column decoding, and utilizing all the cores available for a Spark task.

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-35743
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Oct 08 12:38:14 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tyfc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Oct/21 12:38;stevel@apache.org;If you look at HADOOP-11867 / https://github.com/apache/hadoop/pull/3499 

We are adding a vectored read API to the FSDataInputStream with

* async fetch of different blocks
* order of return == "when the data comes back"
* read into bytebuffer
* caller provides their own bytebuffer factory

Will intially ship with
* base implementation to reorder/coalesce reads
* local FS to use native IO byte buffer reads

For the s3a and abfs object stores, our plan is to coalesce nearby ranges into aggregate ones, then issue multiple ranged GET requests in parallel. If/when the stores support multiple ranges in a GET, we could be even more efficient.

Please have a look @ the API and
1.  See if it will work with your code. Owen's clearly wrote knowing how ORC would make use of it.
1. try to make what you add now be able to support the API when spark is built against a version of hadoop with the API.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Implement lazy decoding for the vectorized Parquet reader
Issue key: SPARK-36528
Issue id: 13395527
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: csun
Creator: csun
Created: 16/Aug/21 18:20
Updated: 06/Jan/23 22:27
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently Spark first decode (e.g., RLE/bit-packed, PLAIN) into column vector and then operate on the decoded data. However, it may be more efficient to directly operate on encoded data, for instance, performing filter or aggregation on RLE-encoded data, or performing comparison over dictionary-encoded string data. This can also potentially work with encodings in Parquet v2 format, such as DELTA_BYTE_ARRAY.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-35743
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-16 18:20:12.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tyeg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: PySpark installation doesn't support Scala 2.13 binaries
Issue key: SPARK-39995
Issue id: 13475524
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: oshevchenko
Creator: oshevchenko
Created: 06/Aug/22 09:08
Updated: 03/Jan/23 03:23
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: [PyPi|https://pypi.org/project/pyspark/] doesn't support Spark binary [installation|https://spark.apache.org/docs/latest/api/python/getting_started/install.html#using-pypi] for Scala 2.13.
Currently, the setup [script|https://github.com/apache/spark/blob/master/python/pyspark/install.py] allows to set versions of Spark, Hadoop (PYSPARK_HADOOP_VERSION), and mirror (PYSPARK_RELEASE_MIRROR) to download needed Spark binaries, but it's always Scala 2.12 compatible binaries. There isn't any parameter to download "spark-3.3.0-bin-hadoop3-scala2.13.tgz".
It's possible to download Spark manually and set the needed SPARK_HOME, but it's hard to use with pip or Poetry.
Also, env vars (e.g. PYSPARK_HADOOP_VERSION) are easy to use with pip and CLI but not possible with package managers like Poetry.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jan 03 03:23:47 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17jcw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Aug/22 04:51;gurwls223;Yeah, I think we should make it available.;;;, 08/Aug/22 12:04;oshevchenko;Thanks [~hyukjin.kwon] for your reply. 
What do you think about support for package managers like [Poetry|https://python-poetry.org/] ? 
Is it possible to add parameters or add scala version into the package name to be able to install Spark with 2.13 since package managers don't support using env vars to configure it?;;;, 10/Aug/22 00:50;gurwls223;It's good to have but I think we should better let the community maintain it as a secondary release channel for now.;;;, 31/Aug/22 17:46;srowen;Would scala version generally matter to python users who would download from Pypi? I envision they're doing something local and probably never care about the JVM side;;;, 05/Sep/22 09:36;oshevchenko;It definitely matters. It impacts the dependencies/packages we can use (e.g. DataSourceV2 API implementation for read and write). It impacts DX (Developer Experience) and installation including CD process for our code.;;;, 03/Jan/23 03:23;gurwls223;For:

{quote}
Also, env vars (e.g. PYSPARK_HADOOP_VERSION) are easy to use with pip and CLI but not possible with package managers like Poetry.
{quote}

We can't do this because of the issue in pip itself, see SPARK-32837;;;, 03/Jan/23 03:23;gurwls223;I think i will be able to pick this up before Spark 3.4.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 08/Aug/22 12:04;oshevchenko;Thanks [~hyukjin.kwon] for your reply. 
What do you think about support for package managers like [Poetry|https://python-poetry.org/] ? 
Is it possible to add parameters or add scala version into the package name to be able to install Spark with 2.13 since package managers don't support using env vars to configure it?;;;
Comment.2: 10/Aug/22 00:50;gurwls223;It's good to have but I think we should better let the community maintain it as a secondary release channel for now.;;;
Comment.3: 31/Aug/22 17:46;srowen;Would scala version generally matter to python users who would download from Pypi? I envision they're doing something local and probably never care about the JVM side;;;
Comment.4: 05/Sep/22 09:36;oshevchenko;It definitely matters. It impacts the dependencies/packages we can use (e.g. DataSourceV2 API implementation for read and write). It impacts DX (Developer Experience) and installation including CD process for our code.;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add the ability to create pvc before creating driver/executor pod
Issue key: SPARK-41781
Issue id: 13515975
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dcoliversun
Creator: dcoliversun
Created: 30/Dec/22 03:20
Updated: 30/Dec/22 07:42
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: Creating pvc after driver/executor pod has Warning event from default-scheduler, such as
{code:java}
error getting PVC "spark/application-exec-1-pvc-0": could not find v1.PersistentVolumeClaim "spark/application-exec-1-pvc-0" {code}
Normal k8s workflow is to create PVC first and schedule pod to mount PVC.

 

We have a scenes that webhook server will try to reschedule pod and pvc to another pod. Because pvc creation after pod, wehbook couldn't find pvc based on pod metadata.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Dec 30 07:42:46 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1efzk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Dec/22 07:42;apachespark;User 'dcoliversun' has created a pull request for this issue:
https://github.com/apache/spark/pull/39306;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark 3.3.0 Netty Buffer Issue
Issue key: SPARK-41570
Issue id: 13514663
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: shamim_er123
Creator: shamim_er123
Created: 19/Dec/22 05:31
Updated: 19/Dec/22 05:31
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 1
Labels: 
Description: Getting below issue while running the spark 3.3.0 

Netty Version used as below

 

<dependency>
            <groupId>io.netty</groupId>
            <artifactId>netty-all</artifactId>
            <version>4.1.74.Final</version>

</dependency>

 

ISSUE:

java.lang.NoSuchMethodError: io.netty.buffer.PooledByteBufAllocator.<init>(ZIIIIIIZ)V
    at org.apache.spark.network.util.NettyUtils.createPooledByteBufAllocator(NettyUtils.java:171)
    at org.apache.spark.network.util.NettyUtils.getSharedPooledByteBufAllocator(NettyUtils.java:142)
    at org.apache.spark.network.client.TransportClientFactory.<init>(TransportClientFactory.java:111)
    at org.apache.spark.network.TransportContext.createClientFactory(TransportContext.java:144)
    at org.apache.spark.rpc.netty.NettyRpcEnv.<init>(NettyRpcEnv.scala:77)
    at org.apache.spark.rpc.netty.NettyRpcEnvFactory.create(NettyRpcEnv.scala:492)
    at org.apache.spark.rpc.RpcEnv$.create(RpcEnv.scala:58)
    at org.apache.spark.SparkEnv$.create(SparkEnv.scala:271)
    at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:194)
    at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:279)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:464)
    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2704)
    at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953)
    at scala.Option.getOrElse(Option.scala:189)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:86)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:459)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:675)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-12-19 05:31:00.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1e82g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Apply 'compute.eager_check' across all the codebase
Issue key: SPARK-37055
Issue id: 13407206
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dchvn
Creator: dchvn
Created: 19/Oct/21 06:00
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: As [~hyukjin.kwon] guide


 1 Make every input validation like this covered by the new configuration. For example:
{code:python}
- a == b
+ def eager_check(f): # Utility function 
+ return not config.compute.eager_check and f() 
+ 
+ eager_check(lambda: a == b)
{code}
2 We should check if the output makes sense although the behaviour is not matched with pandas'. If the output does not make sense, we shouldn't cover it with this configuration.

3 Make this configuration enabled by default so we match the behaviour to pandas' by default.

 

We have to make sure listing which API is affected in the description of 'compute.eager_check'
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): SPARK-36968
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-37002
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Nov 29 06:11:00 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vyf4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): hyukjin.kwon
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Oct/21 07:17;gurwls223;[~dchvn] please feel free to create JIRAs as sub-tasks, and proceed. This will be one of the large items in Spark 3.3 :-).;;;, 19/Oct/21 07:23;gurwls223;Oh just for extra clasification, we should only do this when it triggers some Spark jobs (e.g., Series or DataFrame). For scalar or primitive values, we don't need to guard it.;;;, 20/Oct/21 09:04;yikunkero;Looks like it's a method level results runtime validation between pandas and pandas-on-spark?

What's the next step when some one found eager check failed? Report a bug or back to return the pandas results?;;;, 21/Oct/21 01:38;gurwls223;Actually, it's more to prevent running Spark jobs only for the sake of input validation. For example, assume a pandas API requires to have the same values in its input:

{code}
def abc(df):
    if self.sort_values() != df.sort_values()
        raise Exception("all values have to be same")
{code}

and assume that the input {{df}} contains a very complicated computation chain. For example:

{code}
df = spark.read.csv().sort().repartition().sort().agg(...)
{code}

{code}
another_df.abc(df)  # would result in computing `df` two times (+ sort each df).
{code}

So, this JIRA aims to have the eager check (enabled by default to match with pandas' behaviour) but provide an option to avoid such expensive computation.;;;, 29/Nov/21 04:44;gurwls223;[~dchvn], just checking - are you working on this?;;;, 29/Nov/21 05:01;dchvn;[~hyukjin.kwon] , no, I am not now. I did not find anywhere to apply this conf more :(;;;, 29/Nov/21 05:28;gurwls223;You can, for example, find some instances relying on is_moninotically_increasing (https://github.com/apache/spark/blob/2fe9af8b2b91d0a46782dd6fff57eca8609be105/python/pyspark/pandas/base.py#L703-L758) which is super expensive e.g.) https://github.com/apache/spark/blob/master/python/pyspark/pandas/series.py#L5219 ;;;, 29/Nov/21 05:29;gurwls223;equals is the same too: https://github.com/apache/spark/blob/master/python/pyspark/pandas/series.py#L5842;;;, 29/Nov/21 06:11;dchvn;thanks! I will try to address them;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 19/Oct/21 07:23;gurwls223;Oh just for extra clasification, we should only do this when it triggers some Spark jobs (e.g., Series or DataFrame). For scalar or primitive values, we don't need to guard it.;;;
Comment.2: 20/Oct/21 09:04;yikunkero;Looks like it's a method level results runtime validation between pandas and pandas-on-spark?

What's the next step when some one found eager check failed? Report a bug or back to return the pandas results?;;;
Comment.3: 21/Oct/21 01:38;gurwls223;Actually, it's more to prevent running Spark jobs only for the sake of input validation. For example, assume a pandas API requires to have the same values in its input:

{code}
def abc(df):
    if self.sort_values() != df.sort_values()
        raise Exception("all values have to be same")
{code}

and assume that the input {{df}} contains a very complicated computation chain. For example:

{code}
df = spark.read.csv().sort().repartition().sort().agg(...)
{code}

{code}
another_df.abc(df)  # would result in computing `df` two times (+ sort each df).
{code}

So, this JIRA aims to have the eager check (enabled by default to match with pandas' behaviour) but provide an option to avoid such expensive computation.;;;
Comment.4: 29/Nov/21 04:44;gurwls223;[~dchvn], just checking - are you working on this?;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support ILIKE by Scala/Java, PySpark and R APIs
Issue key: SPARK-36752
Issue id: 13400968
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: maxgekk
Creator: maxgekk
Created: 14/Sep/21 09:42
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Add the ilike function to Scala/Java, Python and R APIs, update docs and examples.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): SPARK-36736, SPARK-36674
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 29 01:09:57 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uvyw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Sep/21 10:19;gurwls223;[~yoda-mon], are you interested in working on this? I saw you added some DSL APIs so thought you might be. cc [~sarutak] too FYI.;;;, 14/Sep/21 10:20;gurwls223;BTW, feel free to separate/create dedicated JIRAs for Scala, PySpark and R if you want to proceed them separately in separate PRs.;;;, 14/Sep/21 11:09;yoda-mon;[~hyukjin.kwon] Thank you, I'd like to work on it.;;;, 14/Sep/21 11:36;gurwls223;Thanks!!;;;, 24/Sep/21 01:45;gurwls223;[~yoda-mon] just cheking. any update on R and Python APIs ;-)?;;;, 29/Sep/21 00:54;yoda-mon;[~hyukjin.kwon] I'm sorry for worrying you ( I took a vacation), I will create a PR for Python today and for R soon.;;;, 29/Sep/21 01:09;gurwls223;it's totally no problem. I was just checking :-).;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 14/Sep/21 10:20;gurwls223;BTW, feel free to separate/create dedicated JIRAs for Scala, PySpark and R if you want to proceed them separately in separate PRs.;;;
Comment.2: 14/Sep/21 11:09;yoda-mon;[~hyukjin.kwon] Thank you, I'd like to work on it.;;;
Comment.3: 14/Sep/21 11:36;gurwls223;Thanks!!;;;
Comment.4: 24/Sep/21 01:45;gurwls223;[~yoda-mon] just cheking. any update on R and Python APIs ;-)?;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Asynchronous State Checkpointing in Structured Streaming
Issue key: SPARK-39592
Issue id: 13468360
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jerrypeng
Creator: jerrypeng
Created: 24/Jun/22 21:00
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: We can reduce the latency of stateful pipelines in Structured Streaming by making state checkpoints asynchronous.  One of the major contributors of latency for stateful pipelines in Structured Streaming can be checkpointing the state changes of every micro-batch.  If we make the state checkpointing asynchronous, we can potentially significantly lower the latency of the pipeline as the state checkpointing won’t or will contribute less to the batch latency.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): SPARK-40025
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 05 02:48:05 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16cl4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Jul/22 02:48;gurwls223;cc [~kabhwan] FYI;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Configurable State Checkpointing Frequency in Structured Streaming
Issue key: SPARK-39593
Issue id: 13468361
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jerrypeng
Creator: jerrypeng
Created: 24/Jun/22 21:01
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Currently, for stateful pipelines state changes are checkpointed for every micro-batch. State checkpoints can contribute significantly to the latency of a micro-batch.  If state is checkpointed less frequently, its effect on batch latency can be amortized.  This can be used in conjunction with asynchronous state checkpointing to further reduce the cost in latency state checkpointing may incur.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): SPARK-40025
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 05 02:48:10 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16clc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Jul/22 02:48;gurwls223;cc [~kabhwan] FYI;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Invoking .repartition(100000) in a unit test causes the unit test to take >20 minutes.
Issue key: SPARK-39602
Issue id: 13468425
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tanin
Creator: tanin
Created: 26/Jun/22 05:07
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Here's a proof of concept: 

{code}
val result = spark
      .createDataset(List("test"))
      .rdd
      .repartition(100000)
      .map { _ =>
        "test"
      }
      .collect()
      .toList
 
    println(result)
{code}

This code takes a very long time in unit test.

We aim to test for correctness in unit test... not testing the repartition. 

Is there a way to make it faster? (e.g. disable partition in test)
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 05 04:28:30 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16czk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Jul/22 02:49;gurwls223;Why don't you decrease the number of partitions?;;;, 05/Jul/22 02:54;tanin;The number of partitions is for illustration. In our actual job, we use something like 40000, 60000.

Currently, what we do is:

{code}
.groupBy(
  ... something ...,
  partitions =  if (spark.conf.getOption("some.sortofourconfig.testMode.enabled").contains("true")) {
    10
  } else {
    60000
  }
)
{code}

But you can imagine that it litters the code a bit.

We need this kind of partitions in prod. Otherwise, it would fail.;;;, 05/Jul/22 04:10;kabhwan;Why not make the number of partitions be configurable by existing technology of configuration (not only Spark config but also any config util)? You should be able to produce different config file per environment (dev, stage, prod) which should achieve this. I don't think Spark can indicate whether end users are running query on their test purpose or not.;;;, 05/Jul/22 04:28;tanin;We certainly can. 

Different points of code require different numbers of partitions. It litters the code quite a lot. This is why I would like to understand if there's a better way than refactoring 5-10 different numbers of partitions into a config file.
;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 05/Jul/22 02:54;tanin;The number of partitions is for illustration. In our actual job, we use something like 40000, 60000.

Currently, what we do is:

{code}
.groupBy(
  ... something ...,
  partitions =  if (spark.conf.getOption("some.sortofourconfig.testMode.enabled").contains("true")) {
    10
  } else {
    60000
  }
)
{code}

But you can imagine that it litters the code a bit.

We need this kind of partitions in prod. Otherwise, it would fail.;;;
Comment.2: 05/Jul/22 04:10;kabhwan;Why not make the number of partitions be configurable by existing technology of configuration (not only Spark config but also any config util)? You should be able to produce different config file per environment (dev, stage, prod) which should achieve this. I don't think Spark can indicate whether end users are running query on their test purpose or not.;;;
Comment.3: 05/Jul/22 04:28;tanin;We certainly can. 

Different points of code require different numbers of partitions. It litters the code quite a lot. This is why I would like to understand if there's a better way than refactoring 5-10 different numbers of partitions into a config file.
;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Dataset planning in a unit test takes a very long time to finish (e.g. >8mins for complex job)
Issue key: SPARK-39603
Issue id: 13468426
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tanin
Creator: tanin
Created: 26/Jun/22 05:11
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: At Stripe, we have a very complex data job. The unit test was running fine when we used RDD.

After we switched to Dataset, the unit test takes considerably longer (e.g. > 8 mins just for planning).

Most of our unit tests only process 1-2 records.

We have tried to investigate it a bit, and we are somewhat sure it's the planning phrase.

We tried disabling almost all optimizers except the ~10 optimizers that can't be disabled. It doesn't impact the test run time at all.

Is there a way to make dataset plan faster in unit test.

Thank you!

(Please excuse us. I may use inaccurate term.)
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 05 02:49:53 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16czs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Jul/22 02:49;gurwls223;Mind showing the reproducer? It's very difficult to assess the problem with just text here.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Inspect the State of Stateful Streaming Pipelines
Issue key: SPARK-39588
Issue id: 13468356
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jerrypeng
Creator: jerrypeng
Created: 24/Jun/22 20:56
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Currently there is no mechanism to query or inspect the state of the stateful streaming pipeline externally.  A tool or API that allows a user to do that would be extremely helpful for debugging.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 05 02:47:38 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16ck8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Jul/22 02:47;gurwls223;cc [~kabhwan] FYI;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Asynchronous I/O API support in Structured Streaming
Issue key: SPARK-39589
Issue id: 13468357
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jerrypeng
Creator: jerrypeng
Created: 24/Jun/22 20:58
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Often in streaming pipelines used to support ETL use cases, API calls to external systems can be found.  To better support and improve performance for such use cases, an asynchronous processing API should be added to Structured Streaming so that external requests can be handled in a more efficient manner.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): SPARK-40025
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 05 02:47:43 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16ckg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Jul/22 02:47;gurwls223;cc [~kabhwan] FYI;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Multiple Stateful Operators in Structured Streaming
Issue key: SPARK-39585
Issue id: 13468353
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jerrypeng
Creator: jerrypeng
Created: 24/Jun/22 20:50
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Currently, Structured Streaming only supports one stateful operator per pipeline.  This constraint excludes Structured Streaming to be used for many use cases such ones involving chained time window aggregations, chained stream-stream outer equality join, stream-stream time interval join followed by time window aggregation.  Enabling multiple stateful operators within a pipeline in Structured Streaming will open up many additional use cases for its usage as well as be on par with other stream processing engines.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): SPARK-40025
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 05 02:47:14 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16cjk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Jul/22 02:47;gurwls223;cc. [~kabhwan] FYI;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Advanced Windowing in Structured Streaming
Issue key: SPARK-39586
Issue id: 13468354
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jerrypeng
Creator: jerrypeng
Created: 24/Jun/22 20:51
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Currently in Structure Streaming there does not exist a user friendly method to define custom windowing strategies. Users can only choose from an existing set of built-in window strategies such tumbling, sliding, session windows.  To improve flexibility of the engine and to support more advanced use cases, Structured Streaming needs to provide an intuitive API that allows users to define custom window boundaries, to trigger windows when they are complete, and, if necessary, evict elements from windows.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): SPARK-40025
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 05 02:47:28 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16cjs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Jul/22 02:47;gurwls223;cc [~kabhwan] FYI;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Schema Evolution for Stateful Pipelines in Structured Streaming 
Issue key: SPARK-39587
Issue id: 13468355
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jerrypeng
Creator: jerrypeng
Created: 24/Jun/22 20:55
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Support for schema evolution of stateful operators is non-existent. There is no clear path to evolve the schema for existing built-in stateful operators.  For use defined stateful operators such as map/flatMapGroupsWithState, there is no path for evolving state as well.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): SPARK-40025
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 05 02:47:33 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16ck0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Jul/22 02:47;gurwls223;cc [~kabhwan] FYI;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: pyspark.pandas.config.OptionError: "No such option: 'mode.chained_assignment'
Issue key: SPARK-38637
Issue id: 13435402
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: prakharsandhu
Creator: prakharsandhu
Created: 23/Mar/22 16:41
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: I replaced import pandas as pd to import pyspark.pandas as pd in my code. 
{code:java}
pd.set_option("mode.chained_assignment", None) {code}
The above command was working with pandas but this option is not available in  pyspark.pandas . 
{code:java}
pyspark.pandas.config.OptionError: "No such option: 'mode.chained_assignment'. Available options are [display.max_rows, 
compute.max_rows, compute.shortcut_limit, compute.ops_on_diff_frames, compute.default_index_type, compute.ordered_head, 
plotting.max_rows, plotting.sample_ratio, plotting.backend]" {code}
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jul 17 18:22:14 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10rag:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Mar/22 02:26;gurwls223;cc [~itholic] [~XinrongM] FYI;;;, 05/Apr/22 11:57;pralabhkumar;[~itholic] can I work on this ;;;, 05/Apr/22 23:13;itholic;[~pralabhkumar] Sure, pleas go ahead!;;;, 17/Jul/22 18:22;prakharsandhu;Is it resolved now?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 05/Apr/22 11:57;pralabhkumar;[~itholic] can I work on this ;;;
Comment.2: 05/Apr/22 23:13;itholic;[~pralabhkumar] Sure, pleas go ahead!;;;
Comment.3: 17/Jul/22 18:22;prakharsandhu;Is it resolved now?;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Fix DataFrame select subset with duplicated columns
Issue key: SPARK-37930
Issue id: 13423001
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dchvn
Creator: dchvn
Created: 17/Jan/22 09:36
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: pandas
{code:java}
>>> pdf
   a
0  1
1  2
2  3
3  4
>>> pdf[['a', 'a']]
   a  a
0  1  1
1  2  2
2  3  3
3  4  4 {code}
pandas on spark
{code:java}
>>> psdf
   a
0  1
1  2
2  3
3  4
>>> psdf[['a', 'a']]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/u02/spark/python/pyspark/pandas/frame.py", line 12077, in __repr__
    pdf = self._get_or_create_repr_pandas_cache(max_display_count)
  File "/u02/spark/python/pyspark/pandas/frame.py", line 12068, in _get_or_create_repr_pandas_cache
    self, "_repr_pandas_cache", {n: self.head(n + 1)._to_internal_pandas()}
  File "/u02/spark/python/pyspark/pandas/frame.py", line 12063, in _to_internal_pandas
    return self._internal.to_pandas_frame
  File "/u02/spark/python/pyspark/pandas/utils.py", line 576, in wrapped_lazy_property
    setattr(self, attr_name, fn(self))
  File "/u02/spark/python/pyspark/pandas/internal.py", line 1055, in to_pandas_frame
    return InternalFrame.restore_index(pdf, **self.arguments_for_restore_index)
  File "/u02/spark/python/pyspark/pandas/internal.py", line 1156, in restore_index
    pdf.columns = pd.Index(
  File "/u02/venv3.9-2/lib/python3.9/site-packages/pandas/core/generic.py", line 5500, in __setattr__
    return object.__setattr__(self, name, value)
  File "pandas/_libs/properties.pyx", line 70, in pandas._libs.properties.AxisProperty.__set__
  File "/u02/venv3.9-2/lib/python3.9/site-packages/pandas/core/generic.py", line 766, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "/u02/venv3.9-2/lib/python3.9/site-packages/pandas/core/internals/managers.py", line 216, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "/u02/venv3.9-2/lib/python3.9/site-packages/pandas/core/internals/base.py", line 57, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 4 elements, new values have 2 elements {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jan 18 09:29:58 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yn5k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Jan/22 01:59;gurwls223;Interesting. we should fix this. cc [~XinrongM][~ueshin][~itholic] FYI;;;, 18/Jan/22 01:59;gurwls223;are you working on this? [~dchvn] [~yikunkero]?;;;, 18/Jan/22 02:19;dchvn;{code:java}
>>> import pandas as pd
>>> pdf = pd.DataFrame([1,2,3,4], columns=['a'])
>>> pdf
   a
0  1
1  2
2  3
3  4
>>> pdf = pdf[['a', 'a']]
>>> pdf
   a  a
0  1  1
1  2  2
2  3  3
3  4  4
>>> pdf[['a', 'a']]
   a  a  a  a
0  1  1  1  1
1  2  2  2  2
2  3  3  3  3
3  4  4  4  4
 {code}
Seem it come from pandas.

[https://github.com/apache/spark/blob/df7447bc62052e3d7391ba23d7220fb8c9b923fd/python/pyspark/pandas/internal.py#L1146];;;, 18/Jan/22 02:22;dchvn;I'm working on this. Thanks;;;, 18/Jan/22 03:30;gurwls223;Thanks [~dchvn]!;;;, 18/Jan/22 05:28;yikunkero;[~dchvn] Thanks for investigation, also update the pandas issue: https://github.com/pandas-dev/pandas/issues/45439 for reference, and not sure it's expected behavior or not. Looks like there are a duplicated to check index duplicated:

idx.duplicated(keep='first') https://pandas.pydata.org/docs/reference/api/pandas.Index.duplicated.html;;;, 18/Jan/22 09:29;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/35240;;;, 18/Jan/22 09:29;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/35240;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 18/Jan/22 01:59;gurwls223;are you working on this? [~dchvn] [~yikunkero]?;;;
Comment.2: 18/Jan/22 02:19;dchvn;{code:java}
>>> import pandas as pd
>>> pdf = pd.DataFrame([1,2,3,4], columns=['a'])
>>> pdf
   a
0  1
1  2
2  3
3  4
>>> pdf = pdf[['a', 'a']]
>>> pdf
   a  a
0  1  1
1  2  2
2  3  3
3  4  4
>>> pdf[['a', 'a']]
   a  a  a  a
0  1  1  1  1
1  2  2  2  2
2  3  3  3  3
3  4  4  4  4
 {code}
Seem it come from pandas.

[https://github.com/apache/spark/blob/df7447bc62052e3d7391ba23d7220fb8c9b923fd/python/pyspark/pandas/internal.py#L1146];;;
Comment.3: 18/Jan/22 02:22;dchvn;I'm working on this. Thanks;;;
Comment.4: 18/Jan/22 03:30;gurwls223;Thanks [~dchvn]!;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: AttributeError: module 'pyspark.pandas' has no attribute 'DateOffset'
Issue key: SPARK-38601
Issue id: 13434575
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: prakharsandhu
Creator: prakharsandhu
Created: 18/Mar/22 12:51
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: I am working on replacing Pandas library to pyspark.pandas Library in my python repo in VS Code. But pyspark.pandas module does not seem to have DateOffset() module similar to what pandas has.

I tried this :
{code:java}
import pyspark.pandas as pd 
clean_df[self._config.general["info_col"]] = (clean_df["cycle_dt"] - pd.DateOffset(months=cycle_info_gap)        ).fillna(clean_df[self._config.general["time_col"]])  {code}
It results in the below error :
{code:java}
Un unanticipated error occurred: module 'pyspark.pandas' has no attribute 'DateOffset'{code}
Is there any alternative for this in pyspark.pandas?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jul 17 18:35:11 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10m7c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Mar/22 02:09;gurwls223;[~prakharsandhu] does it work if you use {{pd.DateOffset}}? cc [~XinrongM] FYI;;;, 19/Mar/22 08:30;prakharsandhu;[~hyukjin.kwon]  yes, it was working previously with Pandas;;;, 19/Mar/22 11:03;gurwls223;[~prakharsandhu] Sorry I meant: 

{code}
import databricks.koalas as ks 
kdf["date_col_2"] = kdf["date_col_1"] - pd.DateOffset(months=cycle_info_gap)
{code};;;, 19/Mar/22 12:46;prakharsandhu;Hi [~hyukjin.kwon] , 

I tried your suggestion but it throws the below error : 
{code:java}
File "C:\Users\abcd\Anaconda3\envs\oden1\lib\site-packages\databricks\koalas\base.py", line 374, in __sub__
    raise TypeError("datetime subtraction can only be applied to datetime series.")
TypeError: datetime subtraction can only be applied to datetime series.
{code};;;, 21/Mar/22 10:16;prakharsandhu;Hi [~hyukjin.kwon] ,
 # Any other alternative that could be tried to achieve this?
 # Is Koalas library not fully developed yet to replace the pandas library?  In most of the documentations it is mentioned that a simple replacement of import pandas to import databricks.koalas would work;;;, 23/Mar/22 15:34;prakharsandhu;Hi [~hyukjin.kwon] , 

Has this DateOffset() function not implemented yet in pyspark 3.2?;;;, 17/Jul/22 18:35;prakharsandhu;Any idea how to make DateOffset work in pyspark.pandas;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 19/Mar/22 08:30;prakharsandhu;[~hyukjin.kwon]  yes, it was working previously with Pandas;;;
Comment.2: 19/Mar/22 11:03;gurwls223;[~prakharsandhu] Sorry I meant: 

{code}
import databricks.koalas as ks 
kdf["date_col_2"] = kdf["date_col_1"] - pd.DateOffset(months=cycle_info_gap)
{code};;;
Comment.3: 19/Mar/22 12:46;prakharsandhu;Hi [~hyukjin.kwon] , 

I tried your suggestion but it throws the below error : 
{code:java}
File "C:\Users\abcd\Anaconda3\envs\oden1\lib\site-packages\databricks\koalas\base.py", line 374, in __sub__
    raise TypeError("datetime subtraction can only be applied to datetime series.")
TypeError: datetime subtraction can only be applied to datetime series.
{code};;;
Comment.4: 21/Mar/22 10:16;prakharsandhu;Hi [~hyukjin.kwon] ,
 # Any other alternative that could be tried to achieve this?
 # Is Koalas library not fully developed yet to replace the pandas library?  In most of the documentations it is mentioned that a simple replacement of import pandas to import databricks.koalas would work;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support dataframe API use jdbc data source in PySpark
Issue key: SPARK-40502
Issue id: 13482339
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: javacaoyu
Creator: javacaoyu
Created: 20/Sep/22 09:29
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: When i using pyspark, i wanna get data from mysql database.  so i want use JDBCRDD like java\scala.

But that is not be supported in PySpark.

 

For some reasons, i can't using DataFrame API, only can use RDD(datastream) API. Even i know the DataFrame can get data from jdbc source fairly well.

 
So i want to implement functionality that can use rdd to get data from jdbc source for PySpark.
 
*But i don't know if that are necessary for PySpark.   so we can discuss it.*
 
{*}If it is necessary for PySpark{*}{*}, i want to contribute to Spark.{*}  
*i hope this Jira task can assigned to me, so i can start working to implement it.*
 
*if not, please close this Jira task.*
 
 
*thanks a lot.*
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 21 06:02:20 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18p2o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Sep/22 01:11;gurwls223;{quote}
For some reasons, i can't using DataFrame API, only can use RDD(datastream) API.
{quote}
What's the reason?;;;, 21/Sep/22 05:58;javacaoyu;I am a teacher
Recently designed Python language basic course, big data direction

PySpark is one of the practical cases, but it is only a simple use of RDD code to complete the basic data processing work, and the use of JDBC data source is a part of the course

 

Because the course is very basic, simple rdd code is suitable as an example.
But if you use DataFrame, you need to explain more content, which is not friendly to novice students

DataFrames(SparkSQL) will be used in future design advanced courses.

So I hope that the extraction of jdbc data may be completed through the api of rdd

 

 

 ;;;, 21/Sep/22 06:02;javacaoyu;When I designed the Python Flink course
It is found that PyFlink does not have the operators sum\min\minby\max\maxby

So I submitted a PR to the flink community and provided the python implementation code of these operators （FLINK-26609 FLINK-26728)

So, again, if jdbc datasource is what pyspark needs, I'd love and have the time to implement it;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 21/Sep/22 05:58;javacaoyu;I am a teacher
Recently designed Python language basic course, big data direction

PySpark is one of the practical cases, but it is only a simple use of RDD code to complete the basic data processing work, and the use of JDBC data source is a part of the course

 

Because the course is very basic, simple rdd code is suitable as an example.
But if you use DataFrame, you need to explain more content, which is not friendly to novice students

DataFrames(SparkSQL) will be used in future design advanced courses.

So I hope that the extraction of jdbc data may be completed through the api of rdd

 

 

 ;;;
Comment.2: 21/Sep/22 06:02;javacaoyu;When I designed the Python Flink course
It is found that PyFlink does not have the operators sum\min\minby\max\maxby

So I submitted a PR to the flink community and provided the python implementation code of these operators （FLINK-26609 FLINK-26728)

So, again, if jdbc datasource is what pyspark needs, I'd love and have the time to implement it;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support `na_filter` for pyspark.pandas.read_csv
Issue key: SPARK-38292
Issue id: 13430057
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: itholic
Creator: itholic
Created: 23/Feb/22 01:31
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: pandas support `na_filter` parameter for `read_csv` function. (https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)

We also want to support this to follow the behavior of pandas.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 27 17:28:55 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zufk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Jun/22 12:28;pralabhkumar;[~itholic] I would like to work on this . ;;;, 16/Jun/22 00:32;gurwls223;please go ahead!;;;, 21/Jun/22 11:45;pralabhkumar;[~itholic] [~hyukjin.kwon] 

Would like to discuss the logic 

The difference comes na_filter = False , when there are missing values . For .eg 

22,,1980-09-26

33,,1980-09-26

 

Pandas with na_filter , read it as its . However Spark will read missing value with null . This happens because of univocity-parsers , which reads missing value as null . 

 

Approach

in case of na_filter. 

Once file is  read  in namespace.py via reader.csv(patj)  , replace missing values with empty string (df.fillna("")). We also need to change the datatype of the column to string (as panda does). 

 

 

Please let me know , if its correct direction , i'll create a PR . ;;;, 21/Jun/22 11:55;gurwls223;can we control the options e.g., emptyValue or nullValue in CSV?;;;, 21/Jun/22 11:56;gurwls223;you could try to leverage the approach like https://github.com/apache/spark/pull/36294 to set empty or null vlaues as non-existent values. ;;;, 22/Jun/22 15:45;pralabhkumar;[~hyukjin.kwon] Thx for the suggestion . 

 

After going through the code (DataFrameReader and Univocity spark parser code) . Here is the analysis .

Example A,,B

A,,B ==> spark.read.option(“nullValue”,”A”) ==> results in null, null, B

Reason for this is 
 * _parse method in_ org.apache.spark.sql.catalyst.csv.UnivocityParser
 * Parse string => A, A,B (settings.setNullValue in com.univocity.parsers.csv.CsvParser replaces the ,, value with A)
 * Now nullSafeDatum will check if (datum == options.{_}nullValue{_} || datum == null) and return null for both the values , since datum = options.nullValue => null, null, B
 * Not sure if this is expected  output since from  com.univocity.parsers.csv.CsvParser point of view expected output should be “A,A,B” after setting .setNullValue("A")

 

*Solution*

Now in case of na_filter ,  what I am thinking is to add one property if ( (na_filter &&  datum == options.{_}nullValue)|| datum == null){_}

_Now if the input string is A,,B and user have set na_filter to False , then_ com.univocity.parsers.csv.CsvParser will return as its is since setNullValue is (“”) 

And then (na_filter &&  datum == options.{_}nullValue) condition become false and{_} converter.apply(datum) , which will leave the value as its . ;;;, 23/Jun/22 10:27;pralabhkumar;[~hyukjin.kwon] Please let me know if its ok . I'll do the same.;;;, 27/Jun/22 17:28;apachespark;User 'pralabhkumar' has created a pull request for this issue:
https://github.com/apache/spark/pull/37009;;;, 27/Jun/22 17:28;apachespark;User 'pralabhkumar' has created a pull request for this issue:
https://github.com/apache/spark/pull/37009;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 16/Jun/22 00:32;gurwls223;please go ahead!;;;
Comment.2: 21/Jun/22 11:45;pralabhkumar;[~itholic] [~hyukjin.kwon] 

Would like to discuss the logic 

The difference comes na_filter = False , when there are missing values . For .eg 

22,,1980-09-26

33,,1980-09-26

 

Pandas with na_filter , read it as its . However Spark will read missing value with null . This happens because of univocity-parsers , which reads missing value as null . 

 

Approach

in case of na_filter. 

Once file is  read  in namespace.py via reader.csv(patj)  , replace missing values with empty string (df.fillna("")). We also need to change the datatype of the column to string (as panda does). 

 

 

Please let me know , if its correct direction , i'll create a PR . ;;;
Comment.3: 21/Jun/22 11:55;gurwls223;can we control the options e.g., emptyValue or nullValue in CSV?;;;
Comment.4: 21/Jun/22 11:56;gurwls223;you could try to leverage the approach like https://github.com/apache/spark/pull/36294 to set empty or null vlaues as non-existent values. ;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: upgrade jackson data mapper to latest 
Issue key: SPARK-40457
Issue id: 13481726
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bilna123
Creator: bilna123
Created: 15/Sep/22 15:10
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Upgrade  jackson-mapper-asl to the latest to resolve CVE-2019-10172
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): SPARK-30466
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Oct 25 12:31:33 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18lbk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Sep/22 03:17;gurwls223;[~bilna123] which Jackson version do you mean?;;;, 20/Sep/22 10:22;bilna123;[~hyukjin.kwon] it is org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13;;;, 21/Sep/22 19:43;bjornjorgensen;[~bilna123]
Yes, there are no version to upgrade to https://github.com/bjornjorgensen/spark/security/dependabot/1 and it's for hadoop version 2. 

But do you find a new version and can you test it with hadoop version 2? 

Edit:
Have a look at 

https://issues.apache.org/jira/browse/HADOOP-17225?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&focusedCommentId=17305360#comment-17305360 
;;;, 11/Oct/22 15:29;bilna123;This link: https://github.com/bjornjorgensen/spark/security/dependabot/1  is giving 404;;;, 12/Oct/22 00:44;gurwls223;We're going to drop Hadoop 2 from Apache Spark 3.4. is this still an issue?;;;, 12/Oct/22 07:17;bilna123;Are we going to remove this dependency from spark's pom file in Spark 3.4?
;;;, 20/Oct/22 06:17;bilna123;[~hyukjin.kwon] Understood. So I think I can mark this as false positive. Thanks for the link;;;, 25/Oct/22 12:31;pj.fanning;Maybe this could be closed as a duplicate of SPARK-30466;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 20/Sep/22 10:22;bilna123;[~hyukjin.kwon] it is org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13;;;
Comment.2: 21/Sep/22 19:43;bjornjorgensen;[~bilna123]
Yes, there are no version to upgrade to https://github.com/bjornjorgensen/spark/security/dependabot/1 and it's for hadoop version 2. 

But do you find a new version and can you test it with hadoop version 2? 

Edit:
Have a look at 

https://issues.apache.org/jira/browse/HADOOP-17225?page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel&focusedCommentId=17305360#comment-17305360 
;;;
Comment.3: 11/Oct/22 15:29;bilna123;This link: https://github.com/bjornjorgensen/spark/security/dependabot/1  is giving 404;;;
Comment.4: 12/Oct/22 00:44;gurwls223;We're going to drop Hadoop 2 from Apache Spark 3.4. is this still an issue?;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Refactor antlr4 syntax file
Issue key: SPARK-37607
Issue id: 13416473
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: melin
Creator: melin
Created: 10/Dec/21 07:39
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Refer to the ShardingSphere project, split into multiple files by type, very clear



https://github.com/apache/shardingsphere/tree/master/shardingsphere-sql-parser/shardingsphere-sql-parser-dialect/shardingsphere-sql-parser-mysql/src/main/antlr4/imports/mysql

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Dec 13 00:35:17 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xjf4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Dec/21 00:35;gurwls223;[~melin] can you double check the link? it shows no page found.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Flaky Test: BasicSchedulerIntegrationSuite - 'super simple job' and 'multi-stage job'
Issue key: SPARK-36375
Issue id: 13392970
Parent id: 
Issue Type: Test
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 02/Aug/21 02:21
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: https://github.com/apache/spark/runs/3216286546

{code}
[info] BasicSchedulerIntegrationSuite:
[info] - super simple job *** FAILED *** (56 milliseconds)
[info]   Map() did not equal Map(0 -> 42, 5 -> 42, 1 -> 42, 6 -> 42, 9 -> 42, 2 -> 42, 7 -> 42, 3 -> 42, 8 -> 42, 4 -> 42) (SchedulerIntegrationSuite.scala:545)
[info]   Analysis:
[info]   HashMap(0: -> 42, 1: -> 42, 2: -> 42, 3: -> 42, 4: -> 42, 5: -> 42, 6: -> 42, 7: -> 42, 8: -> 42, 9: -> 42)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
[info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
[info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
[info]   at org.apache.spark.scheduler.BasicSchedulerIntegrationSuite.$anonfun$new$1(SchedulerIntegrationSuite.scala:545)
[info]   at org.apache.spark.scheduler.SchedulerIntegrationSuite.$anonfun$testScheduler$1(SchedulerIntegrationSuite.scala:98)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:190)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:62)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:62)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563)
[info]   at org.scalatest.Suite.run(Suite.scala:1112)
[info]   at org.scalatest.Suite.run$(Suite.scala:1094)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1563)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:62)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:62)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748)
{code}

{code}
[info] - multi-stage job *** FAILED *** (22 milliseconds)
[info]   org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:
[info] org.apache.spark.SparkContext.<init>(SparkContext.scala:85)
[info] org.apache.spark.scheduler.SchedulerIntegrationSuite.setupScheduler(SchedulerIntegrationSuite.scala:81)
[info] org.apache.spark.scheduler.SchedulerIntegrationSuite.$anonfun$testScheduler$1(SchedulerIntegrationSuite.scala:97)
[info] scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info] org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info] org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info] org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info] org.scalatest.Transformer.apply(Transformer.scala:22)
[info] org.scalatest.Transformer.apply(Transformer.scala:20)
[info] org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info] org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:190)
[info] org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info] org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info] org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info] org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info] org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info] org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:62)
[info] org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info] org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info] org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:62)
[info]   at org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2644)
[info]   at scala.Option.foreach(Option.scala:407)
[info]   at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2641)
[info]   at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2731)
[info]   at org.apache.spark.SparkContext.<init>(SparkContext.scala:95)
[info]   at org.apache.spark.scheduler.SchedulerIntegrationSuite.setupScheduler(SchedulerIntegrationSuite.scala:81)
[info]   at org.apache.spark.scheduler.SchedulerIntegrationSuite.$anonfun$testScheduler$1(SchedulerIntegrationSuite.scala:97)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:190)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:62)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:62)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563)
[info]   at org.scalatest.Suite.run(Suite.scala:1112)
[info]   at org.scalatest.Suite.run$(Suite.scala:1094)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1563)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:62)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:62)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748)
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 02 03:05:33 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0timg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Aug/21 02:22;gurwls223;[~wuyi] do you have any idea on this?;;;, 02/Aug/21 03:05;Ngone51;[~hyukjin.kwon] I'd like to take a look first. Thanks for the ping.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 02/Aug/21 03:05;Ngone51;[~hyukjin.kwon] I'd like to take a look first. Thanks for the ping.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Overflow occurs when reading ANSI day time interval from CSV file
Issue key: SPARK-38520
Issue id: 13433280
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: chongg@nvidia
Creator: chongg@nvidia
Created: 11/Mar/22 06:59
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: *Problem:*

Overflow occurs when reading the following positive intervals, the results become to negative

interval '106751992' day     => INTERVAL '-106751990' DAY

INTERVAL +'+2562047789' hour => INTERVAL '-2562047787' HOUR

interval '153722867281' minute => INTERVAL '-153722867280' MINUTE

 

*Reproduce:*
{code:java}
// days overflow
 scala> val schema = StructType(Seq(StructField("c1",
   DayTimeIntervalType(DayTimeIntervalType.DAY, DayTimeIntervalType.DAY))))
 scala> spark.read.csv(path).show(false)
 +------------------------+
 |_c0                     |
 +------------------------+
 |interval '106751992' day|
 +------------------------+
 scala> spark.read.schema(schema).csv(path).show(false)
 +-------------------------+
 |c1                       |
 +-------------------------+
 |INTERVAL '-106751990' DAY|
 +-------------------------+
  // hour overflow
 scala> val schema = StructType(Seq(StructField("c1",
   DayTimeIntervalType(DayTimeIntervalType.HOUR, DayTimeIntervalType.HOUR))))
 scala> spark.read.csv(path).show(false)
 +----------------------------+
 |_c0                         |
 +----------------------------+
 |INTERVAL +'+2562047789' hour|
 +----------------------------+
 scala> spark.read.schema(schema).csv(path).show(false)
 +---------------------------+
 |c1                         |
 +---------------------------+
 |INTERVAL '-2562047787' HOUR|
 +---------------------------+
 // minute overflow
 scala> val schema = StructType(Seq(StructField("c1",
   DayTimeIntervalType(DayTimeIntervalType.MINUTE, DayTimeIntervalType.MINUTE))))
 scala> spark.read.csv(path).show(false)
 +------------------------------+
 |_c0                           |
 +------------------------------+
 |interval '153722867281' minute|
 +------------------------------+
 scala> spark.read.schema(schema).csv(path).show(false)
 +-------------------------------+
 |c1                             |
 +-------------------------------+
 |INTERVAL '-153722867280' MINUTE|
 +-------------------------------+
{code}
 

*others:*

Also check the negative value is read to positive.

 

others:

should check the negative also
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Mar 14 10:09:28 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10e8w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Mar/22 01:55;gurwls223;[~chongg@nvidia] can you see if it fails with {{spark.sql.ansi.enabled}} enabled?;;;, 14/Mar/22 10:09;apachespark;User 'res-life' has created a pull request for this issue:
https://github.com/apache/spark/pull/35845;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 14/Mar/22 10:09;apachespark;User 'res-life' has created a pull request for this issue:
https://github.com/apache/spark/pull/35845;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Dynamically update the loaded Hive UDF JAR
Issue key: SPARK-37840
Issue id: 13421251
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: melin
Creator: melin
Created: 07/Jan/22 08:44
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: In the production environment, spark ThriftServer needs to be restarted if jar files are updated after UDF files are loaded。
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jan 26 19:01:06 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ycdk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Jan/22 08:46;melin;[~cloud_fan] ;;;, 07/Jan/22 18:23;rakson;We can dynamically update our UDF jars after loading them. I will try to raise a PR soon for this.;;;, 10/Jan/22 01:03;gurwls223;I think we can already overwrite the jars, no?;;;, 11/Jan/22 05:59;cutiechi;[~hyukjin.kwon] No;;;, 11/Jan/22 06:32;gurwls223;ADD FILE|JAR|ARCHIVE tracks timestamps IIRC, and they can update newer files. What's the current behaviour?;;;, 11/Jan/22 07:41;cutiechi;Spark does not currently support delete jar;;;, 11/Jan/22 07:42;cutiechi;[~hyukjin.kwon] But the loading is still from the previous Jar;;;, 11/Jan/22 08:07;rakson;[~cutiechi]  The problem is with `jarClassLoader`. `jarClassLoader` needs to be updated after updated jar is added.;;;, 11/Jan/22 12:36;cutiechi;Yes, but url classloder does not support remove url;;;, 11/Jan/22 15:35;melin;If the JAR file update time changes, create a new classloader;;;, 26/Jan/22 19:01;apachespark;User 'iRakson' has created a pull request for this issue:
https://github.com/apache/spark/pull/35337;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 07/Jan/22 18:23;rakson;We can dynamically update our UDF jars after loading them. I will try to raise a PR soon for this.;;;, 26/Jan/22 19:01;apachespark;User 'iRakson' has created a pull request for this issue:
https://github.com/apache/spark/pull/35337;;;
Comment.2: 10/Jan/22 01:03;gurwls223;I think we can already overwrite the jars, no?;;;
Comment.3: 11/Jan/22 05:59;cutiechi;[~hyukjin.kwon] No;;;
Comment.4: 11/Jan/22 06:32;gurwls223;ADD FILE|JAR|ARCHIVE tracks timestamps IIRC, and they can update newer files. What's the current behaviour?;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM
Issue key: SPARK-39815
Issue id: 13472236
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Ghasemi
Creator: Ghasemi
Created: 19/Jul/22 09:51
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: h3. I have a Spark SQL Program which run in Spark Cluster. Even though the program is finished without any Error, after finishing State of the Application becomes Killed. It shows this error on the log:

 

22/07/19 13:14:50 INFO CoarseGrainedExecutorBackend: Got assigned task 10686

22/07/19 13:14:50 INFO Executor: Running task 232.0 in stage 137.0 (TID 10686)

22/07/19 13:14:50 INFO CoarseGrainedExecutorBackend: Got assigned task 10687

22/07/19 13:14:50 INFO Executor: Running task 233.0 in stage 137.0 (TID 10687)

22/07/19 13:14:50 INFO PythonRunner: Times: total = 6, boot = 2, init = 4, finish = 0

22/07/19 13:14:50 INFO Executor: Finished task 232.0 in stage 137.0 (TID 10686). 1785 bytes result sent to driver

22/07/19 13:14:50 INFO PythonRunner: Times: total = 10, boot = 9, init = 1, finish = 0

22/07/19 13:14:50 INFO Executor: Finished task 233.0 in stage 137.0 (TID 10687). 1785 bytes result sent to driver

22/07/19 13:14:50 INFO CoarseGrainedExecutorBackend: Got assigned task 10688

22/07/19 13:14:50 INFO Executor: Running task 234.0 in stage 137.0 (TID 10688)

22/07/19 13:14:50 INFO CoarseGrainedExecutorBackend: Got assigned task 10689

22/07/19 13:14:50 INFO Executor: Running task 235.0 in stage 137.0 (TID 10689)

22/07/19 13:14:50 INFO PythonRunner: Times: total = 1, boot = 1, init = 0, finish = 0

22/07/19 13:14:50 INFO Executor: Finished task 235.0 in stage 137.0 (TID 10689). 1785 bytes result sent to driver

22/07/19 13:14:50 INFO CoarseGrainedExecutorBackend: Got assigned task 10690

22/07/19 13:14:50 INFO Executor: Running task 236.0 in stage 137.0 (TID 10690)

22/07/19 13:14:50 WARN JdbcUtils: Requested isolation level 1 is not supported; falling back to default isolation level

2 22/07/19 13:14:50 INFO PythonRunner: Times: total = 42, boot = -13, init = 55, finish = 0

22/07/19 13:14:50 INFO Executor: Finished task 231.0 in stage 137.0 (TID 10685). 1785 bytes result sent to driver

22/07/19 13:14:50 INFO CoarseGrainedExecutorBackend: Got assigned task 10691

22/07/19 13:14:50 INFO Executor: Running task 237.0 in stage 137.0 (TID 10691)

22/07/19 13:14:50 INFO PythonRunner: Times: total = 43, boot = -4, init = 47, finish = 0

22/07/19 13:14:50 INFO Executor: Finished task 234.0 in stage 137.0 (TID 10688). 1785 bytes result sent to driver

22/07/19 13:14:50 INFO CoarseGrainedExecutorBackend: Got assigned task 10692

22/07/19 13:14:50 INFO Executor: Running task 238.0 in stage 137.0 (TID 10692)

22/07/19 13:14:50 INFO PythonRunner: Times: total = 43, boot = 2, init = 41, finish = 0

22/07/19 13:14:50 INFO Executor: Finished task 236.0 in stage 137.0 (TID 10690). 1785 bytes result sent to driver

22/07/19 13:14:50 INFO CoarseGrainedExecutorBackend: Got assigned task 10693

22/07/19 13:14:50 INFO Executor: Running task 239.0 in stage 137.0 (TID 10693)

22/07/19 13:14:50 INFO JDBCRDD: closed connection 22/07/19 13:14:50 INFO PythonRunner: Times: total = 44, boot = 3, init = 41, finish = 0

22/07/19 13:14:50 INFO Executor: Finished task 237.0 in stage 137.0 (TID 10691). 1785 bytes result sent to driver

22/07/19 13:14:50 INFO PythonRunner: Times: total = 44, boot = 2, init = 42, finish = 0

22/07/19 13:14:50 INFO Executor: Finished task 238.0 in stage 137.0 (TID 10692). 1785 bytes result sent to driver

22/07/19 13:14:50 INFO Executor: Finished task 219.0 in stage 137.0 (TID 10673). 1785 bytes result sent to driver

22/07/19 13:14:50 INFO PythonRunner: Times: total = 42, boot = 2, init = 40, finish = 0

22/07/19 13:14:50 INFO Executor: Finished task 239.0 in stage 137.0 (TID 10693). 1785 bytes result sent to driver

22/07/19 13:14:50 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown

22/07/19 13:14:50 ERROR CoarseGrainedExecutorBackend: RECEIVED SIGNAL TERM

 
Environment: Ubuntu 20.04

Python 3.8.10

Java 8

 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python
Custom field (Last public comment date): Fri Jul 22 13:18:55 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16z3k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Jul/22 13:44;gurwls223;Does this cause any actual issue? or just error log?;;;, 22/Jul/22 13:18;Ghasemi;No, it is an error LOG. ;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 22/Jul/22 13:18;Ghasemi;No, it is an error LOG. ;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Missing sbin scripts in PySpark packages
Issue key: SPARK-39817
Issue id: 13472272
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: hoeze
Creator: hoeze
Created: 19/Jul/22 13:02
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.2.1, 3.2.2, 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: easyfix
Description: In the PySpark setup.py, only a subset of all scripts is included.
I'm in particular missing the `submit-all.sh` script:
{code:python}
        package_data={
            'pyspark.jars': ['*.jar'],
            'pyspark.bin': ['*'],
            'pyspark.sbin': ['spark-config.sh', 'spark-daemon.sh',
                             'start-history-server.sh',
                             'stop-history-server.sh', ],

            [...]
        },
{code}
 

The solution is super simple, just change 'pyspark.sbin' to:
{code:python}
'pyspark.sbin': ['*'],
{code}
 

I would happily submit a PR to github, but I have no clue on the organizational details.

This would be great to get backported for pyspark 3.2.x as well as 3.3.x soon.
Environment: 
Original Estimate: 300.0
Remaining Estimate: 300.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 300.0
Σ Remaining Estimate: 300.0
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important, Patch
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): python
Custom field (Last public comment date): Thu Jul 21 13:43:17 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16zbk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Jul/22 13:43;gurwls223;pip is designed for using it in Python. I would prefer to avoid people to create a Spark cluster by using pip.;;;
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: WARN WindowExec: No Partition Defined is being printed 4 times. 
Issue key: SPARK-37174
Issue id: 13409347
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bjornjorgensen
Creator: bjornjorgensen
Created: 31/Oct/21 21:53
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Hi I use this code  
{code:java}
f01 = spark.read.json("/home/test_files/falk/flatted110721/F01.json/*.json")
pf01 = f01.to_pandas_on_spark()
pf01 = pf01.rename(columns=lambda x: re.sub(':P$', '', x))
pf01["OBJECT_CONTRACT:DATE_PUBLICATION_NOTICE"] = ps.to_datetime(pf01["OBJECT_CONTRACT:DATE_PUBLICATION_NOTICE"])
pf01.info(){code}
 

 sometimes it prints 
  
{code:java}
 21/10/31 20:38:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
 21/10/31 20:38:04 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
 21/10/31 20:38:08 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
 /opt/spark/python/pyspark/sql/pandas/conversion.py:214: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`
   df[column_name] = series
 /opt/spark/python/pyspark/pandas/utils.py:967: UserWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas Series is expected to be small.
   warnings.warn(message, UserWarning)
 21/10/31 20:38:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
 21/10/31 20:38:18 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.{code}
 
 and some other times it "just" prints 
  
{code:java}
 21/10/31 21:24:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
 21/10/31 21:24:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
 21/10/31 21:24:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
 21/10/31 21:24:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.{code}
Why does it print df[column_name] = series ?
  
 can we remove /opt/spark/python/pyspark/pandas/utils.py:967: ?
 and warnings.warn(message, UserWarning) ?
 and 3 of WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.?

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Nov 02 07:56:59 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wbmw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Nov/21 07:56;gurwls223;This is related to default index, see also https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#default-index-type.

Spark 3.3 targets to remove such warnings for natively supporting global windows. That's slightly orthogonal from this issue though.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Flaky Test: BloomFilterAggregateQuerySuite
Issue key: SPARK-39386
Issue id: 13448500
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dongjoon
Creator: dongjoon
Created: 06/Jun/22 05:24
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL, Tests
Due Date: 
Votes: 0
Labels: 
Description: During Apache Spark 3.3.0 RC5 tests, I found that this test case is very flaky in my environment.
{code:java}
     [info] - Test bloom_filter_agg and might_contain *** FAILED *** (20 seconds, 370 milliseconds)
     [info]   Results do not match for query:
     [info]   Timezone: sun.util.calendar.ZoneInfo[id="America/Los_Angeles",offset=-28800000,dstSavings=3600000,useDaylight=true,transitions=185,lastRule=java.util.SimpleTimeZone[id=America/Los_Angeles,offset=-28800000,dstSavings=3600000,useDaylight=true,startYear=0,startMode=3,startMonth=2,startDay=8,startDayOfWeek=1,startTime=7200000,startTimeMode=0,endMode=3,endMonth=10,endDay=1,endDayOfWeek=1,endTime=7200000,endTimeMode=0]]
     [info]   Timezone Env: 
...

  == Results ==
     [info]   !== Correct Answer - 1 ==   == Spark Answer - 1 ==
     [info]   !struct<>                   struct<positive_membership_test:boolean,negative_membership_test:boolean>
     [info]   ![true,false]               [true,true] (QueryTest.scala:244)
     [info]   org.scalatest.exceptions.TestFailedException:
     [info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
     [info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
     [info]   at org.apache.spark.sql.QueryTest$.newAssertionFailedException(QueryTest.scala:234)
     [info]   at org.scalatest.Assertions.fail(Assertions.scala:933)
     [info]   at org.scalatest.Assertions.fail$(Assertions.scala:929)
     [info]   at org.apache.spark.sql.QueryTest$.fail(QueryTest.scala:234)
     [info]   at org.apache.spark.sql.QueryTest$.checkAnswer(QueryTest.scala:244)
     [info]   at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:151)
     [info]   at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:155)
     [info]   at org.apache.spark.sql.BloomFilterAggregateQuerySuite.$anonfun$new$4(BloomFilterAggregateQuerySuite.scala:98)
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-32268
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 06 10:53:47 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12yuw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Jun/22 05:48;dongjoon;Currently, I set this as a normal flaky test issue instead of `Blocker`.;;;, 06/Jun/22 07:37;yumwang;[~dongjoon] Could you provide your environment. I can't reproduce it.;;;, 06/Jun/22 07:41;dongjoon;In my case, it happens on Linux box. I've been trying on Mac, but I couldn't reproduce either yet, [~yumwang] .

I'll reply on RC5 email to get more attentions. But, this is a non-blocker for now.;;;, 06/Jun/22 10:53;gurwls223;cc [~asomani] FYI;;;
Affects Version/s.1: 
Component/s.1: Tests
Comment.1: 06/Jun/22 07:37;yumwang;[~dongjoon] Could you provide your environment. I can't reproduce it.;;;
Comment.2: 06/Jun/22 07:41;dongjoon;In my case, it happens on Linux box. I've been trying on Mac, but I couldn't reproduce either yet, [~yumwang] .

I'll reply on RC5 email to get more attentions. But, this is a non-blocker for now.;;;
Comment.3: 06/Jun/22 10:53;gurwls223;cc [~asomani] FYI;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Pyspark incompatible with Pypy
Issue key: SPARK-40704
Issue id: 13485170
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ggbaker
Creator: ggbaker
Created: 07/Oct/22 15:11
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Starting Spark with a recent Pypy (>3.6) fails because of an incompatibility between their pickle implementation and cloudpickle:
{quote}{{% PYSPARK_PYTHON=pypy3 ./bin/pyspark}}

{{...}}

{{ModuleNotFoundError: No module named '_pickle'}}
{quote}
 

It seems to be related to [this cloudpickle issue|https://github.com/cloudpipe/cloudpickle/issues/455], which has been fixed upstream. I was able to work around by replacing the Spark-provided cloudpickle (python/pyspark/cloudpickle) with the code from their git repo (and deleting pyspark.zip to purge that copy).

 

 

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 12 03:56:23 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z196dk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Oct/22 03:56;gurwls223;[~ggbaker] Please go ahead and upgrade cloudpickle. See also https://github.com/apache/spark/pull/34705;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support tuple for non-MultiIndex column name.
Issue key: SPARK-37723
Issue id: 13419151
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: itholic
Creator: itholic
Created: 23/Dec/21 07:03
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: pandas API on Spark doesn't support tuple as column name for non-MultiIndex column.
{code:java}
>>> psdf = ps.DataFrame({"A": [1, 2, 3]})
>>> psdf
A
0 1
1 2
2 3
>>> psdf[('a', 'b')] = [4, 5, 6]
Traceback (most recent call last):
...
KeyError: 'Key length (2) exceeds index depth (1)'
{code}
As pandas support this, we should follow the behavior.
{code:java}
>>> pdf = pd.DataFrame({"A": [1, 2, 3]})
>>> pdf[('a', 'b')] = [4, 5, 6]
>>> pdf
   A  (a, b)
0  1       4
1  2       5
2  3       6{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jan 04 01:52:00 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xzg8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Jan/22 16:55;subkanthi;Hi [~itholic] , is someone working on this, I can take on this, some direction would be extremely helpful. Thanks.;;;, 04/Jan/22 00:56;gurwls223;[~itholic] are you working on this?;;;, 04/Jan/22 01:00;itholic;[~subkanthi] You can take this one, please go ahead :);;;, 04/Jan/22 01:24;subkanthi;Thanks, will get started and will ask questions if I need more info.;;;, 04/Jan/22 01:52;itholic;Sure!;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 04/Jan/22 00:56;gurwls223;[~itholic] are you working on this?;;;
Comment.2: 04/Jan/22 01:00;itholic;[~subkanthi] You can take this one, please go ahead :);;;
Comment.3: 04/Jan/22 01:24;subkanthi;Thanks, will get started and will ask questions if I need more info.;;;
Comment.4: 04/Jan/22 01:52;itholic;Sure!;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: pandas and pandas on Spark API parameter naming difference 
Issue key: SPARK-39747
Issue id: 13471082
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ChenyangZhang
Creator: ChenyangZhang
Created: 11/Jul/22 22:50
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Pandas API on Spark
Due Date: 
Votes: 0
Labels: 
Description: I noticed there are some parameter naming differences between pandas and pandas on Spark. For example, in "read_csv", the path parameter is "filepath_or_buffer" for pandas and "path" for pandas on Spark. I wonder why such a difference exists and may I ask to change it to match exactly the same in pandas. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jul 14 00:03:22 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16s00:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Jul/22 00:23;gurwls223;Yeah, we should ideally fix them all;;;, 14/Jul/22 00:03;gurwls223;the reason is that we don't support buffer for now (since that strictly assems that the data is in local). But probably we should fallback to pandas for the time being.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 14/Jul/22 00:03;gurwls223;the reason is that we don't support buffer for now (since that strictly assems that the data is in local). But probably we should fallback to pandas for the time being.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: [PYSPARK] Publish ApacheSparkGitHubActionImage arm64 docker image
Issue key: SPARK-37772
Issue id: 13419801
Parent id: 13381606.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 29/Dec/21 03:06
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jan 07 02:45:34 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y3g8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Dec/21 07:44;yikunkero;There are some depdency conflict on: [https://github.com/scipy/oldest-supported-numpy/issues/39]

pandas and scipy are depends on it so we couldn't install them on aarch64 with pypy3 now.

I submitted a fix: [https://github.com/scipy/oldest-supported-numpy/pull/40]

Now, it's merged, just wait for next release and have a try agiain.;;;, 30/Dec/21 12:32;yikunkero;[https://pypi.org/project/oldest-supported-numpy/0.15/]

 

oldest-supported-numpy v0.15 released, I will add pypy3 to arm image.;;;, 31/Dec/21 01:44;yikunkero;For CPython(such like, python 3.8, python 3.9):

all related can be installed in aarch64.

 

For Pandas with pypy3

pandas are [using `oldest-supported-numpy>=0.10`](https://github.com/pandas-dev/pandas/blob/6bc636672513cf27cb37602976ef88cc12789935/pyproject.toml#L8) , I can install pandas lastest version now.

 

For scipy with pypy3
- there still some conflict on scipy v1.7.x: [https://github.com/scipy/scipy/pull/15317]

- no conflict on scipy master, but better to also sync this info: [https://github.com/scipy/scipy/pull/15318]

 

So we could first skip install scipy first untill scipy v1.8.0 and v1.7.x (maybe v1.7.4) released.;;;, 02/Jan/22 00:45;gurwls223;Actually we have been working on publishing docker images for releases, and Holden has been working on that.

cc [~holden], [~dongjoon] [~gengliang] FYI;;;, 03/Jan/22 03:16;yikunkero;[~hyukjin.kwon] Thanks, if I remembered correctly they are working on spark kubernetes docker official image? This is the github action image for pyspark related test.

 

Here is complete images build PR can meet the pyspark requriment

[https://github.com/Yikun/ApacheSparkGitHubActionImage/pull/4]

it passed in Local SPARK TEST: [https://github.com/Yikun/spark/pull/53]

 ;;;, 07/Jan/22 02:45;yikunkero;Add the PR in https://github.com/dongjoon-hyun/ApacheSparkGitHubActionImage/pull/6;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 30/Dec/21 12:32;yikunkero;[https://pypi.org/project/oldest-supported-numpy/0.15/]

 

oldest-supported-numpy v0.15 released, I will add pypy3 to arm image.;;;
Comment.2: 31/Dec/21 01:44;yikunkero;For CPython(such like, python 3.8, python 3.9):

all related can be installed in aarch64.

 

For Pandas with pypy3

pandas are [using `oldest-supported-numpy>=0.10`](https://github.com/pandas-dev/pandas/blob/6bc636672513cf27cb37602976ef88cc12789935/pyproject.toml#L8) , I can install pandas lastest version now.

 

For scipy with pypy3
- there still some conflict on scipy v1.7.x: [https://github.com/scipy/scipy/pull/15317]

- no conflict on scipy master, but better to also sync this info: [https://github.com/scipy/scipy/pull/15318]

 

So we could first skip install scipy first untill scipy v1.8.0 and v1.7.x (maybe v1.7.4) released.;;;
Comment.3: 02/Jan/22 00:45;gurwls223;Actually we have been working on publishing docker images for releases, and Holden has been working on that.

cc [~holden], [~dongjoon] [~gengliang] FYI;;;
Comment.4: 03/Jan/22 03:16;yikunkero;[~hyukjin.kwon] Thanks, if I remembered correctly they are working on spark kubernetes docker official image? This is the github action image for pyspark related test.

 

Here is complete images build PR can meet the pyspark requriment

[https://github.com/Yikun/ApacheSparkGitHubActionImage/pull/4]

it passed in Local SPARK TEST: [https://github.com/Yikun/spark/pull/53]

 ;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: pyspark.pandas.DataFrame.drop drops dataframe if axis not specified
Issue key: SPARK-39732
Issue id: 13470894
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: itsmeandy
Creator: itsmeandy
Created: 11/Jul/22 02:32
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Pandas API on Spark
Due Date: 
Votes: 0
Labels: 
Description: import pyspark.pandas as pd
data = [\{"Category": 'A', "ID": 1, "Value": 121.44, "Truth": True},
        \{"Category": 'B', "ID": 2, "Value": 300.01, "Truth": False},
        \{"Category": 'C', "ID": 3, "Value": 10.99, "Truth": None},
        \{"Category": 'E', "ID": 4, "Value": 33.87, "Truth": True}
        ]
df = pd.DataFrame(data)
df.display()

--drops dataframe "Query returned no results"
df1=df.drop(["ID","Category"])
df1.display()

--works

df2=df.drop(["ID","Category"], 1)
df2.display()
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jul 14 17:39:20 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16qug:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Jul/22 02:36;itsmeandy;Introduced after 2022.07.04;;;, 13/Jul/22 00:21;gurwls223;[~podongfeng] would you mind taking a look when you find some time?;;;, 13/Jul/22 05:38;podongfeng;OK, will take a look [~hyukjin.kwon];;;, 14/Jul/22 09:49;podongfeng;reproducer:

{code:python}
import pandas as pd
import pyspark.pandas as ps

data = [{"Category": 'A', "ID": 1, "Value": 121.44, "Truth": True},
        {"Category": 'B', "ID": 2, "Value": 300.01, "Truth": False},
        {"Category": 'C', "ID": 3, "Value": 10.99, "Truth": None},
        {"Category": 'E', "ID": 4, "Value": 33.87, "Truth": True}]

pdf = pd.DataFrame(data)
psdf = ps.DataFrame(data)


pdf.drop(["ID","Category"])
pdf.drop(["ID","Category"], 0)
pdf.drop(["ID","Category"], 1)

psdf.drop(["ID","Category"])
psdf.drop(["ID","Category"], 0)
psdf.drop(["ID","Category"], 1)
{code}

pandas threw a KeyError, while ps (pandas API on Spark) returns an empty frame.

{code:python}
pdf.drop(["ID","Category"])

File ~/.zrf/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:6644, in Index.drop(self, labels, errors)
   6642 if mask.any():
   6643     if errors != "ignore":
-> 6644         raise KeyError(f"{list(labels[mask])} not found in axis")
   6645     indexer = indexer[~mask]
   6646 return self.delete(indexer)

KeyError: "['ID', 'Category'] not found in axis"
{code}

Pandas API on Spark:
{code:python}
psdf.drop(["ID","Category"])

Out[4]: 
Empty DataFrame
Columns: [Category, ID, Value, Truth]
Index: []
{code}


that is due to the underlying casting from String to BigInt,


{code:python}
                            cond = ~internal.index_spark_columns[0].isin(
                                [SF.lit(label).cast(self_index_type) for label in index]
                            )
{code}

'ID', 'Category' are both casted to 'null', and then all non-null indices are dropped.

;;;, 14/Jul/22 09:54;podongfeng;I think we may not need to make PS's behavior exactly the same as pandas: (checking whether input labels existing in the indices, which costs an extra action).

Maybe just returning the whole dataframe is acceptable? [~hyukjin.kwon][~xinrong];;;, 14/Jul/22 10:00;podongfeng;[~itsmeandy]  the default value for index is 0, which will try to drop the row indices by the given labels "ID" and "Category". You should explictly set index=1, if you want to drop columns.;;;, 14/Jul/22 13:19;itsmeandy;this is a behavior change from old spark version. I have used this in production code and had an incident due to this. Default axis used to be 1.;;;, 14/Jul/22 13:26;itsmeandy;3.2.0

[https://spark.apache.org/docs/3.3.0/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.drop.html#pyspark.pandas.DataFrame.drop]

3.3.0

[https://spark.apache.org/docs/3.3.0/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.drop.html#pyspark.pandas.DataFrame.drop]

 ;;;, 14/Jul/22 13:26;itsmeandy;Seems weird to change default.;;;, 14/Jul/22 17:31;XinrongM;How about we match pandas behavior/results and utilize `compute.eager_check`([here|https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/options.html#available-options]) to allow users to disable the check?;;;, 14/Jul/22 17:39;XinrongM;Thanks [~itsmeandy] for raising that!

 

Previously, the default was set 1 because only dropping columns was supported at that time; after dropping rows is supported, pandas API on Spark tries to match pandas behavior (e.g. PySpark 3.3 matches pandas 1.3 [DataFrame.drop|https://pandas.pydata.org/pandas-docs/version/1.3.0/reference/api/pandas.DataFrame.drop.html]).;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 13/Jul/22 00:21;gurwls223;[~podongfeng] would you mind taking a look when you find some time?;;;, 14/Jul/22 17:39;XinrongM;Thanks [~itsmeandy] for raising that!

 

Previously, the default was set 1 because only dropping columns was supported at that time; after dropping rows is supported, pandas API on Spark tries to match pandas behavior (e.g. PySpark 3.3 matches pandas 1.3 [DataFrame.drop|https://pandas.pydata.org/pandas-docs/version/1.3.0/reference/api/pandas.DataFrame.drop.html]).;;;
Comment.2: 13/Jul/22 05:38;podongfeng;OK, will take a look [~hyukjin.kwon];;;
Comment.3: 14/Jul/22 09:49;podongfeng;reproducer:

{code:python}
import pandas as pd
import pyspark.pandas as ps

data = [{"Category": 'A', "ID": 1, "Value": 121.44, "Truth": True},
        {"Category": 'B', "ID": 2, "Value": 300.01, "Truth": False},
        {"Category": 'C', "ID": 3, "Value": 10.99, "Truth": None},
        {"Category": 'E', "ID": 4, "Value": 33.87, "Truth": True}]

pdf = pd.DataFrame(data)
psdf = ps.DataFrame(data)


pdf.drop(["ID","Category"])
pdf.drop(["ID","Category"], 0)
pdf.drop(["ID","Category"], 1)

psdf.drop(["ID","Category"])
psdf.drop(["ID","Category"], 0)
psdf.drop(["ID","Category"], 1)
{code}

pandas threw a KeyError, while ps (pandas API on Spark) returns an empty frame.

{code:python}
pdf.drop(["ID","Category"])

File ~/.zrf/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:6644, in Index.drop(self, labels, errors)
   6642 if mask.any():
   6643     if errors != "ignore":
-> 6644         raise KeyError(f"{list(labels[mask])} not found in axis")
   6645     indexer = indexer[~mask]
   6646 return self.delete(indexer)

KeyError: "['ID', 'Category'] not found in axis"
{code}

Pandas API on Spark:
{code:python}
psdf.drop(["ID","Category"])

Out[4]: 
Empty DataFrame
Columns: [Category, ID, Value, Truth]
Index: []
{code}


that is due to the underlying casting from String to BigInt,


{code:python}
                            cond = ~internal.index_spark_columns[0].isin(
                                [SF.lit(label).cast(self_index_type) for label in index]
                            )
{code}

'ID', 'Category' are both casted to 'null', and then all non-null indices are dropped.

;;;
Comment.4: 14/Jul/22 09:54;podongfeng;I think we may not need to make PS's behavior exactly the same as pandas: (checking whether input labels existing in the indices, which costs an extra action).

Maybe just returning the whole dataframe is acceptable? [~hyukjin.kwon][~xinrong];;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Migrate to M1 machines in Jenkins
Issue key: SPARK-37740
Issue id: 13419431
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 26/Dec/21 07:09
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Project Infra
Due Date: 
Votes: 0
Labels: 
Description: See https://mail-archives.apache.org/mod_mbox/spark-dev/202112.mbox/%3CCACdU-dTLuB--1GzAv6XfS-pCrcihhvDpUMrGe%3DfJXUYJpqiX9Q%40mail.gmail.com%3E.

We should revisit all related Jenkins specific codes when M1 machines are ready.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Dec 27 07:11:19 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y160:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Dec/21 00:23;gurwls223;cc [~dongjoon] [~viirya] [~dbtsai] FYI;;;, 27/Dec/21 07:11;dongjoon;Thank you, [~hyukjin.kwon]!;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 27/Dec/21 07:11;dongjoon;Thank you, [~hyukjin.kwon]!;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support read/write.csv() in SparkR
Issue key: SPARK-40103
Issue id: 13477015
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: deshanxiao
Creator: deshanxiao
Created: 16/Aug/22 09:13
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SparkR
Due Date: 22/Aug/22 00:00
Votes: 0
Labels: 
Description: Today, almost languages support the DataFrameReader.csv API, only R is missing. we need to use df.read() to read the csv file. We need a more high-level api for it.

Java:
[DataFrameReader.csv()|https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html]

Scala:
[DataFrameReader.csv()|https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameReader.html#csv(paths:String*):org.apache.spark.sql.DataFrame]

Python:
[DataFrameReader.csv()|https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 17 07:18:54 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17shk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Aug/22 01:29;gurwls223;The main problem is that the signature conflicts with R base API IIRC. We should probably use a different name for this.;;;, 17/Aug/22 06:45;deshanxiao;Yes read.csv, read.csv2 have been used in R utils packages.;;;, 17/Aug/22 07:18;apachespark;User 'deshanxiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/37549;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 17/Aug/22 06:45;deshanxiao;Yes read.csv, read.csv2 have been used in R utils packages.;;;
Comment.2: 17/Aug/22 07:18;apachespark;User 'deshanxiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/37549;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: DS V2 Index Support
Issue key: SPARK-36525
Issue id: 13395514
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: huaxingao
Creator: huaxingao
Created: 16/Aug/21 17:17
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Many data sources support index to improvement query performance. In order to take advantage of the index support in data source, the following APIs will be added for working with indexes:

{code:java}
 public interface SupportsIndex extends Table {

  /**
   * Creates an index.
   *
   * @param indexName the name of the index to be created
   * @param indexType the type of the index to be created. If this is not specified, Spark
   *                  will use empty String.
   * @param columns the columns on which index to be created
   * @param columnsProperties the properties of the columns on which index to be created
   * @param properties the properties of the index to be created
   * @throws IndexAlreadyExistsException If the index already exists.
   */
  void createIndex(String indexName,
      String indexType,
      NamedReference[] columns,
      Map<NamedReference, Map<String, String>> columnsProperties,
      Map<String, String> properties)
      throws IndexAlreadyExistsException;

  /**
   * Drops the index with the given name.
   *
   * @param indexName the name of the index to be dropped.
   * @throws NoSuchIndexException If the index does not exist.
   */
  void dropIndex(String indexName) throws NoSuchIndexException;

  /**
   * Checks whether an index exists in this table.
   *
   * @param indexName the name of the index
   * @return true if the index exists, false otherwise
   */
  boolean indexExists(String indexName);

  /**
   * Lists all the indexes in this table.
   */
  TableIndex[] listIndexes();
}

{code}


Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Nov 29 06:54:24 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tybk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Aug/21 04:39;gurwls223;would you mind outlining the interface?;;;, 14/Oct/21 08:09;dchvn;[~huaxingao], Should we do these functions for supportsIndex in JDBC for the other dialects like Oracle, Postgres, etc.?;;;, 14/Oct/21 14:14;huaxingao;Yes, it would be great if you can please help [~dchvn];;;, 15/Oct/21 01:52;dchvn;[~huaxingao] yes, i'd like to;;;, 29/Nov/21 06:05;LuciferYang;Do we plan to make FileTable support the trait of SupportIndex;;;, 29/Nov/21 06:54;huaxingao;The major reason I work on index support is because I have customers who need this in iceberg. I don't have any plan to make FileTable implement SupportIndex because parquet or ORC doesn't support index. ;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 14/Oct/21 08:09;dchvn;[~huaxingao], Should we do these functions for supportsIndex in JDBC for the other dialects like Oracle, Postgres, etc.?;;;
Comment.2: 14/Oct/21 14:14;huaxingao;Yes, it would be great if you can please help [~dchvn];;;
Comment.3: 15/Oct/21 01:52;dchvn;[~huaxingao] yes, i'd like to;;;
Comment.4: 29/Nov/21 06:05;LuciferYang;Do we plan to make FileTable support the trait of SupportIndex;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Reenable TPC-DS q72 in GitHub Actions
Issue key: SPARK-39903
Issue id: 13473896
Parent id: 
Issue Type: Test
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 28/Jul/22 02:47
Updated: 12/Dec/22 17:51
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.4.0
Fix Version/s: 
Component/s: Tests
Due Date: 
Votes: 0
Labels: 
Description: https://github.com/apache/spark/pull/37289 disabled TPC-DS q72 in GitHub Actions. We should reenable this to recover the test coverage.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-07-28 02:47:16.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z179c8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.4.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Revisit codes in Scala tests
Issue key: SPARK-37743
Issue id: 13419434
Parent id: 13419431.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 26/Dec/21 07:14
Updated: 12/Dec/22 17:51
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Tests
Due Date: 
Votes: 0
Labels: 
Description: {code}
core/src/test/scala/org/apache/spark/deploy/SparkSubmitSuite.scala:  // TODO(SPARK-9603): Building a package is flaky on Jenkins Maven builds.
core/src/test/scala/org/apache/spark/deploy/SparkSubmitTestUtils.scala:      // This test suite has some weird behaviors when executed on Jenkins:
core/src/test/scala/org/apache/spark/deploy/SparkSubmitTestUtils.scala:      // 1. Sometimes it gets extremely slow out of unknown reason on Jenkins.  Here we add a
core/src/test/scala/org/apache/spark/deploy/master/MasterSuite.scala:      // is only 2, while on Jenkins it's 32. For this specific test, 2 available processors, which
external/kafka-0-10-sql/src/test/scala/org/apache/spark/sql/kafka010/KafkaDontFailOnDataLossSuite.scala: * when running on a slow Jenkins machine) before records start to be removed. To make sure a test
project/SparkBuild.scala:      // with Jenkins flakiness.
repl/src/test/scala/org/apache/spark/repl/SparkShellSuite.scala:      // This test suite sometimes gets extremely slow out of unknown reason on Jenkins.  Here we
sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala:    // TODO: Why fs.getContentSummary returns wrong size on Jenkins?
sql/core/src/main/scala/org/apache/spark/sql/execution/command/CommandUtils.scala:    // Seems fs.getContentSummary returns wrong table size on Jenkins. So we use
sql/core/src/test/scala/org/apache/spark/sql/SQLQueryTestSuite.scala:    // the last dot due to a bug in SBT. This makes easier to debug via Jenkins test result
sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreIntegrationSuite.scala:          // Should emit new progresses every 10 ms, but we could be facing a slow Jenkins
sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamingQueryStatusAndProgressSuite.scala:        // Should emit new progresses every 10 ms, but we could be facing a slow Jenkins
sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/CliSuite.scala:      // This test suite sometimes gets extremely slow out of unknown reason on Jenkins.  Here we
sql/hive-thriftserver/src/test/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2Suites.scala:          // started at a time, which is not Jenkins friendly.
sql/hive/src/test/scala/org/apache/spark/sql/hive/client/HiveClientBuilder.scala:  // In order to speed up test execution during development or in Jenkins, you can specify the path
sql/hive/src/test/scala/org/apache/spark/sql/sources/ParquetHadoopFsRelationSuite.scala:  // more cores, the issue can be reproduced steadily.  Fortunately our Jenkins builder meets this
streaming/src/test/scala/org/apache/spark/streaming/util/WriteAheadLogSuite.scala:      // If Jenkins is slow, we may not have a chance to run many threads simultaneously. Having
core/src/test/scala/org/apache/spark/scheduler/TaskSchedulerImplSuite.scala:      // This is to avoid any potential flakiness in the test because of large pauses in jenkins
sql/core/src/test/java/test/org/apache/spark/sql/JavaDataFrameSuite.java:      // (e.g., /home/jenkins/workspace/SparkPullRequestBuilder@2)
sql/core/src/test/resources/sql-tests/inputs/datetime-parsing-invalid.sql:-- in java 8 this case is invalid, but valid in java 11, disabled for jenkins
sql/core/src/test/scala/org/apache/spark/sql/execution/vectorized/ColumnarBatchSuite.scala:    // TODO: Figure out why StringType doesn't work on jenkins.
sql/hive/compatibility/src/test/scala/org/apache/spark/sql/hive/execution/HiveCompatibilitySuite.scala:    // Weird DDL differences result in failures on jenkins.
project/SparkBuild.scala:      // with Jenkins flakiness.
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-26 07:14:27.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y16o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Revisit codes in PySpark, SparkR and other docs
Issue key: SPARK-37744
Issue id: 13419435
Parent id: 13419431.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 26/Dec/21 07:17
Updated: 12/Dec/22 17:51
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark, Tests
Due Date: 
Votes: 0
Labels: 
Description: {code}
R/check-cran.sh:# Jenkins installs arrow. See SPARK-29339.
R/pkg/tests/run-all.R:  # CRAN machines. For Jenkins we should already have SPARK_HOME set.
R/run-tests.sh:    # We have 2 NOTEs: for RoxygenNote and one in Jenkins only "No repository set"
dev/run-pip-tests:# Jenkins has PySpark installed under user sitepackages shared for some reasons.
python/pyspark/sql/tests/test_streaming.py:            # Jenkins is very slow, we don't assert it. If there is something wrong, "lastProgress"
python/pyspark/streaming/tests/test_kinesis.py:        # Don't start the StreamingContext because we cannot test it in Jenkins
dev/create-release/release-build.sh:# This is a band-aid fix to avoid the failure of Maven nightly snapshot in some Jenkins
docs/building-spark.md:## Running Jenkins tests with GitHub Enterprise
docs/building-spark.md:To run tests with Jenkins:
dev/tests/pr_merge_ability.sh:# found at dev/run-tests-jenkins.
dev/tests/pr_merge_ability.sh:# known as `ghprbActualCommit` in `run-tests-jenkins`
dev/tests/pr_merge_ability.sh:# known as `sha1` in `run-tests-jenkins`
dev/tests/pr_public_classes.sh:# found at dev/run-tests-jenkins.
dev/tests/pr_public_classes.sh:# known as `ghprbActualCommit` in `run-tests-jenkins`
docs/building-spark.md:    ./dev/run-tests-jenkins
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-26 07:17:27.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y16w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Tests
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Implement DataFrame.mapInArrow in Scala
Issue key: SPARK-37229
Issue id: 13410443
Parent id: 13410441.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 07/Nov/21 05:10
Updated: 12/Dec/22 17:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-07 05:10:25.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wi9k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: DataFrame.mapInArrow
Issue key: SPARK-37227
Issue id: 13410441
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 07/Nov/21 05:08
Updated: 12/Dec/22 17:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark, SQL
Due Date: 
Votes: 0
Labels: 
Description: h2. Background

*Usability problem*

In Scala APIs, we have added Arrow integration and developer APIs in Apache Spark, for example, at ArrowConverters but this is again inconvenient to use out of the box.

In PySpark, in order to use Arrow format to connect to other external systems, they should manually convert pandas DataFrame in pandas UDF to the Arrow batch, which is inconvenient.


*Technical problem*

pandas UDFs are used in other use cases too. For example, they convert it back to
Arrow batch, and integrate with other systems, see also https://github.com/apache/spark/pull/26783#issue-534127514.
However, it doesn’t vectorize operations because pandas doesn’t support nested structure
natively, and the performance impact seems non-trivial.

In addition, it requires virtually copying during the conversion between pandas and Arrow format that consumes computation (Spark internal format -> Arrow format -> pandas DataFrame -> Arrow format). See https://github.com/apache/spark/pull/26783#issue-534127514 for performance impact.


h2. Other notes:

See also:
- SPARK-30153
- SPARK-26413

h2. Proposal

I would like to propose an API DataFrame.mapInArrow like {{DataFrame.mapInPandas}}, and {{RDD.mapPartitions}}.

The API shape would look like:

*Scala:*

{code}
def mapInArrow(
    f: Iterator[ArrowRecordBatch] => Iterator[ArrowRecordBatch],
    schema: StructType): DataFrame = {
  // ...
}
{code}

{code}
df.mapInArrow(_.map { case arrowBatch: ArrowRecordBatch =>
  // do something with `ArrowRecordBatch` and create new `ArrowRecordBatch`.
  // ...
  arrowBatch
}, df.schema).show()
{code}


*Python:*

{code}
def mapInArrow(
        self,
        func: Callable[Iterator[pyarrow.RecordBatch], Iterator[pyarrow.RecordBatch]],
        schema: StructType) -> DataFrame:
    # ...
{code}

{code}
def do_something(iterator):
    for arrow_batch in iterator:
        # do something with `pyarrow.RecordBatch` and create new `pyarrow.RecordBatch`.
        # ...
        yield arrow_batch

df.mapInPandas(do_something, df.schema).show()
{code}



Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): SPARK-30153
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-07 05:08:51.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wi94:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: SQL
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Keep file permissions for .tar archives
Issue key: SPARK-38632
Issue id: 13435266
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 23/Mar/22 04:51
Updated: 12/Dec/22 17:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: SPARK-38631 disallowed file permissions for .tar archives to work around a security issue. We should restore it back.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-03-23 04:51:22.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10qg8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add Jenkins badge back when M1 machines are ready
Issue key: SPARK-37748
Issue id: 13419465
Parent id: 13419431.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 27/Dec/21 00:21
Updated: 12/Dec/22 17:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Documentation, Project Infra
Due Date: 
Votes: 0
Labels: 
Description: Currently, the badge is removed as Jenkins is retired at https://issues.apache.org/jira/browse/SPARK-37741. Should consider bringing the badge back when M1 machines are ready.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-27 00:21:03.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y1dk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Project Infra
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Revisit codes in testing scripts for Jenkins
Issue key: SPARK-37742
Issue id: 13419433
Parent id: 13419431.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 26/Dec/21 07:11
Updated: 12/Dec/22 17:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Project Infra
Due Date: 
Votes: 0
Labels: 
Description: e.g.)

https://github.com/apache/spark/blob/master/dev/run-tests-jenkins
https://github.com/apache/spark/blob/master/dev/run-tests-jenkins.py
Jenkins specific logics at https://github.com/apache/spark/blob/master/dev/run-tests.py

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-26 07:11:54.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y16g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Fix flaky tests in ImageFileFormatSuite
Issue key: SPARK-40171
Issue id: 13477875
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Gengliang.Wang
Creator: Gengliang.Wang
Created: 22/Aug/22 03:35
Updated: 12/Dec/22 10:35
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.2, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: ML
Due Date: 
Votes: 0
Labels: 
Description: There are 3 test cases that become flaky in the GitHub action tests:

[https://github.com/apache/spark/runs/7941765326?check_suite_focus=true]

We should fix them.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Dec 12 10:35:03 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17xs0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Aug/22 03:45;Gengliang.Wang;cc [~weichenxu123] ;;;, 12/Dec/22 10:35;ahmed.mahran;What's common in these 3 tests and different from the rest:
 - They load a complete directory. *imagePath + "/cls=multichannel/"* is the intersection
 - They set the *dropInvalid* flag

Logs from failed run seem to have expired. Maybe these tests should be re-enabled, and once a failure is captured, logs should be attached to this ticket and tests should then be disabled.;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 12/Dec/22 10:35;ahmed.mahran;What's common in these 3 tests and different from the rest:
 - They load a complete directory. *imagePath + "/cls=multichannel/"* is the intersection
 - They set the *dropInvalid* flag

Logs from failed run seem to have expired. Maybe these tests should be re-enabled, and once a failure is captured, logs should be attached to this ticket and tests should then be disabled.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Clarify that input_file_name returns an URI encoded file name
Issue key: SPARK-41480
Issue id: 13512228
Parent id: 
Issue Type: Documentation
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Kylling
Creator: Kylling
Created: 11/Dec/22 16:54
Updated: 11/Dec/22 17:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: The input_file_name function "Returns the name of the file being read, or empty string if not available." ([docs|[https://spark.apache.org/docs/latest/api/sql/#input_file_name]]). However, the returned file name is URI encoded. We should update the documentation to reflect this.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Dec 11 17:18:56 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1dt20:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Dec/22 17:18;apachespark;User 'jkylling' has created a pull request for this issue:
https://github.com/apache/spark/pull/39024;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark Thrift Server operation log output is empty
Issue key: SPARK-41459
Issue id: 13511016
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: idealspark
Creator: idealspark
Created: 09/Dec/22 06:42
Updated: 11/Dec/22 15:56
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.3.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: when i use jdbc HiveStatement.getQueryLog() to get query log, it return empty.

then i check log on thrift server ,it is also empty.

This bug was introduced in this commit  SPARK-40742  commit id 8e31554bf07cd0f89a45fc393ccc3a77e8a2120d.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Dec 11 15:56:02 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1dlko:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Dec/22 06:55;yumwang;cc [~LuciferYang];;;, 09/Dec/22 07:03;idealspark;pull request

https://github.com/apache/spark/pull/38993/;;;, 09/Dec/22 07:03;apachespark;User 'idealspark' has created a pull request for this issue:
https://github.com/apache/spark/pull/38993;;;, 09/Dec/22 07:03;apachespark;User 'idealspark' has created a pull request for this issue:
https://github.com/apache/spark/pull/38993;;;, 11/Dec/22 15:56;apachespark;User 'idealspark' has created a pull request for this issue:
https://github.com/apache/spark/pull/39023;;;
Affects Version/s.1: 3.3.1
Component/s.1: 
Comment.1: 09/Dec/22 07:03;idealspark;pull request

https://github.com/apache/spark/pull/38993/;;;
Comment.2: 09/Dec/22 07:03;apachespark;User 'idealspark' has created a pull request for this issue:
https://github.com/apache/spark/pull/38993;;;
Comment.3: 09/Dec/22 07:03;apachespark;User 'idealspark' has created a pull request for this issue:
https://github.com/apache/spark/pull/38993;;;
Comment.4: 11/Dec/22 15:56;apachespark;User 'idealspark' has created a pull request for this issue:
https://github.com/apache/spark/pull/39023;;;
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Stage level scheduling, allow to change number of executors
Issue key: SPARK-41449
Issue id: 13510768
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: shay_elbaz
Creator: shay_elbaz
Created: 08/Dec/22 11:48
Updated: 08/Dec/22 11:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.3.1
Fix Version/s: 
Component/s: Scheduler
Due Date: 
Votes: 0
Labels: scheduler
Description: Since the total/max number of executor is constant throughout the application - in dynamic or static allocation - there is loose control over how much GPUs will be requested from the resource manager. 

For example, if an application needs 500 executors for the ETL part (with N cores each), but it needs - *or allowed -* only 50 GPUs for the DL part, in practice it will request at least 500 GPUs from the RM, since `spark.executor.instances` is set to 500. This leads to resource management challenges in multi tenant environments.

A quick workaround is to repartition the RDD to 50 partitions just before switching resources, but it has obvious downsides. 

It would be very helpful if the total/max number of executors could also be configured in the Resource Profile.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-12-08 11:48:25.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1dk1k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.3.1
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: ORC uses Protobuf version vulnerable to CVE-2021-22569
Issue key: SPARK-39738
Issue id: 13470909
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ess-truveta
Creator: ess-truveta
Created: 11/Jul/22 05:07
Updated: 30/Nov/22 18:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.1, 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Our static analysis software detected vulnerability [CVE-2021-22569|https://github.com/protocolbuffers/protobuf/security/advisories/GHSA-wrvw-hg22-4m67], which comes from [ORC-1212] protobuf-java@2.5.0 has CVE-2021-22569 - ASF JIRA (apache.org).

Once ORC has addressed this vulnerability, Spark should upgrade to the next non-vulnerable version. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Nov 30 18:18:45 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16qxs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Nov/22 18:18;ess-truveta;ORC@1.8.0 has upgraded to a non-vulnerable version of protobuf so we should be able to upgrade now to get rid of this vulernability.;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Error Message Improvements in Spark 3.3
Issue key: SPARK-38781
Issue id: 13437564
Parent id: 
Issue Type: Epic
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: maxgekk
Reporter: maxgekk
Creator: maxgekk
Created: 04/Apr/22 08:49
Updated: 28/Nov/22 02:59
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Get together all tickets related to improvements of error messages in Spark like migration to error classes.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): SPARK-38615
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): ghx-label-1
Custom field (Epic Link): 
Custom field (Epic Name): Error Message Improvements in Spark 3.3
Custom field (Epic Status): To Do
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-04 08:49:22.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1145s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Extend test coverage of Analyzer
Issue key: SPARK-40449
Issue id: 13481662
Parent id: 13506154.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: grundprinzip-db
Creator: grundprinzip-db
Created: 15/Sep/22 09:30
Updated: 28/Nov/22 01:59
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Connect
Due Date: 
Votes: 0
Labels: 
Description: Extend the coverage of the proto -> Spark Logical Plan to cover all cases.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-09-15 09:30:45.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18kxc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Log cost time for Get FileStatus in HadoopTableReader
Issue key: SPARK-39128
Issue id: 13443889
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: apachespark
Reporter: chengpan
Creator: 
Created: 09/May/22 04:46
Updated: 24/Nov/22 00:29
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Sometimes fs.globStatus(pathPattern) costs unreasonable time, but there is not log for recording.

!image-2022-05-09-12-43-38-906.png!

￼ !image-2022-05-09-12-44-27-288.png!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon May 09 04:51:03 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z126k8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/May/22 04:51;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/36485;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark Submitter generates a ConfigMap with the same name
Issue key: SPARK-41060
Issue id: 13500278
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Serhii Nesterov
Creator: Serhii Nesterov
Created: 09/Nov/22 03:12
Updated: 11/Nov/22 00:03
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.3.0, 3.3.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: *Description of the issue:*

There's a problem with submitting spark jobs to K8s cluster: the library generates and reuses the same name for config maps (for drivers and executors). Ideally, for each job 2 config maps should be created: for a driver and an executor. However, the library creates only one driver config map for all jobs (in some cases it generates only one executor map for all jobs in the same manner). So, if I run 5 jobs, then only one driver config map will be generated and used for every job.  During those runs we experience issues when deleting pods from the cluster: executors pods are endlessly created and immediately terminated overloading cluster resources.

 

*The reason of the issue:*

This problem occurs because of the *KubernetesClientUtils* class in which we have *configMapNameExecutor* and *configMapNameDriver* as constants. It seems to be incorrect and should be urgently fixed. I've prepared some changes for review to fix the issue (tested in the cluster of our project).

 

*Steps to reproduce the issue:*

 
 # Create a *KubernetesClientApplication* object.
 # Submit at least 2 jobs (sequentially or using *Thread* for running in parallel).

 

*The results of my observations according to the steps are as follows:*
 # Spark 3.1.2 - The same config map in K8S will be overwritten which means all the jobs will point to the same config map.
 # Spark 3.3.* -  For the first job a new config map will be created. For other jobs an exception will be thrown (the K8S Fabric library does not allow to create a new config map with the existing name).
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 11/Nov/22 00:03;Serhii Nesterov;Screenshot 2022-11-09 015432.png;https://issues.apache.org/jira/secure/attachment/13052071/Screenshot+2022-11-09+015432.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): scala
Custom field (Last public comment date): Fri Nov 11 00:03:29 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1brds:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Nov/22 03:48;apachespark;User '19Serhii99' has created a pull request for this issue:
https://github.com/apache/spark/pull/38574;;;, 09/Nov/22 03:48;apachespark;User '19Serhii99' has created a pull request for this issue:
https://github.com/apache/spark/pull/38574;;;, 11/Nov/22 00:03;Serhii Nesterov;After applying the fixes from the pull request config maps are created correctly:

!Screenshot 2022-11-09 015432.png!;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 09/Nov/22 03:48;apachespark;User '19Serhii99' has created a pull request for this issue:
https://github.com/apache/spark/pull/38574;;;
Comment.2: 11/Nov/22 00:03;Serhii Nesterov;After applying the fixes from the pull request config maps are created correctly:

!Screenshot 2022-11-09 015432.png!;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.4.0, EMR-6.5.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Make partial aggregation adaptive
Issue key: SPARK-38505
Issue id: 13433141
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 10/Mar/22 15:01
Updated: 07/Nov/22 13:27
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: We can skip do partial aggregation to avoid spilling if this step does not reduce the number of rows too much.

https://github.com/trinodb/trino/pull/11011


Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): SPARK-36245, SPARK-38506
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Nov 07 13:27:53 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10de8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Mar/22 15:19;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/35806;;;, 10/Mar/22 15:19;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/35806;;;, 07/Nov/22 13:27;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/38534;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 10/Mar/22 15:19;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/35806;;;
Comment.2: 07/Nov/22 13:27;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/38534;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in the execution errors related to partitions
Issue key: SPARK-37946
Issue id: 13423119
Parent id: 13423097.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: maxgekk
Creator: maxgekk
Created: 17/Jan/22 17:48
Updated: 31/Oct/22 22:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Migrate the following errors in QueryExecutionErrors:
* unableToDeletePartitionPathError
* unableToCreatePartitionPathError
* unableToRenamePartitionPathError
* notADatasourceRDDPartitionError
* cannotClearPartitionDirectoryError
* failedToCastValueToDataTypeForPartitionColumnError
* unsupportedPartitionTransformError
* cannotCreateJDBCTableWithPartitionsError
* requestedPartitionsMismatchTablePartitionsError
* dynamicPartitionKeyNotAmongWrittenPartitionPathsError
* cannotRemovePartitionDirError
* alterTableWithDropPartitionAndPurgeUnsupportedError
* invalidPartitionFilterError
* getPartitionMetadataByFilterError
* illegalLocationClauseForViewPartitionError
* partitionColumnNotFoundInSchemaError
* cannotAddMultiPartitionsOnNonatomicPartitionTableError
* cannotDropMultiPartitionsOnNonatomicPartitionTableError
* truncateMultiPartitionUnsupportedError
* dynamicPartitionOverwriteUnsupportedByTableError
* writePartitionExceedConfigSizeWhenDynamicPartitionError

onto use error classes. Throw an implementation of SparkThrowable. Also write a test per every error in QueryExecutionErrorsSuite.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Oct 31 22:11:20 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ynvs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Jan/22 06:35;YActs;[~maxgekk] I will work on this.;;;, 31/Jan/22 10:09;apachespark;User 'yutoacts' has created a pull request for this issue:
https://github.com/apache/spark/pull/35371;;;, 31/Oct/22 22:11;khalidmammadov9@gmail.com;Hi [~maxgekk], I see this one is not done yet here: partitionColumnNotFoundInSchemaError

Can I look into it?

Also, there are some more waiting to be done in QueryExecutionErrors.scala e.g.

stateNotDefinedOrAlreadyRemovedError

cannotSetTimeoutDurationError

cannotGetEventTimeWatermarkError

cannotSetTimeoutTimestampError

batchMetadataFileNotFoundError

....

Shall I look into these as well?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 31/Jan/22 10:09;apachespark;User 'yutoacts' has created a pull request for this issue:
https://github.com/apache/spark/pull/35371;;;
Comment.2: 31/Oct/22 22:11;khalidmammadov9@gmail.com;Hi [~maxgekk], I see this one is not done yet here: partitionColumnNotFoundInSchemaError

Can I look into it?

Also, there are some more waiting to be done in QueryExecutionErrors.scala e.g.

stateNotDefinedOrAlreadyRemovedError

cannotSetTimeoutDurationError

cannotGetEventTimeWatermarkError

cannotSetTimeoutTimestampError

batchMetadataFileNotFoundError

....

Shall I look into these as well?;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: SVD: matrix U has wrong row order
Issue key: SPARK-40920
Issue id: 13491754
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: LeoIV
Creator: LeoIV
Created: 26/Oct/22 11:58
Updated: 27/Oct/22 11:28
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: MLlib, PySpark
Due Date: 
Votes: 0
Labels: 
Description: When performing SVD on a RowMatrix, the matrix U has the wrong row order and the original matrix is not correctly restored with the given matrix. 

 

Consider the following code:
{code:java}
x_np = np.random.random((14, 3)) # the size matters, it works for smaller sizes
x = ctx.parallelize(x_np).zipWithIndex().map(
    lambda r: [MatrixEntry(r[1], i, r[0][i]) for i in range(len(r[0]))])
x = CoordinateMatrix(x.flatMap(lambda x: x))
x_inv = matrix_inverse(x) {code}
with 
{code:java}
def matrix_inverse(matrix: CoordinateMatrix) -> DenseMatrix:
    mtrx = matrix.toRowMatrix()
    svd = matrix.toRowMatrix().computeSVD(k=mtrx.numCols(), computeU=True, rCond=1e-15)  # do the SVD

    s_inv = 1 / svd.s
    mtrx_orig = matrix.toBlockMatrix().blocks.first()[1].toArray()
    u_dense = mtrx_orig @ (svd.V.toArray() * s_inv[np.newaxis, :])
    cov_inv = np.matmul(svd.V.toArray(), np.multiply(s_inv[:, np.newaxis], u_dense.T))
    u_from_spark = np.array(svd.U.rows.map(lambda x: x.toArray()).collect())
    return DenseMatrix(numRows=cov_inv.shape[0], numCols=cov_inv.shape[1],
                       values=cov_inv.ravel(order="F"))  # return inverse as dense matrix {code}
Then, u_dense is the correct U but differs from the U produced by Spark. In particular, the U in Spark does not return the correct pseudoinverse and U@[S@V.T|mailto:S@V.T] does not reproduce the input matrix. 

 

With the following input matrix x

!image-2022-10-26-13-58-52-998.png!

I get the following u_dense

!image-2022-10-26-13-59-04-608.png!

but the following u_from_spark

!image-2022-10-26-13-59-13-425.png!

 

On careful inspection, it seems that the row order is wrong.

 
Environment: Python 3.10, multi-core machine, no cluster
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 26/Oct/22 11:58;LeoIV;image-2022-10-26-13-58-52-998.png;https://issues.apache.org/jira/secure/attachment/13051440/image-2022-10-26-13-58-52-998.png, 26/Oct/22 11:59;LeoIV;image-2022-10-26-13-59-04-608.png;https://issues.apache.org/jira/secure/attachment/13051441/image-2022-10-26-13-59-04-608.png, 26/Oct/22 11:59;LeoIV;image-2022-10-26-13-59-13-425.png;https://issues.apache.org/jira/secure/attachment/13051442/image-2022-10-26-13-59-13-425.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 3.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Oct 27 11:28:39 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1aau8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Oct/22 19:46;srowen;So, first, to reproduce the problem more reliably, stick a .repartition(10) or something after .zipWithIndex(). The problem here hinges on what ordering is preserved where. In a simple case with 1 partition, everything works as expected in this code snippet.

However the distributed representations here generally don't preserve row order in the RDD; they rely on row indices. To recover the original ordering, try using .toIndexedRowMatrix() instead, and then call .sortBy(lambda r: r.index) on svd.U and svd.V first. I believe that will give you the expected result, or at least it gave me the same answers as scipy's SVD.

Now, I think this is confusing. In particular, there is "RowMatrix" which lacks indices, and which is returned in several places, and without indices you'd really expect that (for instance) CoordinateMatrix.toRowMatrix has rows ordered by the coordinates, but it doesn't. I think that's a bug, let me chew on the implications of fixing that while you check if that's the issue.;;;, 27/Oct/22 11:28;LeoIV;Using  .repartition(10), .toIndexedRowMatrix(), and .sortBy(lambda r: r.index) produces the correct U - without sorting, the order is mixed up. 

I agree that this behavior is unexpected; the rows should be in the right order. ;;;
Affects Version/s.1: 
Component/s.1: PySpark
Comment.1: 27/Oct/22 11:28;LeoIV;Using  .repartition(10), .toIndexedRowMatrix(), and .sortBy(lambda r: r.index) produces the correct U - without sorting, the order is mixed up. 

I agree that this behavior is unexpected; the rows should be in the right order. ;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Row-level Runtime Filtering cannot be enabled when externalTable has no stats
Issue key: SPARK-40793
Issue id: 13486243
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: chenminghua8
Creator: chenminghua8
Created: 14/Oct/22 02:53
Updated: 25/Oct/22 03:24
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: patch
Description: When using external tables, Row-level Runtime Filtering cannot be enabled anyway without performing analysis on the table to generate statistics. In actual use, external tables often do not have statistics, but it is also hoped that the execution efficiency can be improved through Row-level Runtime Filtering. The reason why Row-level Runtime Filtering cannot be enabled is: 'InjectRuntimeFilter' calls the 'satisfyByteSizeRequirement' method to determine whether the application side plan's aggregated scan size meets the requirements, and because there is no statistical data, the application side plan's aggregated scan size is equal to 0, which cannot satisfy the requirement to enable Row -level Runtime Filtering requirements.
In order to enable Row-level Runtime Filtering even when the external table has no statistics, add the RUNTIME_FILTER_{_}ENABLED_WHEN_NO_STATS parameter. When RUNTIME_FILTER{_}_ENABLED_WHEN_NO_STATS is configured to true and the external table has no statistics, the 'satisfyByteSizeRequirement' method returns true so that Row-level Runtime Filtering may be enabled .
Environment:  
{panel:title=Use tpcds to generate test data}
test:runMain com.databricks.spark.sql.perf.tpcds.GenTPCDSData -d /data/project/tpcds-kit/tpcds-kit/tools -s 500 -l hdfs://hadoop01:8020/user/apple/tpcds -f parquet
{panel}
 
{panel:title=Use spark-sql-perf's TPCDSTables class to create external tables to perform tpcds q24a tests like the following}
   import com.databricks.spark.sql.perf.tpcds.TPCDSTables;

   ....................

    String dsdgenDir = "/data/project/tpcds-kit/tpcds-kit/tools";
    String scaleFactor = "500";
    String format = "parquet";
    String rootDir = "hdfs://hadoop01:8020/user/apple/tpcds";
    String databaseName = "tpcds_500g";
    String q24a = ...............
    TPCDSTables tables = new TPCDSTables(sqlContext, dsdgenDir, scaleFactor, false, false);
    tables.createExternalTables(rootDir, format, databaseName, true, true, "");
    //tables.analyzeTables(databaseName, true, "");
    Dataset<Row> queryResult = spark.sql(testSql);
    queryResult.show();
{panel}
{panel:title=Spark Sql generates the following execution plan that did not use Row-level Runtime Filtering}
== Physical Plan ==
AdaptiveSparkPlan (121)
+- == Final Plan ==
   * SerializeFromObject (75)
   +- MapPartitions (74)
      +- DeserializeToObject (73)
         +- * Sort (72)
            +- AQEShuffleRead (71)
               +- ShuffleQueryStage (70), Statistics(sizeInBytes=119.6 KiB, rowCount=1.52E+3)
                  +- Exchange (69)
                     +- * Filter (68)
                        +- * HashAggregate (67)
                           +- AQEShuffleRead (66)
                              +- ShuffleQueryStage (65), Statistics(sizeInBytes=139.7 KiB, rowCount=1.62E+3)
                                 +- Exchange (64)
                                    +- * HashAggregate (63)
                                       +- * HashAggregate (62)
                                          +- AQEShuffleRead (61)
                                             +- ShuffleQueryStage (60), Statistics(sizeInBytes=1425.0 KiB, rowCount=9.15E+3)
                                                +- Exchange (59)
                                                   +- * HashAggregate (58)
                                                      +- * Project (57)
                                                         +- * SortMergeJoin Inner (56)
                                                            :- * Sort (48)
                                                            :  +- AQEShuffleRead (47)
                                                            :     +- ShuffleQueryStage (46), Statistics(sizeInBytes=60.9 MiB, rowCount=3.54E+5)
                                                            :        +- Exchange (45)
                                                            :           +- * Project (44)
                                                            :              +- * SortMergeJoin Inner (43)
                                                            :                 :- * Sort (35)
                                                            :                 :  +- AQEShuffleRead (34)
                                                            :                 :     +- ShuffleQueryStage (33), Statistics(sizeInBytes=47.7 MiB, rowCount=3.67E+5)
                                                            :                 :        +- Exchange (32)
                                                            :                 :           +- * Project (31)
                                                            :                 :              +- * BroadcastHashJoin Inner BuildRight (30)
                                                            :                 :                 :- * Project (24)
                                                            :                 :                 :  +- * BroadcastHashJoin Inner BuildRight (23)
                                                            :                 :                 :     :- * Project (16)
                                                            :                 :                 :     :  +- * SortMergeJoin Inner (15)
                                                            :                 :                 :     :     :- * Sort (7)
                                                            :                 :                 :     :     :  +- ShuffleQueryStage (6), Statistics(sizeInBytes=60.0 GiB, rowCount=1.34E+9)
                                                            :                 :                 :     :     :     +- Exchange (5)
                                                            :                 :                 :     :     :        +- * Project (4)
{color:#de350b}                                                            :                 :                 :     :     :           +- * Filter (3){color}
                                                            :                 :                 :     :     :              +- * ColumnarToRow (2)
                                                            :                 :                 :     :     :                 +- Scan parquet tpcds_500.store_sales (1)
                                                            :                 :                 :     :     +- * Sort (14)
                                                            :                 :                 :     :        +- ShuffleQueryStage (13), Statistics(sizeInBytes=3.2 GiB, rowCount=1.44E+8)
                                                            :                 :                 :     :           +- Exchange (12)
                                                            :                 :                 :     :              +- * Project (11)
                                                            :                 :                 :     :                 +- * Filter (10)
                                                            :                 :                 :     :                    +- * ColumnarToRow (9)
                                                            :                 :                 :     :                       +- Scan parquet tpcds_500.store_returns (8)
                                                            :                 :                 :     +- BroadcastQueryStage (22), Statistics(sizeInBytes=1030.3 KiB, rowCount=100)
                                                            :                 :                 :        +- BroadcastExchange (21)
                                                            :                 :                 :           +- * Project (20)
                                                            :                 :                 :              +- * Filter (19)
                                                            :                 :                 :                 +- * ColumnarToRow (18)
                                                            :                 :                 :                    +- Scan parquet tpcds_500.store (17)
                                                            :                 :                 +- BroadcastQueryStage (29), Statistics(sizeInBytes=1280.0 KiB, rowCount=5.97E+3)
                                                            :                 :                    +- BroadcastExchange (28)
                                                            :                 :                       +- * Filter (27)
                                                            :                 :                          +- * ColumnarToRow (26)
                                                            :                 :                             +- Scan parquet tpcds_500.item (25)
                                                            :                 +- * Sort (42)
                                                            :                    +- AQEShuffleRead (41)
                                                            :                       +- ShuffleQueryStage (40), Statistics(sizeInBytes=438.8 MiB, rowCount=6.76E+6)
                                                            :                          +- Exchange (39)
                                                            :                             +- * Filter (38)
                                                            :                                +- * ColumnarToRow (37)
                                                            :                                   +- Scan parquet tpcds_500.customer (36)
                                                            +- * Sort (55)
                                                               +- AQEShuffleRead (54)
                                                                  +- ShuffleQueryStage (53), Statistics(sizeInBytes=203.8 MiB, rowCount=3.34E+6)
                                                                     +- Exchange (52)
                                                                        +- * Filter (51)
                                                                           +- * ColumnarToRow (50)
                                                                              +- Scan parquet tpcds_500.customer_address (49)

 

{color:#de350b}(3) Filter [codegen id : 1]{color}
{color:#de350b}Input [6]: [ss_item_sk#62, ss_customer_sk#63, ss_store_sk#67, ss_ticket_number#69L, ss_net_paid#80, ss_sold_date_sk#83]{color}
{color:#de350b}Condition : (((isnotnull(ss_ticket_number#69L) AND isnotnull(ss_item_sk#62)) AND isnotnull(ss_store_sk#67)) AND isnotnull(ss_customer_sk#63)){color}
{panel}
 

 

 

 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): https://github.com/apache/spark/pull/38213
Custom field (Fix version (Component)): 
Custom field (Flags): Patch
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Oct 25 03:24:23 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19cxs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Oct/22 05:59;chenminghua8;I added the parameter RUNTIME_FILTER__ENABLED_WHEN_NO_STATS to SQLConf, and modified the satisfyByteSizeRequirement method of InjectRuntimeFilter:

{color:#4c9aff}maxScanSize >={color}
{color:#4c9aff}        conf.getConf(SQLConf.RUNTIME_BLOOM_FILTER_APPLICATION_SIDE_SCAN_SIZE_THRESHOLD){color}

change to：

{color:#4c9aff}if (conf.runtimeFilterEnabledWhenOnStats) {{color}
{color:#4c9aff}      maxScanSize <= 0 || maxScanSize >={color}
{color:#4c9aff}        conf.getConf(SQLConf.RUNTIME_BLOOM_FILTER_APPLICATION_SIDE_SCAN_SIZE_THRESHOLD){color}
{color:#4c9aff}    } else {{color}
{color:#4c9aff}      maxScanSize >={color}
{color:#4c9aff}        conf.getConf(SQLConf.RUNTIME_BLOOM_FILTER_APPLICATION_SIDE_SCAN_SIZE_THRESHOLD){color}
{color:#4c9aff}    }{color}

{color:#4c9aff}}

set RUNTIME_FILTER__ENABLED_WHEN_NO_STATS to true，perform tpcds q24a tests.It uses a run time filter, and the execution time is also reduced from 1.9 minutes to 1.1 minutes. Its execution plan is as follows:

 

== Physical Plan ==
AdaptiveSparkPlan (123)
+- == Final Plan ==
   * SerializeFromObject (77)
   +- MapPartitions (76)
      +- DeserializeToObject (75)
         +- * Sort (74)
            +- AQEShuffleRead (73)
               +- ShuffleQueryStage (72), Statistics(sizeInBytes=119.6 KiB, rowCount=1.52E+3)
                  +- Exchange (71)
                     +- * Filter (70)
                        +- * HashAggregate (69)
                           +- AQEShuffleRead (68)
                              +- ShuffleQueryStage (67), Statistics(sizeInBytes=139.7 KiB, rowCount=1.62E+3)
                                 +- Exchange (66)
                                    +- * HashAggregate (65)
                                       +- * HashAggregate (64)
                                          +- AQEShuffleRead (63)
                                             +- ShuffleQueryStage (62), Statistics(sizeInBytes=1425.0 KiB, rowCount=9.15E+3)
                                                +- Exchange (61)
                                                   +- * HashAggregate (60)
                                                      +- * Project (59)
                                                         +- * SortMergeJoin Inner (58)
                                                            :- * Sort (50)
                                                            :  +- AQEShuffleRead (49)
                                                            :     +- ShuffleQueryStage (48), Statistics(sizeInBytes=60.9 MiB, rowCount=3.54E+5)
                                                            :        +- Exchange (47)
                                                            :           +- * Project (46)
                                                            :              +- * SortMergeJoin Inner (45)
                                                            :                 :- * Sort (37)
                                                            :                 :  +- AQEShuffleRead (36)
                                                            :                 :     +- ShuffleQueryStage (35), Statistics(sizeInBytes=47.7 MiB, rowCount=3.67E+5)
                                                            :                 :        +- Exchange (34)
                                                            :                 :           +- * Project (33)
                                                            :                 :              +- * BroadcastHashJoin Inner BuildRight (32)
                                                            :                 :                 :- * Project (26)
                                                            :                 :                 :  +- * BroadcastHashJoin Inner BuildRight (25)
                                                            :                 :                 :     :- * Project (18)
                                                            :                 :                 :     :  +- * SortMergeJoin Inner (17)
                                                            :                 :                 :     :     :- * Sort (8)
                                                            :                 :                 :     :     :  +- AQEShuffleRead (7)
                                                            :                 :                 :     :     :     +- ShuffleQueryStage (6), Statistics(sizeInBytes=167.1 MiB, rowCount=3.65E+6)
                                                            :                 :                 :     :     :        +- Exchange (5)
                                                            :                 :                 :     :     :           +- * Project (4)
{color:#de350b}                                                            :                 :                 :     :     :              +- * Filter (3){color}
                                                            :                 :                 :     :     :                 +- * ColumnarToRow (2)
                                                            :                 :                 :     :     :                    +- Scan parquet tpcds_500.store_sales (1)
                                                            :                 :                 :     :     +- * Sort (16)
                                                            :                 :                 :     :        +- AQEShuffleRead (15)
                                                            :                 :                 :     :           +- ShuffleQueryStage (14), Statistics(sizeInBytes=3.2 GiB, rowCount=1.44E+8)
                                                            :                 :                 :     :              +- Exchange (13)
                                                            :                 :                 :     :                 +- * Project (12)
                                                            :                 :                 :     :                    +- * Filter (11)
                                                            :                 :                 :     :                       +- * ColumnarToRow (10)
                                                            :                 :                 :     :                          +- Scan parquet tpcds_500.store_returns (9)
                                                            :                 :                 :     +- BroadcastQueryStage (24), Statistics(sizeInBytes=1030.3 KiB, rowCount=100)
                                                            :                 :                 :        +- BroadcastExchange (23)
                                                            :                 :                 :           +- * Project (22)
                                                            :                 :                 :              +- * Filter (21)
                                                            :                 :                 :                 +- * ColumnarToRow (20)
                                                            :                 :                 :                    +- Scan parquet tpcds_500.store (19)
                                                            :                 :                 +- BroadcastQueryStage (31), Statistics(sizeInBytes=1280.0 KiB, rowCount=5.97E+3)
                                                            :                 :                    +- BroadcastExchange (30)
                                                            :                 :                       +- * Filter (29)
                                                            :                 :                          +- * ColumnarToRow (28)
                                                            :                 :                             +- Scan parquet tpcds_500.item (27)
                                                            :                 +- * Sort (44)
                                                            :                    +- AQEShuffleRead (43)
                                                            :                       +- ShuffleQueryStage (42), Statistics(sizeInBytes=438.8 MiB, rowCount=6.76E+6)
                                                            :                          +- Exchange (41)
                                                            :                             +- * Filter (40)
                                                            :                                +- * ColumnarToRow (39)
                                                            :                                   +- Scan parquet tpcds_500.customer (38)
                                                            +- * Sort (57)
                                                               +- AQEShuffleRead (56)
                                                                  +- ShuffleQueryStage (55), Statistics(sizeInBytes=203.8 MiB, rowCount=3.34E+6)
                                                                     +- Exchange (54)
                                                                        +- * Filter (53)
                                                                           +- * ColumnarToRow (52)
                                                                              +- Scan parquet tpcds_500.customer_address (51)

{color:#de350b}(3) Filter [codegen id : 1]
Input [6]: [ss_item_sk#62, ss_customer_sk#63, ss_store_sk#67, ss_ticket_number#69L, ss_net_paid#80, ss_sold_date_sk#83|#62, ss_customer_sk#63, ss_store_sk#67, ss_ticket_number#69L, ss_net_paid#80, ss_sold_date_sk#83]
Condition : (((((isnotnull(ss_ticket_number#69L) AND isnotnull(ss_item_sk#62)) AND isnotnull(ss_store_sk#67)) AND isnotnull(ss_customer_sk#63)) AND might_contain(Subquery subquery#1608, [id=#26590|#26590], xxhash64(ss_store_sk#67, 42))) AND might_contain(Subquery subquery#1611, [id=#26603|#26603], xxhash64(ss_item_sk#62, 42))){color};;;, 18/Oct/22 03:29;chenminghua8;I think a better way to improve this problem: when the InjectRuntimeFilter.maxScanByteSize method detects that there is no statistics, it will get the HDFS file size of the corresponding table as the value of Statistics.sizeInBytes, which is to delay loading Statistics.sizeInBytes . This allows both Row-level Runtime Filtering to be enabled when the table is not being analyzed and that Row-level Runtime Filtering cannot be misused.;;;, 25/Oct/22 02:44;chenminghua8;[https://github.com/apache/spark/pull/38381] is the mplementation of Statistics.sizeInBytes lazy loading.;;;, 25/Oct/22 03:24;apachespark;User 'chenminghua8' has created a pull request for this issue:
https://github.com/apache/spark/pull/38381;;;
Affects Version/s.1: 3.4.0
Component/s.1: 
Comment.1: 18/Oct/22 03:29;chenminghua8;I think a better way to improve this problem: when the InjectRuntimeFilter.maxScanByteSize method detects that there is no statistics, it will get the HDFS file size of the corresponding table as the value of Statistics.sizeInBytes, which is to delay loading Statistics.sizeInBytes . This allows both Row-level Runtime Filtering to be enabled when the table is not being analyzed and that Row-level Runtime Filtering cannot be misused.;;;
Comment.2: 25/Oct/22 02:44;chenminghua8;[https://github.com/apache/spark/pull/38381] is the mplementation of Statistics.sizeInBytes lazy loading.;;;
Comment.3: 25/Oct/22 03:24;apachespark;User 'chenminghua8' has created a pull request for this issue:
https://github.com/apache/spark/pull/38381;;;
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add fair-share scheduling integration test
Issue key: SPARK-38190
Issue id: 13427979
Parent id: 
Issue Type: Test
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 11/Feb/22 10:07
Updated: 24/Oct/22 19:25
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-02-11 10:07:49.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zhp4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add volcano module to release-build.sh
Issue key: SPARK-38621
Issue id: 13435019
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 22/Mar/22 03:15
Updated: 24/Oct/22 19:24
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build, Kubernetes, Project Infra
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Oct 24 19:24:29 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10oxk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Mar/22 03:24;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35922;;;, 24/Oct/22 19:24;dongjoon;I converted this to the independent Jira issue.;;;
Affects Version/s.1: 
Component/s.1: Kubernetes
Comment.1: 24/Oct/22 19:24;dongjoon;I converted this to the independent Jira issue.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Upgrade Apache zookeeper to get rid of CVE-2020-10663
Issue key: SPARK-40758
Issue id: 13485704
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bilna123
Creator: bilna123
Created: 11/Oct/22 15:15
Updated: 20/Oct/22 06:12
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In order to resolve security vulnerability CVE-2020-10663, upgrade Apache zookeeper to 3.8.0
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Oct 20 06:12:59 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z199n4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Oct/22 17:44;srowen;Does the CVE affect Spark?
Can we update ZK without breaking other things?
Go ahead and try with a pull request;;;, 20/Oct/22 06:12;bilna123;https://issues.apache.org/jira/browse/ZOOKEEPER-3933 This link says the reported CVE is false positive. So I think we can close this.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 20/Oct/22 06:12;bilna123;https://issues.apache.org/jira/browse/ZOOKEEPER-3933 This link says the reported CVE is false positive. So I think we can close this.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support s3 and hdfs paths for keytab file while being passed
Issue key: SPARK-40842
Issue id: 13486981
Parent id: 13486978.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: av010
Creator: av010
Created: 19/Oct/22 05:17
Updated: 19/Oct/22 05:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: keytab file should be able to be specified using s3 or s3a or hdfs scheme - currently this is not supported on spark 3.3.0 to my knowledge.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-10-19 05:17:09.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19hfc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Spark Submit
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support k8s secret as keytab file when using kerberos on spark on kubernetes
Issue key: SPARK-40841
Issue id: 13486980
Parent id: 13486978.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: av010
Creator: av010
Created: 19/Oct/22 05:15
Updated: 19/Oct/22 05:15
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: spark recognizes a secret and is able to use that to create a keytab for it to propagate to executors to use when using kerberized hive metastore setup.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-10-19 05:15:59.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19hf4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Spark Submit
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support Keytab file passing as a k8s secret
Issue key: SPARK-40840
Issue id: 13486978
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: av010
Creator: av010
Created: 19/Oct/22 05:14
Updated: 19/Oct/22 05:14
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: Currently I can use 
{code:java}
--conf spark.kerberos.keytab=local://{code}
It would be useful if I can make keytab file as a kubernetes secret and then reference the secret here instead of keytab file. Keytab file needs to be mounted using templates and it gets to be tedious needlessly when running driver on cluster mode.

 

I can help if directed on how to get this done.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-10-19 05:14:25.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19heo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Spark Submit
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Missing handling a catalog name in destination tables in `RenameTableExec`
Issue key: SPARK-40804
Issue id: 13486448
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tom_tanaka
Creator: tom_tanaka
Created: 15/Oct/22 14:43
Updated: 19/Oct/22 02:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.3, 3.2.2, 3.3.0
Fix Version/s: 3.1.3, 3.2.2, 3.3.0
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently `RenameTableExec` only handles an empty namespace for destination tables as the following current spec:

(3.3.0 is picked up) [https://github.com/apache/spark/blob/v3.3.0/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/RenameTableExec.scala#L48]

 
{code:java}
# L48
    val qualifiedNewIdent = if (newIdent.namespace.isEmpty) {
      Identifier.of(oldIdent.namespace, newIdent.name)
    } else newIdent{code}
This part doesn't handle the case where a destination table is specified catalog.db.table. For example, Apache Iceberg is expected to handle the destination table as catalog.db.table in the document; [https://iceberg.apache.org/docs/latest/spark-ddl/#alter-table--rename-to|https://iceberg.apache.org/docs/latest/spark-ddl/#alter-table--rename-to.]

 

If catalog.db.table is passed to "ALTER TABLE <src> RENAME TO <dst>" query, there's a difference of handling namespaces between source and destination tables. Specifically, source tables can be correctly handled as *[db]* for its namespace, but destination tables are handled as *[catalog, db]* for its namespace.
Environment: This depends on Spark versions, since the version in which ALTER TABLE RENAME TO is added.
Original Estimate: 172800.0
Remaining Estimate: 172800.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 172800.0
Σ Remaining Estimate: 172800.0
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): https://github.com/apache/spark/pull/38268
Custom field (Fix version (Component)): 
Custom field (Flags): Patch
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): scala
Custom field (Last public comment date): Sat Oct 15 14:53:01 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19e7c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Oct/22 14:52;apachespark;User 'tomtongue' has created a pull request for this issue:
https://github.com/apache/spark/pull/38268;;;, 15/Oct/22 14:53;apachespark;User 'tomtongue' has created a pull request for this issue:
https://github.com/apache/spark/pull/38268;;;
Affects Version/s.1: 3.2.2
Component/s.1: 
Comment.1: 15/Oct/22 14:53;apachespark;User 'tomtongue' has created a pull request for this issue:
https://github.com/apache/spark/pull/38268;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Spark 3.3.0 doesn't works with Hive 3.1.2
Issue key: SPARK-40736
Issue id: 13485477
Parent id: 
Issue Type: Bug
Status: Reopened
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: pratik.malani
Creator: pratik.malani
Created: 10/Oct/22 17:10
Updated: 18/Oct/22 13:59
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core, SQL
Due Date: 
Votes: 0
Labels: Hive, spark
Description: Hive 2.3.9 is impacted with CVE-2021-34538, so trying to use the Hive 3.1.2.

Using Spark 3.3.0 with Hadoop 3.3.4 and Hive 3.1.2, getting below error when starting the Thriftserver

 
{noformat}
Exception in thread "main" java.lang.IllegalAccessError: tried to access class org.apache.hive.service.server.HiveServer2$ServerOptionsProcessor from class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:92)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala){noformat}
Using below command to start the Thriftserver

 

*spark-class org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 spark-internal*

 

Have set the SPARK_HOME correctly.

 

The same works well with Hive 2.3.9, but fails when we upgrade to Hive 3.1.2.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 12/Oct/22 12:49;pratik.malani;image-2022-10-12-18-19-24-455.png;https://issues.apache.org/jira/secure/attachment/13050823/image-2022-10-12-18-19-24-455.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Oct 18 13:59:13 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1989c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Oct/22 12:52;pratik.malani;Hi All,

Removed hive-service jar from the classpath.

Now Spark Thriftserver has started but facing another issue while querying the database using the thriftserver.
{noformat}
java.lang.NullPointerException
        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:809)
        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:702)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:650)
        at org.apache.hadoop.hive.metastore.ObjectStore.unCacheDataNucleusClassLoaders(ObjectStore.java:9708)
        at org.apache.hadoop.hive.ql.session.SessionState.unCacheDataNucleusClassLoaders(SessionState.java:1802)
        at org.apache.hadoop.hive.ql.session.SessionState.close(SessionState.java:1777)
        at org.apache.hive.service.cli.session.HiveSessionImpl.close(HiveSessionImpl.java:669)
        at org.apache.hive.service.cli.session.SessionManager.closeSession(SessionManager.java:295)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLSessionManager.closeSession(SparkSQLSessionManager.scala:91)
        at org.apache.hive.service.cli.CLIService.closeSession(CLIService.java:238)
        at org.apache.hive.service.cli.thrift.ThriftCLIService$1.deleteContext(ThriftCLIService.java:107)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:325)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
Exception in thread "HiveServer2-Handler-Pool: Thread-68" java.lang.NoSuchMethodError: org.apache.hadoop.hive.ql.QueryState.<init>(Lorg/apache/hadoop/hive/conf/HiveConf;Ljava/util/Map;Z)V
        at org.apache.hive.service.cli.operation.Operation.<init>(Operation.java:89)
        at org.apache.hive.service.cli.operation.ExecuteStatementOperation.<init>(ExecuteStatementOperation.java:34)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.<init>(SparkExecuteStatementOperation.scala:50)
        at org.apache.spark.sql.hive.thriftserver.server.SparkSQLOperationManager.newExecuteStatementOperation(SparkSQLOperationManager.scala:55)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:481)
        at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:472)
        at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:310)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:455)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1557)
        at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1542)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:38)
        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:313)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750){noformat}
After deep investigation, found out Spark 3.3.0 is creating the QueryState using constructor

[https://jar-download.com/artifacts/org.apache.spark/spark-hive-thriftserver_2.12/3.3.0/source-code/org/apache/hive/service/cli/operation/Operation.java]

!image-2022-10-12-18-19-24-455.png|width=581,height=183!

This constructor initialization has been removed from the latest hive QueryState.java file

[https://jar-download.com/artifacts/org.apache.hive/hive-exec/3.1.2/source-code/org/apache/hadoop/hive/ql/QueryState.java]

Hive is using Builder pattern while creating the QueryState object. 
Can Spark help to resolve the code to be compatible with Spark 3.1.2?

 ;;;, 15/Oct/22 09:37;yumwang;Could you just upgrade Hive metastore to 3.1.2?;;;, 18/Oct/22 09:23;pratik.malani;Hi [~yumwang] 
Have upgraded the Hive Metastore to 3.1.2.
The issue is with the Spark code trying to create QueryState object using the constructor approach instead of builder pattern.

 ;;;, 18/Oct/22 10:24;yumwang;Do you copy hive related jars to ${SPARK_HOME}/jars and then start the Thriftserver?;;;, 18/Oct/22 10:27;pratik.malani;Hi [~yumwang] yes;;;, 18/Oct/22 12:48;yumwang;Please do not copy Hive related jars to ${SPARK_HOME}/jars. Just set  spark.sql.hive.metastore.version=3.1.2 and spark.sql.hive.metastore.jars=maven is enough.;;;, 18/Oct/22 13:59;pratik.malani;Hi [~yumwang] 
The jars are set in SPARK_HOME as well as in a path set under 

spark.sql.hive.metastore.jars.

The same setup works well with Hive 2.3.9 and even now it is working fine with customized Hive 3.1.2 jars.
So from my opinion, SPARK_HOME is not a concern over here.
Only concern is, with customized Hive, the spark jobs do not get complete after the execution. They get stuck and spark jobs stays in Running state forever.;;;
Affects Version/s.1: 
Component/s.1: SQL
Comment.1: 15/Oct/22 09:37;yumwang;Could you just upgrade Hive metastore to 3.1.2?;;;
Comment.2: 18/Oct/22 09:23;pratik.malani;Hi [~yumwang] 
Have upgraded the Hive Metastore to 3.1.2.
The issue is with the Spark code trying to create QueryState object using the constructor approach instead of builder pattern.

 ;;;
Comment.3: 18/Oct/22 10:24;yumwang;Do you copy hive related jars to ${SPARK_HOME}/jars and then start the Thriftserver?;;;
Comment.4: 18/Oct/22 10:27;pratik.malani;Hi [~yumwang] yes;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Certain aggregations cause extra exchange steps on unioned and bucketed tables
Issue key: SPARK-40824
Issue id: 13486714
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mtong
Creator: mtong
Created: 17/Oct/22 22:46
Updated: 17/Oct/22 22:46
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: An extension to https://issues.apache.org/jira/browse/SPARK-22898

 

Currently working on a POC where we store aggregations of features across different datasets. I have noticed that when you try to do certain aggregation operations across multiple tables, spark will introduce an extra exchange step

 
{code:java}
# initializing the tables
sql("""
    CREATE TABLE t1 (`id` BIGINT, `value` INT)
    USING PARQUET
    CLUSTERED BY (id)
    INTO 1 BUCKETS
    """)
sql("""
    CREATE TABLE t2 (`id` BIGINT, `value` INT)
    USING PARQUET
    CLUSTERED BY (id)
    INTO 1 BUCKETS
    """)
sql("INSERT INTO TABLE t1 VALUES(1, 2)")
sql("INSERT INTO TABLE t2 VALUES(1, 3)")

# aggregation, note the exchange after the union operation
sql("""
    SELECT id, COUNT(*)
    FROM (SELECT id FROM t1 UNION SELECT id FROM t2)
    GROUP BY id
    """).explain()

== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[id#92L], functions=[count(1)])
   +- HashAggregate(keys=[id#92L], functions=[partial_count(1)])
      +- HashAggregate(keys=[id#92L], functions=[])
         +- Exchange hashpartitioning(id#92L, 100), ENSURE_REQUIREMENTS, [id=#202]
            +- HashAggregate(keys=[id#92L], functions=[])
               +- Union
                  :- FileScan parquet default.t1[id#92L] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark-warehouse/t1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 1 out of 1
                  +- FileScan parquet default.t2[id#94L] Batched: true, Bucketed: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark-warehouse/t2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 1 out of 1
 {code}
This seems like an issue with the query optimizer because if you use a different set and order of operations (in this case groupby/count on individual tables, join the tables, then infer the union count from the joined values), you get a query plan that doesn't have this exchange step
{code:java}
sql("""
    SELECT t1_agg.id, t1_agg.count + t2_agg.count as count
    FROM (SELECT id, COUNT(*) as count from t1 GROUP BY id) as t1_agg
    JOIN (SELECT id, COUNT(*) as count from t2 GROUP BY id) as t2_agg ON t1_agg.id=t2_agg.id
""").explain()

# note the lack of an exchange step
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [id#92L, (count#121L + count#122L) AS count#123L]
   +- SortMergeJoin [id#92L], [id#94L], Inner
      :- Sort [id#92L ASC NULLS FIRST], false, 0
      :  +- HashAggregate(keys=[id#92L], functions=[count(1)])
      :     +- HashAggregate(keys=[id#92L], functions=[partial_count(1)])
      :        +- Filter isnotnull(id#92L)
      :           +- FileScan parquet default.t1[id#92L] Batched: true, Bucketed: true, DataFilters: [isnotnull(id#92L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark-warehouse/t1], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 1 out of 1
      +- Sort [id#94L ASC NULLS FIRST], false, 0
         +- HashAggregate(keys=[id#94L], functions=[count(1)])
            +- HashAggregate(keys=[id#94L], functions=[partial_count(1)])
               +- Filter isnotnull(id#94L)
                  +- FileScan parquet default.t2[id#94L] Batched: true, Bucketed: true, DataFilters: [isnotnull(id#94L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark-warehouse/t2], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint>, SelectedBucketsCount: 1 out of 1 {code}
It feels like the first union->aggregate query should not have an exchange step similar to the second one.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-10-17 22:46:20.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19fug:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: NullPointerException with UTF8String.getBaseObject() when UDF
Issue key: SPARK-40541
Issue id: 13482824
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: garretwilson
Creator: garretwilson
Created: 22/Sep/22 20:30
Updated: 14/Oct/22 18:43
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I'm using Spark 3.3.0 on Windows with Java 17. I have a UDF that returns several columns using:

{code}
StructType schema = createStructType(List.of(… createStructField("bar", StringType, false)));
UserDefinedFunction foobarUdf = udf((String foo) -> {
  …
}, schema).asNondeterministic();
{code}

Note that I specify {{false}} for {{bar}}'s nullability. It turns out that {{foobarUdf}} actually returns {{null}} for {{bar}} sometimes. In the relational database world, I would expect that if my integrity constraint wasn't met, the database would say, "you put {{null}} in {{bar}}, but {{bar}} is not nullable".

What I did _not_ expect is what Spark does: it hits a {{NullPointerException}} and has a nervous breakdown:

{noformat}
[ERROR] Exception in task 0.0 in stage 8.0 (TID 5)
java.lang.NullPointerException: Cannot invoke "org.apache.spark.unsafe.types.UTF8String.getBaseObject()" because "input" is null
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
        at scala.collection.Iterator$$anon$9.next(Iterator.scala:577)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:833)
[WARN] Lost task 0.0 in stage 8.0 (TID 5) (xps-13-9310 executor driver): java.lang.NullPointerException: Cannot invoke "org.apache.spark.unsafe.types.UTF8String.getBaseObject()" because "input" is null
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
        at scala.collection.Iterator$$anon$9.next(Iterator.scala:577)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:833)
{noformat}

It finally fails with:

{noformat}
[ERROR] Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 5) (xps-13-9310 executor driver): java.lang.NullPointerException: Cannot invoke "org.apache.spark.unsafe.types.UTF8String.getBaseObject()" because "input" is null
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
        at scala.collection.Iterator$$anon$9.next(Iterator.scala:577)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:833)
{noformat}

See also [Spark dies with NullPointerException UTF8String.getBaseObject() "input" is null|https://stackoverflow.com/q/73815800].

The irony is that when I actually intend to mark a column as non-nullable when reading in data, Spark ignores me. See [Spark 3.3.0 not honoring my schema nullability in Java|https://stackoverflow.com/q/73476202].
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Oct 14 18:41:06 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18s1k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Oct/22 03:41;ivan.sadikov;What is the question here? Does marking column as nullable make it work? 

Code generation inlines the field access if you don't have nullability, so we know the field is non-nullable. Making it nullable should add the check and mitigate the issue.;;;, 14/Oct/22 17:10;garretwilson;{quote}What is the question here?{quote}

There is no question. This is a bug report. Under no circumstances should a correctly-behaving application throw a {{NullPointerException}}. That is a very bad, broken thing to happen.

{quote}Making it nullable should add the check and mitigate the issue.{quote}

Right. By "mitigate" I assume you mean "work around the bug". Yes, I am working around the bug by telling Spark the columns is nullable. But that doesn't fix the bug.

If you create a relational schema indicating that a relational database column as non-nullable, and then attempt to insert a tuple with a {{null}} value in that column, you will get a semantic error related to a constraint violation. That is an appropriate error. It is a data validation error.

Yes, I know Spark is not a relational database; that was not my point. A {{NullPointerException}} is a programming error, not a data validation error. That is why I'm filing this bug.;;;, 14/Oct/22 18:41;ivan.sadikov;I was asking about the actual problem. It is not clear what you are reporting in the ticket, it reads as a statement, there is no expected behaviour or root cause analysis, or steps to reproduce. As it is, it is not a very good bug report.

Spark is not a database. Any assertions in the generated code, especially row based could affect performance. NullPointerException has a fairly descriptive error: {{{}Cannot invoke "org.apache.spark.unsafe.types.UTF8String.getBaseObject()" because "input" is null{}}}. IMHO, in this case the mitigation should be the right way to fix it for you as it is partially a user error: the UDF returns nulls for a non-null column. 

I can take a look to see how to improve the error. Meanwhile, you can try to disable whole stage code gen and add to your bug report what error you get and whether that one is descriptive enough.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 14/Oct/22 17:10;garretwilson;{quote}What is the question here?{quote}

There is no question. This is a bug report. Under no circumstances should a correctly-behaving application throw a {{NullPointerException}}. That is a very bad, broken thing to happen.

{quote}Making it nullable should add the check and mitigate the issue.{quote}

Right. By "mitigate" I assume you mean "work around the bug". Yes, I am working around the bug by telling Spark the columns is nullable. But that doesn't fix the bug.

If you create a relational schema indicating that a relational database column as non-nullable, and then attempt to insert a tuple with a {{null}} value in that column, you will get a semantic error related to a constraint violation. That is an appropriate error. It is a data validation error.

Yes, I know Spark is not a relational database; that was not my point. A {{NullPointerException}} is a programming error, not a data validation error. That is why I'm filing this bug.;;;
Comment.2: 14/Oct/22 18:41;ivan.sadikov;I was asking about the actual problem. It is not clear what you are reporting in the ticket, it reads as a statement, there is no expected behaviour or root cause analysis, or steps to reproduce. As it is, it is not a very good bug report.

Spark is not a database. Any assertions in the generated code, especially row based could affect performance. NullPointerException has a fairly descriptive error: {{{}Cannot invoke "org.apache.spark.unsafe.types.UTF8String.getBaseObject()" because "input" is null{}}}. IMHO, in this case the mitigation should be the right way to fix it for you as it is partially a user error: the UDF returns nulls for a non-null column. 

I can take a look to see how to improve the error. Meanwhile, you can try to disable whole stage code gen and add to your bug report what error you get and whether that one is descriptive enough.;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in the compilation errors of partitions
Issue key: SPARK-37940
Issue id: 13423111
Parent id: 13423097.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: maxgekk
Creator: maxgekk
Created: 17/Jan/22 17:16
Updated: 12/Oct/22 01:19
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Migrate the following errors in QueryCompilationErrors:
* unsupportedIfNotExistsError
* nonPartitionColError
* missingStaticPartitionColumn
* alterV2TableSetLocationWithPartitionNotSupportedError
* invalidPartitionSpecError
* partitionNotSpecifyLocationUriError
* describeDoesNotSupportPartitionForV2TablesError
* tableDoesNotSupportPartitionManagementError
* tableDoesNotSupportAtomicPartitionManagementError
* alterTableRecoverPartitionsNotSupportedForV2TablesError
* partitionColumnNotSpecifiedError
* invalidPartitionColumnError
* multiplePartitionColumnValuesSpecifiedError
* cannotUseDataTypeForPartitionColumnError
* cannotUseAllColumnsForPartitionColumnsError
* partitionColumnNotFoundInSchemaError
* mismatchedTablePartitionColumnError

onto use error classes. Throw an implementation of SparkThrowable. Also write a test per every error in QueryCompilationErrorsSuite.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): SPARK-38689
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 12 01:19:39 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ynu0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Jan/22 03:41;senthh;I will work on this;;;, 30/Mar/22 18:58;maxgekk;[~senthh] Is there any progress?;;;, 11/Oct/22 13:50;maxgekk;[~panbingkun] Would you like to work on this?;;;, 12/Oct/22 01:19;panbingkun;[~maxgekk] okay, I will work on this!;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 30/Mar/22 18:58;maxgekk;[~senthh] Is there any progress?;;;
Comment.2: 11/Oct/22 13:50;maxgekk;[~panbingkun] Would you like to work on this?;;;
Comment.3: 12/Oct/22 01:19;panbingkun;[~maxgekk] okay, I will work on this!;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: CTAS should respect TBLPROPERTIES during execution
Issue key: SPARK-40719
Issue id: 13485316
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dongjoon
Creator: dongjoon
Created: 09/Oct/22 10:49
Updated: 09/Oct/22 11:04
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, Spark `CTAS` has inconsistent behavior when we use `STORED AS` and `USING`.

{code}
spark-sql> CREATE TABLE t1 STORED AS PARQUET TBLPROPERTIES (parquet.compression 'zstd') AS SELECT 1;

$ ls spark-warehouse/t1
_SUCCESS
part-00000-c28a99f0-a88f-448d-a60d-e8204bf2f3a7-c000.zstd.parquet
{code}

{code}
spark-sql> CREATE TABLE t2 USING PARQUET TBLPROPERTIES (parquet.compression 'zstd') AS SELECT 1;

$ ls spark-warehouse/t2
_SUCCESS
part-00000-3c5853d0-308d-4571-86c3-3e1a31eacfe6-c000.snappy.parquet
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Oct 09 10:54:26 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z197a0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Oct/22 10:54;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38180;;;, 09/Oct/22 10:54;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38180;;;
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 09/Oct/22 10:54;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/38180;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Update embedded JQuery javascript resources
Issue key: SPARK-40683
Issue id: 13484872
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: apurtell
Creator: apurtell
Created: 06/Oct/22 20:52
Updated: 06/Oct/22 20:52
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark 3.3 currently ships with embedded versions of JQuery subject to these vulnerability notices:

CVE-2019-11358
CVE-2020-7656
CVE-2020-11023

This issue is not meant to imply a security issue in Spark itself.

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-10-06 20:52:41.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z194js:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark creates an optional columns in hive table for fields that are not null
Issue key: SPARK-40507
Issue id: 13482442
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dasarianil
Creator: dasarianil
Created: 20/Sep/22 18:04
Updated: 06/Oct/22 20:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Dataframe saveAsTable sets all columns as optional/nullable while creating the table here  

[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L531]

(`outputColumns.toStructType.asNullable`)

This makes source parquet schema and hive table schema doesn't match and is problematic when large dataframe(s) process uses hive as temporary storage to avoid the memory pressure. 

Hive 3.x supports non null constraints on table columns. Please add support for non null constraints on Spark sql hive table. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Oct 06 20:48:05 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18pow:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Oct/22 20:48;dasarianil;does anyone have thoughts on this ?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Push down join condition evaluation
Issue key: SPARK-36290
Issue id: 13391910
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 26/Jul/21 15:06
Updated: 02/Oct/22 02:39
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: {code:scala}
    val numRows = 1024 * 1024 * 15
    spark.sql(s"CREATE TABLE t1 using parquet AS select id as a, id as b from range(${numRows}L)")
    spark.sql(s"CREATE TABLE t2 using parquet AS select id as a, id as b from range(${numRows}L)")
    val benchmark = new Benchmark("Benchmark push down join condition evaluation", numRows, minNumIters = 5)

    Seq(false, true).foreach { pushDownEnabled =>
      val name = s"Join Condition Evaluation ${if (pushDownEnabled) s"(Pushdown)" else ""}"
      benchmark.addCase(name) { _ =>
        withSQLConf("spark.sql.pushDownJoinConditionEvaluationevaluation" -> s"$pushDownEnabled") {
          spark.sql("SELECT t1.* FROM t1 JOIN t2 ON translate(t1.a, '123', 'abc') = translate(t2.a, '123', 'abc')").write.format("noop").mode("Overwrite").save()
        }
      }
    }
    benchmark.run()
{code}


{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark push down join condition evaluation:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-----------------------------------------------------------------------------------------------------------------------------
Join Condition Evaluation                              32459          34521        1465          0.5        2063.7       1.0X
Join Condition Evaluation (Pushdown)                   19483          20350         812          0.8        1238.7       1.7X
{noformat}



Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-40626
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Oct 02 02:39:38 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tc34:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Jul/21 15:38;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/33522;;;, 29/Jul/21 07:35;yumwang;{code:java}
spark.sql("create table t1 using parquet select cast(id as string) as a, cast(id as string) as b from range(1)")
spark.sql("create table t2 using parquet select cast(id as string) as x from range(1)")
spark.sql("set spark.sql.autoBroadcastJoinThreshold=-1")
spark.sql("SELECT t1.* FROM t1 JOIN t2 ON coalesce(t1.a, t1.b)=t2.x").show
{code}
The {{Coalesce}} evaluation:
{noformat}
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.Pmod_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$6(ShuffleExchangeExec.scala:311)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$6$adapted(ShuffleExchangeExec.scala:311)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$14(ShuffleExchangeExec.scala:380)


	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.SortExec$$anon$1.computePrefix(SortExec.scala:90)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:137)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.smj_findNextJoinRows_0$(Unknown Source)




	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.smj_findNextJoinRows_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)
{noformat}


;;;, 02/Oct/22 02:38;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/38071;;;, 02/Oct/22 02:39;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/38071;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 29/Jul/21 07:35;yumwang;{code:java}
spark.sql("create table t1 using parquet select cast(id as string) as a, cast(id as string) as b from range(1)")
spark.sql("create table t2 using parquet select cast(id as string) as x from range(1)")
spark.sql("set spark.sql.autoBroadcastJoinThreshold=-1")
spark.sql("SELECT t1.* FROM t1 JOIN t2 ON coalesce(t1.a, t1.b)=t2.x").show
{code}
The {{Coalesce}} evaluation:
{noformat}
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.Pmod_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$6(ShuffleExchangeExec.scala:311)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$6$adapted(ShuffleExchangeExec.scala:311)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$14(ShuffleExchangeExec.scala:380)


	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.SortExec$$anon$1.computePrefix(SortExec.scala:90)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:137)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.smj_findNextJoinRows_0$(Unknown Source)




	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.smj_findNextJoinRows_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:778)
{noformat}


;;;
Comment.2: 02/Oct/22 02:38;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/38071;;;
Comment.3: 02/Oct/22 02:39;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/38071;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve correlated subqueries
Issue key: SPARK-35553
Issue id: 13380903
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: allisonwang-db
Creator: allisonwang-db
Created: 28/May/21 04:36
Updated: 29/Sep/22 15:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: This umbrella ticket is used to keep track of correlated subquery improvements and bug fixes. 

The goal is to gradually remove the restrictions in CheckAnalysis for correlated subqueries.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-05-28 04:36:22.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rgbc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: `V2ScanRelationPushDown` supports extracting predicates in the output set
Issue key: SPARK-40608
Issue id: 13483802
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wangzhun
Creator: wangzhun
Created: 29/Sep/22 09:52
Updated: 29/Sep/22 09:54
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently does not push down `(ID = 20220921) OR (ID = 20220910)`, if `ID` is a partition field, it makes sense to implement push down
{code:java}
SELECT * FROM h2.test.people WHERE ID = 20220921 OR (ID =20220910 and split(NAME, ',')[0] == 'fred') {code}
h3. before
{code:java}
== Analyzed Logical Plan ==
NAME: string, ID: int
Project [NAME#5, ID#6]
+- Filter ((ID#6 = 20220921) OR ((ID#6 = 20220910) AND (split(NAME#5, ,, -1)[0] = fred)))
   +- SubqueryAlias h2.test.people
         +- RelationV2[NAME#5, ID#6] h2.test.people test.people

== Optimized Logical Plan ==
Filter ((ID#6 = 20220921) OR ((ID#6 = 20220910) AND (split(NAME#5, ,, -1)[0] = fred)))
+- RelationV2[NAME#5, ID#6] test.people

== Physical Plan ==
*(1) Filter ((ID#6 = 20220921) OR ((ID#6 = 20220910) AND (split(NAME#5, ,, -1)[0] = fred)))
+- *(1) Scan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@56c42964 [NAME#5,ID#6] PushedFilters: [], ReadSchema: struct<NAME:string,ID:int> {code}
h3. after
{code:java}
== Analyzed Logical Plan ==
NAME: string, ID: int
Project [NAME#5, ID#6]
+- Filter ((ID#6 = 20220921) OR ((ID#6 = 20220910) AND (split(NAME#5, ,, -1)[0] = fred)))
   +- SubqueryAlias h2.test.people
         +- RelationV2[NAME#5, ID#6] h2.test.people test.people 

== Optimized Logical Plan ==
Filter ((ID#6 = 20220921) OR ((ID#6 = 20220910) AND (split(NAME#5, ,, -1)[0] = fred)))
+- RelationV2[NAME#5, ID#6] test.people

== Physical Plan ==
*(1) Filter ((ID#6 = 20220921) OR ((ID#6 = 20220910) AND (split(NAME#5, ,, -1)[0] = fred)))
+- *(1) Scan org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCScan$$anon$1@56c42964 [NAME#5,ID#6] PushedFilters: [(ID = 20220921) OR (ID = 20220910)], ReadSchema: struct<NAME:string,ID:int>{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-09-29 09:52:51.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18xz4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Override Spark Core_2.12 (v3.3.0) logging configuration
Issue key: SPARK-39804
Issue id: 13471969
Parent id: 
Issue Type: Question
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jd-gen
Creator: jd-gen
Created: 18/Jul/22 05:42
Updated: 28/Sep/22 19:15
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: I'm using Grails 2.5.4 and trying to use _SparkSession_ instance for generating a Parquet output. Recently, upgraded the spark core and it's related dependencies to their latest version(v3.3.0).

 

During the SparkSession builder() initialization, I notice that some extra logs are getting displayed:

 
{noformat}
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
22/07/13 11:58:54 WARN Utils: Your hostname, XY resolves to a loopback address: 127.0.1.1; using 1XX.1XX.0.1XX instead (on interface wlo1)
22/07/13 11:58:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/07/13 11:58:54 INFO SparkContext: Running Spark version 3.3.0
22/07/13 11:58:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/07/13 11:58:54 INFO ResourceUtils: ==============================================================
22/07/13 11:58:54 INFO ResourceUtils: No custom resources configured for spark.driver.
22/07/13 11:58:54 INFO ResourceUtils: ==============================================================
22/07/13 11:58:54 INFO SparkContext: Submitted application: ABCDE
22/07/13 11:58:54 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/07/13 11:58:54 INFO ResourceProfile: Limiting resource is cpu
22/07/13 11:58:54 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/07/13 11:58:54 INFO SecurityManager: Changing view acls to: xy
22/07/13 11:58:54 INFO SecurityManager: Changing modify acls to: xy
22/07/13 11:58:54 INFO SecurityManager: Changing view acls groups to: 
22/07/13 11:58:54 INFO SecurityManager: Changing modify acls groups to: 
22/07/13 11:58:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(xy); groups with view permissions: Set(); users  with modify permissions: Set(xy); groups with modify permissions: Set()
22/07/13 11:58:54 INFO Utils: Successfully started service 'sparkDriver' on port 39483.
22/07/13 11:58:54 INFO SparkEnv: Registering MapOutputTracker
22/07/13 11:58:54 INFO SparkEnv: Registering BlockManagerMaster
22/07/13 11:58:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/07/13 11:58:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/07/13 11:58:54 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/07/13 11:58:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cf39a58e-e5bc-4a26-b92a-d945a0deb8e7
22/07/13 11:58:55 INFO MemoryStore: MemoryStore started with capacity 2004.6 MiB
22/07/13 11:58:55 INFO SparkEnv: Registering OutputCommitCoordinator
22/07/13 11:58:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/07/13 11:58:55 INFO Executor: Starting executor ID driver on host 1XX.1XX.0.1XX
22/07/13 11:58:55 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
22/07/13 11:58:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33993.
22/07/13 11:58:55 INFO NettyBlockTransferService: Server created on 192.168.0.135:33993
22/07/13 11:58:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/07/13 11:58:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.0.135, 33993, None)
22/07/13 11:58:55 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.0.135:33993 with 2004.6 MiB RAM, BlockManagerId(driver, 192.168.0.135, 33993, None)
22/07/13 11:58:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.0.135, 33993, None)
22/07/13 11:58:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.0.135, 33993, None){noformat}
 

Before initializing the SparkSession instance using the builder() method, I've configured the logger level programmatically by:
{code:java}
Configurator.setLevel("org", Level.ERROR)
Configurator.setLevel("org.apache.spark", Level.ERROR)
Configurator.setLevel("akka", Level.ERROR)
Configurator.setLevel("scala", Level.ERROR)
Configurator.setLevel("java", Level.ERROR)
Configurator.setLevel("org.slf4j", Level.ERROR)
Configurator.setLevel("com", Level.ERROR)
Configurator.setLevel("javax", Level.ERROR)
Configurator.setLevel("jakarta", Level.ERROR)
Configurator.setLevel("io", Level.ERROR)
Configurator.setLevel("net", Level.ERROR)
 {code}
I notice that it's picking the default _log4j2.properties_ file of Spark. Is there a way I can override the logging configuration programatically or disable this default logging so that these extra logs don't appear ?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 28 19:15:46 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16xgo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Sep/22 19:15;cornelcreanga;Spark will only load the default file when no custom configuration was declared (it check to see if log4j2 is using the 

org.apache.logging.log4j.core.config.DefaultConfiguration)

If you need an example how to declare a configuration programmatically you can take a look on my git repo [here|[http://example.com|https://github.com/cornelcreanga/spark-playground/blob/master/examples/src/main/scala/com/creanga/playground/spark/example/logging/CustomConfigurationFactory.java]]. ;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: NullPointerException: Cannot invoke invalidateSerializedMapOutputStatusCache() because "shuffleStatus" is null
Issue key: SPARK-40582
Issue id: 13483476
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: garretwilson
Creator: garretwilson
Created: 27/Sep/22 15:02
Updated: 28/Sep/22 01:51
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: I'm running a simple little Spark 3.3.0 pipeline on Windows 10 using Java 17 and UDFs. I hardly do anything interesting, and now when I run the pipeline on only 30,000 records I'm getting this:

{noformat}
[ERROR] Error in removing shuffle 2
java.lang.NullPointerException: Cannot invoke "org.apache.spark.ShuffleStatus.invalidateSerializedMapOutputStatusCache()" because "shuffleStatus" is null
        at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1(MapOutputTracker.scala:882)
        at org.apache.spark.MapOutputTrackerMaster.$anonfun$unregisterShuffle$1$adapted(MapOutputTracker.scala:881)
        at scala.Option.foreach(Option.scala:437)
        at org.apache.spark.MapOutputTrackerMaster.unregisterShuffle(MapOutputTracker.scala:881)
        at org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$3(BlockManagerStorageEndpoint.scala:59)
        at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.scala:17)
        at org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)
        at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678)
        at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:833)
{noformat}

I searched and couldn't find any of the principal terms in the error message.

Disconcerting that Spark is breaking at what seems to be a fundamental part of processing, and with a {{NullPointerException}} at that.

I have already asked this question on [Stack Overflow|https://stackoverflow.com/q/73732970], and even posted a bounty, with no solutions. (The only answer so far is from someone who doesn't even use Spark and just posted links.)

_Update:_ Now it just happened with only 1000 records. But then I reran the pipeline immediately with no changes, and it succeeded. So this {{NullPointerException}} bug is nondeterministic. Not good at all.

_Update:_ Now it just happened with only 10 records. But then as before I reran the pipeline immediately with no changes, and it succeeded.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 28 00:04:56 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18vyw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Sep/22 17:09;LuciferYang;Do you use Scala 2.13? [~garretwilson] 

 ;;;, 27/Sep/22 17:13;garretwilson;{quote}Do you use Scala 2.13?{quote}

Yes. (Sorry; I misread the question earlier.) But my application is in Java 17. I only use Scala 2.13 insofar as I'm required to specify a Scala version in the dependency:

{code:xml}
<scala.version>2.13</scala.version>
<spark.version>3.3.0</spark.version>
…
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_${scala.version}</artifactId>
  <version>${spark.version}</version>
  <exclusions>
    <exclusion>
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-slf4j-impl</artifactId>
    </exclusion>
  </exclusions>
</dependency>
{code}

(Don't get me started about logging. See SPARK-40489 for that.);;;, 27/Sep/22 17:37;LuciferYang;It looks like a known bug of Scala 2.13.8 and fixed by SPARK-39553,  I think Spark 3.3.1 will include  this fix;;;, 27/Sep/22 17:45;garretwilson;That's awesome to hear, Yang! Thanks for the good news.;;;, 27/Sep/22 17:47;garretwilson;Will Spark be updated to use the newer version of Scala when it's released rather than just providing a workaround?

For example, it looks like [this has already been fixed in Scala|https://github.com/scala/bug/issues/12613] since June 2022 in v2.13.8. Can I just upgrade the Scala dependencies somehow, or does Spark constantly saddle me with an outdated version of Scala and until Spark decides to upgrade?

(I wish Spark weren't based on Scala for this very sort of issue. I wish we weren't forced to juggle the versions of both the Spark library and another language, and be hit with another layer of bugs because of this.);;;, 28/Sep/22 00:04;LuciferYang;Yes, Scala 2.13.9 fixes this issue, but 2.13.9 has an [incompatible syntax problem|https://github.com/scala/bug/issues/12641#issuecomment-1252344400], so Spark 3.4 will wait for Scala 2.13.10 to upgrade;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 27/Sep/22 17:13;garretwilson;{quote}Do you use Scala 2.13?{quote}

Yes. (Sorry; I misread the question earlier.) But my application is in Java 17. I only use Scala 2.13 insofar as I'm required to specify a Scala version in the dependency:

{code:xml}
<scala.version>2.13</scala.version>
<spark.version>3.3.0</spark.version>
…
<dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_${scala.version}</artifactId>
  <version>${spark.version}</version>
  <exclusions>
    <exclusion>
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-slf4j-impl</artifactId>
    </exclusion>
  </exclusions>
</dependency>
{code}

(Don't get me started about logging. See SPARK-40489 for that.);;;
Comment.2: 27/Sep/22 17:37;LuciferYang;It looks like a known bug of Scala 2.13.8 and fixed by SPARK-39553,  I think Spark 3.3.1 will include  this fix;;;
Comment.3: 27/Sep/22 17:45;garretwilson;That's awesome to hear, Yang! Thanks for the good news.;;;
Comment.4: 27/Sep/22 17:47;garretwilson;Will Spark be updated to use the newer version of Scala when it's released rather than just providing a workaround?

For example, it looks like [this has already been fixed in Scala|https://github.com/scala/bug/issues/12613] since June 2022 in v2.13.8. Can I just upgrade the Scala dependencies somehow, or does Spark constantly saddle me with an outdated version of Scala and until Spark decides to upgrade?

(I wish Spark weren't based on Scala for this very sort of issue. I wish we weren't forced to juggle the versions of both the Spark library and another language, and be hit with another layer of bugs because of this.);;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: The distributed runtime has one more identical process with a small amount of data on the master
Issue key: SPARK-40564
Issue id: 13483199
Parent id: 
Issue Type: Question
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: littleNing
Creator: littleNing
Created: 26/Sep/22 12:26
Updated: 27/Sep/22 02:28
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: When I ran my program with the Dataframe structure in Pyspark.PANDAS, there is an abnormal extra process on the master. My dataframe contains three columns named "id", "path", and "category". It contains more than 300,000 pieces of data in total, and the "id" values are only 1, 2, 3, and 4. When I use "groupBy (" id ").apply(func)", my four nodes run normally, but there is an abnormal process in the master, which contains 1001 pieces of data. This process also executes the code in "func" and is divided into four parts, each part contains more than 200 pieces of data. When I collect the results in each node, I can only collect the results of 1001 data points, and the results of 300,000 data points are lost. When I tried to reduce the number of data to about 20,000, this problem still occurred and the data volume was still 1001. I suspect there is a problem with the implementation of this API.I tried setting the number of data partitions to 4, but the problem didn't go away.The value of the dataframe, part of the code, and the output of the exception process are attached
Environment: Hadoop 3.3.1

蟒蛇3.8

火花3.3.0

pyspark 3.3.0

ubuntu 20.04
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 26/Sep/22 13:01;littleNing;Part of the code.png;https://issues.apache.org/jira/secure/attachment/13049764/Part+of+the+code.png, 26/Sep/22 13:01;littleNing;The output of the abnormal process.png;https://issues.apache.org/jira/secure/attachment/13049765/The+output+of+the+abnormal+process.png, 26/Sep/22 13:01;littleNing;Value of df.png;https://issues.apache.org/jira/secure/attachment/13049766/Value+of+df.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 3.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-09-26 12:26:28.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18uao:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add showIndex function 
Issue key: SPARK-40566
Issue id: 13483208
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: KaiXinXIaoLei
Creator: KaiXinXIaoLei
Created: 26/Sep/22 13:24
Updated: 27/Sep/22 01:29
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I find there isn't a showIndex function.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Sep 27 01:29:34 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18uco:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Sep/22 01:28;apachespark;User 'huleilei' has created a pull request for this issue:
https://github.com/apache/spark/pull/38007;;;, 27/Sep/22 01:29;apachespark;User 'huleilei' has created a pull request for this issue:
https://github.com/apache/spark/pull/38007;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 27/Sep/22 01:29;apachespark;User 'huleilei' has created a pull request for this issue:
https://github.com/apache/spark/pull/38007;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Remove min heap setting in Kubernetes Dockerfile entrypoint
Issue key: SPARK-40505
Issue id: 13482373
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bryanck
Creator: bryanck
Created: 20/Sep/22 11:59
Updated: 20/Sep/22 12:12
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.2, 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: The entrypoint script for the Kubernetes Dockerfile sets the Java min heap setting (-Xms) to be the same as the max setting (-Xmx) for the executor process. This prevents the JVM from shrinking the heap and can lead to excessive memory usage in some scenarios. Removing the min heap setting is consistent with YARN executor startup.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Sep 20 12:12:36 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18pa8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Sep/22 12:11;apachespark;User 'bryanck' has created a pull request for this issue:
https://github.com/apache/spark/pull/37950;;;, 20/Sep/22 12:12;apachespark;User 'bryanck' has created a pull request for this issue:
https://github.com/apache/spark/pull/37950;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 20/Sep/22 12:12;apachespark;User 'bryanck' has created a pull request for this issue:
https://github.com/apache/spark/pull/37950;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Allow job status tracking with jobGroupId
Issue key: SPARK-40475
Issue id: 13481928
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: anuragmantri
Creator: anuragmantri
Created: 16/Sep/22 18:45
Updated: 16/Sep/22 20:04
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core, Web UI
Due Date: 
Votes: 0
Labels: 
Description: Spark let's us group jobs together by setting a job group id. This is useful to check the job group in the web UI. For example

{{spark.sparkContext().setJobGroup("mygroup_id")}}

We have a use-case where we would like to have a long running Spark application and have jobs submitted to it. We would like to programmatically check the status of the jobs created by this group id. For example, [SQLStatusStore|#L41]] has `executionList()` which returns a map of jobs to the status. There is no way to filter this based on jobGroupId. 

This Jira is to add ability to get fine grained job statues by jobGroupId.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Sep 16 20:04:17 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18mjk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Sep/22 20:04;dongjoon;Thank you for filing a JIRA, [~anuragmantri] . Go for it!;;;
Affects Version/s.1: 
Component/s.1: Web UI
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Abort result stage directly when it failed caused by FetchFailed
Issue key: SPARK-40455
Issue id: 13481692
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 15/Sep/22 11:41
Updated: 15/Sep/22 12:03
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.0.0, 3.1.2, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Here's a very serious bug：

When result stage failed caused by FetchFailedException,  the previous condition to determine whether result stage retries are allowed is {color:#ff0000}numMissingPartitions < resultStage.numTasks{color}. 

 

If this condition holds on retry, but the other tasks at the current result stage are not killed, when result stage was resubmit, it would got wrong partitions to recalculation.
{code:java}
// DAGScheduler#submitMissingTasks
 
// Figure out the indexes of partition ids to compute.
val partitionsToCompute: Seq[Int] = stage.findMissingPartitions() {code}
It is possible that the number of partitions to be recalculated is smaller than the actual number of partitions at result stage
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 15 12:03:39 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18l40:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Sep/22 12:03;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/37899;;;, 15/Sep/22 12:03;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/37899;;;
Affects Version/s.1: 3.1.2
Component/s.1: 
Comment.1: 15/Sep/22 12:03;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/37899;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Simplify join condition of form (a==b) || (a==null&&b==null) to a<=>b
Issue key: SPARK-40177
Issue id: 13477952
Parent id: 
Issue Type: Task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ayaga
Creator: ayaga
Created: 22/Aug/22 08:13
Updated: 13/Sep/22 08:25
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: If the join condition is like key1==key2 || (key1==null && key2==null), join is executed as Broadcast Nested Loop Join as this condition doesn't satisfy equi join condition. BNLJ takes more time as compared to Sort merge or broadcast join. This condition can be converted to key1<=>key2 to make the join execute as Broadcast or sort merge join.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Aug 23 09:08:13 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17y94:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Aug/22 08:14;ayaga;Working on the PR;;;, 23/Aug/22 09:07;apachespark;User 'ayushi-agarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/37625;;;, 23/Aug/22 09:08;apachespark;User 'ayushi-agarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/37625;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 23/Aug/22 09:07;apachespark;User 'ayushi-agarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/37625;;;
Comment.2: 23/Aug/22 09:08;apachespark;User 'ayushi-agarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/37625;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Increase pandas API coverage in PySpark
Issue key: SPARK-36394
Issue id: 13393332
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: XinrongM
Reporter: XinrongM
Creator: XinrongM
Created: 03/Aug/21 20:45
Updated: 07/Sep/22 06:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Increase pandas API coverage in PySpark.

 

Especially, pending PRs [https://github.com/databricks/koalas/pulls] should be ported. Existing tickets are created for porting purposes, please avoid working on that.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): SPARK-36395
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-34849, SPARK-40327
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-03 20:45:26.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tkuo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): ueshin
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use spark config to configure the parameters of volcano podgroup
Issue key: SPARK-40350
Issue id: 13480105
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: biaobiao.sun
Creator: biaobiao.sun
Created: 05/Sep/22 09:47
Updated: 05/Sep/22 09:57
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: Now we use volcano as our scheduler， we need  specify  the following configuration options:

 

 
{code:java}
spark.kubernetes.scheduler.name=volcano
spark.kubernetes.scheduler.volcano.podGroupTemplateFile=/path/to/podgroup-template.yaml

spark.kubernetes.driver.pod.featureSteps=org.apache.spark.deploy.k8s.features.VolcanoFeatureStep

spark.kubernetes.executor.pod.featureSteps=org.apache.spark.deploy.k8s.features.VolcanoFeatureStep {code}
 

 

we should use configMap to mount the /path/to/podgroup-template.yaml file

 

If we use spark config to specify the parameters of the podgroup, it will be much more convenient,we don't need configmap to mount static files

 

In our scenario, we need to dynamically specify the volcano queue, but it is not convenient to create a static podgroup configuration file to mount 

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Sep 05 09:57:46 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18bf4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Sep/22 09:57;apachespark;User 'zheniantoushipashi' has created a pull request for this issue:
https://github.com/apache/spark/pull/37802;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support of Columnar result(ColumnarBatch) in org.apache.spark.sql.Dataset flatMap, transform, etc
Issue key: SPARK-40325
Issue id: 13480022
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: igor.suhorukov
Creator: igor.suhorukov
Created: 04/Sep/22 11:48
Updated: 04/Sep/22 11:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Java API, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Sometimes result of data transformation in JVM program available from native code in Apache Arrow columnar data format. Current Dataset API require unnecessary data transform from columnar format wrapper into row with additional allocation on JVM heap. 

In this proposed feature I ask for propagation of columnar data in DatasetAPI without unnecessary InternalRow->Row->InternalRow conversion.

 

Current solution use [ColumnarBatch wrapper|https://github.com/igor-suhorukov/spark3/blob/master/src/main/java/com/github/igorsuhorukov/arrow/spark/ArrowDataIterator.java] on top of ArrowColumnVector and rowExpressionEncoder.createDeserializer() to transform data [into Row|https://github.com/igor-suhorukov/spark3/blob/c655d4b6058fdd4529aa59093edfe2333d96fb05/src/main/java/com/github/igorsuhorukov/arrow/spark/ArrowDataIterator.java#L53]

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-27396
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-09-04 11:48:38.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18ax4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Spark Core
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support more than Integer.MAX_VALUE of the same join key
Issue key: SPARK-40306
Issue id: 13479696
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wankun
Creator: wankun
Created: 01/Sep/22 15:01
Updated: 01/Sep/22 15:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: For SMJ, the number of the same join key records of the right table is greater than Integer.MAX_VALUE, the result will be incorrect. 
Before SMJ JOIN, we will put the records of the same join key into the ExternalAppendOnlyUnsafeRowArray. ExternalAppendOnlyUnsafeRowArray.numRows overflow may cause OOM. During SMJ JOIN, SpillableArrayIterator.startIndex overflow may cause incorrect result.

For example, one of our production table:

!image-2022-09-01-23-02-15-955.png!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 01/Sep/22 15:02;wankun;image-2022-09-01-23-02-15-955.png;https://issues.apache.org/jira/secure/attachment/13048866/image-2022-09-01-23-02-15-955.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 01 15:10:50 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z188wo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Sep/22 15:10;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/37759;;;, 01/Sep/22 15:10;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/37759;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 01/Sep/22 15:10;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/37759;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Security Vulnerability CVE-2018-11793 due to mesos-1.4.3-shaded-protobuf.jar
Issue key: SPARK-40123
Issue id: 13477240
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: khakunin
Creator: khakunin
Created: 17/Aug/22 13:19
Updated: 31/Aug/22 17:43
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Mesos
Due Date: 
Votes: 0
Labels: security-issue
Description: Hello Team,
We are facing this vulnerability on Spark Installation 3.3.3 , Can we please upgrade the version of mesos in our installation to address this vulnerability. 


||Package||cve||cvss||severity||pkg_version||fixed_in_pkg||pkg_path||
|1|org.apache.mesos_mesos|CVE-2018-11793|7|high|1.4.0|1.7.1, 1.6.2, 1.5.2, 1.4.3|/opt/domino/spark/python/build/lib/pyspark/jars/mesos-1.4.0-shaded-protobuf.jar|


In our source code i found that the depedant version of mesos jar is 1.4.3 

user@ThinkPad-E14-02:~/Downloads/spark-master$ grep -ir mesos- * 
core/src/main/scala/org/apache/spark/scheduler/SchedulerBackend.scala: * TaskSchedulerImpl. We assume a Mesos-like model where the application gets resource offers as
*dev/deps/spark-deps-hadoop-2-hive-2.3:mesos/1.4.3/shaded-protobuf/mesos-1.4.3-shaded-protobuf.jar
dev/deps/spark-deps-hadoop-3-hive-2.3:mesos/1.4.3/shaded-protobuf/mesos-1.4.3-shaded-protobuf.jar
*
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Patch
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 31 17:43:43 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17tv4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/Aug/22 17:43;srowen;Mesos is deprecated, but, if you want you can open a PR to update this. I think this is going away in Spark 4 but probably fine to just update the lib now;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Logging isn't configurable via log4j2 with hadoop-provided profile
Issue key: SPARK-40246
Issue id: 13478946
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kimahriman
Creator: kimahriman
Created: 27/Aug/22 17:44
Updated: 27/Aug/22 17:50
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 0
Labels: 
Description: When building Spark with -Phadoop-provided (or using the 3.3.0 build without Hadoop), there is no slf implementation provided for log4j2, so the default log4j2 properties are ignored and logging isn't configurable via SparkContext.setLogLevel.

Reproduction on a fresh Ubuntu container:

 
{noformat}
apt-get update
apt-get install -y wget
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz
wget https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-without-hadoop.tgz
tar -xvf hadoop-3.3.4.tar.gz -C /opt
tar -xvf spark-3.3.0-bin-without-hadoop.tgz -C /opt
export HADOOP_HOME=/opt/hadoop-3.3.4/
export SPARK_HOME=/opt/spark-3.3.0-bin-without-hadoop/
apt install -y openjdk-11-jre-headless python3
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/
export SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)
$SPARK_HOME/bin/pyspark
{noformat}
The default log level starts at INFO and you can't change it with sc.setLogLevel
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Aug 27 17:50:04 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z184cg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Aug/22 17:50;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/37694;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in the execution errors of casting
Issue key: SPARK-37944
Issue id: 13423117
Parent id: 13423097.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: maxgekk
Creator: maxgekk
Created: 17/Jan/22 17:34
Updated: 22/Aug/22 10:23
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Migrate the following errors in QueryExecutionErrors:
* failedToCastValueToDataTypeForPartitionColumnError
* invalidInputSyntaxForNumericError
* cannotCastToDateTimeError
* invalidInputSyntaxForBooleanError
* nullLiteralsCannotBeCastedError

onto use error classes. Throw an implementation of SparkThrowable. Also write a test per every error in QueryExecutionErrorsSuite.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 22 10:23:27 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ynvc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Jan/22 03:46;senthh;I will work on this;;;, 30/Mar/22 18:52;maxgekk;[~senthh] Any updates?;;;, 26/Jul/22 06:37;goutamghosh;[~maxgekk] can I work on this ?    invalidInputSyntaxForNumericError and cannotCastToDateTimeError are not present in 
QueryExecutionErrors any more and invalidInputSyntaxForBooleanError is already using error classes.;;;, 26/Jul/22 07:32;maxgekk;> can I work on this ?

[~goutamghosh] Sure, go ahead.;;;, 22/Aug/22 10:23;apachespark;User 'goutam-git' has created a pull request for this issue:
https://github.com/apache/spark/pull/37613;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 30/Mar/22 18:52;maxgekk;[~senthh] Any updates?;;;
Comment.2: 26/Jul/22 06:37;goutamghosh;[~maxgekk] can I work on this ?    invalidInputSyntaxForNumericError and cannotCastToDateTimeError are not present in 
QueryExecutionErrors any more and invalidInputSyntaxForBooleanError is already using error classes.;;;
Comment.3: 26/Jul/22 07:32;maxgekk;> can I work on this ?

[~goutamghosh] Sure, go ahead.;;;
Comment.4: 22/Aug/22 10:23;apachespark;User 'goutam-git' has created a pull request for this issue:
https://github.com/apache/spark/pull/37613;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Error while creating dataset in Java spark-3.x using Encoders bean with Dense Vector. (Issue arises when updating spark from 2.4 to 3.x)
Issue key: SPARK-40074
Issue id: 13476727
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: anujgrgv
Creator: anujgrgv
Created: 15/Aug/22 00:28
Updated: 22/Aug/22 01:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.2.2, 3.3.0
Fix Version/s: 
Component/s: Java API, ML, SQL
Due Date: 
Votes: 0
Labels: 
Description: Encountered a compatibility issue while upgrading spark from 2.4 to 3.x (also scala is upgraded from 2.11 to 2.12). 

This java code below used to work with spark 2.4 but when migrated to 3.x it gives the error (mentioned below) I have done my own research but couldn't find a solution or any related information.
 

 
{code:java|title=Code.java|borderStyle=solid}
public void test() {

final SparkSession spark = SparkSession.builder()
.appName("Test")
.getOrCreate();

DenseClass denseFactor1 = new DenseClass( new DenseVector( new double[]{0.13, 0.24}));

DenseClass denseFactor2 = new DenseClass( new DenseVector( new double[]{0.24, 0.32}));

final List<DenseClass> inputsNew = Arrays.asList(denseFactor1, denseFactor2);

final Dataset<DenseClass> denseVectorDf = spark.createDataset(inputsNew, Encoders.bean(DenseClass.class));

denseVectorDf.printSchema();
}


public static class DenseClass implements Serializable

{ private org.apache.spark.ml.linalg.DenseVector denseVector; }{code}
The error occurs while creating the dataset *denseVectorDf* .

Error
 
{noformat}
}}
{{org.apache.spark.sql.AnalysisException: Cannot up cast `denseVector` from struct<> to struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.
The type path of the target object is:
 - field (class: "org.apache.spark.ml.linalg.DenseVector", name: "denseVector")
You can either add an explicit cast to the input data or choose a higher precision type of the field in the target object}}

{{{noformat}
I have tried to use _double_ instead of dense vector and it works just fine, but fails on using the dense vector with encoders bean.

 

StackOverflow link for the issue: [https://stackoverflow.com/questions/73313660/error-while-creating-dataset-in-java-spark-3-x-using-encoders-bean-with-dense-ve]

 
Environment: Scala 2.12

Spark 3.x
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): java, scala
Custom field (Last public comment date): 2022-08-15 00:28:22.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17qps:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.2
Component/s.1: ML
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: JDBC connection to Hive Metastore fails without first calling any .jdbc call
Issue key: SPARK-40108
Issue id: 13477132
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: chajath
Creator: chajath
Created: 16/Aug/22 22:58
Updated: 16/Aug/22 22:58
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Tested on pyspark==3.3.0. When talking to hive metastore with MySQL backend, I installed MySQL driver with spark.jars.packages, alongside with other necessary settings:

ss = SparkSession.builder.master('local[*]')\
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.3," +
        "org.apache.hadoop:hadoop-common:3.3.3,mysql:mysql-connector-java:8.0.30") \   .config("spark.executor.memory", "10g") \
    .config("spark.driver.memory", "10g") \
    .config("spark.memory.offHeap.enabled","true") \
    .config("spark.memory.offHeap.size","32g")  \
    .config("spark.hadoop.javax.jdo.option.ConnectionURL", "jdbc:mysql://localhost:3306/hive") \
    .config("spark.hadoop.javax.jdo.option.ConnectionUserName", "yyyy") \
    .config("spark.hadoop.javax.jdo.option.ConnectionPassword", "xxxx") \
    .config("spark.hadoop.javax.jdo.option.ConnectionDriverName", "com.mysql.cj.jdbc.Driver") \
    .config("spark.sql.hive.metastore.sharedPrefixes", "com.mysql") \
    .config("spark.sql.warehouse.dir", "s3://xxxx-yyyy/") \
    .enableHiveSupport() \
    .appName("hms_test").config(conf=conf).getOrCreate()

Now, if I just do: ss.sql("SHOW DATABASES;").show() I get a lot of errors, saying:

Unable to open a test connection to the given database. JDBC url = jdbc:mysql://localhost:3306/hive, username = yyyy. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------
java.sql.SQLException: No suitable driver found for jdbc:mysql://localhost:3306/hive
    at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:702)
    at java.sql/java.sql.DriverManager.getConnection(DriverManager.java:189)
    at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)
    at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)
...

However, if I do any "jdbc" read, even if the call ends up in an error, then the call to Hive Metastore seem to succeed without any issue:

try:
    _ = ss.read.format("jdbc") \
        .option("url", "jdbc:mysql://localhost:3306/hive") \
        .option("query", "SHOW TABLES;") \
        .option("driver", "com.mysql.cj.jdbc.Driver").load()
except:
    pass

ss.sql("SHOW DATABASES;").show() # this now works fine.
Environment: PySpark==3.3.0
Java 11
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-08-16 22:58:15.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17t7k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add shuffle index cache expire time policy to avoid unused continuous memory consumption
Issue key: SPARK-40083
Issue id: 13476807
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wangshengjie
Creator: wangshengjie
Created: 15/Aug/22 11:33
Updated: 15/Aug/22 13:03
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Shuffle, YARN
Due Date: 
Votes: 0
Labels: 
Description: In our production environment, we found some applicaitons finished already about 2 days and its cache still in memory, so we could add guava cache expire time policy to save memory.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 15 13:02:43 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17r7c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Aug/22 11:34;wangshengjie;I'm working on this, a pr will be submitted later.;;;, 15/Aug/22 11:47;apachespark;User 'wangshengjie123' has created a pull request for this issue:
https://github.com/apache/spark/pull/37522;;;, 15/Aug/22 13:02;wangshengjie;Maybe i also should add expire policy for push merge shuffle manager ;;;
Affects Version/s.1: 
Component/s.1: YARN
Comment.1: 15/Aug/22 11:47;apachespark;User 'wangshengjie123' has created a pull request for this issue:
https://github.com/apache/spark/pull/37522;;;
Comment.2: 15/Aug/22 13:02;wangshengjie;Maybe i also should add expire policy for push merge shuffle manager ;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: ParquetSchemaConverter fails match schema by id
Issue key: SPARK-39997
Issue id: 13475543
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zhenw
Creator: zhenw
Created: 06/Aug/22 13:31
Updated: 06/Aug/22 14:03
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
{code:scala}
  test("SPARK-38094: absence of field ids: reading nested schema struct field renamed") {
    withTempDir { dir =>
      // now with nested schema/complex type

      val innerTypeRenamed = new StructType().add("c1", IntegerType, true, withId(6));
      val readSchema =
        new StructType()
          .add("c", ArrayType(innerTypeRenamed), true, withId(3))
          .add("e", IntegerType, true, withId(5))


      val innerType = new StructType().add("c0", IntegerType, true, withId(6))
      val writeSchema =
        new StructType()
          .add("c", ArrayType(innerType), true, withId(3))
          .add("randomName", StringType, true)

      val writeData = Seq(Row(Seq(Row(100)), "text"), Row(Seq(Row(100)), "more"))

      spark.createDataFrame(writeData.asJava, writeSchema)
        .write.mode("overwrite").parquet(dir.getCanonicalPath)

      withAllParquetReaders {
        checkAnswer(spark.read.schema(readSchema).parquet(dir.getCanonicalPath),
          // a, b, c, d all couldn't be found
          Row(Seq(Row(100)), null) :: Row(Seq(Row(100)), null) :: Nil)
      }
    }
  }
{code}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): SPARK-38094, SPARK-36935
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Aug 06 14:00:52 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17jh4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Aug/22 14:00;apachespark;User 'zinking' has created a pull request for this issue:
https://github.com/apache/spark/pull/37428;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: It's necessary to materialize BroadcastQueryStage first, because the BroadcastQueryStage does not  timeout in AQE. 
Issue key: SPARK-39950
Issue id: 13474690
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 02/Aug/22 12:06
Updated: 02/Aug/22 12:15
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.1, 3.2.2, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Aug 02 12:14:59 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17e88:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Aug/22 12:14;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/37362;;;
Affects Version/s.1: 3.2.2
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Print applicationId once applied from yarn rm
Issue key: SPARK-39850
Issue id: 13473111
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: LiDongwei
Creator: LiDongwei
Created: 24/Jul/22 09:27
Updated: 24/Jul/22 09:42
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: YARN
Due Date: 
Votes: 0
Labels: 
Description: As we all know,between client gets application from yarn and submits the application to yarn,there is still a lot work to do . if a application fails during these works,user can not easily find out the application id.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jul 24 09:42:30 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z174i0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Jul/22 09:42;apachespark;User 'DongweiLee' has created a pull request for this issue:
https://github.com/apache/spark/pull/37265;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add environment bin folder to R/Python subprocess PATH
Issue key: SPARK-39659
Issue id: 13469736
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: rshkv
Creator: rshkv
Created: 02/Jul/22 02:48
Updated: 22/Jul/22 14:08
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Some Python packages rely on non-Python executables which are usually made available on the {{PATH}} through something like {{{}conda activate{}}}.

When using Spark with conda-pack environments added via {{{}spark.archives{}}}, Python packages aren't able to find conda-installed executables because Spark doesn't update {{{}PATH{}}}.

E.g.
{code:java|title=test.py}
# This only works if kaleido-python can find the conda-installed executable
fig = px.scatter(px.data.iris(), x="sepal_length", y="sepal_width", color="species")
fig.write_image("figure.png", engine="kaleido")
{code}
and
{code:java}
./bin/spark-submit --master yarn --deploy-mode cluster --archives environment.tar.gz#environment --conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./environment/bin/python test.py
{code}
will throw
{code:java}
Traceback (most recent call last):
  File "/tmp/hadoop-hadoop/nm-local-dir/usercache/wraschkowski/appcache/application_1656456739406_0012/container_1656456739406_0012_01_000001/kaleido-test.py", line 7, in <module>
    fig.write_image("figure.png", engine="kaleido")
  File "/tmp/hadoop-hadoop/nm-local-dir/usercache/wraschkowski/appcache/application_1656456739406_0012/container_1656456739406_0012_01_000001/environment/lib/python3.10/site-packages/plotly/basedatatypes.py", line 3829, in write_image
    return pio.write_image(self, *args, **kwargs)
  File "/tmp/hadoop-hadoop/nm-local-dir/usercache/wraschkowski/appcache/application_1656456739406_0012/container_1656456739406_0012_01_000001/environment/lib/python3.10/site-packages/plotly/io/_kaleido.py", line 267, in write_image
    img_data = to_image(
  File "/tmp/hadoop-hadoop/nm-local-dir/usercache/wraschkowski/appcache/application_1656456739406_0012/container_1656456739406_0012_01_000001/environment/lib/python3.10/site-packages/plotly/io/_kaleido.py", line 144, in to_image
    img_bytes = scope.transform(
  File "/tmp/hadoop-hadoop/nm-local-dir/usercache/wraschkowski/appcache/application_1656456739406_0012/container_1656456739406_0012_01_000001/environment/lib/python3.10/site-packages/kaleido/scopes/plotly.py", line 153, in transform
    response = self._perform_transform(
  File "/tmp/hadoop-hadoop/nm-local-dir/usercache/wraschkowski/appcache/application_1656456739406_0012/container_1656456739406_0012_01_000001/environment/lib/python3.10/site-packages/kaleido/scopes/base.py", line 293, in _perform_transform
    self._ensure_kaleido()
  File "/tmp/hadoop-hadoop/nm-local-dir/usercache/wraschkowski/appcache/application_1656456739406_0012/container_1656456739406_0012_01_000001/environment/lib/python3.10/site-packages/kaleido/scopes/base.py", line 176, in _ensure_kaleido
    proc_args = self._build_proc_args()
  File "/tmp/hadoop-hadoop/nm-local-dir/usercache/wraschkowski/appcache/application_1656456739406_0012/container_1656456739406_0012_01_000001/environment/lib/python3.10/site-packages/kaleido/scopes/base.py", line 123, in _build_proc_args
    proc_args = [self.executable_path(), self.scope_name]
  File "/tmp/hadoop-hadoop/nm-local-dir/usercache/wraschkowski/appcache/application_1656456739406_0012/container_1656456739406_0012_01_000001/environment/lib/python3.10/site-packages/kaleido/scopes/base.py", line 99, in executable_path
    raise ValueError(
ValueError: 
The kaleido executable is required by the kaleido Python library, but it was not included
in the Python package and it could not be found on the system PATH.

Searched for included kaleido executable at:
    /tmp/hadoop-hadoop/nm-local-dir/usercache/wraschkowski/appcache/application_1656456739406_0012/container_1656456739406_0012_01_000001/environment/lib/python3.10/site-packages/kaleido/executable/kaleido 

Searched for executable 'kaleido' on the following system PATH:
    /usr/local/sbin
    /usr/local/bin
    /usr/sbin
    /usr/bin
    /sbin
    /bin
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 22 14:08:59 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16jso:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Jul/22 02:52;rshkv;The way we solve this in our fork is by doing something like
{code:scala}
PythonUtils.appendDirToEnvironmentPath(Paths.get(pythonExec).toAbsolutePath.getParent, builder)
{code}
with
{code:scala}
  /**
   * Append the directory to the subprocess' PATH environment variable.
   *
   * This allows the Python subprocess to find additional executables when the environment
   * containing those executables was added at runtime (e.g. via sc.addArchive()).
   */
  def appendDirToEnvironmentPath(dir: Path, processBuilder: ProcessBuilder): Unit = {
    processBuilder.environment().compute("PATH", (_, oldPath) =>
      Option(oldPath).map(_ + File.pathSeparator + dir).getOrElse(dir.toString))
  }
{code}
inside the driver- and executor-side PythonRunners.;;;, 02/Jul/22 02:55;rshkv;Alternatively, one could update {{PATH}} to point to something like {{./environment/bin/}}.

-But when using k8s, we don't know the location of the environment on driver beforehand, because it's unarchived under {{SparkFiles}}.- Actually, this isn't right. For drivers in k8s cluster mode, the environment is downloaded into the working directory. So we can add the working directory to {{PATH}}. 

Still, that's inconvenient to do because you need to modify infrastructure, i.e. set {{PATH}} on YARN nodes or in the K8s image.;;;, 02/Jul/22 02:58;rshkv;Anyway, wanted to get your thoughts on this.

If you think that adding the parent folder of the Python executable is the right move (i.e. for {{./environment/bin/python}} we do {{$PATH:./environment/bin}}, I can put up a PR.;;;, 02/Jul/22 03:18;rshkv;Another way could be to add a config like {{spark.(driver|executor).extraPathDirs}} which the PythonRunner (and RRunner) apply to their {{ProcessBuilder}}.;;;, 22/Jul/22 14:08;rshkv;[~hyukjin.kwon], we talked about this at the conference. Basically Spark somewhat supports running Python from environments. But it doesn't run "conda activate" and the equivalent for other package managers. We could approximate "conda activate" by updating the PATH. 

Curious if you think this is a general problem. Or if you think Spark shouldn't be solving it. Cluster owners should solve it by setting PATH themselves.

The reason we don't do that is that we want the {{PATH}} elements to be absolute locations (to support e.g. {{Popen}} following a {{chdir}}) and for YARN we don't know the working directory location in advance.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 02/Jul/22 02:55;rshkv;Alternatively, one could update {{PATH}} to point to something like {{./environment/bin/}}.

-But when using k8s, we don't know the location of the environment on driver beforehand, because it's unarchived under {{SparkFiles}}.- Actually, this isn't right. For drivers in k8s cluster mode, the environment is downloaded into the working directory. So we can add the working directory to {{PATH}}. 

Still, that's inconvenient to do because you need to modify infrastructure, i.e. set {{PATH}} on YARN nodes or in the K8s image.;;;
Comment.2: 02/Jul/22 02:58;rshkv;Anyway, wanted to get your thoughts on this.

If you think that adding the parent folder of the Python executable is the right move (i.e. for {{./environment/bin/python}} we do {{$PATH:./environment/bin}}, I can put up a PR.;;;
Comment.3: 02/Jul/22 03:18;rshkv;Another way could be to add a config like {{spark.(driver|executor).extraPathDirs}} which the PythonRunner (and RRunner) apply to their {{ProcessBuilder}}.;;;
Comment.4: 22/Jul/22 14:08;rshkv;[~hyukjin.kwon], we talked about this at the conference. Basically Spark somewhat supports running Python from environments. But it doesn't run "conda activate" and the equivalent for other package managers. We could approximate "conda activate" by updating the PATH. 

Curious if you think this is a general problem. Or if you think Spark shouldn't be solving it. Cluster owners should solve it by setting PATH themselves.

The reason we don't do that is that we want the {{PATH}} elements to be absolute locations (to support e.g. {{Popen}} following a {{chdir}}) and for YARN we don't know the working directory location in advance.;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Passing an empty Metadata object to Column.as() should clear the metadata
Issue key: SPARK-39838
Issue id: 13472830
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kupferk
Creator: kupferk
Created: 22/Jul/22 05:54
Updated: 22/Jul/22 06:54
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: h2. Description

The Spark DataFrame API allows developers to attach arbiotrary metadata to individual columns as key/value pairs. The attachment is performed via the method "Column.as(name, metadata)". This works as expected, as long as the metadata object is not empty. But when passing an empty metadata object, the final column in the resulting DataFrame will still hold the metadata of the original incoming column, i.e. you cannot use this method to essentially reset the metadata of a column.

This is not the expected behaviour and has changed in Spark 3.3.0. In Spark 3.2.1 and earlier, passing an empty metadata object to the method "Column.as(name, metadata)" resets the columns metadata as expected.

h2. Steps to Reproduce

The following code snippet will show the issue in Spark shell:
{code:scala}
import org.apache.spark.sql.types.MetadataBuilder

// Create a DataFrame with one column with Metadata attached
val df1 = spark.range(1,10)
    .withColumn("col_with_metadata", col("id").as("col_with_metadata", new MetadataBuilder().putString("metadata", "value").build()))

// Create a derived DataFrame which should reset the metadata of the column
val df2 = df1.select(col("col_with_metadata").as("col_without_metadata", new MetadataBuilder().build()))

// Display metadata of both DataFrames columns
println(s"df1 metadata: ${df1.schema("col_with_metadata").metadata}")
println(s"df2 metadata: ${df2.schema("col_without_metadata").metadata}")
{code} 

This code results in the following lines printed onto the console
{code}
df1 metadata: {"metadata":"value"}
df2 metadata: {"metadata":"value"}
{code}

This result does not meet my expectations. I expect that df1 has non-empty metadata, but df2 should have empty metadata. But this is not the case, df2 still holds the same metadata as df1.

h2. Analysis

I think the problem stems from the changes in the method "trimNonTopLevelAliases" in the class AliasHelper:
{code:scala}
  protected def trimNonTopLevelAliases[T <: Expression](e: T): T = {
    val res = e match {
      case a: Alias =>
        val metadata = if (a.metadata == Metadata.empty) {
          None
        } else {
          Some(a.metadata)
        }
        a.copy(child = trimAliases(a.child))(
          exprId = a.exprId,
          qualifier = a.qualifier,
          explicitMetadata = metadata,
          nonInheritableMetadataKeys = a.nonInheritableMetadataKeys)
      case a: MultiAlias =>
        a.copy(child = trimAliases(a.child))
      case other => trimAliases(other)
    }

    res.asInstanceOf[T]
  }
{code}

The method will remove any empty metadata object from an Alias, which in turn means that Alias will inherit its childs metadata.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 22 06:54:52 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z172rk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Jul/22 06:53;kupferk;Please find a PR at https://github.com/apache/spark/pull/37251;;;, 22/Jul/22 06:54;apachespark;User 'kupferk' has created a pull request for this issue:
https://github.com/apache/spark/pull/37251;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 22/Jul/22 06:54;apachespark;User 'kupferk' has created a pull request for this issue:
https://github.com/apache/spark/pull/37251;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: EliminateInnerJoin to support convert inner join to left semi join
Issue key: SPARK-37899
Issue id: 13422560
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 13/Jan/22 15:52
Updated: 20/Jul/22 09:58
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 20 09:58:36 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ykfs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Jan/22 16:39;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/35194;;;, 20/Jul/22 09:57;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37236;;;, 20/Jul/22 09:58;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37236;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 20/Jul/22 09:57;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37236;;;
Comment.2: 20/Jul/22 09:58;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/37236;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: How to treat/eliminate CVE-2021-4048 (reported for arpack_combined_all-0.1.jar)
Issue key: SPARK-39793
Issue id: 13471801
Parent id: 
Issue Type: Question
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: alexnb
Creator: alexnb
Created: 15/Jul/22 14:49
Updated: 15/Jul/22 14:49
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: MLlib
Due Date: 
Votes: 1
Labels: 
Description: The following CVE is reported for arpack_combined_all-0.1.jar which is used in  org.apache.spark:spark-graphx_2.13 which in turn is used in mllib : [https://nvd.nist.gov/vuln/detail/CVE-2021-4048]

Questions: how relevant is this issue, can it be safely ignored?

It seems that arpack_combined_all-0.1.jar is really needed because when removing it from the CLASSPATH, a NoClassDefFoundError: org/netlib/blas/Sdot is reported.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-07-15 14:49:14.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16wfc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support adding maven packages while pip
Issue key: SPARK-39779
Issue id: 13471568
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: serenaruan
Creator: serenaruan
Created: 14/Jul/22 10:03
Updated: 14/Jul/22 15:16
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 1
Labels: maven, package, pyspark, python
Description: The goal is to support adding maven packages to a pip installable package, including adding jars and resolvers, so that when spark gets booted up it can automatically look for the maven packages in the classpath and install corresponding dependencies.

 

This idea comes up because currently for a python package, which depends on jars like pyspark internally use reflection on spark source code, if we want to make it work, there're two steps: 1. pip install the python package. 2. Add the jar into spark configuration while we start spark session, for example through spark.jars.packages.

If we can support the proposed functionality, we could ideally just add the package name and resolver while we pip install the package, and when spark session starts, it can look for those configurations inside python classpath and install them if they're not existed. This will simplify the process of all python developers who internally depends on maven packages and make pyspark more user-friendly.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python, scala
Custom field (Last public comment date): 2022-07-14 10:03:09.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16uzs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): mhamilton
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Binary array operations can be faster if one side is a constant
Issue key: SPARK-39746
Issue id: 13471050
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dvogelbacher
Creator: dvogelbacher
Created: 11/Jul/22 17:24
Updated: 11/Jul/22 17:24
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Array operations such as [ArraysOverlap|https://github.com/apache/spark/blob/79f133b7bbc1d9aa6a20dd8a34ec120902f96155/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/collectionOperations.scala#L1367] are optimized to put all the elements of the smaller array into a HashSet, if elements properly support equals. 
However, if one of the arrays is a constant, we could do much better as we don't have to reconstruct the HashSet for each row, we could construct it just once and send it to all the executors. This would improve runtime by a constant factor.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-07-11 17:24:12.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16rsw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: PrometheusServlet: add "TYPE" comment to exposed metrics
Issue key: SPARK-39619
Issue id: 13468619
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ebarault
Creator: ebarault
Created: 27/Jun/22 14:07
Updated: 11/Jul/22 09:20
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.0.0, 3.1.0, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Input/Output
Due Date: 
Votes: 0
Labels: 
Description: The PrometheusServlet sink does not include the usual comments when exposing the metrics in the prometheus format

e.g. `# TYPE nginx_ingress_controller_ingress_upstream_latency_seconds summary`

which prevents some client/integrations that depend on them to assess the metric type to work properly.

For example the AWS cloudwatch agent prometheus plugin attempts to get the metric type from the TYPE comment and considers any metric with no type as unsuported and hence drops it. 

As a result the cloudwatch agent prometheus drops all the metrics exposed by the PrometheusServlet.

[https://github.com/aws/amazon-cloudwatch-agent/blob/1f654cf69c1269073673ba2f636738c556248a31/plugins/inputs/prometheus_scraper/metrics_type_handler.go#L190]

 

This would be solved by adding the TYPE comments to the metrics exposed by the PrometheusServlet sink.

 

_*references:*_
 - [https://dzlab.github.io/bigdata/2020/07/03/spark3-monitoring-1/]

 - [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/metrics/sink/PrometheusServlet.scala]
 - [https://github.com/prometheus/docs/blob/main/content/docs/instrumenting/exposition_formats.md]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jul 11 09:20:05 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16e6g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Jul/22 09:20;ebarault;A PR was provided here: 
https://github.com/apache/spark/pull/37153;;;
Affects Version/s.1: 3.1.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: spark-core: sonatype-2021-1215 & sonatype-2021-1216 vulnerabilities from com.twitter:chill
Issue key: SPARK-39730
Issue id: 13470888
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ess-truveta
Creator: ess-truveta
Created: 10/Jul/22 23:06
Updated: 10/Jul/22 23:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.3, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Our static application security test showed that Spark Core has the following vulnerabilities due to a transitive dependency on com.esotericsoftware:kryo-shaded@4.0.2 via com.twitter:chill@0.10.0.

[34733 - kryo:DeserializeStringFuzzer: Uncaught exception in com.esotericsoftware.kryo.serializers.FieldSerializer.read - oss-fuzz (chromium.org)|https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=34733]

[34646 - kryo:DeserializeCollectionsFuzzer: Uncaught exception with empty stacktrace - oss-fuzz (chromium.org)|https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=34646]

Both vulnerabilities were addressed in kryo@5..3.0+. Once chill has been upgraded ([Upgrade com.esotericsoftware:kryo-shaded:jar:4.0.2 to avoid security risk · Issue #665 · twitter/chill (github.com)|https://github.com/twitter/chill/issues/665]), Spark Core should be updated as well. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-07-10 23:06:48.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16qt4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Reuse exchange across subqueries is broken with AQE if subquery side exchange materialized first
Issue key: SPARK-39690
Issue id: 13470238
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kapilks_ms
Creator: kapilks_ms
Created: 06/Jul/22 05:46
Updated: 06/Jul/22 06:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When trying to reuse Exchange of a subquery in main plan, if the Exchange inside subquery materialize first then main ASPE node won't have that stage info (in [stageToReplace|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/adaptive/AdaptiveSparkPlanExec.scala#L243]) to replace in current logical plan. This will cause AQE to produce new candidate physical plan without reusing the exchange present inside subquery. And depending on how complex the inner plan is (no. of exchanges) AQE could choose plan without ReusedExchange. 

We have seen with multiple queries with our private build. This can happen in DPP also.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 06 06:11:59 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16mt4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Jul/22 06:11;apachespark;User 'mskapilks' has created a pull request for this issue:
https://github.com/apache/spark/pull/37098;;;, 06/Jul/22 06:11;apachespark;User 'mskapilks' has created a pull request for this issue:
https://github.com/apache/spark/pull/37098;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 06/Jul/22 06:11;apachespark;User 'mskapilks' has created a pull request for this issue:
https://github.com/apache/spark/pull/37098;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Upgrade to spark 3.3.0 is causing error "Cannot grow BufferHolder by size -179446840 because the size is negative"
Issue key: SPARK-39608
Issue id: 13468454
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: isaace
Creator: isaace
Created: 26/Jun/22 15:48
Updated: 05/Jul/22 02:52
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 1
Labels: 
Description: Hi,

 

We recently upgraded to version 3.3.0.
The upgrade is causing the following error "Cannot grow BufferHolder by size -179446840 because the size is negative"

 

I can't find information on this on the internet, when reverting to spark 3.2.1 it works.

 

Full exception:

org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 36.0 failed 4 times, most recent failure: Lost task 1.3 in stage 36.0 (TID 2873) (172.24.214.133 executor 4): java.lang.IllegalArgumentException: Cannot grow BufferHolder by size -143657042 because the size is negative
        at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:67)
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:165)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.smj_consumeFullOuterJoinRow_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)
        at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
        at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)
        at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)
        at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)
        at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)
        at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)
        at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)
        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:327)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)
        at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:1020)
        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)
        at org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3161)
        at org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3160)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)
        at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)
        at org.apache.spark.sql.Dataset.count(Dataset.scala:3160)
Caused by: java.lang.IllegalArgumentException: Cannot grow BufferHolder by size -143657042 because the size is negative
        at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:67)
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:165)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.smj_consumeFullOuterJoinRow_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)
        at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
        at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)
        at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)
        at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)
        at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)
        at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)
        at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)
        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:327)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)
" errorMessage="Job aborted due to stage failure: Task 1 in stage 36.0 failed 4 times, most recent failure: Lost task 1.3 in stage 36.0 (TID 2873) (172.24.214.133 executor 4): java.lang.IllegalArgumentException: Cannot grow BufferHolder by size -143657042 because the size is negative
        at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:67)
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:165)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.smj_consumeFullOuterJoinRow_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)
        at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
        at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)
        at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)
        at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)
        at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)
        at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)
        at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)
        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:327)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)

Driver stacktrace:" exception.type="class org.apache.spark.SparkException" message="com.sap.mlr.spark.jobs.ModelBuilderMain.main:52 - org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 36.0 failed 4 times, most recent failure: Lost task 1.3 in stage 36.0 (TID 2873) (172.24.214.133 executor 4): java.lang.IllegalArgumentException: Cannot grow BufferHolder by size -143657042 because the size is negative
        at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:67)
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.grow(UnsafeWriter.java:63)
        at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:165)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.smj_consumeFullOuterJoinRow_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage24.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:779)
        at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
        at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)
        at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)
        at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)
        at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)
        at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)
        at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)
        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:327)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base/java.lang.Thread.run(Unknown Source)

 

BR,

Isaac.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 01 09:22:05 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16d5s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Jul/22 09:22;dcoliversun;Could you share more information? Such as spark application code or generated code;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: V2Catalog rename not support newIdent with catalog
Issue key: SPARK-39527
Issue id: 13450902
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 20/Jun/22 06:00
Updated: 22/Jun/22 13:47
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
{code:java}

  test("rename a table") {
    sql("ALTER TABLE h2.test.empty_table RENAME TO h2.test.empty_table2")
    checkAnswer(
      sql("SHOW TABLES IN h2.test"),
      Seq(Row("test", "empty_table2")))
  }
{code}


{code:java}
[info] - rename a table *** FAILED *** (2 seconds, 358 milliseconds)
[info]   org.apache.spark.sql.catalyst.analysis.NoSuchNamespaceException: Failed table renaming from test.empty_table to h2.test.empty_table2
[info]   at org.apache.spark.sql.jdbc.H2Dialect$.classifyException(H2Dialect.scala:117)
[info]   at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.classifyException(JdbcUtils.scala:1176)
[info]   at org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog.$anonfun$renameTable$1(JDBCTableCatalog.scala:102)
[info]   at org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog.$anonfun$renameTable$1$adapted(JDBCTableCatalog.scala:100)
[info]   at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.withConnection(JdbcUtils.scala:1184)
[info]   at org.apache.spark.sql.execution.datasources.v2.jdbc.JDBCTableCatalog.renameTable(JDBCTableCatalog.scala:100)
[info]   at org.apache.spark.sql.execution.datasources.v2.RenameTableExec.run(RenameTableExec.scala:51)
[info]   at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
[info]   at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
[info]   at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
[info]   at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[info]   at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:111)
[info]   at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:171)
[info]   at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[info]   at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[info]   at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[info]   at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
[info]   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
[info]   at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[info]   at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[info]   at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[info]   at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
[info]   at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
{code}


Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jun 22 12:22:49 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z13dm8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Jun/22 12:22;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36919;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: [Yarn] Diagnostics of yarn UI did not display the exception of driver when driver exit before regiserAM
Issue key: SPARK-39541
Issue id: 13454534
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: liangyongyuan
Creator: liangyongyuan
Created: 21/Jun/22 10:22
Updated: 22/Jun/22 02:30
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: YARN
Due Date: 
Votes: 0
Labels: 
Description: If commit a job in yarn cluster mode and driver exited before registerAM，Diagnostics of yarn UI did not show the exception that was throwed by driver .Yarn UI only show :

Application application_xxx failed 1 times (global limit =10; local limit is =1) due to AM Container for appattempt_xxx_000001 exited with exitCode: 13

 

User must view spark log to find the real reason.for example,spark log shows 
{code:java}
2022-06-21,17:58:28,273 ERROR org.apache.spark.deploy.yarn.ApplicationMaster: User class threw exception: java.lang.ArithmeticException: / by zero
java.lang.ArithmeticException: / by zero
	at org.examples.appErrorDemo3$.main(appErrorDemo3.scala:10)
	at org.examples.appErrorDemo3.main(appErrorDemo3.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:736) {code}
 

The reason of this issue is that if driver would not call unregisterAM exited before registerAM ，then yarn UI could not show the real diagnostic information.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jun 22 02:30:55 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z13z94:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Jun/22 10:27;liangyongyuan;I want to try to solve this problem. I already have a solution and have tested it;;;, 22/Jun/22 02:30;apachespark;User 'lyy-pineapple' has created a pull request for this issue:
https://github.com/apache/spark/pull/36952;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 22/Jun/22 02:30;apachespark;User 'lyy-pineapple' has created a pull request for this issue:
https://github.com/apache/spark/pull/36952;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: setPredictionCol for OneVsRest does not persist when saving model to disk
Issue key: SPARK-39544
Issue id: 13454564
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kobakhit
Creator: kobakhit
Created: 21/Jun/22 12:32
Updated: 21/Jun/22 12:38
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: ML
Due Date: 
Votes: 0
Labels: 
Description: The naming of rawPredcitionCol in OneVsRest does not persist after saving and loading a trained model. This becomes an issue when I try to stack multiple One Vs Rest models in a pipeline. Code example below. 
{code:java}
from pyspark.ml.classification import LinearSVC, OneVsRest, OneVsRestModel

data_path = "/sample_multiclass_classification_data.txt"
df = spark.read.format("libsvm").load(data_path)
lr = LinearSVC(regParam=0.01)

# set the name of rawPrediction column
ovr = OneVsRest(classifier=lr, rawPredictionCol = 'raw_prediction')
print(ovr.getRawPredictionCol())

model = ovr.fit(df)model_path = 'temp' + "/ovr_model"

# save and read back in
model.write().overwrite().save(model_path)
model2 = OneVsRestModel.load(model_path)
model2.getRawPredictionCol()

Output:
raw_prediction
'rawPrediction' {code}
 

 
Environment: Python 3.6

Spark 3.2
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-06-21 12:32:11.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z13zfs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.0.1
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Parquet bit-packing de/encode optimization
Issue key: SPARK-39480
Issue id: 13450249
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Fang-Xie
Creator: Fang-Xie
Created: 15/Jun/22 14:45
Updated: 20/Jun/22 06:12
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Current Spark use Parquet-mr as parquet reader/writer library, but the built-in bit-packing en/decode is not efficient enough. 

Our optimization for Parquet bit-packing en/decode with jdk.incubator.vector in Open JDK18 brings prominent performance improvement.

Due to Vector API is added to OpenJDK since 16, So this optimization request JDK16 or higher.

*Below are our test results*

Functional test is based on open-source parquet-mr Bit-pack decoding function: *_public final void unpack8Values(final byte[] in, final int inPos, final int[] out, final int outPos)_* __

compared with our implementation with vector API *_public final void unpack8Values_vec(final byte[] in, final int inPos, final int[] out, final int outPos)_*

We tested 10 pairs (open source parquet bit unpacking vs ours optimized vectorized SIMD implementation) decode function with bit width=\{1,2,3,4,5,6,7,8,9,10}, below are test results:

!image-2022-06-15-22-50-12-759.png|width=513,height=259!

 

We integrated our bit-packing decode implementation into parquet-mr, tested the parquet batch reader ability from Spark VectorizedParquetRecordReader which get parquet column data by the batch way. We construct parquet file with different row count and column count, the column data type is Int32, the maximum int value is 127 which satisfies bit pack encode with bit width=7,   the count of the row is from 10k to 100 million and the count of the column is from 1 to 4.

!image-2022-06-15-22-52-56-792.png|width=328,height=167!

!image-2022-06-15-22-53-35-937.png|width=354,height=175!

!image-2022-06-15-22-54-07-288.png|width=352,height=173!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 15/Jun/22 14:50;Fang-Xie;image-2022-06-15-22-50-12-759.png;https://issues.apache.org/jira/secure/attachment/13045131/image-2022-06-15-22-50-12-759.png, 15/Jun/22 14:52;Fang-Xie;image-2022-06-15-22-52-56-792.png;https://issues.apache.org/jira/secure/attachment/13045132/image-2022-06-15-22-52-56-792.png, 15/Jun/22 14:53;Fang-Xie;image-2022-06-15-22-53-35-937.png;https://issues.apache.org/jira/secure/attachment/13045133/image-2022-06-15-22-53-35-937.png, 15/Jun/22 14:54;Fang-Xie;image-2022-06-15-22-54-07-288.png;https://issues.apache.org/jira/secure/attachment/13045134/image-2022-06-15-22-54-07-288.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 4.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important, Patch
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jun 15 15:38:01 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z139ls:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.3.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Jun/22 15:38;apachespark;User 'Fang-Xie' has created a pull request for this issue:
https://github.com/apache/spark/pull/36878;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: HiveMetastore serializable exception with Spark 3.3.0
Issue key: SPARK-39513
Issue id: 13450780
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tcarland
Creator: tcarland
Created: 19/Jun/22 01:41
Updated: 20/Jun/22 00:12
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes, SQL
Due Date: 
Votes: 0
Labels: 
Description: Running this command-line with provided configuration works fine in Spark 3.2.1: 

 
{code:java}
${SPARK_HOME}/bin/spark-shell \
>       --master k8s://https://10.96.0.1:443 \
>       --deploy-mode client \
>       --name spark-shell --conf spark.scheduler.minRegisteredResourcesRatio=1 --conf spark.executor.instances=8 --conf spark.executor.cores=1 --conf spark.executor.limit.cores=2 --conf spark.executor.memory=4g --conf spark.driver.cores=1 --conf spark.driver.limit.cores=1 --conf spark.driver.memory=1g --conf spark.kubernetes.container.image=<myimagerepo>/spark:v3.2.1-thebe-2206.10 --conf spark.kubernetes.driver.pod.name=spark-shell-driver-jglxk --conf spark.kubernetes.namespace=spark --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark --conf spark.hadoop.fs.s3a.endpoint=http://10.96.45.95:80 --conf spark.hadoop.fs.s3a.access.key=<myaccesskey> --conf spark.hadoop.fs.s3a.secret.key=<mysecretkey> --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.block.size=512M --conf spark.hadoop.fs.s3a.committer.magic.enabled=false --conf spark.hadoop.fs.s3a.committer.name=directory --conf spark.hadoop.fs.s3a.committer.staging.abort.pending.uploads=true --conf spark.hadoop.fs.s3a.committer.staging.conflict-mode=append --conf spark.hadoop.fs.s3a.committer.staging.tmp.path=/tmp/staging --conf spark.hadoop.fs.s3a.committer.staging.unique-filenames=true --conf spark.hadoop.fs.s3a.committer.threads=2048 --conf spark.hadoop.fs.s3a.connection.establish.timeout=5000 --conf spark.hadoop.fs.s3a.connection.maximum=8192 --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --conf spark.hadoop.fs.s3a.connection.timeout=200000 --conf spark.hadoop.fs.s3a.fast.upload.active.blocks=2048 --conf spark.hadoop.fs.s3a.fast.upload.buffer=disk --conf spark.hadoop.fs.s3a.fast.upload=true --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.max.total.tasks=2048 --conf spark.hadoop.fs.s3a.multipart.size=512M --conf spark.hadoop.fs.s3a.multipart.threshold=512M --conf spark.hadoop.fs.s3a.socket.recv.buffer=65536 --conf spark.hadoop.fs.s3a.socket.send.buffer=65536 --conf spark.hadoop.fs.s3a.threads.max=2048 --conf spark.eventLog.dir=s3a://spark/spark-logs --conf spark.eventLog.enabled=true --conf spark.hadoop.metastore.catalog.default=hive --conf spark.sql.warehouse.dir=s3a//hive/warehouse --conf spark.sql.hive.metastore.dir=s3a://hive/warehouse --conf spark.sql.hive.metastore.version=3.1.2 --conf spark.sql.hive.metastore.jars=path --conf spark.sql.hive.metastore.jars.path=file:///opt/hive/lib/*.jar --conf spark.hadoop.hive.metastore.uris=thrift://hive-metastore.hive.svc.cluster.local:9083
 
{code}
 

With Spark 3.3.0 image, this same command yields the following exception when running spark.catalog functions, eg. spark.catalog.listDatabases().show()

 

 
{noformat}
2022-06-19T00:57:13,687 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Opened a connection to metastore, current connections: 1          2022-06-19T00:57:13,701 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Connected to metastore.                                           
2022-06-19T00:57:13,701 INFO [main] org.apache.hadoop.hive.metastore.RetryingMetaStoreClient - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.
metadata.SessionHiveMetaStoreClient ugi=root (auth:SIMPLE) retries=1 delay=1 lifetime=0                                                                      
java.lang.NoClassDefFoundError: scala/Serializable 
  at java.base/java.lang.ClassLoader.defineClass1(Native Method)         
  at java.base/java.lang.ClassLoader.defineClass(Unknown Source)         
  at java.base/java.security.SecureClassLoader.defineClass(Unknown Source)                                                                  
  at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(Unknown Source)                                                                  
  at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(Unknown Source)           
  at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(Unknown Source)                      
  at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(Unknown Source)                             
  at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(Unknown Source)                                                               
  at java.base/java.lang.ClassLoader.loadClass(Unknown Source)                                                                        
  at org.apache.spark.sql.catalyst.analysis.RewriteDeleteFromTable$.apply(RewriteDeleteFromTable.scala:39) 
  at org.apache.spark.sql.catalyst.analysis.RewriteDeleteFromTable$.apply(RewriteDeleteFromTable.scala:37) 
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)            
  at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:169)
  at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:165)  
  at scala.collection.immutable.List.foldLeft(List.scala:79)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)                                                               at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)    
  at scala.collection.immutable.List.foreach(List.scala:333) 
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200) 
  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)           
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)      
  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172) 
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223) 
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)                           
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)   
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)  
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)  
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) 
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185) 
  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)  
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185) 
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)   
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)  
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76) 
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)        
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66History {noformat}
 

 

{{Not sure what I am missing between the two versions. Using spark 3.3.0 with hadoop 3.3.2 and hive 3.1.3}}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 20 00:08:02 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z13cv4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Jun/22 10:15;yumwang;[~tcarland] What is your java version?;;;, 19/Jun/22 15:30;tcarland;Java 11 for both cases. 

Hive 3.1.3 image build from  openjdk:11-slim

Spark images built from openjdk:11-jre-slim;;;, 20/Jun/22 00:08;tcarland;[~yumwang]  I can reproduce this with YARN as scheduler as well, just using spark-defaults.conf and simply running 'spark-shell'. This works correctly in spark 3.2.1.   Here is the cluster's spark-defaults.conf: 

 
{code:java}
spark.authenticate=false
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.executorIdleTimeout=60
spark.dynamicAllocation.minExecutors=0
spark.dynamicAllocation.schedulerBacklogTimeout=1
spark.shuffle.service.enabled=true
spark.shuffle.service.port=7337
spark.master=yarn
spark.submit.deployMode=client
spark.ui.killEnabled=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.driver.memory=1g
spark.executor.memory=1g
spark.sql.hive.metastore.jars=path
spark.sql.hive.metastore.jars.path=file:///opt/TDH/hive/lib/*.jar
spark.sql.hive.metastore.version=3.1.2
spark.hadoop.hive.metastore.uris=thrift://callisto:9083
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs://callisto:8020/tmp/spark/applicationHistory
spark.history.fs.logDirectory=hdfs://callisto:8020/tmp/spark/applicationHistory
spark.yarn.historyServer.address=http://callisto:18080
spark.yarn.jars=local:/opt/TDH/spark/jars/*
spark.driver.extraLibraryPath=$HADOOP_HOME/lib/native
spark.executor.extraLibraryPath=$HADOOP_HOME/lib/native
spark.yarn.am.extraLibraryPath=$HADOOP_HOME/lib/native
spark.hadoop.mapreduce.application.classpath=
spark.hadoop.yarn.application.classpath=
{code}
 
{noformat}
 
Spark context Web UI available at http://callisto.<mydomain>.net:4040
Spark context available as 'sc' (master = yarn, app id = application_1655681960811_0002).
Spark session available as 'spark'.
scala> spark.catalog.listDatabases().show()
2022-06-19T17:06:04,935 INFO [main] org.apache.hadoop.hive.conf.HiveConf - Found configuration file file:/opt/TDH/hive/conf/hive-site.xml
Hive Session ID = 0b3f641c-d862-4e78-93da-88e6d872d328
2022-06-19T17:06:05,104 INFO [main] SessionState - Hive Session ID = 0b3f641c-d862-4e78-93da-88e6d872d328
2022-06-19T17:06:05,265 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Trying to connect to metastore with URI thrift://callisto.charltontechnology.net:9083
2022-06-19T17:06:05,290 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Opened a connection to metastore, current connections: 1
2022-06-19T17:06:05,315 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Connected to metastore.
2022-06-19T17:06:05,315 INFO [main] org.apache.hadoop.hive.metastore.RetryingMetaStoreClient - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=tca (auth:SIMPLE) retries=1 delay=1 lifetime=0
java.lang.NoClassDefFoundError: scala/Serializable
  at java.base/java.lang.ClassLoader.defineClass1(Native Method)
  at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)
{noformat}
 ;;;
Affects Version/s.1: 
Component/s.1: SQL
Comment.1: 19/Jun/22 15:30;tcarland;Java 11 for both cases. 

Hive 3.1.3 image build from  openjdk:11-slim

Spark images built from openjdk:11-jre-slim;;;
Comment.2: 20/Jun/22 00:08;tcarland;[~yumwang]  I can reproduce this with YARN as scheduler as well, just using spark-defaults.conf and simply running 'spark-shell'. This works correctly in spark 3.2.1.   Here is the cluster's spark-defaults.conf: 

 
{code:java}
spark.authenticate=false
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.executorIdleTimeout=60
spark.dynamicAllocation.minExecutors=0
spark.dynamicAllocation.schedulerBacklogTimeout=1
spark.shuffle.service.enabled=true
spark.shuffle.service.port=7337
spark.master=yarn
spark.submit.deployMode=client
spark.ui.killEnabled=true
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.driver.memory=1g
spark.executor.memory=1g
spark.sql.hive.metastore.jars=path
spark.sql.hive.metastore.jars.path=file:///opt/TDH/hive/lib/*.jar
spark.sql.hive.metastore.version=3.1.2
spark.hadoop.hive.metastore.uris=thrift://callisto:9083
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs://callisto:8020/tmp/spark/applicationHistory
spark.history.fs.logDirectory=hdfs://callisto:8020/tmp/spark/applicationHistory
spark.yarn.historyServer.address=http://callisto:18080
spark.yarn.jars=local:/opt/TDH/spark/jars/*
spark.driver.extraLibraryPath=$HADOOP_HOME/lib/native
spark.executor.extraLibraryPath=$HADOOP_HOME/lib/native
spark.yarn.am.extraLibraryPath=$HADOOP_HOME/lib/native
spark.hadoop.mapreduce.application.classpath=
spark.hadoop.yarn.application.classpath=
{code}
 
{noformat}
 
Spark context Web UI available at http://callisto.<mydomain>.net:4040
Spark context available as 'sc' (master = yarn, app id = application_1655681960811_0002).
Spark session available as 'spark'.
scala> spark.catalog.listDatabases().show()
2022-06-19T17:06:04,935 INFO [main] org.apache.hadoop.hive.conf.HiveConf - Found configuration file file:/opt/TDH/hive/conf/hive-site.xml
Hive Session ID = 0b3f641c-d862-4e78-93da-88e6d872d328
2022-06-19T17:06:05,104 INFO [main] SessionState - Hive Session ID = 0b3f641c-d862-4e78-93da-88e6d872d328
2022-06-19T17:06:05,265 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Trying to connect to metastore with URI thrift://callisto.charltontechnology.net:9083
2022-06-19T17:06:05,290 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Opened a connection to metastore, current connections: 1
2022-06-19T17:06:05,315 INFO [main] org.apache.hadoop.hive.metastore.HiveMetaStoreClient - Connected to metastore.
2022-06-19T17:06:05,315 INFO [main] org.apache.hadoop.hive.metastore.RetryingMetaStoreClient - RetryingMetaStoreClient proxy=class org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient ugi=tca (auth:SIMPLE) retries=1 delay=1 lifetime=0
java.lang.NoClassDefFoundError: scala/Serializable
  at java.base/java.lang.ClassLoader.defineClass1(Native Method)
  at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)
{noformat}
 ;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: BrotliCodec doesn't support Apple Silicon on MacOS
Issue key: SPARK-36731
Issue id: 13400687
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dongjoon
Creator: dongjoon
Created: 13/Sep/21 02:17
Updated: 16/Jun/22 18:55
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build, SQL
Due Date: 
Votes: 0
Labels: 
Description: {code}
[info] Caused by: java.lang.UnsatisfiedLinkError: Couldn't load native library 'brotli'. [LoaderResult: os.name="Mac OS X", os.arch="aarch64", os.version="11.5.2", java.vm.name="OpenJDK 64-Bit Server VM", java.vm.version="25.302-b08", java.vm.vendor="Azul Systems, Inc.", alreadyLoaded="null", loadedFromSystemLibraryPath="false", nativeLibName="libbrotli.dylib", temporaryLibFile="/Users/dongjoon/APACHE/spark-merge/target/tmp/brotli8243220902047076449/libbrotli.dylib", libNameWithinClasspath="/lib/darwin-aarch64/libbrotli.dylib", usedThisClassloader="false", usedSystemClassloader="false", java.library.path="/Users/dongjoon/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:."]
[info] 	at org.meteogroup.jbrotli.libloader.BrotliLibraryLoader.loadBrotli(BrotliLibraryLoader.java:35)
[info] 	at org.apache.hadoop.io.compress.BrotliCodec.<init>(BrotliCodec.java:40)
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-35781
Outward issue link (Reference): SPARK-36670
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Sep 13 02:18:54 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uu8g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Sep/21 02:18;dongjoon;Thanks to SPARK-36670, we identified this issue. Thank you, [~viirya];;;
Affects Version/s.1: 
Component/s.1: SQL
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Non-deterministic UDF executed multiple times when combined with withField
Issue key: SPARK-38485
Issue id: 13432930
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tanelk
Creator: tanelk
Created: 09/Mar/22 18:11
Updated: 07/Jun/22 01:33
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: Correctness
Description: When adding fields to a result of a non-deterministic UDF, that returns a struct, then that UDF is executed multiple times (once per field) for each row.

In this UT df1 passes, but df2 fails with something like:
"279751724 did not equal -1023188908"

{code}
  test("SPARK-XXXXX: non-deterministic UDF should be called once when adding fields") {
    val nondeterministicUDF = udf((s: Int) => {
      val r = Random.nextInt()
      // Both values should be the same
      GroupByKey(r, r)
    }).asNondeterministic()

    val df1 = spark.range(5).select(nondeterministicUDF($"id"))
    df1.collect().foreach {
      row => assert(row.getStruct(0).getInt(0) == row.getStruct(0).getInt(1))
    }

    val df2 = spark.range(5).select(nondeterministicUDF($"id").withField("new", lit(7)))
    df2.collect().foreach {
      row => assert(row.getStruct(0).getInt(0) == row.getStruct(0).getInt(1))
    }
  }
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jun 07 01:33:24 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10c3k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Apr/22 20:26;srowen;I don't think there are guarantees about the number of times it is executed.;;;, 17/Apr/22 14:31;tanelk;Is there then even any point in having non-deterministic methods in spark? Some optimizations are disabled for them do avoid similar situations.;;;, 07/Jun/22 01:33;joshrosen;It looks like the child expression of the `UpdateFields` plan node will be evaluated multiple times. If the child is non-deterministic then this can lead to the correctness issues seen here.

Other parts of Spark contain logic to avoid duplication of non-deterministic expressions. For example, Spark won't collapse adjacent projections if doing so would lead to the duplication of a non-deterministic expression. We can exploit that existing project collapsing rule to work around this bug: 
{code:java}
val df3 = spark
  .range(5)
  .select(nondeterministicUDF($"id").as("udfOutput"))
  .select($"udfOutput".withField("new", lit(7))) {code}
This generates the following plan:
{code:java}
== Parsed Logical Plan ==
'Project [unresolvedalias(update_fields('udfOutput, WithField(new, 7)), Some(org.apache.spark.sql.Column$$Lambda$9491/376740337@3cd91041))]
+- Project [UDF(cast(id#9499682L as int)) AS udfOutput#9499684]
   +- Range (0, 5, step=1, splits=Some(4))

== Analyzed Logical Plan ==
update_fields(udfOutput, WithField(7)): struct<a:int,b:int,new:int>
Project [update_fields(udfOutput#9499684, WithField(new, 7)) AS update_fields(udfOutput, WithField(7))#9499689]
+- Project [UDF(cast(id#9499682L as int)) AS udfOutput#9499684]
   +- Range (0, 5, step=1, splits=Some(4))

== Optimized Logical Plan ==
Project [if (isnull(udfOutput#9499684)) null else named_struct(a, udfOutput#9499684.a, b, udfOutput#9499684.b, new, 7) AS update_fields(udfOutput, WithField(7))#9499689]
+- Project [UDF(cast(id#9499682L as int)) AS udfOutput#9499684]
   +- Range (0, 5, step=1, splits=Some(4))

== Physical Plan ==
*(1) Project [if (isnull(udfOutput#9499684)) null else named_struct(a, udfOutput#9499684.a, b, udfOutput#9499684.b, new, 7) AS update_fields(udfOutput, WithField(7))#9499689]
+- *(1) Project [UDF(cast(id#9499682L as int)) AS udfOutput#9499684]
   +- *(1) ColumnarToRow
      +- PhotonResultStage
         +- PhotonRange Range (0, 5, step=1, splits=4) {code}
Here the UDF is evaluated once in the first project and the result of the evaluation is re-used in the withField.

Given this, one potential way to fix this problem could be to add an optimizer rule which performs what is effectively the opposite of project collapsing: if we see a non-deterministic child of UpdateFields then introduce a new projection to prevent duplication of the expression (essentially automating the workaround I'm suggesting here).;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 17/Apr/22 14:31;tanelk;Is there then even any point in having non-deterministic methods in spark? Some optimizations are disabled for them do avoid similar situations.;;;
Comment.2: 07/Jun/22 01:33;joshrosen;It looks like the child expression of the `UpdateFields` plan node will be evaluated multiple times. If the child is non-deterministic then this can lead to the correctness issues seen here.

Other parts of Spark contain logic to avoid duplication of non-deterministic expressions. For example, Spark won't collapse adjacent projections if doing so would lead to the duplication of a non-deterministic expression. We can exploit that existing project collapsing rule to work around this bug: 
{code:java}
val df3 = spark
  .range(5)
  .select(nondeterministicUDF($"id").as("udfOutput"))
  .select($"udfOutput".withField("new", lit(7))) {code}
This generates the following plan:
{code:java}
== Parsed Logical Plan ==
'Project [unresolvedalias(update_fields('udfOutput, WithField(new, 7)), Some(org.apache.spark.sql.Column$$Lambda$9491/376740337@3cd91041))]
+- Project [UDF(cast(id#9499682L as int)) AS udfOutput#9499684]
   +- Range (0, 5, step=1, splits=Some(4))

== Analyzed Logical Plan ==
update_fields(udfOutput, WithField(7)): struct<a:int,b:int,new:int>
Project [update_fields(udfOutput#9499684, WithField(new, 7)) AS update_fields(udfOutput, WithField(7))#9499689]
+- Project [UDF(cast(id#9499682L as int)) AS udfOutput#9499684]
   +- Range (0, 5, step=1, splits=Some(4))

== Optimized Logical Plan ==
Project [if (isnull(udfOutput#9499684)) null else named_struct(a, udfOutput#9499684.a, b, udfOutput#9499684.b, new, 7) AS update_fields(udfOutput, WithField(7))#9499689]
+- Project [UDF(cast(id#9499682L as int)) AS udfOutput#9499684]
   +- Range (0, 5, step=1, splits=Some(4))

== Physical Plan ==
*(1) Project [if (isnull(udfOutput#9499684)) null else named_struct(a, udfOutput#9499684.a, b, udfOutput#9499684.b, new, 7) AS update_fields(udfOutput, WithField(7))#9499689]
+- *(1) Project [UDF(cast(id#9499682L as int)) AS udfOutput#9499684]
   +- *(1) ColumnarToRow
      +- PhotonResultStage
         +- PhotonRange Range (0, 5, step=1, splits=4) {code}
Here the UDF is evaluated once in the first project and the result of the evaluation is re-used in the withField.

Given this, one potential way to fix this problem could be to add an optimizer rule which performs what is effectively the opposite of project collapsing: if we see a non-deterministic child of UpdateFields then introduce a new projection to prevent duplication of the expression (essentially automating the workaround I'm suggesting here).;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Redact table/partition properties
Issue key: SPARK-39336
Issue id: 13447444
Parent id: 
Issue Type: Task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 30/May/22 04:48
Updated: 03/Jun/22 02:30
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-05-30 04:48:09.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12scg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in org.apache.spark.security
Issue key: SPARK-38474
Issue id: 13432767
Parent id: 13423097.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bozhang
Creator: bozhang
Created: 09/Mar/22 05:20
Updated: 27/May/22 05:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri May 27 05:00:29 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10b3c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/May/22 12:55;sandeep.katta2007;I will start working on this;;;, 27/May/22 04:59;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/36695;;;, 27/May/22 05:00;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/36695;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 27/May/22 04:59;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/36695;;;
Comment.2: 27/May/22 05:00;apachespark;User 'sandeep-katta' has created a pull request for this issue:
https://github.com/apache/spark/pull/36695;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Keep the Spark SQL API reference URL as same as before.
Issue key: SPARK-39288
Issue id: 13446818
Parent id: 
Issue Type: Test
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: itholic
Creator: itholic
Created: 25/May/22 12:43
Updated: 26/May/22 00:33
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Documentation, PySpark
Due Date: 
Votes: 0
Labels: 
Description: After https://github.com/apache/spark/pull/36647 is merged, the PySpark SQL API reference URL is changed, since the file structure is changed from `pyspark.sql.rst` to `pyspark.sql/*.rst`.

We should keep it same as before to keep the consistency with older documents versions.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed May 25 12:44:07 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12ojk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/May/22 12:44;itholic;I'm working on this;;;
Affects Version/s.1: 
Component/s.1: PySpark
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Error occurs when cast  a big enough long/float to timestamp
Issue key: SPARK-39209
Issue id: 13445441
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: chongg@nvidia
Creator: chongg@nvidia
Created: 17/May/22 14:04
Updated: 24/May/22 10:24
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description:  
Got an error when casting a big enough long to a timestamp, should get the max timestamp according to the code in `Cast.scala`:

 
{code:java}
private[this] def longToTimestamp(t: Long): Long = SECONDS.toMicros(t)

// the logic of SECONDS.toMicros is:
static long x(long d, long m, long over) {     
    if (d > Long.MAX_VALUE / 1000000L) return Long.MAX_VALUE;     
    if (d < -(Long.MAX_VALUE / 1000000L)) return Long.MIN_VALUE;     
    return d * m; 
}{code}
 
 

Reproduce steps:
{code:java}
$SPARK_HOME/bin/spark-shell 
import spark.implicits._ 
val df = Seq((Long.MaxValue / 1000000) + 1).toDF("a") 
df.selectExpr("cast(a as timestamp)").collect()

// the result is right  

Array[org.apache.spark.sql.Row] = Array([294247-01-10 12:00:54.775807])
 

import org.apache.spark.sql.types._ 
import org.apache.spark.sql.Row 
val schema = StructType(Array(StructField("a", LongType))) 
val data = Seq(Row((Long.MaxValue / 1000000) + 1)) 
val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema) 
df.selectExpr("cast(a as timestamp)").collect()

// ANSI or non-ANSI, both throws error
// spark.conf.set("spark.sql.ansi.enabled", true)   
// spark.conf.set("spark.sql.ansi.enabled", false)
// error occurs: 

java.lang.RuntimeException: Error while decoding: java.lang.ArithmeticException: long overflow
createexternalrow(staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, ObjectType(class java.sql.Timestamp), toJavaTimestamp, input[0, timestamp, true], true, false), StructField(a,TimestampType,true))
  at org.apache.spark.sql.errors.QueryExecutionErrors$.expressionDecodingError(QueryExecutionErrors.scala:1047)
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:184)
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:172)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
  at scala.collection.TraversableLike.map(TraversableLike.scala:286)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)
  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2971)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2971)
  ... 51 elided
Caused by: java.lang.ArithmeticException: long overflow
  at java.lang.Math.multiplyExact(Math.java:892)
  at org.apache.spark.sql.catalyst.util.DateTimeUtils$.millisToMicros(DateTimeUtils.scala:213)
  at org.apache.spark.sql.catalyst.util.RebaseDateTime$.rebaseGregorianToJulianMicros(RebaseDateTime.scala:362)
  at org.apache.spark.sql.catalyst.util.RebaseDateTime$.rebaseGregorianToJulianMicros(RebaseDateTime.scala:386)
  at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaTimestamp(DateTimeUtils.scala:146)
  at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaTimestamp(DateTimeUtils.scala)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:181)
  ... 69 more
{code}
 

Another similar issue when casting a float to a timestamp

The code should not overflow in non-ANSI mode: 
{code:java}
if (d.isNaN || d.isInfinite) null else (d * MICROS_PER_SECOND).toLong{code}
 
Reproduce steps:
{code:java}
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
val data = Seq(
  Row((Long.MaxValue / 1000000 + 100).toDouble),
  Row((-(Long.MaxValue / 1000000) - 100).toDouble))
val schema = StructType(Array(StructField("a", DoubleType)))
val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
df.selectExpr("cast(a as timestamp)").collect()

// Error
java.lang.RuntimeException: Error while decoding: java.lang.ArithmeticException: long overflow  
createexternalrow(staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, ObjectType(class java.sql.Timestamp), toJavaTimestamp, input[0, timestamp, true], true, false), StructField(a,TimestampType,true))  
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:186)  
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:173)  
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)  
  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)  
  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)  
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)  
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)  
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)  
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)  
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)  
  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2965)  
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)  
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)  
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)  
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)  
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)  
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)  
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)  
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2965)  
  ... 51 elided  
Caused by: java.lang.ArithmeticException: long overflow  
  at java.lang.Math.multiplyExact(Math.java:892)  
  at org.apache.spark.sql.catalyst.util.DateTimeUtils$.millisToMicros(DateTimeUtils.scala:202)  
  at org.apache.spark.sql.catalyst.util.RebaseDateTime$.rebaseGregorianToJulianMicros(RebaseDateTime.scala:361)  
  at org.apache.spark.sql.catalyst.util.RebaseDateTime$.rebaseGregorianToJulianMicros(RebaseDateTime.scala:385)  
  at org.apache.spark.sql.catalyst.util.DateTimeUtils$.toJavaTimestamp(DateTimeUtils.scala:135)  
  at org.apache.spark.sql.catalyst.util.DateTimeUtils.toJavaTimestamp(DateTimeUtils.scala)  
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificSafeProjection.apply(Unknown Source)  
  at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:182)  
  ... 69 more  

// But if show, the result is OK.
df.selectExpr("cast(a as timestamp)").show(false)
+-----------------------------+
|a                            |
+-----------------------------+
|+294247-01-10 12:00:54.775807|
|-290308-12-22 04:04:48.224192|
+-----------------------------+
{code}
Environment: Spark 3.3.0
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-05-17 14:04:54.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12g1s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Provide query context in runtime error of cast overflow
Issue key: SPARK-39188
Issue id: 13445000
Parent id: 13434942.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: maxgekk
Reporter: maxgekk
Creator: maxgekk
Created: 15/May/22 07:30
Updated: 23/May/22 00:08
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: See castingCauseOverflowError and the error class CAST_CAUSES_OVERFLOW
https://github.com/apache/spark/blob/1b37f19876298e995596a30edc322c856ea1bbb4/sql/catalyst/src/main/scala/org/apache/spark/sql/errors/QueryExecutionErrors.scala#L97-L101
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): SPARK-38908
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-05-15 07:30:01.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12dd4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Hive client should not gather statistic by default.
Issue key: SPARK-39043
Issue id: 13442039
Parent id: 
Issue Type: Task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 27/Apr/22 10:22
Updated: 19/May/22 15:12
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When use `InsertIntoHiveTable`, when insert overwrite partition, it will call
Hive.loadPartition(), in this method, when `hive.stats.autogather` is true(default is true)

 

{code:java}
// Some comments here
public String getFoo()
      if (oldPart == null) {
        newTPart.getTPartition().setParameters(new HashMap<String,String>());
        if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {
          StatsSetupConst.setBasicStatsStateForCreateTable(newTPart.getParameters(),
              StatsSetupConst.TRUE);
        }

public static void setBasicStatsStateForCreateTable(Map<String, String> params, String setting) {
  if (TRUE.equals(setting)) {
    for (String stat : StatsSetupConst.supportedStats) {
      params.put(stat, "0");
    }
  }
  setBasicStatsState(params, setting);
} 

public static final String[] supportedStats = {NUM_FILES,ROW_COUNT,TOTAL_SIZE,RAW_DATA_SIZE};
{code}




Then it set default rowNum as 0, but since spark will update numFiles and rawSize, so rowNum remain 0.

This impact other system like presto's CBO.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri May 06 09:37:58 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11v7c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Apr/22 10:57;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36377;;;, 06/May/22 09:37;cloud_fan;Issue resolved by pull request 36377
[https://github.com/apache/spark/pull/36377];;;
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 06/May/22 09:37;cloud_fan;Issue resolved by pull request 36377
[https://github.com/apache/spark/pull/36377];;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in org.apache.spark.input
Issue key: SPARK-38463
Issue id: 13432756
Parent id: 13423097.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bozhang
Creator: bozhang
Created: 09/Mar/22 05:17
Updated: 14/May/22 11:14
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat May 14 11:14:43 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10b0w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/May/22 11:14;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36551;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in org.apache.spark.partial
Issue key: SPARK-38470
Issue id: 13432763
Parent id: 13423097.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bozhang
Creator: bozhang
Created: 09/Mar/22 05:19
Updated: 14/May/22 02:02
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat May 14 02:02:47 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10b2g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/May/22 02:02;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36548;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Convert *AlreadyExistsException to use error classes
Issue key: SPARK-39185
Issue id: 13444920
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: srielau
Creator: srielau
Created: 14/May/22 00:17
Updated: 14/May/22 00:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: XXX already exists is a pretty common error condition.
We want to handle it as an error class
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-05-14 00:17:57.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12cvc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): cloud_fan
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in org.apache.spark.mapred
Issue key: SPARK-38466
Issue id: 13432759
Parent id: 13423097.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bozhang
Creator: bozhang
Created: 09/Mar/22 05:18
Updated: 13/May/22 13:01
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri May 13 13:01:18 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10b1k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/May/22 13:01;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36540;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Optimize FIRST when used as a single aggregate function
Issue key: SPARK-39169
Issue id: 13444727
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: vli-databricks
Creator: vli-databricks
Created: 12/May/22 23:56
Updated: 13/May/22 01:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When `FIRST` is a single aggregate function in `Aggregate` we could either rewrite whole query or optimize execution logic. 
 * Plan => `SELECT FIRST(<col>) FROM <table>` => `SELECT <col> FROM <table> LIMIT 1`. Note that setting `ignoreNulls` to `true` should block such rewrite since returns could differ in case all values of <col> are `NULL`
 * Execution => `SELECT FIRST(<col>) FROM <table> GROUP BY <some_col>` => short circuit iteration per key once a value for `FIRST` is set.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri May 13 01:00:34 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12bog:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/May/22 01:00;apachespark;User 'vli-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/36527;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve the test coverage for PySpark
Issue key: SPARK-38492
Issue id: 13433008
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: pralabhkumar
Reporter: itholic
Creator: itholic
Created: 10/Mar/22 05:42
Updated: 09/May/22 01:31
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark, Tests
Due Date: 
Votes: 0
Labels: 
Description: Currently, PySpark test coverage is around 91% according to codecov report: [https://app.codecov.io/gh/apache/spark|https://app.codecov.io/gh/apache/spark]

Since there are still 9% missing tests, so I think it would be great to improve our test coverage.

Of course we might not target to 100%, but as much as possible, to the level that we can currently cover with CI.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Apr 17 07:47:30 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10cko:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Mar/22 05:58;itholic;Each sub-task again contains several Python files.

So, if you are not going to work on the entire module, create a new sub-task under each sub-task and proceed!

e.g. If you want to add the test for `pyspark/pandas/frame.py` and `pyspark/pandas/series.py`, please create the sub-task under the https://issues.apache.org/jira/browse/SPARK-38493.

The title will be like "Improve the test coverage for pyspark/pandas/frame.py & pyspark/pandas/series.py";;;, 17/Apr/22 07:47;pralabhkumar;on it . Thx;;;
Affects Version/s.1: 
Component/s.1: Tests
Comment.1: 17/Apr/22 07:47;pralabhkumar;on it . Thx;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use v2 commands by default
Issue key: SPARK-36588
Issue id: 13397201
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: imback82
Reporter: cloud_fan
Creator: cloud_fan
Created: 25/Aug/21 14:16
Updated: 06/May/22 10:12
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: It's been a while after we introduce the v2 commands, and I think it's time to use v2 commands by default even for the session catalog, with a legacy config to fall back to the v1 commands.

We can do this one command by one command, with tests for both the v1 and v2 versions. The tests should help us understand the behavior difference between v1 and v2 commands, so that we can:
 # fix the v2 commands to match the v1 behavior
 # or accept the behavior difference and write migration guide

We can reuse the test framework built in https://issues.apache.org/jira/browse/SPARK-33381
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Dec 10 06:03:46 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u8qg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Aug/21 14:17;cloud_fan;[~imback82] do you want to drive this project?;;;, 25/Aug/21 15:55;imback82;Yes. Thanks [~cloud_fan].;;;, 29/Sep/21 05:00;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/34137;;;, 10/Dec/21 06:03;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/34861;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 25/Aug/21 15:55;imback82;Yes. Thanks [~cloud_fan].;;;
Comment.2: 29/Sep/21 05:00;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/34137;;;
Comment.3: 10/Dec/21 06:03;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/34861;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add bitwise functions for the BINARY data type
Issue key: SPARK-36811
Issue id: 13402312
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mkaravel
Creator: mkaravel
Created: 20/Sep/21 20:25
Updated: 06/May/22 10:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Add four new SQL functions operating on the `BINARY` data type for performing bitwise operations: BITAND, BITOR, BITXOR, and BITNOT.

The BITAND, BITOR, and BITXOR functions take two byte strings as input and return the bitwise AND, OR, or XOR of the two input byte strings. The byte size of the result is the maximum of the byte sizes of the inputs, while the result is computed by aligning the two strings with respect to their least significant byte, and left-padding with zeros the shorter of the two inputs.

The BITNOT function is a unary function and returns the input byte string with all the bits negated.
Environment: 
Original Estimate: 1209600.0
Remaining Estimate: 1209600.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 1209600.0
Σ Remaining Estimate: 1209600.0
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Sep 21 15:01:40 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v49c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.3.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Sep/21 15:01;apachespark;User 'mkaravel' has created a pull request for this issue:
https://github.com/apache/spark/pull/34056;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Keep consistent order of columns with user specify for v1 table
Issue key: SPARK-37517
Issue id: 13415009
Parent id: 13397201.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: xiaopenglei
Creator: xiaopenglei
Created: 02/Dec/21 10:11
Updated: 06/May/22 10:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Dec 02 11:38:42 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xaeo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Dec/21 11:37;apachespark;User 'Peng-Lei' has created a pull request for this issue:
https://github.com/apache/spark/pull/34780;;;, 02/Dec/21 11:38;apachespark;User 'Peng-Lei' has created a pull request for this issue:
https://github.com/apache/spark/pull/34780;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 02/Dec/21 11:38;apachespark;User 'Peng-Lei' has created a pull request for this issue:
https://github.com/apache/spark/pull/34780;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Escape dot character in partition names
Issue key: SPARK-37722
Issue id: 13419139
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ivan.sadikov
Creator: ivan.sadikov
Created: 23/Dec/21 05:32
Updated: 06/May/22 10:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Some file systems (for example, ABFS) do not support file names/paths ending with the {{.}} character. The following error is thrown: 
{code:java}
Caused by: java.lang.IllegalArgumentException: ABFS does not allow files or directories to end with a dot.  {code}
We should escape dots in partition names by treating {{.}} character as a special one.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Dec 23 05:47:45 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xzdk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Dec/21 05:47;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/34995;;;, 23/Dec/21 05:47;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/34995;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 23/Dec/21 05:47;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/34995;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Comparation between string(great than Int.MaxValue, such as '2147483648') and int return null
Issue key: SPARK-37736
Issue id: 13419407
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: lijiahong
Creator: lijiahong
Created: 25/Dec/21 08:26
Updated: 06/May/22 10:09
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Comparation between string(great than Int.MaxValue, such as '2147483648') and int return null:

```

>select '2147483648'>1;

NULL

```

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Dec 25 08:54:12 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y10o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Dec/21 08:53;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/35018;;;, 25/Dec/21 08:54;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/35018;;;
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 25/Dec/21 08:54;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/35018;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Migrate SHOW CREATE TABLE to use V2 command by default
Issue key: SPARK-37477
Issue id: 13414199
Parent id: 13397201.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: xiaopenglei
Creator: xiaopenglei
Created: 29/Nov/21 02:09
Updated: 06/May/22 10:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-29 02:09:35.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0x5f4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Migrate DESCRIBE TABLE to use V2 command by default
Issue key: SPARK-37890
Issue id: 13422378
Parent id: 13397201.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: imback82
Reporter: imback82
Creator: imback82
Created: 13/Jan/22 03:36
Updated: 06/May/22 10:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Migrate DESCRIBE TABLE to use V2 command by default.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jan 13 03:37:10 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yjbc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Jan/22 03:37;imback82;Working on this.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Replace the error classes related to invalid parameters by `INVALID_PARAMETER_VALUE`
Issue key: SPARK-38119
Issue id: 13426784
Parent id: 13423097.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: maxgekk
Reporter: maxgekk
Creator: maxgekk
Created: 06/Feb/22 09:35
Updated: 06/May/22 10:05
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Re-use INVALID_PARAMETER_VALUE instead of more specific error classes:
* ILLEGAL_SUBSTRING
* INVALID_ARRAY_INDEX
* INVALID_ARRAY_INDEX_IN_ELEMENT_AT
* INVALID_FRACTION_OF_SECOND
* INVALID_INPUT_SYNTAX_FOR_NUMERIC_TYPE
* MAP_KEY_DOES_NOT_EXIST
* MAP_KEY_DOES_NOT_EXIST_IN_ELEMENT_AT
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): SPARK-38001
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Feb 06 09:36:43 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zae0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Feb/22 09:36;maxgekk;I am working on this.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support Alter Table/Partition Concatenate command
Issue key: SPARK-39036
Issue id: 13441982
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gabry.wu
Creator: gabry.wu
Created: 27/Apr/22 06:33
Updated: 06/May/22 00:15
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core, SQL
Due Date: 
Votes: 0
Labels: 
Description: Hi, folks, 

In Hive, we can use following command to merge small files, however, there is not a corresponding command to do that in Spark SQL. 

I believe it's useful and it's not enough only using AQE.  Is anyone working on this to merge small files? If not, I want to create a PR to implement it

 
{code:java}
ALTER TABLE table_name [PARTITION (partition_key = 'partition_value' [, ...])] CONCATENATE;{code}
 

[https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AlterTable/PartitionConcatenate]

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri May 06 00:15:26 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11uuo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/May/22 00:15;gabry.wu;[~hyukjin.kwon] What do you know about that?  Is anyone working on this to merge small files?;;;
Affects Version/s.1: 
Component/s.1: SQL
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add longMessage and parameters to error-class
Issue key: SPARK-39101
Issue id: 13443224
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: srielau
Creator: srielau
Created: 05/May/22 00:45
Updated: 05/May/22 00:45
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: We want to allow error-classes.json to contain documentation beyond the short message.
this information can be used for doc generation.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-05-05 00:45:01.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z122hc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add option for partial nested field writes with nullable fields.
Issue key: SPARK-38408
Issue id: 13431773
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 03/Mar/22 17:39
Updated: 02/May/22 19:35
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: If a nested field has nullable entries a user shouldn't have to specify all of the fields, instead the nullable fields can have the default value of null. This will allow for more effective schema migration. We might want to put this behind a feature flag though.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-34378
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon May 02 19:29:42 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z104zk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Mar/22 19:54;xkrogen;Sounds very similar to what was done in SPARK-34378 for the Avro datasource specifically. Is what you're discussing a general feature, or specific to a datasource? I don't quite follow the use-case.;;;, 02/May/22 19:29;holden;More general;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 02/May/22 19:29;holden;More general;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use V2 Filter in V2 file source
Issue key: SPARK-36730
Issue id: 13400677
Parent id: 13396564.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: huaxingao
Creator: huaxingao
Created: 12/Sep/21 23:25
Updated: 24/Apr/22 06:03
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Use V2 Filters in V2 file source, e.g. FileScan, FileScanBuilder
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Apr 24 06:03:38 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uu68:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Sep/21 05:35;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33973;;;, 24/Apr/22 06:02;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/36332;;;, 24/Apr/22 06:03;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/36332;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 24/Apr/22 06:02;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/36332;;;
Comment.2: 24/Apr/22 06:03;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/36332;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: DS V2 supports push down collection functions
Issue key: SPARK-38900
Issue id: 13439626
Parent id: 13438855.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Zhixiong Chen
Creator: Zhixiong Chen
Created: 14/Apr/22 10:02
Updated: 18/Apr/22 07:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Apr 18 07:10:29 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11grk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Apr/22 07:45;beliefer;Could you do the job?;;;, 18/Apr/22 07:10;Zhixiong Chen;Yes. I can do this job.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 18/Apr/22 07:10;Zhixiong Chen;Yes. I can do this job.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: cast as char/varchar result is string, not expect data type
Issue key: SPARK-38902
Issue id: 13439650
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yghu
Creator: yghu
Created: 14/Apr/22 12:05
Updated: 15/Apr/22 01:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.1, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: when cast column to char/varchar type, result is string, not expected data type
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-14 12:05:17.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11gww:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: More comprehensive DSV2 push down capabilities
Issue key: SPARK-38788
Issue id: 13437775
Parent id: 
Issue Type: Epic
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: maxgekk
Reporter: maxgekk
Creator: maxgekk
Created: 05/Apr/22 07:36
Updated: 13/Apr/22 16:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Get together all tickets related to push down (filters) via Datasource V2 APIs.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): ghx-label-14
Custom field (Epic Link): 
Custom field (Epic Name): More comprehensive DSV2 push down capabilities
Custom field (Epic Status): To Do
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 13 16:08:06 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z115fs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Apr/22 22:38;xkrogen;What's the relationship between this and SPARK-38852? Seems like they are laying out the same goal?;;;, 13/Apr/22 05:31;maxgekk;[~xkrogen] This epic ticket gets together all related activities in Spark 3.3 for the release note.;;;, 13/Apr/22 16:08;xkrogen;Yeah, I got that, but isn't SPARK-38852 trying to do the same thing? Or are they targeting different functionality..? The descriptions seem the same to me.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 13/Apr/22 05:31;maxgekk;[~xkrogen] This epic ticket gets together all related activities in Spark 3.3 for the release note.;;;
Comment.2: 13/Apr/22 16:08;xkrogen;Yeah, I got that, but isn't SPARK-38852 trying to do the same thing? Or are they targeting different functionality..? The descriptions seem the same to me.;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: ANSI enhancements in Spark 3.3
Issue key: SPARK-38860
Issue id: 13438948
Parent id: 
Issue Type: Epic
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: maxgekk
Reporter: maxgekk
Creator: maxgekk
Created: 11/Apr/22 12:44
Updated: 11/Apr/22 12:51
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Get together all issues related to ANSI enhancements in Spark 3.3.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-35030, SPARK-33354, SPARK-34246
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): ghx-label-3
Custom field (Epic Link): 
Custom field (Epic Name): ANSI enhancements in Spark 3.3
Custom field (Epic Status): To Do
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-11 12:44:18.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11clk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Fix translate metadata col filters
Issue key: SPARK-38843
Issue id: 13438746
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 09/Apr/22 23:30
Updated: 09/Apr/22 23:53
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Actually it can't be pushed down:
{noformat}
07:24:45.131 ERROR org.apache.spark.sql.execution.datasources.FileSourceStrategy: Pushed Filters: IsNotNull(_metadata.file_path),StringContains(_metadata.file_path,data/f0)
{noformat}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Apr 09 23:53:33 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11bcw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Apr/22 23:52;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36126;;;, 09/Apr/22 23:53;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36126;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 09/Apr/22 23:53;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36126;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use error classes in the compilation errors of windows
Issue key: SPARK-38110
Issue id: 13426668
Parent id: 13423097.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: maxgekk
Creator: maxgekk
Created: 04/Feb/22 16:43
Updated: 07/Apr/22 11:28
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Migrate the following errors in QueryCompilationErrors:
* windowSpecificationNotDefinedError
* windowAggregateFunctionWithFilterNotSupportedError
* windowFunctionInsideAggregateFunctionNotAllowedError
* expressionWithoutWindowExpressionError
* expressionWithMultiWindowExpressionsError
* windowFunctionNotAllowedError
* cannotSpecifyWindowFrameError
* windowFrameNotMatchRequiredFrameError
* windowFunctionWithWindowFrameNotOrderedError
* multiTimeWindowExpressionsNotSupportedError
* sessionWindowGapDurationDataTypeError
* invalidLiteralForWindowDurationError
* emptyWindowExpressionError
* foundDifferentWindowFunctionTypeError

onto use error classes. Throw an implementation of SparkThrowable. Also write a test per every error in QueryCompilationErrorsSuite.

*Feel free to split this to sub-tasks.*
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): SPARK-38687
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-38296
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Apr 07 11:28:38 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z9o8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): maxgekk
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Apr/22 11:28;goutamghosh;[~maxgekk]  can I work on this ?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: DS V2 Filter support
Issue key: SPARK-36555
Issue id: 13396564
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: huaxingao
Creator: huaxingao
Created: 22/Aug/21 16:02
Updated: 05/Apr/22 07:39
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The motivation of adding DSV2 filters:
The values in V1 filters are Scala types. When translating catalyst Expression to V1 filers, we have to call convertToScala to convert from Catalyst types used internally in rows to standard Scala types, and later convert Scala types back to Catalyst types. This is very inefficient. In V2 filters, we use Expression for filter values, so the conversion from Catalyst types to Scala types and Scala types back to Catalyst types are avoided.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): SPARK-38788
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-22 16:02:36.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u4sw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: More operator pushdown API in Data Source V2 
Issue key: SPARK-36640
Issue id: 13398741
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: cloud_fan
Creator: cloud_fan
Created: 01/Sep/21 15:49
Updated: 05/Apr/22 07:37
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: As of Spark 3.2, we have DS v2 pushdown APIs for Filter, Aggregate, and column pruning. We may need more like Limit, Sort, Sample, etc.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): SPARK-38788
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-09-01 15:49:40.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ui88:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve the implement of percentile_cont
Issue key: SPARK-38685
Issue id: 13436408
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: beliefer
Creator: beliefer
Created: 29/Mar/22 11:51
Updated: 29/Mar/22 12:45
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 29 12:45:52 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10xfs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Mar/22 12:45;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/36003;;;, 29/Mar/22 12:45;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/36003;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 29/Mar/22 12:45;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/36003;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Avoid unnecessary sort in FileFormatWriter if user has specified sort in AQE
Issue key: SPARK-38578
Issue id: 13434244
Parent id: 13407393.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ulysses
Creator: ulysses
Created: 17/Mar/22 02:08
Updated: 21/Mar/22 13:27
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: FileFormatWriter will check and add an implicit sort for dynamic partition columns or bucket columns according to the input physical plan. The check became always failure since AQE AdaptiveSparkPlanExec has no outputOrdering.

That casues a redundant sort if user has specified a sort which satisfies the required ordering (dynamic partition and bucket columns).
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Mar 21 13:27:00 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10k5s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Mar/22 13:27;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/35924;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Upgrade Kafka to 3.1.1
Issue key: SPARK-38602
Issue id: 13434625
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dongjoon
Creator: dongjoon
Created: 18/Mar/22 21:21
Updated: 18/Mar/22 21:21
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-03-18 21:21:35.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10mig:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Interval types are not truncated to the expected endField when creating a DataFrame via Duration
Issue key: SPARK-38577
Issue id: 13434239
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: chongg@nvidia
Creator: chongg@nvidia
Created: 17/Mar/22 01:18
Updated: 17/Mar/22 15:56
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: *Problem:*

ANSI interval types are store as long internally.

The long value are not truncated to the expected endField when creating a DataFrame via Duration.

 

*Reproduce:*

Create a "day to day" interval, the seconds are not truncated, see below code.

The internal long is not {*}86400 * 1000000{*}, but it's ({*}86400 + 1)  * 1000000{*}{*}{{*}}

 
{code:java}
  test("my test") {
    val data = Seq(Row(Duration.ofDays(1).plusSeconds(1)))
    val schema = StructType(Array(
      StructField("t", DayTimeIntervalType(DayTimeIntervalType.DAY, DayTimeIntervalType.DAY))
    ))
    val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
    df.show()
  } {code}
 

 

After debug, the {{endField}} is always {{SECOND}} in {{{}durationToMicros{}}}, see below:

 
{code:java}
  // IntervalUtils class

  def durationToMicros(duration: Duration): Long = {
    durationToMicros(duration, DT.SECOND)   // always SECOND
  }

  def durationToMicros(duration: Duration, endField: Byte)

{code}
Seems should use different endField which could be [DAY, HOUR, MINUTE, SECOND]

Or Spark can throw an exception to avoid truncating.
Environment: Spark 3.3.0 snapshot version

 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Mar 17 15:56:34 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10k4o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Mar/22 15:56;revans2;This is especially problematic because it is really inconsistent.
{code:scala}
val data = Seq(Row(Duration.ofDays(1).plusSeconds(1)), Row(Duration.ofDays(2).plusMinutes(2)))

val schema = StructType(Array(StructField("dur", DayTimeIntervalType(DayTimeIntervalType.DAY, DayTimeIntervalType.DAY))))

val df = spark.createDataFrame(spark.sparkContext.parallelize(data), schema)

df.selectExpr("dur", "CAST(dur AS long)", "CAST('1970-1-1' as timestamp) + dur as ts").show()
+----------------+---+-------------------+
|             dur|dur|                 ts|
+----------------+---+-------------------+
|INTERVAL '1' DAY|  1|1970-01-02 00:00:01|
|INTERVAL '2' DAY|  2|1970-01-03 00:02:00|
+----------------+---+-------------------+
 
df.select(col("dur"), col("dur").cast(DayTimeIntervalType()).alias("default_dur")).show(truncate = false)
+----------------+-----------------------------------+
|dur             |default_dur                        |
+----------------+-----------------------------------+
|INTERVAL '1' DAY|INTERVAL '1 00:00:01' DAY TO SECOND|
|INTERVAL '2' DAY|INTERVAL '2 00:02:00' DAY TO SECOND|
+----------------+-----------------------------------+
{code}
Casting the values to different types will truncate it if dropping precision, but increasing precision or doing math with it does not.

Saving the data to parquet keeps the data exactly the same as was input, but doing it to CSV truncates it.
{code:scala}
df.write.parquet("./tmp")

val df2 = spark.read.parquet("./tmp")

df2.selectExpr("dur", "CAST(dur AS long)", "CAST('1970-1-1' as timestamp) + dur as ts").show()
+----------------+---+-------------------+
|             dur|dur|                 ts|
+----------------+---+-------------------+
|INTERVAL '1' DAY|  1|1970-01-02 00:00:01|
|INTERVAL '2' DAY|  2|1970-01-03 00:02:00|
+----------------+---+-------------------+

 

df.write.csv("./tmp_csv")

val df3 = spark.read.schema(schema).csv("./tmp_csv")

df3.selectExpr("dur", "CAST(dur AS long)", "CAST('1970-1-1' as timestamp) + dur as ts").show()
+----------------+---+-------------------+
|             dur|dur|                 ts|
+----------------+---+-------------------+
|INTERVAL '2' DAY|  2|1970-01-03 00:00:00|
|INTERVAL '1' DAY|  1|1970-01-02 00:00:00|
+----------------+---+-------------------+
 {code}
This is all also true in the python API.

 

I would expect to get an error when importing the data, or have Spark truncate/fix the data when it is imported so I don't get inconsistent and confusing results with it.

 

If this works as expected, then I would like to see it documented better what is happening.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Deduplicate the right side of left semi/anti join
Issue key: SPARK-36245
Issue id: 13391150
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 21/Jul/21 14:52
Updated: 10/Mar/22 15:03
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Deduplicate the right side of left semi/anti join to improve query performance.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): SPARK-38505
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): SPARK-37597
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 21 15:00:30 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0t7e8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Jul/21 15:00;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/33465;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve the test coverage for pyspark/ml module
Issue key: SPARK-38495
Issue id: 13433013
Parent id: 13433008.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: itholic
Creator: itholic
Created: 10/Mar/22 05:48
Updated: 10/Mar/22 05:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Currently, ml module has 90% of test coverage.

We could improve the test coverage by adding the missing tests for ml module.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-03-10 05:48:34.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10cls:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve the test coverage for pyspark/mllib module
Issue key: SPARK-38494
Issue id: 13433012
Parent id: 13433008.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: itholic
Creator: itholic
Created: 10/Mar/22 05:48
Updated: 10/Mar/22 05:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Currently, mllib module has 88% of test coverage.

We could improve the test coverage by adding the missing tests for mllib module.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-03-10 05:48:05.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10clk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Avoid duplicating complex partitioning expressions
Issue key: SPARK-38282
Issue id: 13429840
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tanelk
Creator: tanelk
Created: 22/Feb/22 07:03
Updated: 08/Mar/22 02:33
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Spark will duplicate all non-trivial expressions in Window.partitionBy, that will result in duplicate exchanges and WindowExec nodes.

An example unit test:
{code}
  test("SPARK-38282: Avoid duplicating complex partitioning expressions") {
    val group = functions.col("id") % 2
    val min = functions.min("id").over(Window.partitionBy(group))
    val max = functions.max("id").over(Window.partitionBy(group))

    val df1 = spark.range(1, 4)
      .withColumn("ratio", max / min)

    val df2 = spark.range(1, 4)
      .withColumn("min", min)
      .withColumn("max", max)
      .select(col("id"), (col("max") / col("min")).as("ratio"))

    Seq(df1, df2).foreach { df =>
      checkAnswer(
        df,
        Seq(Row(1L, 3.0), Row(2L, 1.0), Row(3L, 3.0)))

      val windows = collect(df.queryExecution.executedPlan) {
        case w: WindowExec => w
      }
      assert(windows.size == 1)
    }
  }
{code}

The query plan for this (_w0#5L and _w1#6L are duplicates):
{code}
Window [min(id#2L) windowspecdefinition(_w1#6L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we1#8L], [_w1#6L]
   +- *(4) Sort [_w1#6L ASC NULLS FIRST], false, 0
      +- AQEShuffleRead coalesced
         +- ShuffleQueryStage 1
            +- Exchange hashpartitioning(_w1#6L, 5), ENSURE_REQUIREMENTS, [id=#256]
               +- *(3) Project [id#2L, _w1#6L, _we0#7L]
                  +- Window [max(id#2L) windowspecdefinition(_w0#5L, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#7L], [_w0#5L]
                     +- *(2) Sort [_w0#5L ASC NULLS FIRST], false, 0
                        +- AQEShuffleRead coalesced
                           +- ShuffleQueryStage 0
                              +- Exchange hashpartitioning(_w0#5L, 5), ENSURE_REQUIREMENTS, [id=#203]
                                 +- *(1) Project [id#2L, (id#2L % 2) AS _w0#5L, (id#2L % 2) AS _w1#6L]
                                    +- *(1) Range (1, 4, step=1, splits=2)
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 08 02:33:37 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zt4g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Feb/22 07:08;tanelk;[~cloud_fan], any ideas how to improve this? 
I could submit a PR, but I'm not sure, what would be the best way here.;;;, 07/Mar/22 15:49;cloud_fan;cc [~yumwang] [~chengsu] ;;;, 08/Mar/22 02:33;yumwang;Some related PRs:
https://github.com/apache/spark/pull/33522
https://github.com/apache/spark/pull/34334;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 07/Mar/22 15:49;cloud_fan;cc [~yumwang] [~chengsu] ;;;
Comment.2: 08/Mar/22 02:33;yumwang;Some related PRs:
https://github.com/apache/spark/pull/33522
https://github.com/apache/spark/pull/34334;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Unify get preferred locations for shuffle in AQE
Issue key: SPARK-38401
Issue id: 13431635
Parent id: 13407393.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ulysses
Creator: ulysses
Created: 03/Mar/22 07:12
Updated: 03/Mar/22 08:31
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: It has several issues with method `ShuffledRowRDD#getPreferredLocations`.
 * it does not respect the config `spark.shuffle.reduceLocality.enabled`, so we can not disable it.
 * it does not respect `REDUCER_PREF_LOCS_FRACTION`, so it has no effect if DAG schedule task to an executor who has less data. In worse, driver will take more memory to store the useless locations.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Mar 03 08:31:17 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1044w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Mar/22 08:31;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/35719;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support Create/Drop/Alter Catalog SQL
Issue key: SPARK-38359
Issue id: 13431169
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: melin
Creator: melin
Created: 01/Mar/22 07:39
Updated: 01/Mar/22 07:46
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Support Create/Drop/Alter Catalog，Add the catalog dynamically at runtime
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-03-01 07:39:31.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z101a0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Make the output of V1/V2 "desc extended table" consistent
Issue key: SPARK-38350
Issue id: 13430990
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: Gengliang.Wang
Reporter: Gengliang.Wang
Creator: Gengliang.Wang
Created: 28/Feb/22 13:00
Updated: 28/Feb/22 13:23
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The V1 "DESC TABLE EXTENDED" command contains info of "Database" and "Table", which can be used by external tools like DBT.

However, the V2 version contains only one field "name" representing "catalog.database.table". External tools can't recognize it. Also, it is weird to have different command output from the same command.

We should fix it.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Feb 28 13:22:57 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1006g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Feb/22 13:22;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35681;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Error Classes AnalysisException is not propagated in FunctionRegistry
Issue key: SPARK-38296
Issue id: 13430070
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: andrewfmurphy
Creator: andrewfmurphy
Created: 23/Feb/22 03:43
Updated: 24/Feb/22 00:24
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: While trying to migrate invalidLiteralForWindowDurationError to use Error Classes (SPARK-38110), I realized that the error class is not propagating correctly. This is because the AnalysisException is caught and rethrown at [FunctionRegistry:154|https://github.com/apache/spark/blob/43e93b581ea5f7a1ba6cf943e6624f6847ebc3a8/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala#L154] without a case for Error Classes. I will attach a fix for this specific error here to avoid bundling too many changes into one PR for SPARK-38110. I anticipate that similar changes will need to be made where AnalysisException is rethrown as the migration continues.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-38110
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Feb 24 00:24:35 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zuig:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Feb/22 00:23;apachespark;User 'andrewfmurphy' has created a pull request for this issue:
https://github.com/apache/spark/pull/35638;;;, 24/Feb/22 00:24;apachespark;User 'andrewfmurphy' has created a pull request for this issue:
https://github.com/apache/spark/pull/35638;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 24/Feb/22 00:24;apachespark;User 'andrewfmurphy' has created a pull request for this issue:
https://github.com/apache/spark/pull/35638;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add `DataFrame.resample` for pandas API on Spark.
Issue key: SPARK-38264
Issue id: 13429578
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: itholic
Creator: itholic
Created: 21/Feb/22 00:11
Updated: 21/Feb/22 00:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Implement the function DataFrame.resample for pandas API on Spark to follow the behavior of pandas (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html).
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-02-21 00:11:41.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zrio:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Contains Join for Spark SQL
Issue key: SPARK-38238
Issue id: 13429037
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wankun
Creator: wankun
Created: 17/Feb/22 09:06
Updated: 17/Feb/22 09:32
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently Spark SQL uses a Broadcast Nested Loop join when it has to execute the following string contains query:
{code:sql}
SELECT a.text, b.pattern
FROM fact_table a
JOIN patterns b
ON a.text like concat('%', b.pattern, '%');
{code}
OR
{code:sql}
SELECT a.text, b.pattern
FROM fact_table a
JOIN patterns b
ON position(b.pattern, a.text) > 0;
{code}
If there are many patterns to match in the left table, the query many execute for a long time.

Actually this kind of join is called *Multi-Pattern String Matching* or {*}Multi-Way String Matching{*}, and many algorithms try to improve this kind of matching. One of the well-knowing algorithm is [*Aho–Corasick algorithm*|https://en.wikipedia.org/wiki/Aho%E2%80%93Corasick_algorithm]

The basic idea of optimizing this kindof query is to transform all the patterns into a trie tree and broadcast it. So each row of the fact table only need to match its content against the trie tree once.

The query will go from *O(M * N * m * n)* to *O(M * m * max( n ))*
M = number of records in the fact table
N = number of records in the patterns table
m = row length of the fact table
n = row length of the patterns table
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Feb 17 09:24:08 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zo7c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Feb/22 09:23;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/35550;;;, 17/Feb/22 09:24;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/35550;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 17/Feb/22 09:24;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/35550;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Provide monitoring REST API for Structured Streaming
Issue key: SPARK-38234
Issue id: 13428917
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yeskarthik
Creator: yeskarthik
Created: 16/Feb/22 20:40
Updated: 17/Feb/22 01:59
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: In [SPARK-38234|https://issues.apache.org/jira/browse/SPARK-38234] {-}{-}Structured Streaming is added to the history server and a "Structured Streaming" tab appears in the history UI when a streaming query is present. However, even though a store exists for it and the data is presented in the UI, this data is not exposed as a REST API. This data can be used for monitoring, detecting streaming and to build custom dashboards. This monitoring API will be similar to the monitoring APIs that are present for DStreams - refer [SPARK-18470|https://issues.apache.org/jira/browse/SPARK-18470].

In this change, we plan to add two simple APIs that expose the data in the store and can be used to monitor streaming queries. 
h3. *Summary API*

To list the summary of all existing streaming queries.

GET {{/\{appId}/sql/streamingqueries}}

Response is list of {_}StreamingQueryData{_}.
h3. *Progress API*

To list the progress events of a specific streaming query by {_}runId{_}. 

User can also specify how many of the most recent events needs to be retrieved by using the _last_ query parameter. By default, we can return the most recent progress event i.e. last is set to 1.

GET {{{}/\{appId}/sql/streamingqueries/\{runId}/progress?last={N{}}}}

Response is list of {_}StreamingQueryProgress{_}.

*Note:* We are not introducing new objects for the response since we are just returning the data from the store without aggregation, these are existing event structures.

Attached sample I/O and screenshots.{{{{}}{}}}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 16/Feb/22 21:36;yeskarthik;StreamingAPI-SS1.jpg;https://issues.apache.org/jira/secure/attachment/13040134/StreamingAPI-SS1.jpg, 16/Feb/22 21:36;yeskarthik;StreamingAPI-SS2.jpg;https://issues.apache.org/jira/secure/attachment/13040135/StreamingAPI-SS2.jpg, 16/Feb/22 21:32;yeskarthik;StreamingAPIsSampleIO.txt;https://issues.apache.org/jira/secure/attachment/13040133/StreamingAPIsSampleIO.txt
Custom field (Affects version (Component)): 
Custom field (Attachment count): 3.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Feb 17 00:34:27 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zngw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Feb/22 20:58;kabhwan;(I just changed the fixed versions - it is reserved to represent which versions the "fix" is landed.);;;, 16/Feb/22 21:03;yeskarthik;Thanks. I will send the PR soon.;;;, 16/Feb/22 23:07;apachespark;User 'yeskarthik' has created a pull request for this issue:
https://github.com/apache/spark/pull/35547;;;, 16/Feb/22 23:08;apachespark;User 'yeskarthik' has created a pull request for this issue:
https://github.com/apache/spark/pull/35547;;;, 17/Feb/22 00:34;apachespark;User 'yeskarthik' has created a pull request for this issue:
https://github.com/apache/spark/pull/35548;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 16/Feb/22 21:03;yeskarthik;Thanks. I will send the PR soon.;;;
Comment.2: 16/Feb/22 23:07;apachespark;User 'yeskarthik' has created a pull request for this issue:
https://github.com/apache/spark/pull/35547;;;
Comment.3: 16/Feb/22 23:08;apachespark;User 'yeskarthik' has created a pull request for this issue:
https://github.com/apache/spark/pull/35547;;;
Comment.4: 17/Feb/22 00:34;apachespark;User 'yeskarthik' has created a pull request for this issue:
https://github.com/apache/spark/pull/35548;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: The columns in state schema should be relaxed to be nullable
Issue key: SPARK-38205
Issue id: 13428313
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kabhwan
Creator: kabhwan
Created: 14/Feb/22 11:20
Updated: 15/Feb/22 22:52
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Starting from SPARK-27237, Spark validates the schema of state across query runs to make sure it doesn't fall into more weird issue like SIGSEGV on the runtime.

The comparison logic is reasonable in terms of nullability; it has below matrices:
||existing schema||new schema||allowed||
|nullable|nullable|O|
|nullable|non-nullable|O|
|non-nullable|nullable|X|
|non-nullable|non-nullable|O|

What we miss here is, the nullability of the column can be changed in the optimizer (mostly nullable to non-nullable), and the optimization about nullability could be applied differently with any simple changes.

So this scenario is hypothetically possible:

1. At the first run of the query, optimizer marks some columns from nullable to non-nullable, and it goes to the schema of the state. (state schema has a column with non-nullable)
2. At the second run of the query (possibly with code modification or upgrading Spark version), optimizer no longer marks such columns from nullable to non-nullable, and it goes with comparison of the schema of the state (existing vs new), comparing non-nullable (existing) vs nullable (new), which is NOT allowed.

In terms of storage view for state store, it is not required to determine the column as non-nullable vs nullable. Interface-wise, state store has no concept of schema; so it is safe to relax such constraint, and open the chance for optimizer to do whatever it wants and doesn't break stateful operators.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Feb 15 22:52:36 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zjrc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Feb/22 22:52;kabhwan;I realized the output schema should also be nullable (since the operator will produce output from state), and now puzzled whether there may be cases I’m going to break the existing query (DSv2 sink may check the nullability when writing).

I guess another way is never changing the nullability on optimizer and keep the nullability check in state. I would rely on less invasive way if there is one, since the lifetime of streaming query is long, across Spark versions, and compatibility is a major concern.;;;
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Streaming dropDuplicates should remove out-of-watermark keys
Issue key: SPARK-38212
Issue id: 13428481
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: viirya
Creator: viirya
Created: 15/Feb/22 06:14
Updated: 15/Feb/22 06:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Currently streaming dropDuplicates simply stores all states forever if no watermark-attached key exists.

For streaming stateful operator, this seems counterintuitive behavior if we consider other stateful operations such as streaming joining.

It also means the number of state will grow up infinitely as we observe in real application. But it doesn't make sense to add a time column into dedup keys because we don't really deduplicate rows by [key, time] but only by [key].

More reasonable streaming dedup seems to remove out-of-watermark states if the input data (not key) has watermark. This will be a behavior change, for streaming queries with dedup operation.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Feb 15 06:18:42 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zkso:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Feb/22 06:18;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/35521;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Selectively include EXTERNAL TABLE source files via REGEX
Issue key: SPARK-38209
Issue id: 13428360
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: melin
Creator: melin
Created: 14/Feb/22 15:47
Updated: 14/Feb/22 15:47
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: https://issues.apache.org/jira/browse/HIVE-951

CREATE EXTERNAL TABLE should allow users to cherry-pick files via regular expression.
CREATE EXTERNAL TABLE was designed to allow users to access data that exists outside of Hive, and
currently makes the assumption that all of the files located under the supplied path should be included
in the new table. Users frequently encounter directories containing multiple
datasets, or directories that contain data in heterogeneous schemas, and it's often
impractical or impossible to adjust the layout of the directory to meet the requirements of
CREATE EXTERNAL TABLE. A good example of this problem is creating an external table based
on the contents of an S3 bucket.

One way to solve this problem is to extend the syntax of CREATE EXTERNAL TABLE
as follows:

CREATE EXTERNAL TABLE
...
LOCATION path [file_regex]
...
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-02-14 15:47:32.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zk1s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Allow for API user to disable Shuffle Operations while running locally
Issue key: SPARK-37536
Issue id: 13415251
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: RodBoavida
Creator: RodBoavida
Created: 03/Dec/21 10:36
Updated: 06/Feb/22 18:33
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: We have been using Spark on local mode, as a small embedded, in-memory SQL DB for microservices.

Spark's powerful SQL features, and flexibility enables developers to build efficient data querying solutions. Due to the nature of our solution dealing with small datasets, which required to be queried through SQL, on very low latencies, we found the embedded approach a very good model.

We found through experimentation, that Spark on local mode, would gain significant performance improvements (on average between 20-30%) by disabling the shuffling on aggregation operations - ShuffleExchangeExec.

I will be raising a PR, to propose introducing a new configuration variable 

*spark.sql.localMode.shuffle.enabled*

This variable will default to true, and will be checked on the QueryExecution EnsureRequirements creation time, in conjunction with checking if Spark is running on local mode, will keep the execution plans unchanged if the value is false.

Looking forward any comments and feedback.
Environment: Spark running in local mode
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Feb 05 10:24:45 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xbwg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Feb/22 10:24;apachespark;User 'risinga' has created a pull request for this issue:
https://github.com/apache/spark/pull/35402;;;, 05/Feb/22 10:24;apachespark;User 'risinga' has created a pull request for this issue:
https://github.com/apache/spark/pull/35402;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 05/Feb/22 10:24;apachespark;User 'risinga' has created a pull request for this issue:
https://github.com/apache/spark/pull/35402;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improving schema filtering flexibility
Issue key: SPARK-36986
Issue id: 13406128
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: RodBoavida
Creator: RodBoavida
Created: 12/Oct/21 11:48
Updated: 05/Feb/22 10:13
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Our spark usage, requires us to build an external schema and pass it on while creating a DataSet.

While working through this, I found an optimization would improve greatly Spark's flexibility to query external schema management.

Scope: ability to retrieve a field's name and schema in one single call, requesting to return a tupple by index. 

Means extending the StructType class to support an additional method

This is what the function would look like:

/**
 * Returns the index and field structure by name.
 * If it doesn't find it, returns None.
 * Avoids two client calls/loops to obtain consolidated field info.
*
*/
def getIndexAndFieldByName(name: String): Option[(Int, StructField)] = \{   val field = nameToField.get(name)   if(field.isDefined) {     Some((fieldIndex(name), field.get))   }
else \{     None   }
}

This is particularly useful from an efficiency perspective, when we're parsing a Json structure and we want to check for every field what is the name and field type already defined in the schema

I will create a corresponding branch for PR review, assuming that there are no concerns with the above proposal.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): Schema management improvements 
1 - Retrieving a field name and type from a schema based on its index
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Oct 21 17:41:14 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vrsg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Oct/21 17:41;apachespark;User 'risinga' has created a pull request for this issue:
https://github.com/apache/spark/pull/34359;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Make links for stderr/stdout on Spark on Kube configurable
Issue key: SPARK-38090
Issue id: 13426256
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: holden
Reporter: holden
Creator: holden
Created: 02/Feb/22 18:45
Updated: 02/Feb/22 18:45
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.2, 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: Unlike YARN different clusters store pod logs in different locations. We should allow people to configure the links so that they can go a web UI for their clusters stderr/stdout or print out the kubectl commands for users who don't have a link configured.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-02-02 18:45:18.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z74w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Improve the performance of DS V2 aggregate push-down
Issue key: SPARK-38065
Issue id: 13425515
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: beliefer
Creator: beliefer
Created: 29/Jan/22 01:58
Updated: 29/Jan/22 02:09
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, the build-in scan builder that implement SupportsPushDownAggregates has itself's config to control aggregate push-down.
We can advance the judgement of the config which controls aggregate push-down.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Jan 29 02:09:51 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z2mo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Jan/22 02:09;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/35360;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: SQL Adaptive Query Execution QA: Phase 2
Issue key: SPARK-37063
Issue id: 13407393
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dongjoon
Creator: dongjoon
Created: 19/Oct/21 21:58
Updated: 27/Jan/22 20:01
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-33828
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 20 01:48:25 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vzko:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.3.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Oct/21 01:48;ulysses;thank you [~dongjoon] for creating this umbrella !;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Refactor FileDataSourceBaseSuite
Issue key: SPARK-38043
Issue id: 13425162
Parent id: 
Issue Type: Task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 27/Jan/22 13:21
Updated: 27/Jan/22 13:30
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Refactor FileDataSourceBaseSuite to build a test frame for datasource
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jan 27 13:30:28 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z0gg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Jan/22 13:30;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/35342;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Push down limit through Aggregate if it is group only
Issue key: SPARK-37989
Issue id: 13424197
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 23/Jan/22 04:46
Updated: 24/Jan/22 10:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jan 24 10:00:35 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yuj4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Jan/22 05:52;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/35287;;;, 24/Jan/22 06:32;yumwang;Benchmark 1

{code:scala}
import org.apache.spark.benchmark.Benchmark
import org.apache.spark.sql.catalyst.optimizer.LimitPushDown
val numRows = 1024 * 1024 * 1000

spark.sql(s"CREATE TABLE t1 using parquet AS SELECT id AS a, id AS b, id AS c FROM range(1, ${numRows}L, 1, 5)")
val benchmark = new Benchmark("Push down limit through Aggregate if it is group only", numRows, minNumIters = 1)
Seq(LimitPushDown.ruleName, "").foreach { execludedRules =>
  benchmark.addCase(s"Push down ${if (execludedRules.length > 0) "disabled" else "enabled" }") { _ =>
    withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> execludedRules) {
      spark.sql("SELECT distinct * FROM t1 LIMIT 5000").write.format("noop").mode("Overwrite").save()
    }
  }
}

benchmark.run()
{code}


{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Push down limit through Aggregate if it is group only:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
Push down disabled                                            751700         751700           0          1.4         716.9       1.0X
Push down enabled                                             266913         266913           0          3.9         254.5       2.8X
{noformat}
;;;, 24/Jan/22 07:11;yumwang;Benchmark 2

{code:scala}
import org.apache.spark.benchmark.Benchmark
import org.apache.spark.sql.catalyst.optimizer.LimitPushDown
val numRows = 1024 * 1024 * 1000

spark.sql(s"CREATE TABLE t1 using parquet AS SELECT id % 10000 AS a, id % 100000 AS b, id % 1000000 AS c FROM range(1, ${numRows}L, 1, 5)")
val benchmark = new Benchmark("Push down limit through Aggregate if it is group only", numRows, minNumIters = 1)
Seq(LimitPushDown.ruleName, "").foreach { execludedRules =>
  benchmark.addCase(s"Push down ${if (execludedRules.length > 0) "disabled" else "enabled" }") { _ =>
    withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> execludedRules) {
      spark.sql("SELECT distinct * FROM t1 LIMIT 5000").write.format("noop").mode("Overwrite").save()
    }
  }
}

benchmark.run()
{code}


{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Push down limit through Aggregate if it is group only:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
Push down disabled                                            151617         151617           0          6.9         144.6       1.0X
Push down enabled                                             135625         135625           0          7.7         129.3       1.1X
{noformat}
;;;, 24/Jan/22 07:54;yumwang;Benchmark 3
{noformat}
import org.apache.spark.benchmark.Benchmark
import org.apache.spark.sql.catalyst.optimizer.LimitPushDown
val numRows = 1024 * 1024 * 1000

spark.sql(s"CREATE TABLE t1 using parquet AS SELECT id % 1000 AS a, id % 10 AS b, id % 100 AS c FROM range(1, ${numRows}L, 1, 5)")
val benchmark = new Benchmark("Push down limit through Aggregate if it is group only", numRows, minNumIters = 1)
Seq(LimitPushDown.ruleName, "").foreach { execludedRules =>
  benchmark.addCase(s"Push down ${if (execludedRules.length > 0) "disabled" else "enabled" }") { _ =>
    withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> execludedRules) {
      spark.sql("SELECT distinct * FROM t1 LIMIT 5000").write.format("noop").mode("Overwrite").save()
    }
  }
}

benchmark.run()
{noformat}


{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Push down limit through Aggregate if it is group only:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
Push down disabled                                             37318          37318           0         28.1          35.6       1.0X
Push down enabled                                              37940          37940           0         27.6          36.2       1.0X
{noformat}
;;;, 24/Jan/22 08:15;yumwang;Benchmark 4

{code:scala}
import org.apache.spark.benchmark.Benchmark
import org.apache.spark.sql.catalyst.optimizer.LimitPushDown
val numRows = 1024 * 1024 * 1000

spark.sql(s"CREATE TABLE t1 using parquet AS SELECT id % 1000 AS a, id % 10 AS b, id % 100 AS c FROM range(1, ${numRows}L, 1, 5)")
val benchmark = new Benchmark("Push down limit through Aggregate if it is group only", numRows, minNumIters = 1)
Seq(LimitPushDown.ruleName, "").foreach { execludedRules =>
  benchmark.addCase(s"Push down ${if (execludedRules.length > 0) "disabled" else "enabled" }") { _ =>
    withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> execludedRules) {
      spark.sql("SELECT distinct * FROM t1 LIMIT 10000000").write.format("noop").mode("Overwrite").save()
    }
  }
}

benchmark.run()
{code}

{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Push down limit through Aggregate if it is group only:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
Push down disabled                                             42968          42968           0         24.4          41.0       1.0X
Push down enabled                                              47639          47639           0         22.0          45.4       0.9X
{noformat}
;;;, 24/Jan/22 10:00;yumwang;Benchmark 5
{code:java}
import org.apache.spark.benchmark.Benchmark
import org.apache.spark.sql.catalyst.optimizer.LimitPushDown
val numRows = 1024 * 1024 * 1000

spark.sql(s"CREATE TABLE t1 using parquet AS SELECT id % (${numRows} / 3) AS a, id % (${numRows} / 3) AS b, id % (${numRows} / 3) AS c FROM range(1, ${numRows}L, 1, 5)")
val benchmark = new Benchmark("Push down limit through Aggregate if it is group only", numRows, minNumIters = 1)
Seq(LimitPushDown.ruleName, "").foreach { execludedRules =>
  benchmark.addCase(s"Push down ${if (execludedRules.length > 0) "disabled" else "enabled" }") { _ =>
    withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> execludedRules) {
      spark.sql("SELECT distinct * FROM t1 LIMIT 1000").write.format("noop").mode("Overwrite").save()
    }
  }
}

benchmark.run()
{code}

{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Push down limit through Aggregate if it is group only:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
Push down disabled                                            752585         752585           0          1.4         717.7       1.0X
Push down enabled                                             266496         266496           0          3.9         254.1       2.8X
{noformat}

;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 24/Jan/22 06:32;yumwang;Benchmark 1

{code:scala}
import org.apache.spark.benchmark.Benchmark
import org.apache.spark.sql.catalyst.optimizer.LimitPushDown
val numRows = 1024 * 1024 * 1000

spark.sql(s"CREATE TABLE t1 using parquet AS SELECT id AS a, id AS b, id AS c FROM range(1, ${numRows}L, 1, 5)")
val benchmark = new Benchmark("Push down limit through Aggregate if it is group only", numRows, minNumIters = 1)
Seq(LimitPushDown.ruleName, "").foreach { execludedRules =>
  benchmark.addCase(s"Push down ${if (execludedRules.length > 0) "disabled" else "enabled" }") { _ =>
    withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> execludedRules) {
      spark.sql("SELECT distinct * FROM t1 LIMIT 5000").write.format("noop").mode("Overwrite").save()
    }
  }
}

benchmark.run()
{code}


{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Push down limit through Aggregate if it is group only:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
Push down disabled                                            751700         751700           0          1.4         716.9       1.0X
Push down enabled                                             266913         266913           0          3.9         254.5       2.8X
{noformat}
;;;
Comment.2: 24/Jan/22 07:11;yumwang;Benchmark 2

{code:scala}
import org.apache.spark.benchmark.Benchmark
import org.apache.spark.sql.catalyst.optimizer.LimitPushDown
val numRows = 1024 * 1024 * 1000

spark.sql(s"CREATE TABLE t1 using parquet AS SELECT id % 10000 AS a, id % 100000 AS b, id % 1000000 AS c FROM range(1, ${numRows}L, 1, 5)")
val benchmark = new Benchmark("Push down limit through Aggregate if it is group only", numRows, minNumIters = 1)
Seq(LimitPushDown.ruleName, "").foreach { execludedRules =>
  benchmark.addCase(s"Push down ${if (execludedRules.length > 0) "disabled" else "enabled" }") { _ =>
    withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> execludedRules) {
      spark.sql("SELECT distinct * FROM t1 LIMIT 5000").write.format("noop").mode("Overwrite").save()
    }
  }
}

benchmark.run()
{code}


{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Push down limit through Aggregate if it is group only:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
Push down disabled                                            151617         151617           0          6.9         144.6       1.0X
Push down enabled                                             135625         135625           0          7.7         129.3       1.1X
{noformat}
;;;
Comment.3: 24/Jan/22 07:54;yumwang;Benchmark 3
{noformat}
import org.apache.spark.benchmark.Benchmark
import org.apache.spark.sql.catalyst.optimizer.LimitPushDown
val numRows = 1024 * 1024 * 1000

spark.sql(s"CREATE TABLE t1 using parquet AS SELECT id % 1000 AS a, id % 10 AS b, id % 100 AS c FROM range(1, ${numRows}L, 1, 5)")
val benchmark = new Benchmark("Push down limit through Aggregate if it is group only", numRows, minNumIters = 1)
Seq(LimitPushDown.ruleName, "").foreach { execludedRules =>
  benchmark.addCase(s"Push down ${if (execludedRules.length > 0) "disabled" else "enabled" }") { _ =>
    withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> execludedRules) {
      spark.sql("SELECT distinct * FROM t1 LIMIT 5000").write.format("noop").mode("Overwrite").save()
    }
  }
}

benchmark.run()
{noformat}


{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Push down limit through Aggregate if it is group only:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
Push down disabled                                             37318          37318           0         28.1          35.6       1.0X
Push down enabled                                              37940          37940           0         27.6          36.2       1.0X
{noformat}
;;;
Comment.4: 24/Jan/22 08:15;yumwang;Benchmark 4

{code:scala}
import org.apache.spark.benchmark.Benchmark
import org.apache.spark.sql.catalyst.optimizer.LimitPushDown
val numRows = 1024 * 1024 * 1000

spark.sql(s"CREATE TABLE t1 using parquet AS SELECT id % 1000 AS a, id % 10 AS b, id % 100 AS c FROM range(1, ${numRows}L, 1, 5)")
val benchmark = new Benchmark("Push down limit through Aggregate if it is group only", numRows, minNumIters = 1)
Seq(LimitPushDown.ruleName, "").foreach { execludedRules =>
  benchmark.addCase(s"Push down ${if (execludedRules.length > 0) "disabled" else "enabled" }") { _ =>
    withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> execludedRules) {
      spark.sql("SELECT distinct * FROM t1 LIMIT 10000000").write.format("noop").mode("Overwrite").save()
    }
  }
}

benchmark.run()
{code}

{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_281-b09 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Push down limit through Aggregate if it is group only:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
-------------------------------------------------------------------------------------------------------------------------------------
Push down disabled                                             42968          42968           0         24.4          41.0       1.0X
Push down enabled                                              47639          47639           0         22.0          45.4       0.9X
{noformat}
;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: dev/reformat version required warning is confused
Issue key: SPARK-37988
Issue id: 13424191
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 23/Jan/22 02:16
Updated: 23/Jan/22 02:36
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build, PySpark
Due Date: 
Votes: 0
Labels: 
Description: Oh no! 💥 💔 💥 The required version `21.12b0` does not match the running version `21.7b0`!

But I'm don't it's the warnning is for `black` before I take a look on dev/reformat code.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jan 23 02:35:58 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yuhs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Jan/22 02:35;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35284;;;
Affects Version/s.1: 
Component/s.1: PySpark
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support ALTER TABLE SWAP - atomically swap all content and metadata between two tables
Issue key: SPARK-37927
Issue id: 13422962
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: YActs
Creator: YActs
Created: 17/Jan/22 06:19
Updated: 17/Jan/22 06:21
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Add ALTER TABLE SWAP to improve user experience with Spark SQL. Other DBMSs have the feature:
 * Snowflake: [https://docs.snowflake.com/en/sql-reference/sql/alter-table.html#parameters]
 * MySQL(not exactly the same statement but the same transaction can be done): [https://dev.mysql.com/doc/refman/8.0/en/rename-table.html]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jan 17 06:21:13 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ymww:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Jan/22 06:21;YActs;I'm working on it.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Cleanup ShuffleBlockResolver from polluted methods to create a developer API
Issue key: SPARK-37881
Issue id: 13422256
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: attilapiros
Reporter: attilapiros
Creator: attilapiros
Created: 12/Jan/22 14:35
Updated: 12/Jan/22 14:42
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: ShuffleBlockResolver is intended to be part of a generic Shuffle API but currently it contains local disk specific (and pushed based shuffle) methods. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jan 12 14:42:35 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yik8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Jan/22 14:42;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/35180;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support clear shuffle dependencies eagerly for thrift server
Issue key: SPARK-37877
Issue id: 13422205
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Qin Yao
Creator: Qin Yao
Created: 12/Jan/22 10:29
Updated: 12/Jan/22 10:55
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jan 12 10:55:50 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yi94:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Jan/22 10:55;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/35178;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Skip equal check in LocIndexer._select_cols_by_iterable if eager_check disable
Issue key: SPARK-37868
Issue id: 13421980
Parent id: 13407206.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 11/Jan/22 13:29
Updated: 11/Jan/22 13:34
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jan 11 13:34:36 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ygvc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Jan/22 13:33;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35167;;;, 11/Jan/22 13:34;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35167;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 11/Jan/22 13:34;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35167;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Migrate Spark Arm Job from Jenkins to GitHub Actions
Issue key: SPARK-35607
Issue id: 13381606
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 02/Jun/21 03:37
Updated: 06/Jan/22 07:59
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Project Infra
Due Date: 
Votes: 0
Labels: 
Description: There were two Arm CI jobs in AMP lab:

[https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-python-arm/]

[https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-arm/]

As note from ML[1], AMPLab jenkins had been shut down at the end of 2021.

We should consider to migrate the Arm job from jenkins to somewhere like github action. Unfortunately, the github action doen't support native arm64 vitualenv yet.

 * Github Action self hosted
 ** Running jobs in self-hosted ARM VM.
 ** Reuse many actions and build script
 ** Easy to migrate when official GA arm support ready

 

[1] [https://www.mail-archive.com/dev@spark.apache.org/msg28480.html]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-06-02 03:37:37.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rkls:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add ARM based self-hosted runner to Apache Spark
Issue key: SPARK-37759
Issue id: 13419614
Parent id: 13381606.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 28/Dec/21 02:50
Updated: 06/Jan/22 07:55
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 0
Labels: 
Description: For Spark Test Runner:

{code:bash}
# Prepare the custom runner
mkdir actions-runner && cd actions-runner
curl -o actions-runner-linux-arm64-2.286.0-airflow1.tar.gz -L https://github.com/ashb/runner/releases/download/v2.286.0-airflow1/actions-runner-linux-arm64-2.286.0-airflow1.tar.gz
tar xzf ./actions-runner-linux-arm64-2.286.0-airflow1.tar.gz

# Configure the self-hosted runner, the Apache Infra Team will give a $TOKEN
# 8U16G for Spark
./config.sh --url https://github.com/apache/spark --token $TOKEN --labels ubuntu-20.04-arm64

# Test on connecting
./run.sh

# Start it as a service
sudo ./svc.sh install 
sudo ./svc.sh start
{code}

For PySpark Test Runner:
{code:bash}
# Prepare the custom runner
mkdir actions-runner && cd actions-runner
curl -o actions-runner-linux-arm64-2.286.0-airflow1.tar.gz -L https://github.com/ashb/runner/releases/download/v2.286.0-airflow1/actions-runner-linux-arm64-2.286.0-airflow1.tar.gz
tar xzf ./actions-runner-linux-arm64-2.286.0-airflow1.tar.gz

# Configure the self-hosted runner, the Apache Infra Team will give a $TOKEN
# 4U8G for PySpark
./config.sh --url https://github.com/apache/spark --token $TOKEN --labels ubuntu-20.04-arm64-pyspark

# Test on connecting
./run.sh

# Start it as a service
sudo ./svc.sh install 
sudo ./svc.sh start
{code}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-28 02:50:49.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y2ao:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: [PYSPARK] Enable PySpark scheduled job on ARM based self-hosted runner
Issue key: SPARK-37758
Issue id: 13419613
Parent id: 13381606.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 28/Dec/21 02:47
Updated: 06/Jan/22 03:37
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 0
Labels: 
Description: cpython 3.9

pypy3
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jan 06 03:37:50 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y2ag:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Dec/21 07:49;yikunkero;cpython 3.9 passed:

https://github.com/Yikun/spark/runs/4656963889?check_suite_focus=true;;;, 03/Jan/22 03:26;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35088;;;, 03/Jan/22 03:27;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35088;;;, 06/Jan/22 03:37;yikunkero;
{code:java}
sudo apt install docker.io
sudo gpasswd -a $USER docker
newgrp docker
{code}

The pyspark runner is based on docker, the above cmd is the complete OS packages depency.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 03/Jan/22 03:26;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35088;;;
Comment.2: 03/Jan/22 03:27;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35088;;;
Comment.3: 06/Jan/22 03:37;yikunkero;
{code:java}
sudo apt install docker.io
sudo gpasswd -a $USER docker
newgrp docker
{code}

The pyspark runner is based on docker, the above cmd is the complete OS packages depency.;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: [Scala] Enable Spark test scheduled job on ARM based self-hosted runner
Issue key: SPARK-37757
Issue id: 13419612
Parent id: 13381606.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 28/Dec/21 02:46
Updated: 06/Jan/22 03:34
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jan 06 03:34:29 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y2a8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Dec/21 02:56;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35049;;;, 29/Dec/21 02:57;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35049;;;, 30/Dec/21 07:47;yikunkero;* DistributedSuite.caching in memory, replicated (encryption = on)
 * DistributedSuite.caching on disk, replicated 2 (encryption = on)
 * DistributedSuite.caching in memory and disk, serialized, replicated (encryption = on)

These tests are flaky in ARM, all other tests passed

30355 tests run, 679 skipped, 3 failed.

[1][https://github.com/Yikun/spark/pull/47/checks?check_run_id=4652714227];;;, 06/Jan/22 03:11;yikunkero;`apt install libssl-dev`, then it's stable to passed, because need to upgrade openssl to 1.1.x, see related: https://issues.apache.org/jira/browse/CRYPTO-139;;;, 06/Jan/22 03:34;yikunkero; 
{code:java}
sudo apt update

sudo apt-get install --no-install-recommends -y software-properties-common git libxml2-dev pkg-config curl wget openjdk-8-jdk libpython3-dev python3-pip python3-setuptools python3.8 python3.9 python-is-python3 libssl-dev

sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-arm64/jre/bin/java{code}
 
The above cmd is the complete os package dependecy for spark test runner.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 29/Dec/21 02:57;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35049;;;
Comment.2: 30/Dec/21 07:47;yikunkero;* DistributedSuite.caching in memory, replicated (encryption = on)
 * DistributedSuite.caching on disk, replicated 2 (encryption = on)
 * DistributedSuite.caching in memory and disk, serialized, replicated (encryption = on)

These tests are flaky in ARM, all other tests passed

30355 tests run, 679 skipped, 3 failed.

[1][https://github.com/Yikun/spark/pull/47/checks?check_run_id=4652714227];;;
Comment.3: 06/Jan/22 03:11;yikunkero;`apt install libssl-dev`, then it's stable to passed, because need to upgrade openssl to 1.1.x, see related: https://issues.apache.org/jira/browse/CRYPTO-139;;;
Comment.4: 06/Jan/22 03:34;yikunkero; 
{code:java}
sudo apt update

sudo apt-get install --no-install-recommends -y software-properties-common git libxml2-dev pkg-config curl wget openjdk-8-jdk libpython3-dev python3-pip python3-setuptools python3.8 python3.9 python-is-python3 libssl-dev

sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-arm64/jre/bin/java{code}
 
The above cmd is the complete os package dependecy for spark test runner.;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Decommission logs too frequent when waiting migration to finish
Issue key: SPARK-36543
Issue id: 13395881
Parent id: 13069723.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: apachespark
Reporter: Ngone51
Creator: Ngone51
Created: 18/Aug/21 12:55
Updated: 04/Jan/22 09:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.0, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: {code:java}
21/08/18 08:14:31 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:31 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:31 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:32 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:32 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:32 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:33 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:33 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:33 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:34 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:34 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:34 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:35 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:35 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:35 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:36 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:36 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:36 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:37 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:37 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:37 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:38 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:38 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:38 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:39 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:39 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:39 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:40 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:40 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:40 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:41 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:41 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:41 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:42 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:42 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:42 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:43 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:43 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:43 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
21/08/18 08:14:44 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
21/08/18 08:14:44 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
21/08/18 08:14:44 INFO CoarseGrainedExecutorBackend: All blocks not yet migrated.
...{code}
It takes some time to migrate data (shuffle or rdd). Logging per second is too frequent. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jan 04 09:00:06 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u0l4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): holden
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Sep/21 19:45;holden;We could make the logging less frequent I agree. Anyone want to take this on? I'd be happy to review.;;;, 04/Jan/22 09:00;apachespark;User 'sungpeo' has created a pull request for this issue:
https://github.com/apache/spark/pull/35094;;;
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 04/Jan/22 09:00;apachespark;User 'sungpeo' has created a pull request for this issue:
https://github.com/apache/spark/pull/35094;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Performance improvement to hash joins with many duplicate keys
Issue key: SPARK-37175
Issue id: 13409348
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bersprockets
Creator: bersprockets
Created: 01/Nov/21 00:14
Updated: 29/Dec/21 02:23
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I noticed that HashedRelations with many duplicate keys perform significantly slower than HashedRelations with similar number of entries but few or no duplicate keys.

A hypothesis:
 * Because of the order in which rows are appended to the map, rows for a given key are typically non-adjacent in memory, resulting in poor locality.
 * The map would perform better if all rows for a given key are next to each other in memory.

To test this hypothesis, I made a [somewhat brute force change to HashedRelation|https://github.com/apache/spark/compare/master...bersprockets:hash_rel_play] to reorganize the map such that all rows for a given key are adjacent in memory. This yielded some performance improvements, at least in my contrived examples:

(Run on a Intel-based MacBook Pro with 4 cores/8 hyperthreads):

Example 1:
 Shuffled Hash Join, LongHashedRelation:
 Stream side: 300M rows
 Build side: 90M rows, but only 200K unique keys
 136G output rows
|Join strategy|Time (in seconds)|Notes|
|Shuffled hash join (No reorganization)|1092| |
|Shuffled hash join (with reorganization)|234|4.6 times faster than regular SHJ|
|Sort merge join|164|This beats the SHJ when there are lots of duplicate keys, I presume because of better cache locality on both sides of the join|

Example 2:
 Broadcast Hash Join, LongHashedRelation:
 Stream side: 350M rows
 Build side 9M rows, but only 18K unique keys
 175G output rows
|Join strategy|Time (in seconds)|Notes|
|Broadcast hash join (No reorganization)|872| |
|Broadcast hash join (with reorganization)|263|3 times faster than regular BHJ|
|Sort merge join|174|This beats the BHJ when there are lots of duplicate keys, I presume because of better cache locality on both sides of the join|

Example 3:
 Shuffled Hash Join, UnsafeHashedRelation
 Stream side: 300M rows
 Build side 90M rows, but only 200K unique keys
 135G output rows
|Join strategy|Time (in seconds)|Notes|
|Shuffled Hash Join (No reorganization)|3154| |
|Shuffled Hash Join (with reorganization)|533|5.9 times faster|
|Sort merge join|190|This beats the SHJ when there are lots of duplicate keys, I presume because of better cache locality on both sides of the join|

Example 4:
 Broadcast Hash Join, UnsafeHashedRelation:
 Stream side: 70M rows
 Build side 9M rows, but only 18K unique keys
 35G output rows
|Join strategy|Time (in seconds)|Notes|
|Broadcast hash join (No reorganization)|849| |
|Broadcast hash join (with reorganization)|130|6.5 times faster|
|Sort merge join|46|This beats the BHJ when there are lots of duplicate keys, I presume because of better cache locality on both sides of the join|

The code for these examples is attached here [^hash_rel_examples.txt]

Even the brute force approach could be useful in production if
 * Toggled by a feature flag
 * Reorganizes only if the ratio of keys to rows drops below some threshold
 * Falls back to using the original map if building the new map results in a memory-related SparkException.

Another incidental lesson is that sort merge join seems to outperform broadcast hash join when the build side has lots of duplicate keys. So maybe a long term improvement would be to avoid hash joins (broadcast or shuffle) if there are many duplicate keys.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 01/Nov/21 00:14;bersprockets;hash_rel_examples.txt;https://issues.apache.org/jira/secure/attachment/13035537/hash_rel_examples.txt
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Dec 29 02:23:54 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wbn4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Nov/21 02:26;sumeet.gajjar;I am working on this.;;;, 29/Dec/21 02:23;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/35047;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 29/Dec/21 02:23;apachespark;User 'sumeetgajjar' has created a pull request for this issue:
https://github.com/apache/spark/pull/35047;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add new option to CSVOption ignoreEmptyLines
Issue key: SPARK-37745
Issue id: 13419436
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: igreenfi
Creator: igreenfi
Created: 26/Dec/21 08:19
Updated: 26/Dec/21 08:19
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In many cases, user need to read full CSV file with all empty lines so it will be good to have such option the default will be true so the default behavior will not changed
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-26 08:19:28.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y174:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Create a plan for top & frequency for pandas-on-Spark optimization
Issue key: SPARK-37711
Issue id: 13418951
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: itholic
Creator: itholic
Created: 22/Dec/21 07:26
Updated: 23/Dec/21 00:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When invoking the DataFrame.describe() in pandas API on Spark, the multiple Spark job is run as much as number of columns to retrieve the `top` and `freq` status.

Top is the most common value in column, and the freq is the count of the most common value.

We should write a util in Scala side, and make it return key value (count) in one Spark job. e.g.) Dataset.mapPartitions and calculate the summation of the key and value (count). Such APIs are missing in PySpark so we would have to write one in Scala side.

See the [https://github.com/apache/spark/pull/34931#discussion_r772220260] for more detail.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-22 07:26:38.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xy80:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Cleanup direct usage of OkHttpClient
Issue key: SPARK-37687
Issue id: 13418485
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 20/Dec/21 03:06
Updated: 20/Dec/21 03:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: - There are some problem (such as IPV6 based cluster support) on okhttpclient v3, but it's a little bit diffcult to upgrade to v4 [1]

- Kubernetes client are also consider to support other clients [2] rather than single okhttpclient.

- Kubernetes client add a abstract layer [3] to address supporting httpclient, okhttp client as one of supported http clients.

 

So, we better to consider to cleanup okhttpclient direct usage and use the httpclient which kubernetes client diret supported to reduce the potential risk in future upgrade.

 

See also:

[[1]https://github.com/fabric8io/kubernetes-client/issues/2632|https://github.com/fabric8io/kubernetes-client/issues/2632]

[2][https://github.com/fabric8io/kubernetes-client/issues/3663#issuecomment-997402993]

[3] [https://github.com/fabric8io/kubernetes-client/issues/3547]

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-20 03:06:21.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xvco:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Nondeterministic type checker error in data tests
Issue key: SPARK-37683
Issue id: 13418375
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zero323
Creator: zero323
Created: 18/Dec/21 12:40
Updated: 18/Dec/21 13:14
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Occasionally, I see different errors around {{_jvm}}, {{_wrapped}} and {{_jsparkSession}} popping out in data tests

{code}
E     ../../spark/python/pyspark/sql/catalog:71: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/catalog:72: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/catalog:349: error: Cannot determine type of "_wrapped"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/catalog:360: error: Cannot determine type of "_wrapped"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/column:920: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/pandas/conversion:400: error: Cannot determine type of "_wrapped"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/pandas/conversion:407: error: Cannot determine type of "_wrapped"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/pandas/conversion:413: error: Cannot determine type of "_wrapped"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/pandas/conversion:602: error: Cannot determine type of "_wrapped"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/pandas/conversion:604: error: Cannot determine type of "_wrapped"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/pandas/conversion:620: error: Cannot determine type of "_jvm"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/pandas/conversion:621: error: Cannot determine type of "_jvm"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/pandas/conversion:622: error: Cannot determine type of "_wrapped"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/readwriter:115: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/readwriter:307: error: Cannot determine type of "_jvm"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/readwriter:308: error: Cannot determine type of "_jvm"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/session:283: error: Cannot determine type of "_jvm"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/session:284: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/streaming:377: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/udf:225: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/udf:456: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/udf:509: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/udf:510: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
E     ../../spark/python/pyspark/sql/udf:533: error: Cannot determine type of "_jsparkSession"  [has-type] (diff)
{code}

The set of errors is not always the same, so it indicates so issues with type checker.

I am going to add {{casts}} and / or annotations for these lines, so we don't get flaky tests.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Dec 18 13:14:25 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xuo8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Dec/21 12:41;zero323;cc [~hyukjin.kwon] FYI;;;, 18/Dec/21 13:14;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/34946;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 18/Dec/21 13:14;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/34946;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Flexible ways of launching executors
Issue key: SPARK-37572
Issue id: 13415941
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: functicons
Creator: functicons
Created: 08/Dec/21 02:10
Updated: 15/Dec/21 17:58
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Deploy
Due Date: 
Votes: 2
Labels: 
Description: Currently Spark launches executor processes by constructing and running commands [1], for example:
{code:java}
/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/jre/bin/java -cp /opt/spark-3.2.0-bin-hadoop3.2/conf/:/opt/spark-3.2.0-bin-hadoop3.2/jars/* -Xmx1024M -Dspark.driver.port=35729 org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@example.host:35729 --executor-id 0 --hostname 100.116.124.193 --cores 6 --app-id app-20211207131146-0002 --worker-url spark://Worker@100.116.124.193:45287 {code}
But there are use cases which require more flexible ways of launching executors. In particular, our use case is that we run Spark in standalone mode, Spark master and workers are running in VMs. We want to allow Spark app developers to provide custom container images to customize the job runtime environment (typically Java and Python dependencies), so executors (which run the job code) need to run in Docker containers.

After reading the source code, we found that the concept of Spark Command Runner might be a good solution. Basically, we want to introduce an optional Spark command runner in Spark, so that instead of running the command to launch executor directly, it passes the command to the runner, the runner then runs the command with its own strategy which could be running in Docker, or by default running the command directly.

The runner is specified through an env variable `SPARK_COMMAND_RUNNER`, which by default could be a simple script like:
{code:java}
#!/bin/bash
exec "$@" {code}
or in the case of Docker container:
{code:java}
#!/bin/bash
docker run ... – "$@" {code}
 

I already have a patch for this feature and have tested in our environment.

 

[1]: [https://github.com/apache/spark/blob/v3.2.0/core/src/main/scala/org/apache/spark/deploy/worker/CommandUtils.scala#L52]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-08 02:10:25.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xg54:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Fall back to SortMergeJoin if BroadcastHashJoin has OOM issue
Issue key: SPARK-37642
Issue id: 13417315
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 14/Dec/21 08:21
Updated: 14/Dec/21 08:21
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: Fall back to SortMergeJoin if BroadcastHashJoin has OOM issue.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-14 08:21:05.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xo4w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Migrate SHOW TABLES EXTENDED to use v2 command by default
Issue key: SPARK-37637
Issue id: 13417271
Parent id: 13397201.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: imback82
Creator: imback82
Created: 14/Dec/21 05:12
Updated: 14/Dec/21 05:12
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Migrate SHOW TABLES EXTENDED to use v2 command by default
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Dec 14 05:12:52 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xnv4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Dec/21 05:12;imback82;Working on this.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support window frame ORDER BY i ROWS BETWEEN (('foo' < 'foobar')::integer) PRECEDING AND CURRENT ROW
Issue key: SPARK-37612
Issue id: 13416743
Parent id: 13276725.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: beliefer
Creator: beliefer
Created: 11/Dec/21 03:31
Updated: 11/Dec/21 03:32
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: SELECT array_agg(i) OVER w
FROM range(1,6) i
WINDOW w AS (ORDER BY i ROWS BETWEEN (('foo' < 'foobar')::integer) PRECEDING AND CURRENT ROW);
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-11 03:31:06.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xkls:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: ShuffledRowRDD get preferred locations order by reduce size
Issue key: SPARK-37559
Issue id: 13415568
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ulysses
Creator: ulysses
Created: 06/Dec/21 11:51
Updated: 06/Dec/21 12:05
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The coalesced partition can contain several reduce partitions. The preferred locations of the RDD partition should be the biggest reduce partition before coalesced. So it can get a better data locality and reduce the network traffic.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Dec 06 12:04:57 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xdug:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Dec/21 12:04;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34820;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support set parallel through data source properties
Issue key: SPARK-37549
Issue id: 13415455
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 05/Dec/21 12:15
Updated: 05/Dec/21 13:08
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: {code:scala}
spark.read.option("Parallel", 10).parquet("path/to/parquet")
{code}

{code:sql}
CREATE TABLE test_parallel (
  foo STRING,
  bar STRING)
USING parquet
OPTIONS (
  PARALLEL '1000'
)
{code}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Dec 05 13:08:20 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xd5k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Dec/21 13:07;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34810;;;, 05/Dec/21 13:08;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34810;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 05/Dec/21 13:08;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34810;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Replace single projection Expand with Project
Issue key: SPARK-37538
Issue id: 13415304
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tanelk
Creator: tanelk
Created: 03/Dec/21 15:02
Updated: 03/Dec/21 15:12
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Dec 03 15:12:51 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xc80:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Dec/21 15:12;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/34800;;;, 03/Dec/21 15:12;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/34800;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 03/Dec/21 15:12;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/34800;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Change default spark.io.compression.codec to zstd
Issue key: SPARK-37535
Issue id: 13415241
Parent id: 13362809.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 03/Dec/21 09:51
Updated: 03/Dec/21 10:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: To workaround this issue:
{noformat}
org.apache.spark.shuffle.FetchFailedException: Stream is corrupted
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:830)
	at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:926)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.sparkproject.guava.io.ByteStreams.read(ByteStreams.java:899)
	at org.sparkproject.guava.io.ByteStreams.readFully(ByteStreams.java:733)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:127)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:494)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:50)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:730)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:255)
	at org.apache.spark.sql.execution.SortExecBase.$anonfun$doExecute$1(SortExec.scala:266)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:913)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:913)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:388)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:315)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:129)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:486)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1379)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:489)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Stream is corrupted
	at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:259)
	at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)
	at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:922)
	... 32 more
{noformat}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Dec 03 10:11:12 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xbu8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Dec/21 10:11;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34798;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support cast aware output partitioning and required if it can up cast
Issue key: SPARK-37502
Issue id: 13414484
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ulysses
Creator: ulysses
Created: 30/Nov/21 08:37
Updated: 30/Nov/21 09:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: If a `Cast` is up cast then it should be without any truncating or precision lose or possible runtime failures. So the output partitioning should be same with/without `Cast` if the `Cast` is up cast.

Let's say we have a query:
{code:java}
-- v1: c1 int
-- v2: c2 long

SELECT * FROM v2 JOIN (SELECT c1, count(*) FROM v1 GROUP BY c1) v1 ON v1.c1 = v2.c2
{code}
The executed plan contains three shuffle nodes which looks like:
{code:java}
SortMergeJoin
  Exchange(cast(c1 as bigint))
    HashAggregate
      Exchange(c1)
        Scan v1
  Exchange(c2)
    Scan v2
{code}
We can simplify the plan using two shuffle nodes:
{code:java}
SortMergeJoin
  HashAggregate
    Exchange(c1)
      Scan v1
  Exchange(c2)
    Scan v2
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Nov 30 09:00:37 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0x76g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Nov/21 09:00;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34755;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support subexpression elimination in lambda functions
Issue key: SPARK-37466
Issue id: 13413921
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kimahriman
Creator: kimahriman
Created: 26/Nov/21 00:44
Updated: 26/Nov/21 00:44
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: https://issues.apache.org/jira/browse/SPARK-37019 will add codegen support for higher order functions. However we can't support subexpression elimination inside of lambda functions because subexpressions are evaluated once per row at the beginning of the codegen. Common expressions inside lambda functions can easily result in performance degradation due to multiple evaluations of the same expression. Subexpression elimination inside of lambda functions needs to be handled specially to be evaluated once per function invocation.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-26 00:44:32.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0x3pc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: pyspark.{ml, mllib}.linalg.Vector API
Issue key: SPARK-37431
Issue id: 13413019
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zero323
Creator: zero323
Created: 21/Nov/21 13:50
Updated: 21/Nov/21 13:51
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: ML, MLlib, PySpark
Due Date: 
Votes: 0
Labels: 
Description: At the moment both {{Vector}} implementations have minimal API. {{mllib}}

{code:python}
class Vector(object):
    __UDT__ = ....
    def toArray(self): ...
    def asML(self): ...
{code}

{{ml}}

{code:python}
class Vector(object):
    __UDT__ = ...
    def toArray(self): ...

{code}

which doesn't cover actual API being used.

This causes typing issues, when we expect any {{Vector}}.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-37234
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): https://github.com/apache/spark/pull/34513#discussion_r744444312
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-21 13:50:37.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wy4w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: MLlib
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Inline type hints for files in python/pyspark/mllib
Issue key: SPARK-37233
Issue id: 13410500
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: dchvn
Reporter: dchvn
Creator: dchvn
Created: 08/Nov/21 01:53
Updated: 21/Nov/21 11:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): SPARK-37396
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-08 01:53:01.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wim0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark K8s tests are broken
Issue key: SPARK-37359
Issue id: 13412355
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: holden
Reporter: holden
Creator: holden
Created: 17/Nov/21 20:47
Updated: 17/Nov/21 20:54
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: Kubernetes, Tests
Due Date: 
Votes: 0
Labels: 
Description: For decom: We should look for "Decommission executors" instead of "Remove reason statistics:", the listenerbus doesn't have strong guarantees and depending on it makes this test flaky.

 

For R : It has not passed in weeks, lets disable it.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Nov 17 20:53:27 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wu1c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Nov/21 20:52;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/34636;;;, 17/Nov/21 20:53;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/34636;;;
Affects Version/s.1: 3.2.1
Component/s.1: Tests
Comment.1: 17/Nov/21 20:53;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/34636;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Unify v1 and v2 ALTER TABLE .. REPLACE COLUMNS tests
Issue key: SPARK-37306
Issue id: 13411492
Parent id: 13339335.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: imback82
Creator: maxgekk
Created: 12/Nov/21 14:17
Updated: 12/Nov/21 14:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Extract ALTER TABLE .. REPLACE COLUMNS tests to the common place to run them for V1 and v2 datasources. Some tests can be places to V1 and V2 specific test suites.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): SPARK-37045
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-12 14:17:31.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0woq8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Unify v1 and v2 ALTER TABLE .. ADD COLUMNS tests
Issue key: SPARK-37045
Issue id: 13407116
Parent id: 13339335.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: maxgekk
Reporter: imback82
Creator: maxgekk
Created: 18/Oct/21 16:59
Updated: 12/Nov/21 14:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Extract ALTER TABLE .. ADD COLUMNS tests to the common place to run them for V1 and v2 datasources. Some tests can be places to V1 and V2 specific test suites.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): SPARK-37306
Outward issue link (Cloners): SPARK-37031
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Nov 10 14:01:11 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vxv4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Nov/21 14:01;maxgekk;I am working on this.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Run PyArrow tests on Python 3.10
Issue key: SPARK-37246
Issue id: 13410714
Parent id: 
Issue Type: Bug
Status: Reopened
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: XinrongM
Creator: XinrongM
Created: 08/Nov/21 23:54
Updated: 09/Nov/21 16:13
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Though pyarrow doesn’t support python 3.10 yet [https://pypi.org/project/pyarrow/,] we wanted to adjust PySpark codes gradually to support Python 3.10.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Nov 09 16:12:48 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wjxk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Nov/21 01:28;dongjoon;Thank you for leading this, [~XinrongM].;;;, 09/Nov/21 02:51;dongjoon;Hi, may I ask the reason why you filed JIRAs and marked as `Invalid` immediately, [~XinrongM]? I expected that you are going to make a PR for them. 
cc [~hyukjin.kwon];;;, 09/Nov/21 03:14;XinrongM;Oh sorry for the confusion. I was about to resolve the issue.

Test failures were due to my environmental issue.

Now they pass on my local after rebuilding Spark.

CC [~dongjoon] ;;;, 09/Nov/21 16:12;dongjoon;Never mind. This is also valuable. I converted this umbrella JIRA into a new issue `Run PyArrow tests on Python 3.10` to reuse the JIRA ID.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 09/Nov/21 02:51;dongjoon;Hi, may I ask the reason why you filed JIRAs and marked as `Invalid` immediately, [~XinrongM]? I expected that you are going to make a PR for them. 
cc [~hyukjin.kwon];;;
Comment.2: 09/Nov/21 03:14;XinrongM;Oh sorry for the confusion. I was about to resolve the issue.

Test failures were due to my environmental issue.

Now they pass on my local after rebuilding Spark.

CC [~dongjoon] ;;;
Comment.3: 09/Nov/21 16:12;dongjoon;Never mind. This is also valuable. I converted this umbrella JIRA into a new issue `Run PyArrow tests on Python 3.10` to reuse the JIRA ID.;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Filter push down through window
Issue key: SPARK-37226
Issue id: 13410404
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: yumwang
Reporter: yumwang
Creator: yumwang
Created: 06/Nov/21 15:48
Updated: 06/Nov/21 16:38
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: {code:scala}
spark.sql("CREATE TABLE t1 using parquet as select id as a, id as b from range(1000)")
spark.sql("select * from (SELECT a, count(*) cnt, row_number() over (order by a desc) as rn from t1 group by a) where rn <= 10").explain(true)
{code}

We can optimize this query:
{noformat}
== Optimized Logical Plan ==
Filter (rn#4 <= 10)
+- Window [row_number() windowspecdefinition(a#7L DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rn#4], [a#7L DESC NULLS LAST]
   +- GlobalLimit 10
      +- LocalLimit 10
         +- Sort [a#7L DESC NULLS LAST], true
            +- Aggregate [a#7L], [a#7L, count(1) AS cnt#3L]
               +- Project [a#7L]
                  +- Relation default.t1[a#7L,b#8L] parquet
{noformat}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Nov 06 16:38:50 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wi5c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Nov/21 16:38;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34504;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support Application Timeouts on YARN
Issue key: SPARK-37215
Issue id: 13410208
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Qin Yao
Creator: Qin Yao
Created: 05/Nov/21 09:23
Updated: 05/Nov/21 09:41
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: YARN
Due Date: 
Votes: 0
Labels: 
Description: Since #YARN-3813/2.9.0/3.0.0, YARN supports ApplicationTimeouts. It helps enforce lifetime application SLAs. Currently, lifetime indicates the overall time spent by an application in YARN. It is calculated from its submit time to finish time, including running time and the waiting time for resource allocation.

YARN allows admins to set lifetime of an application at leaf-queue. It also allows users to set it programmatically. During application submission, user can set it in  ApplicationSubmissionContext#setApplicationTimeouts(Map<ApplicationTimeoutType, Long> applicationTimeouts).

 

So far, YARN supports for one timeout type - LIFETIME 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Nov 05 09:41:24 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wgxs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Nov/21 09:40;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34491;;;, 05/Nov/21 09:41;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34491;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 05/Nov/21 09:41;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34491;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve error messages under ANSI mode
Issue key: SPARK-37148
Issue id: 13408954
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: allisonwang-db
Creator: allisonwang-db
Created: 28/Oct/21 17:23
Updated: 28/Oct/21 17:23
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Improve error messages when ANSI mode is enabled. Many exceptions thrown under ANSI mode can be disruptive to users. We should provide clear error messages with workarounds.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-10-28 17:23:32.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0w97k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add __all__ to pyspark/pandas/*/__init__.py
Issue key: SPARK-37142
Issue id: 13408831
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dchvn
Creator: dchvn
Created: 28/Oct/21 06:36
Updated: 28/Oct/21 07:16
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Oct 28 07:16:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0w8g8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Oct/21 07:16;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34416;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Cancel all running job after AQE plan finished
Issue key: SPARK-37043
Issue id: 13407062
Parent id: 13407393.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ulysses
Creator: ulysses
Created: 18/Oct/21 12:02
Updated: 26/Oct/21 01:20
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: We see stage was still running after AQE plan finished. This is because the plan which contains a empty join has been converted to `LocalTableScanExec` during `AQEOptimizer`, but the other side of this join is still running (shuffle map stage).

 

It's no meaning to keep running the stage, It's better to cancel the running stage after AQE plan finished in case wasting the task resource.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Oct 26 01:20:34 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vxj4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Oct/21 12:30;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34316;;;, 19/Oct/21 22:36;xkrogen;[~ulysses] any concerns if I make this a sub-task of SPARK-37063 to track it alongside other AQE fixes?;;;, 25/Oct/21 15:43;xkrogen;Converted to subtask of SPARK-37063.;;;, 26/Oct/21 01:20;ulysses;[~xkrogen] sorry for the late reply, I agree with mark this issue as the subtask of SPARK-37063.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 19/Oct/21 22:36;xkrogen;[~ulysses] any concerns if I make this a sub-task of SPARK-37063 to track it alongside other AQE fixes?;;;
Comment.2: 25/Oct/21 15:43;xkrogen;Converted to subtask of SPARK-37063.;;;
Comment.3: 26/Oct/21 01:20;ulysses;[~xkrogen] sorry for the late reply, I agree with mark this issue as the subtask of SPARK-37063.;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add Spark error classes to error message and enforce test coverage
Issue key: SPARK-37092
Issue id: 13407857
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: karenfeng
Creator: karenfeng
Created: 22/Oct/21 00:45
Updated: 22/Oct/21 00:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: We should add Spark error classes to the error message and make sure that all error classes are tested. This will help us understand the error cases, remove dead code, and improve the error messages and classes as we refactor the error messages.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Oct 22 00:48:15 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0w2fs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Oct/21 00:48;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/34342;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Push extra predicates through non-join
Issue key: SPARK-37074
Issue id: 13407578
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tanelk
Creator: tanelk
Created: 20/Oct/21 15:54
Updated: 20/Oct/21 16:36
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: In the Optimizer we could partially push some predicates through a non-join nodes, that produce new columns: Aggregate, Generate, Window.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 20 16:36:44 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0w0ps:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Oct/21 16:36;apachespark;User 'tanelk' has created a pull request for this issue:
https://github.com/apache/spark/pull/34339;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Pull out ordering expressions
Issue key: SPARK-36763
Issue id: 13401246
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 15/Sep/21 06:26
Updated: 20/Oct/21 02:36
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Similar to [PullOutGroupingExpressions|https://github.com/apache/spark/blob/7fd3f8f9ec55b364525407213ba1c631705686c5/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PullOutGroupingExpressions.scala#L48]. We can pull out ordering expressions to improve order performance. For example:
{code:scala}
sql("create table t1(a int, b int) using parquet")
sql("insert into t1 values (1, 2)")
sql("insert into t1 values (3, 4)")
sql("select * from t1 order by a - b").explain
{code}
{noformat}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Sort [(a#12 - b#13) ASC NULLS FIRST], true, 0
   +- Exchange rangepartitioning((a#12 - b#13) ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [id=#39]
      +- FileScan parquet default.t1[a#12,b#13]
{noformat}
The {{Subtract}} will be evaluated 4 times.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 20 02:36:32 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uxoo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Sep/21 11:15;Rabbid;I'm working on this.:D;;;, 22/Sep/21 02:45;yumwang;Benchmark and benchmark result:
{code:scala}
import org.apache.spark.benchmark.Benchmark
val numRows = 1024 * 1024 * 10
spark.sql(s"CREATE TABLE t1 using parquet AS select id AS a, id AS b FROM range(${numRows}L)")
val benchmark = new Benchmark("Benchmark pull out ordering expressions", numRows, minNumIters = 5)

Seq(false, true).foreach { pullOutEnabled =>
  val name = s"Pull out ordering expressions ${if (pullOutEnabled) "(Enabled)" else "(Disabled)"}"
  benchmark.addCase(name) { _ =>
    withSQLConf("spark.sql.pullOutOrderingExpressions" -> s"$pullOutEnabled") {
      spark.sql("SELECT t1.* FROM t1 ORDER BY translate(t1.a, '123', 'abc')").write.format("noop").mode("Overwrite").save()
    }
  }
}
benchmark.run()
{code}
{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark pull out ordering expressions:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Pull out ordering expressions (Disabled)           9232           9753         867          1.1         880.4       1.0X
Pull out ordering expressions (Enabled)            7084           7462         370          1.5         675.5       1.3X
{noformat};;;, 20/Oct/21 02:36;apachespark;User 'RabbidHY' has created a pull request for this issue:
https://github.com/apache/spark/pull/34334;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 22/Sep/21 02:45;yumwang;Benchmark and benchmark result:
{code:scala}
import org.apache.spark.benchmark.Benchmark
val numRows = 1024 * 1024 * 10
spark.sql(s"CREATE TABLE t1 using parquet AS select id AS a, id AS b FROM range(${numRows}L)")
val benchmark = new Benchmark("Benchmark pull out ordering expressions", numRows, minNumIters = 5)

Seq(false, true).foreach { pullOutEnabled =>
  val name = s"Pull out ordering expressions ${if (pullOutEnabled) "(Enabled)" else "(Disabled)"}"
  benchmark.addCase(name) { _ =>
    withSQLConf("spark.sql.pullOutOrderingExpressions" -> s"$pullOutEnabled") {
      spark.sql("SELECT t1.* FROM t1 ORDER BY translate(t1.a, '123', 'abc')").write.format("noop").mode("Overwrite").save()
    }
  }
}
benchmark.run()
{code}
{noformat}
Java HotSpot(TM) 64-Bit Server VM 1.8.0_251-b08 on Mac OS X 10.15.7
Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
Benchmark pull out ordering expressions:  Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
Pull out ordering expressions (Disabled)           9232           9753         867          1.1         880.4       1.0X
Pull out ordering expressions (Enabled)            7084           7462         370          1.5         675.5       1.3X
{noformat};;;
Comment.2: 20/Oct/21 02:36;apachespark;User 'RabbidHY' has created a pull request for this issue:
https://github.com/apache/spark/pull/34334;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: when we change the complex column's comment of hive table , it should succeed but fails
Issue key: SPARK-36955
Issue id: 13405555
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Resol1992
Creator: Resol1992
Created: 08/Oct/21 07:16
Updated: 09/Oct/21 01:29
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 1. create a hive table
{code:java}
CREATE TABLE tbl
STORED AS PARQUET
AS SELECT
 '1st' AS key,
 '2nd' AS value,
 map('name', 'Bob') as name;
{code}
2. alter complex column's comment
{code:java}
ALTER TABLE tbl CHANGE COLUMN name name MAP<STRING, STRING> COMMENT 'temp comment';
{code}
it fails with the following error:
{code:java}
ALTER TABLE CHANGE COLUMN is not supported for changing column 'name' with type 'MapType(StringType,StringType,false)' to 'name' with type 'MapType(StringType,StringType,true)'ALTER TABLE CHANGE COLUMN is not supported for changing column 'name' with type 'MapType(StringType,StringType,false)' to 'name' with type 'MapType(StringType,StringType,true)'org.apache.spark.sql.AnalysisException: ALTER TABLE CHANGE COLUMN is not supported for changing column 'name' with type 'MapType(StringType,StringType,false)' to 'name' with type 'MapType(StringType,StringType,true)' at org.apache.spark.sql.execution.command.AlterTableChangeColumnCommand.run(ddl.scala:349) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84) at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97) at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457) at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93) at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80) at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220) at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:92) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89) at org.apache.spark.sql.hive.test.TestHiveSparkSession.$anonfun$sql$1(TestHive.scala:240) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775) at org.apache.spark.sql.hive.test.TestHiveSparkSession.sql(TestHive.scala:238) at org.apache.spark.sql.test.SQLTestUtilsBase.$anonfun$sql$1(SQLTestUtils.scala:231) at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$new$161(StatisticsSuite.scala:1546) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462) at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:305) at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:303) at org.apache.spark.sql.StatisticsCollectionTestBase.withTable(StatisticsCollectionTestBase.scala:42) at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$new$160(StatisticsSuite.scala:1535) at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$new$160$adapted(StatisticsSuite.scala:1534) at org.apache.spark.sql.catalyst.plans.SQLHelper.withTempPath(SQLHelper.scala:69) at org.apache.spark.sql.catalyst.plans.SQLHelper.withTempPath$(SQLHelper.scala:66) at org.apache.spark.sql.QueryTest.withTempPath(QueryTest.scala:34) at org.apache.spark.sql.hive.StatisticsSuite.$anonfun$new$159(StatisticsSuite.scala:1534) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226) at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:190) at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224) at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236) at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218) at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:62) at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234) at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227) at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:62) at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413) at scala.collection.immutable.List.foreach(List.scala:431) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475) at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269) at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268) at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563) at org.scalatest.Suite.run(Suite.scala:1112) at org.scalatest.Suite.run$(Suite.scala:1094) at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1563) at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273) at org.scalatest.SuperEngine.runImpl(Engine.scala:535) at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273) at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272) at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:62) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:62) at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45) at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1322) at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1316) at scala.collection.immutable.List.foreach(List.scala:431) at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1316) at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:993) at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:971) at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1482) at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:971) at org.scalatest.tools.Runner$.run(Runner.scala:798) at org.scalatest.tools.Runner.run(Runner.scala) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:133) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:27)
{code}
3. If we change the simple column's comment, like column `key`, it succeeds.
{code:java}
ALTER TABLE tbl CHANGE COLUMN key key STRING COMMENT 'temp comment';
{code}
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-10-08 07:16:11.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vo94:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Implement Series.combine
Issue key: SPARK-36402
Issue id: 13393351
Parent id: 13393332.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: XinrongM
Creator: XinrongM
Created: 03/Aug/21 23:17
Updated: 07/Oct/21 11:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Oct 07 11:18:56 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tkyw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Aug/21 11:47;dc-heros;working on this;;;, 27/Aug/21 11:46;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33858;;;, 27/Aug/21 11:46;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33858;;;, 07/Oct/21 11:18;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34212;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 27/Aug/21 11:46;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33858;;;
Comment.2: 27/Aug/21 11:46;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33858;;;
Comment.3: 07/Oct/21 11:18;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34212;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Group exception messages in core/resource
Issue key: SPARK-36096
Issue id: 13389205
Parent id: 13342440.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: allisonwang-db
Creator: allisonwang-db
Created: 12/Jul/21 18:14
Updated: 06/Oct/21 03:04
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 'core/src/main/scala/org/apache/spark/resource'
|| Filename                            ||   Count ||
| ResourceAllocator.scala             |       4 |
| ResourceDiscoveryScriptPlugin.scala |       3 |
| ResourceInformation.scala           |       2 |
| ResourceProfile.scala               |       4 |
| ResourceProfileManager.scala        |       2 |
| ResourceUtils.scala                 |      11 |

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 06 03:04:12 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0svf4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Jul/21 09:18;apachespark;User 'Viethd27' has created a pull request for this issue:
https://github.com/apache/spark/pull/33320;;;, 13/Jul/21 09:19;apachespark;User 'Viethd27' has created a pull request for this issue:
https://github.com/apache/spark/pull/33320;;;, 28/Jul/21 07:36;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33554;;;, 06/Oct/21 03:04;apachespark;User 'phamhuyhoang97' has created a pull request for this issue:
https://github.com/apache/spark/pull/34192;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 13/Jul/21 09:19;apachespark;User 'Viethd27' has created a pull request for this issue:
https://github.com/apache/spark/pull/33320;;;
Comment.2: 28/Jul/21 07:36;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33554;;;
Comment.3: 06/Oct/21 03:04;apachespark;User 'phamhuyhoang97' has created a pull request for this issue:
https://github.com/apache/spark/pull/34192;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Group exception messages in core/status
Issue key: SPARK-36100
Issue id: 13389210
Parent id: 13342440.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: allisonwang-db
Creator: allisonwang-db
Created: 12/Jul/21 18:18
Updated: 06/Oct/21 02:39
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 'core/src/main/scala/org/apache/spark/status'
|| Filename             ||   Count ||
| AppStatusStore.scala |       3 |
| KVUtils.scala        |       1 |
| LiveEntity.scala     |       2 |


'core/src/main/scala/org/apache/spark/status/api/v1'
|| Filename                     ||   Count ||
| ApiRootResource.scala        |       4 |
| OneApplicationResource.scala |      11 |
| SimpleDateParam.scala        |       1 |
| StagesResource.scala         |       4 |

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 06 02:39:20 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0svg8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Jul/21 07:37;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33555;;;, 06/Oct/21 02:38;apachespark;User 'phamhuyhoang97' has created a pull request for this issue:
https://github.com/apache/spark/pull/34191;;;, 06/Oct/21 02:39;apachespark;User 'phamhuyhoang97' has created a pull request for this issue:
https://github.com/apache/spark/pull/34191;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 06/Oct/21 02:38;apachespark;User 'phamhuyhoang97' has created a pull request for this issue:
https://github.com/apache/spark/pull/34191;;;
Comment.2: 06/Oct/21 02:39;apachespark;User 'phamhuyhoang97' has created a pull request for this issue:
https://github.com/apache/spark/pull/34191;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Group exception messages in core/deploy
Issue key: SPARK-36102
Issue id: 13389212
Parent id: 13342440.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: allisonwang-db
Creator: allisonwang-db
Created: 12/Jul/21 18:20
Updated: 06/Oct/21 01:51
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 'core/src/main/scala/org/apache/spark/deploy'
|| Filename                      ||   Count ||
| FaultToleranceTest.scala      |       1 |
| PythonRunner.scala            |       1 |
| RRunner.scala                 |       2 |
| SparkHadoopUtil.scala         |       2 |
| SparkSubmit.scala             |       7 |
| SparkSubmitArguments.scala    |       3 |
| StandaloneResourceUtils.scala |       1 |


'core/src/main/scala/org/apache/spark/deploy/history'
|| Filename                         ||   Count ||
| ApplicationCache.scala           |       2 |
| EventLogFileWriters.scala        |       2 |
| FsHistoryProvider.scala          |       5 |
| HistoryServer.scala              |       2 |
| HistoryServerMemoryManager.scala |       1 |


'core/src/main/scala/org/apache/spark/deploy/master'
|| Filename     ||   Count ||
| Master.scala |       2 |


'core/src/main/scala/org/apache/spark/deploy/rest'
|| Filename                        ||   Count ||
| RestSubmissionClient.scala      |      11 |
| StandaloneRestServer.scala      |       2 |
| SubmitRestProtocolMessage.scala |       5 |
| SubmitRestProtocolRequest.scala |       1 |


'core/src/main/scala/org/apache/spark/deploy/security'
|| Filename                              ||   Count ||
| HadoopFSDelegationTokenProvider.scala |       1 |


'core/src/main/scala/org/apache/spark/deploy/worker'
|| Filename           ||   Count ||
| DriverRunner.scala |       2 |
| Worker.scala       |       3 |


'core/src/main/scala/org/apache/spark/deploy/worker/ui'
|| Filename      ||   Count ||
| LogPage.scala |       2 |
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 06 01:51:58 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0svgo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Jul/21 14:18;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33540;;;, 06/Oct/21 01:51;apachespark;User 'thangnd197' has created a pull request for this issue:
https://github.com/apache/spark/pull/34189;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 06/Oct/21 01:51;apachespark;User 'thangnd197' has created a pull request for this issue:
https://github.com/apache/spark/pull/34189;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Group exception messages in core/util
Issue key: SPARK-36099
Issue id: 13389208
Parent id: 13342440.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: allisonwang-db
Creator: allisonwang-db
Created: 12/Jul/21 18:17
Updated: 06/Oct/21 01:42
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 'core/src/main/scala/org/apache/spark/util'
|| Filename                 ||   Count ||
| AccumulatorV2.scala      |       4 |
| ClosureCleaner.scala     |       1 |
| DependencyUtils.scala    |       1 |
| KeyLock.scala            |       1 |
| ListenerBus.scala        |       1 |
| NextIterator.scala       |       1 |
| SerializableBuffer.scala |       2 |
| ThreadUtils.scala        |       4 |
| Utils.scala              |      16 |


'core/src/main/scala/org/apache/spark/util/collection'
|| Filename              ||   Count ||
| AppendOnlyMap.scala   |       1 |
| CompactBuffer.scala   |       1 |
| ImmutableBitSet.scala |       6 |
| MedianHeap.scala      |       1 |
| OpenHashSet.scala     |       2 |


'core/src/main/scala/org/apache/spark/util/io'
|| Filename                ||   Count ||
| ChunkedByteBuffer.scala |       1 |


'core/src/main/scala/org/apache/spark/util/logging'
|| Filename           ||   Count ||
| DriverLogger.scala |       1 |


'core/src/main/scala/org/apache/spark/util/random'
|| Filename            ||   Count ||
| RandomSampler.scala |       1 |
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 06 01:42:45 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0svfs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Jul/21 07:54;Shockang;[~allisonwang-db] Could I working on this issue?;;;, 16/Jul/21 20:38;allisonwang-db;[~Shockang] Yes of course!;;;, 27/Jul/21 07:03;dc-heros;[~Shockang] how is your progress, I already have the work done on my local repo and will make a pull request soon;;;, 27/Jul/21 08:43;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33534;;;, 27/Jul/21 14:23;Shockang;You should tell me in advance.My code has written more than 200 lines and is preparing to submit pr….[~dc-heros];;;, 27/Jul/21 14:25;Shockang;I think you should ask my permission, or my time will be wasted.[~dc-heros];;;, 27/Jul/21 14:35;Shockang;I’m sorry for my gaffe.[~dc-heros];;;, 27/Jul/21 14:41;Shockang;It's good for you to wait a day. I was going to submit the PR tonight. Why is it so coincidental![~dc-heros];;;, 28/Jul/21 01:27;dc-heros;Sorry I wasn't checking the comment recently, I've done the work for the spark core but didn't create a pull request because I've been waiting for the approve in SPARK-36095.

Again, truly sorry for your wasted time.  [~Shockang];;;, 06/Oct/21 01:42;apachespark;User 'thangnd197' has created a pull request for this issue:
https://github.com/apache/spark/pull/34188;;;, 06/Oct/21 01:42;apachespark;User 'thangnd197' has created a pull request for this issue:
https://github.com/apache/spark/pull/34188;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 16/Jul/21 20:38;allisonwang-db;[~Shockang] Yes of course!;;;, 06/Oct/21 01:42;apachespark;User 'thangnd197' has created a pull request for this issue:
https://github.com/apache/spark/pull/34188;;;
Comment.2: 27/Jul/21 07:03;dc-heros;[~Shockang] how is your progress, I already have the work done on my local repo and will make a pull request soon;;;
Comment.3: 27/Jul/21 08:43;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33534;;;
Comment.4: 27/Jul/21 14:23;Shockang;You should tell me in advance.My code has written more than 200 lines and is preparing to submit pr….[~dc-heros];;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Allow coercing of an interval expression to a specific interval type 
Issue key: SPARK-36923
Issue id: 13404758
Parent id: 13234582.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: maxgekk
Creator: maxgekk
Created: 04/Oct/21 09:51
Updated: 04/Oct/21 09:51
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Interval expressions should allow for coercing an expression returning an interval to a specific interval type. E.g. (date1 - date2) INTERVAL DAY
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-10-04 09:51:24.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vjcg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Migrate all ParsedStatement to the new v2 command framework
Issue key: SPARK-36586
Issue id: 13397195
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: cloud_fan
Creator: cloud_fan
Created: 25/Aug/21 14:02
Updated: 28/Sep/21 01:51
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The ParsedStatement needs to be pattern matched in two analyzer rules and results to a lot of duplicated code.

The new v2 command framework defines a few basic logical plan nodes such as UnresolvedTable, and we only need to resolve these basic nodes, and pattern match v2 commands only once in the rule `ResolveSessionCatalog` for v1 command fallback.

We should migrate all the ParsedStatement to v2 command framework.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Sep 28 01:21:10 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u8p4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Aug/21 14:11;cloud_fan;[~imback82] do you want to drive this?;;;, 28/Sep/21 01:21;imback82;[~cloud_fan] totally missed this ping (never got an email notification). :) Looks like the work has started and I will also chip in. Thanks!;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 28/Sep/21 01:21;imback82;[~cloud_fan] totally missed this ping (never got an email notification). :) Looks like the work has started and I will also chip in. Thanks!;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Stop task result getter properly on spark context stopping
Issue key: SPARK-36842
Issue id: 13403118
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: lxian2
Creator: lxian2
Created: 24/Sep/21 10:51
Updated: 24/Sep/21 11:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: org.apache.spark.scheduler.TaskSchedulerImpl#stop doesn't handle exception properly. If one component throws exceptions on stopping, the exception is thrown and TaskSchedulerImpl.stop() will not be executed completely.

For example if backend.stop() fails, then taskResultGetter.stop() won't be executed. The result is that after a couple of restart of the spark context, there will be a lot of '

task-result-getter' threads retained.

 

!image-2021-09-24-18-50-57-072.png!

!image-2021-09-24-18-51-03-837.png!

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Sep 24 11:10:30 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v988:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Sep/21 11:10;apachespark;User 'lxian' has created a pull request for this issue:
https://github.com/apache/spark/pull/34098;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support DPP if there is no selective predicate on the filtering side
Issue key: SPARK-36840
Issue id: 13403054
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 24/Sep/21 06:08
Updated: 24/Sep/21 06:41
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Sep 24 06:41:00 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v8u0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Sep/21 06:41;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34070;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Remove Guava from Spark binary distribution
Issue key: SPARK-36828
Issue id: 13402795
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: csun
Creator: csun
Created: 22/Sep/21 21:19
Updated: 22/Sep/21 21:19
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 0
Labels: 
Description: After SPARK-36676, we should consider removing Guava from Spark's binary distribution. It is currently only required by a few libraries such as curator-client.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): SPARK-36676
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-09-22 21:19:06.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v78g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Create shaded Hive module and upgrade to higher version of Guava
Issue key: SPARK-36676
Issue id: 13399614
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: csun
Creator: csun
Created: 06/Sep/21 16:33
Updated: 22/Sep/21 21:19
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently Spark is tied with Guava from Hive which is of version 14. This proposes to create a separate module {{hive-shaded}} which shades dependencies from Hive and subsequently allows us to upgrade Guava independently.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): SPARK-36828
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Sep 14 06:06:16 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0unm0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Sep/21 16:33;csun;Will post a PR soon;;;, 14/Sep/21 06:05;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33989;;;, 14/Sep/21 06:06;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33989;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 14/Sep/21 06:05;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33989;;;
Comment.2: 14/Sep/21 06:06;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33989;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support broadcast nested loop join hint for equi-join
Issue key: SPARK-36823
Issue id: 13402611
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ulysses
Creator: ulysses
Created: 22/Sep/21 04:59
Updated: 22/Sep/21 06:44
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: For the join if one side is small and other side is large, the shuffle overhead is also very big. Due to the 
bhj limitation, we can only broadcast right side for left join and left side for right join. So for the other case, we can try to use `BroadcastNestedLoopJoin` as the join strategy.

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 22 06:44:53 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v63k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Sep/21 06:44;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34069;;;, 22/Sep/21 06:44;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34069;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 22/Sep/21 06:44;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34069;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Join estimation support LeftExistence join type
Issue key: SPARK-36716
Issue id: 13400408
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 10/Sep/21 09:00
Updated: 13/Sep/21 05:38
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Join estimation support LeftExistence join type. This can benefit tpcds q10.

Before:
{noformat}
TakeOrderedAndProject (51)
+- * HashAggregate (50)
   +- Exchange (49)
      +- * HashAggregate (48)
         +- * Project (47)
            +- * SortMergeJoin Inner (46)
               :- * Sort (40)
               :  +- Exchange (39)
               :     +- * Project (38)
               :        +- * BroadcastHashJoin Inner BuildRight (37)
               :           :- * Project (31)
               :           :  +- * Filter (30)
               :           :     +- SortMergeJoin ExistenceJoin(exists#1) (29)
               :           :        :- SortMergeJoin ExistenceJoin(exists#2) (21)
               :           :        :  :- * SortMergeJoin LeftSemi (13)
               :           :        :  :  :- * Sort (5)
               :           :        :  :  :  +- Exchange (4)
               :           :        :  :  :     +- * Filter (3)
               :           :        :  :  :        +- * ColumnarToRow (2)
               :           :        :  :  :           +- Scan parquet default.customer (1)
               :           :        :  :  +- * Sort (12)
               :           :        :  :     +- Exchange (11)
               :           :        :  :        +- * Project (10)
               :           :        :  :           +- * BroadcastHashJoin Inner BuildRight (9)
               :           :        :  :              :- * ColumnarToRow (7)
               :           :        :  :              :  +- Scan parquet default.store_sales (6)
               :           :        :  :              +- ReusedExchange (8)
               :           :        :  +- * Sort (20)
               :           :        :     +- Exchange (19)
               :           :        :        +- * Project (18)
               :           :        :           +- * BroadcastHashJoin Inner BuildRight (17)
               :           :        :              :- * ColumnarToRow (15)
               :           :        :              :  +- Scan parquet default.web_sales (14)
               :           :        :              +- ReusedExchange (16)
               :           :        +- * Sort (28)
               :           :           +- Exchange (27)
               :           :              +- * Project (26)
               :           :                 +- * BroadcastHashJoin Inner BuildRight (25)
               :           :                    :- * ColumnarToRow (23)
               :           :                    :  +- Scan parquet default.catalog_sales (22)
               :           :                    +- ReusedExchange (24)
               :           +- BroadcastExchange (36)
               :              +- * Project (35)
               :                 +- * Filter (34)
               :                    +- * ColumnarToRow (33)
               :                       +- Scan parquet default.customer_address (32)
               +- * Sort (45)
                  +- Exchange (44)
                     +- * Filter (43)
                        +- * ColumnarToRow (42)
                           +- Scan parquet default.customer_demographics (41)
{noformat}

After:
{noformat}
TakeOrderedAndProject (48)
+- * HashAggregate (47)
   +- Exchange (46)
      +- * HashAggregate (45)
         +- * Project (44)
            +- * BroadcastHashJoin Inner BuildLeft (43)
               :- BroadcastExchange (39)
               :  +- * Project (38)
               :     +- * BroadcastHashJoin Inner BuildRight (37)
               :        :- * Project (31)
               :        :  +- * Filter (30)
               :        :     +- SortMergeJoin ExistenceJoin(exists#1) (29)
               :        :        :- SortMergeJoin ExistenceJoin(exists#2) (21)
               :        :        :  :- * SortMergeJoin LeftSemi (13)
               :        :        :  :  :- * Sort (5)
               :        :        :  :  :  +- Exchange (4)
               :        :        :  :  :     +- * Filter (3)
               :        :        :  :  :        +- * ColumnarToRow (2)
               :        :        :  :  :           +- Scan parquet default.customer (1)
               :        :        :  :  +- * Sort (12)
               :        :        :  :     +- Exchange (11)
               :        :        :  :        +- * Project (10)
               :        :        :  :           +- * BroadcastHashJoin Inner BuildRight (9)
               :        :        :  :              :- * ColumnarToRow (7)
               :        :        :  :              :  +- Scan parquet default.store_sales (6)
               :        :        :  :              +- ReusedExchange (8)
               :        :        :  +- * Sort (20)
               :        :        :     +- Exchange (19)
               :        :        :        +- * Project (18)
               :        :        :           +- * BroadcastHashJoin Inner BuildRight (17)
               :        :        :              :- * ColumnarToRow (15)
               :        :        :              :  +- Scan parquet default.web_sales (14)
               :        :        :              +- ReusedExchange (16)
               :        :        +- * Sort (28)
               :        :           +- Exchange (27)
               :        :              +- * Project (26)
               :        :                 +- * BroadcastHashJoin Inner BuildRight (25)
               :        :                    :- * ColumnarToRow (23)
               :        :                    :  +- Scan parquet default.catalog_sales (22)
               :        :                    +- ReusedExchange (24)
               :        +- BroadcastExchange (36)
               :           +- * Project (35)
               :              +- * Filter (34)
               :                 +- * ColumnarToRow (33)
               :                    +- Scan parquet default.customer_address (32)
               +- * Filter (42)
                  +- * ColumnarToRow (41)
                     +- Scan parquet default.customer_demographics (40)
{noformat}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Sep 13 05:35:01 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0usig:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Sep/21 09:18;007akan;I'm working on.;;;, 13/Sep/21 05:35;apachespark;User '007akuan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33974;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 13/Sep/21 05:35;apachespark;User '007akuan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33974;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support Series.__and__ for Integral
Issue key: SPARK-36671
Issue id: 13399491
Parent id: 13393332.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dc-heros
Creator: dc-heros
Created: 06/Sep/21 02:49
Updated: 09/Sep/21 12:25
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: {code:python}
>>> pser1 = pd.Series([1, 2, 3])
>>> pser2 = pd.Series([4, 5, 6, 7])
>>> pser1 & pser2
0    False
1    False
2     True
3    False
dtype: bool

>>> pser1 = pd.Series([1, 2, 3])
>>> pser2 = pd.Series([4, 5, 6])
>>> pser1 & pser2
0    0
1    0
2    2
dtype: int64

>>> pser1 = ps.Series([1, 2, 3])
>>> pser2 = ps.Series([4, 5, 6, 7])
>>> pser1 & pser2
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/dgd/spark/python/pyspark/pandas/base.py", line 423, in __and__
    return self._dtype_op.__and__(self, other)
  File "/Users/dgd/spark/python/pyspark/pandas/data_type_ops/base.py", line 317, in __and__
    raise TypeError("Bitwise and can not be applied to %s." % self.pretty_name)
TypeError: Bitwise and can not be applied to integrals.

{code}
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 09 12:25:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0umuo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Sep/21 07:59;dc-heros;working on this;;;, 09/Sep/21 12:25;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33945;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 09/Sep/21 12:25;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33945;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: special timestamps values support for path filters - modifiedBefore/modifiedAfter
Issue key: SPARK-36662
Issue id: 13399103
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Qin Yao
Creator: Qin Yao
Created: 03/Sep/21 07:37
Updated: 03/Sep/21 08:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: support today, now, tomorrow, etc in path filter modifiedBefore/modifiedAfter
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Sep 03 08:06:29 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ukgg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Sep/21 08:06;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33908;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Implement __setitem__ of label-based MultiIndex
Issue key: SPARK-36648
Issue id: 13398809
Parent id: 13393332.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: XinrongM
Creator: XinrongM
Created: 01/Sep/21 21:38
Updated: 01/Sep/21 21:38
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-09-01 21:38:43.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uinc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Implement __getitem__ of label-based MultiIndex
Issue key: SPARK-36628
Issue id: 13398522
Parent id: 13393332.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: XinrongM
Creator: XinrongM
Created: 31/Aug/21 18:20
Updated: 01/Sep/21 21:30
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description:  
{code:java}
>>> psdf = ps.DataFrame({'a':[1, 2, 3], 'b':[4,5,6], 'c':[7,8,9]}, index=pd.MultiIndex.from_tuples([('a', 'x'), ('a', 'y'), ('b', 'z')]))

>>> psdf.loc[('a', 'x')]
Traceback (most recent call last):
...
KeyError: 'x'{code}
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Aug 31 23:03:49 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ugvk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/Aug/21 23:02;apachespark;User 'xinrong-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/33881;;;, 31/Aug/21 23:03;apachespark;User 'xinrong-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/33881;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 31/Aug/21 23:03;apachespark;User 'xinrong-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/33881;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support using SQL string as a general Data Source V2 Expression
Issue key: SPARK-36641
Issue id: 13398742
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: cloud_fan
Creator: cloud_fan
Created: 01/Sep/21 15:55
Updated: 01/Sep/21 15:57
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: In Data Source V2 APIs, we kind of need to fork the catalyst Expressions and create their public versions. This is going to be a long journey. In the meanwhile, we can add a general Data Source V2 expression that contains standard SQL string (following ANSI standard).
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-09-01 15:55:22.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ui8g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support access and read parquet file by column index
Issue key: SPARK-36634
Issue id: 13398613
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Qin Yao
Creator: Qin Yao
Created: 01/Sep/21 08:16
Updated: 01/Sep/21 08:25
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Add a config spark.sql.parquet.columnIndexAccess 

When true, we access the parquet files by column index instead of catalyst schema mapping at the executor side. This is useful when the parquet file meta is inconsistent with those in Metastore
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 01 08:25:49 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uhfs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Sep/21 08:25;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33888;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve InsertIntoHadoopFsRelation file commit logic
Issue key: SPARK-36562
Issue id: 13396683
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 23/Aug/21 12:46
Updated: 24/Aug/21 05:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Improve InsertIntoHadoopFsRelation file commit logic
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-23 12:46:20.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u5jc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Investigate native support for raw data containing commas
Issue key: SPARK-36570
Issue id: 13396795
Parent id: 13393332.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: XinrongM
Creator: XinrongM
Created: 24/Aug/21 01:16
Updated: 24/Aug/21 01:16
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: For raw data containing commas as thousands separator, pandas handles it automatically. We should recognize them as well.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-24 01:16:06.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u688:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add cache for Orc Metadata
Issue key: SPARK-36516
Issue id: 13395387
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 16/Aug/21 05:13
Updated: 16/Aug/21 06:29
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Similar to SPARK-33449, this jira is used to tracking file meta support for ORC fileformat
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 16 06:29:48 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0txjc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Aug/21 05:14;LuciferYang;cc [~dongjoon];;;, 16/Aug/21 06:29;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33748;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 16/Aug/21 06:29;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33748;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Implement Index.putmask
Issue key: SPARK-36403
Issue id: 13393352
Parent id: 13393332.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: XinrongM
Creator: XinrongM
Created: 03/Aug/21 23:19
Updated: 14/Aug/21 17:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Aug 14 17:11:14 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tkz4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Aug/21 17:10;apachespark;User 'beobest2' has created a pull request for this issue:
https://github.com/apache/spark/pull/33744;;;, 14/Aug/21 17:11;apachespark;User 'beobest2' has created a pull request for this issue:
https://github.com/apache/spark/pull/33744;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 14/Aug/21 17:11;apachespark;User 'beobest2' has created a pull request for this issue:
https://github.com/apache/spark/pull/33744;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Enable binary operations with list-like Python objects
Issue key: SPARK-36437
Issue id: 13393796
Parent id: 13393332.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: XinrongM
Creator: XinrongM
Created: 05/Aug/21 21:54
Updated: 11/Aug/21 02:58
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 11 02:58:50 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tnps:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Aug/21 02:58;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33696;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Implement map for indexes
Issue key: SPARK-36395
Issue id: 13393333
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: XinrongM
Creator: XinrongM
Created: 03/Aug/21 20:48
Updated: 10/Aug/21 18:21
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): SPARK-36394
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-03 20:48:22.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tkuw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: extractJoinKeysWithColStats support EqualNullSafe
Issue key: SPARK-36162
Issue id: 13389853
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 15/Jul/21 14:24
Updated: 06/Aug/21 03:54
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: sql("select * from date_dim join item on d_date_sk = i_item_sk").explain("cost")
{noformat}
== Optimized Logical Plan ==
Join Inner, (d_date_sk#0 = i_item_sk#28), Statistics(sizeInBytes=1.0 B, rowCount=0)
:- Relation default.date_dim[d_date_sk#0,d_date_id#1,d_date#2,d_month_seq#3,d_week_seq#4,d_quarter_seq#5,d_year#6,d_dow#7,d_moy#8,d_dom#9,d_qoy#10,d_fy_year#11,d_fy_quarter_seq#12,d_fy_week_seq#13,d_day_name#14,d_quarter_name#15,d_holiday#16,d_weekend#17,d_following_holiday#18,d_first_dom#19,d_last_dom#20,d_same_day_ly#21,d_same_day_lq#22,d_current_day#23,... 4 more fields] parquet, Statistics(sizeInBytes=17.6 MiB, rowCount=7.30E+4)
+- Relation default.item[i_item_sk#28,i_item_id#29,i_rec_start_date#30,i_rec_end_date#31,i_item_desc#32,i_current_price#33,i_wholesale_cost#34,i_brand_id#35,i_brand#36,i_class_id#37,i_class#38,i_category_id#39,i_category#40,i_manufact_id#41,i_manufact#42,i_size#43,i_formulation#44,i_color#45,i_units#46,i_container#47,i_manager_id#48,i_product_name#49] parquet, Statistics(sizeInBytes=85.2 MiB, rowCount=2.04E+5)
{noformat}

sql("select * from date_dim join item on d_date_sk <=> i_item_sk").explain("cost")
{noformat}
== Optimized Logical Plan ==
Join Inner, (d_date_sk#0 <=> i_item_sk#28), Statistics(sizeInBytes=9.2 TiB, rowCount=1.49E+10)
:- Relation default.date_dim[d_date_sk#0,d_date_id#1,d_date#2,d_month_seq#3,d_week_seq#4,d_quarter_seq#5,d_year#6,d_dow#7,d_moy#8,d_dom#9,d_qoy#10,d_fy_year#11,d_fy_quarter_seq#12,d_fy_week_seq#13,d_day_name#14,d_quarter_name#15,d_holiday#16,d_weekend#17,d_following_holiday#18,d_first_dom#19,d_last_dom#20,d_same_day_ly#21,d_same_day_lq#22,d_current_day#23,... 4 more fields] parquet, Statistics(sizeInBytes=17.6 MiB, rowCount=7.30E+4)
+- Relation default.item[i_item_sk#28,i_item_id#29,i_rec_start_date#30,i_rec_end_date#31,i_item_desc#32,i_current_price#33,i_wholesale_cost#34,i_brand_id#35,i_brand#36,i_class_id#37,i_class#38,i_category_id#39,i_category#40,i_manufact_id#41,i_manufact#42,i_size#43,i_formulation#44,i_color#45,i_units#46,i_container#47,i_manager_id#48,i_product_name#49] parquet, Statistics(sizeInBytes=85.2 MiB, rowCount=2.04E+5)
{noformat}

https://github.com/apache/spark/blob/d6a68e0b67ff7de58073c176dd097070e88ac831/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/JoinEstimation.scala#L329-L339

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Aug 06 03:54:04 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sze8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Aug/21 03:53;apachespark;User 'changvvb' has created a pull request for this issue:
https://github.com/apache/spark/pull/33662;;;, 06/Aug/21 03:54;apachespark;User 'changvvb' has created a pull request for this issue:
https://github.com/apache/spark/pull/33662;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 06/Aug/21 03:54;apachespark;User 'changvvb' has created a pull request for this issue:
https://github.com/apache/spark/pull/33662;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Implement 'weights' and 'axis' in sample at DataFrame and Series
Issue key: SPARK-36436
Issue id: 13393795
Parent id: 13393332.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: XinrongM
Creator: XinrongM
Created: 05/Aug/21 21:49
Updated: 05/Aug/21 21:49
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-05 21:49:32.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tnpk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Implement DataFrame.mode
Issue key: SPARK-36397
Issue id: 13393338
Parent id: 13393332.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: XinrongM
Creator: XinrongM
Created: 03/Aug/21 21:38
Updated: 03/Aug/21 22:47
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Aug 03 22:47:37 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tkw0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Aug/21 22:47;apachespark;User 'xinrong-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/33625;;;, 03/Aug/21 22:47;apachespark;User 'xinrong-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/33625;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 03/Aug/21 22:47;apachespark;User 'xinrong-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/33625;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: date_trunc returns incorrect output
Issue key: SPARK-36065
Issue id: 13388527
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: sumeet.gajjar
Creator: sumeet.gajjar
Created: 09/Jul/21 03:33
Updated: 30/Jul/21 08:39
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: date_trunc, sql, timestamp
Description: Hi,

Running date_trunc on any hour of "1891-10-01" returns incorrect output for "Europe/Bratislava" timezone.
Use the following steps in order to reproduce the issue:
 * Run spark-shell using:

{code:java}
TZ="Europe/Bratislava" ./bin/spark-shell --conf spark.driver.extraJavaOptions='-Duser.timezone=Europe/Bratislava' --conf spark.executor.extraJavaOptions='-Duser.timezone=Europe/Bratislava' --conf spark.sql.session.timeZone="Europe/Bratislava"{code}

 * Generate test data:

{code:java}
((0 until 9).map(hour => s"1891-10-01 00:0$hour:00") ++ (10 until 24).map(hour => s"1891-10-01 00:$hour:00")).toDF("ts_string").createOrReplaceTempView("temp_ts")
{code}

 * Run query:

{code:java}
sql("select ts_string, cast(ts_string as TIMESTAMP) as ts, date_trunc('day', ts_string) from temp_ts").show(false)
{code}

 * Output:

{code:java}
+-------------------+-------------------+--------------------------+
|ts_string          |ts                 |date_trunc(day, ts_string)|
+-------------------+-------------------+--------------------------+
|1891-10-01 00:00:00|1891-10-01 00:02:16|1891-10-01 00:02:16       |
|1891-10-01 00:01:00|1891-10-01 00:03:16|1891-10-01 00:02:16       |
|1891-10-01 00:02:00|1891-10-01 00:04:16|1891-10-01 00:02:16       |
|1891-10-01 00:03:00|1891-10-01 00:03:00|1891-10-01 00:02:16       |
|1891-10-01 00:04:00|1891-10-01 00:04:00|1891-10-01 00:02:16       |
|1891-10-01 00:05:00|1891-10-01 00:05:00|1891-10-01 00:02:16       |
|1891-10-01 00:06:00|1891-10-01 00:06:00|1891-10-01 00:02:16       |
|1891-10-01 00:07:00|1891-10-01 00:07:00|1891-10-01 00:02:16       |
|1891-10-01 00:08:00|1891-10-01 00:08:00|1891-10-01 00:02:16       |
|1891-10-01 00:10:00|1891-10-01 00:10:00|1891-10-01 00:02:16       |
|1891-10-01 00:11:00|1891-10-01 00:11:00|1891-10-01 00:02:16       |
|1891-10-01 00:12:00|1891-10-01 00:12:00|1891-10-01 00:02:16       |
|1891-10-01 00:13:00|1891-10-01 00:13:00|1891-10-01 00:02:16       |
|1891-10-01 00:14:00|1891-10-01 00:14:00|1891-10-01 00:02:16       |
|1891-10-01 00:15:00|1891-10-01 00:15:00|1891-10-01 00:02:16       |
|1891-10-01 00:16:00|1891-10-01 00:16:00|1891-10-01 00:02:16       |
|1891-10-01 00:17:00|1891-10-01 00:17:00|1891-10-01 00:02:16       |
|1891-10-01 00:18:00|1891-10-01 00:18:00|1891-10-01 00:02:16       |
|1891-10-01 00:19:00|1891-10-01 00:19:00|1891-10-01 00:02:16       |
|1891-10-01 00:20:00|1891-10-01 00:20:00|1891-10-01 00:02:16       |
+-------------------+-------------------+--------------------------+
only showing top 20 rows
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 30 08:39:19 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sr8o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Jul/21 17:12;sumeet.gajjar;cc [~maxgekk];;;, 30/Jul/21 08:39;petertoth;I think the output is correct as there was a time zone change (+00:02:16) at 1891-10-01 00:00:00 in Bratislava and that means that 1891-10-01 00:00:00 = 1891-10-01 00:02:16.
I found this site that shows the TZ changes: https://www.timeanddate.com/time/zone/slovakia/bratislava;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 30/Jul/21 08:39;petertoth;I think the output is correct as there was a time zone change (+00:02:16) at 1891-10-01 00:00:00 in Bratislava and that means that 1891-10-01 00:00:00 = 1891-10-01 00:02:16.
I found this site that shows the TZ changes: https://www.timeanddate.com/time/zone/slovakia/bratislava;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use atomic renames for diskstore block writes & do "fall-through" reads
Issue key: SPARK-36330
Issue id: 13392398
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 28/Jul/21 19:09
Updated: 28/Jul/21 19:09
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: This is a potential follow-up for SPARK-36058. Since we could end up in the situation where we have a partial write we can't do fall through reads to the diskStore. A solution to this is to use atomic renames on successful write of a block which would make it safe to assume that any on-disk representation of a block represented the full block.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-28 19:09:57.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tf3k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add shuffle checksum support for push-based shuffle
Issue key: SPARK-36284
Issue id: 13391783
Parent id: 13375878.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Ngone51
Creator: Ngone51
Created: 26/Jul/21 04:48
Updated: 26/Jul/21 04:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-26 04:48:01.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tbaw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Make the dev-run-integration-tests & docker-image-tool able to use sbt build output
Issue key: SPARK-36278
Issue id: 13391636
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 23/Jul/21 22:37
Updated: 23/Jul/21 22:37
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build, Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: maven build (even with zinc) are relatively slow and being able to use sbt incremental builds could speed it up for folks who are doing Kube integration test work.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-23 22:37:41.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tae8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Kubernetes
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark Kube Integration tests must be run from project root
Issue key: SPARK-36233
Issue id: 13391005
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 20/Jul/21 21:20
Updated: 20/Jul/21 21:20
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes, Tests
Due Date: 
Votes: 0
Labels: 
Description: Otherwise they fail to resolve various configuration files. We should check that the PWD is in the project root and error otherwise.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-20 21:20:36.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0t6i0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Tests
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add flag to allow Driver to request for OPPORTUNISTIC containers
Issue key: SPARK-36219
Issue id: 13390811
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: chaosju
Creator: chaosju
Created: 20/Jul/21 03:22
Updated: 20/Jul/21 03:24
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: YARN
Due Date: 
Votes: 0
Labels: 
Description: YARN-2882 and YARN-4335 introduces the concept of container ExecutionTypes and specifically OPPORTUNISTIC containers.
The default ExecutionType is GUARANTEED. This JIRA proposes to allow users to provide hints via config to the SPARK framework as to the number of containers it would like to schedule as OPPORTUNISTIC.


like MAPREDUCE-6703 .
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-20 03:22:34.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0t5aw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support explain final plan in AQE
Issue key: SPARK-36174
Issue id: 13389952
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ulysses
Creator: ulysses
Created: 16/Jul/21 04:37
Updated: 16/Jul/21 06:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The executed plan will be changed during running in AQE, however the current implementation of explain does not support this.

As the AQE is enabled by default, user may want to get the final plan using query, so it should make sense to add a new grammar to support it.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 16 06:10:43 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0t008:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Jul/21 06:10;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/33386;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: An Impl of skew key detection and data inflation optimization
Issue key: SPARK-36087
Issue id: 13389065
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: podongfeng
Creator: podongfeng
Created: 12/Jul/21 09:20
Updated: 12/Jul/21 09:45
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: design doc: [https://docs.google.com/document/d/1jYGlipQdirhuRR3_x1PflAYdMow7StOHOLUjSw6Vk9s/edit?usp=sharing]

1, introduce {{ShuffleExecAccumulator}} in ShuffleExchangeExec to support arbitrary statistics;

2, impl a key sampling {{ShuffleExecAccumulator}} to detect skew keys and show debug info on SparkUI;

3, in {{OptimizeSkewedJoin}}, estimate the joined size of each partition based on the sampled keys, and split a partition if it is not split yet and its estimated joined size is too larger.

 

data inflation case:
{code:java}
spark.conf.set("spark.sql.adaptive.enabled", true)
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", true)
spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes", "80")
spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "80")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "-1")
spark.conf.set("spark.sql.adaptive.coalescePartitions.minPartitionNum", "1")
spark.conf.set("spark.sql.shuffle.partitions", 10)

sc.setLogLevel("INFO")

val df1 = spark.range(0, 1000000, 1, 9).select(col("id").as("key1"), col("id").as("value1"), hash(col("id")).mod(100).as("hash1")).withColumn("key1", when(col("hash1") === lit(0), lit(0)).otherwise(col("key1")))

val df2 = spark.range(0, 500000, 1, 7).select(col("id").as("key2"), col("id").as("value2"), hash(col("id")).mod(100).as("hash2")).withColumn("key2", when(col("hash2") === lit(0), lit(0)).otherwise(col("key2")))

df1.join(df2, col("key1") === col("key2")).write.mode("overwrite").parquet("/tmp/result_0")

spark.conf.set("spark.sql.adaptive.skewJoin.inflation.enabled", true)
spark.conf.set("spark.sql.adaptive.skewJoin.inflation.factor", 10)
spark.conf.set("spark.sql.adaptive.shuffle.sampleSizePerPartition", 100)
spark.conf.set("spark.sql.adaptive.shuffle.detectSkewness", true)

df1.join(df2, col("key1") === col("key2")).write.mode("overwrite").parquet("/tmp/result_1"){code}
 

it also partially resolve https://issues.apache.org/jira/browse/SPARK-35596

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jul 12 09:32:43 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0suk0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Jul/21 09:32;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/33298;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add v2 implementation for hive table
Issue key: SPARK-39797
Issue id: 13471912
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: kaifeiYi
Creator: kaifeiYi
Created: 17/Jul/22 13:11
Updated: 03/Jun/24 18:20
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: V2 interface has been developed for a long time, Spark datasource will be implemented based on V2 in the future, we intend to add V2 support for Hive tables in this issue
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 03 18:20:11 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16x40:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Jul/22 13:14;kaifeiYi;kindly ping [~cloud_fan] , Do we plan to implement V2 Hive?;;;, 03/Jun/24 18:20;viirya;Btw, there are some third-party implementation of Hive v2 datasource, e.g., https://github.com/apache/kyuubi/commit/d822b3eba3050eb6a3908cf9c6c059ca29f82100;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 03/Jun/24 18:20;viirya;Btw, there are some third-party implementation of Hive v2 datasource, e.g., https://github.com/apache/kyuubi/commit/d822b3eba3050eb6a3908cf9c6c059ca29f82100;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: When using INSERT OVERWRITE with Spark CTEs they may not be fully resolved
Issue key: SPARK-48101
Issue id: 13578007
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 02/May/24 22:27
Updated: 02/May/24 23:04
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.4.0, 3.5.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Repro:

```sql

DROP TABLE IF EXISTS local.cte1;
DROP TABLE IF EXISTS local.cte2;
DROP TABLE IF EXISTS local.cte3;
CREATE TABLE local.cte1 (id INT, fname STRING);
CREATE TABLE local.cte2 (id2 INT);
CREATE TABLE local.cte3 (id INT);
WITH test_fake AS (SELECT * FROM local.cte1 WHERE id = 1 AND id2 = 1), test_fake2 AS (SELECT * FROM local.cte2 WHERE id2 = 1) INSERT OVERWRITE TABLE local.cte3 SELECT id2 as id FROM test_fake2;
WITH test_fake AS (SELECT * FROM local.cte1 WHERE id = 1 AND id2 = 1), test_fake2 AS (SELECT * FROM local.cte2 WHERE id2 = 1) SELECT id2 as id FROM test_fake2;

```

 

Here we would expect both of the last two SQL expressions to fail, but instead only the first one does.

 

There are more complicated cases, and in those cases, the invalid CTE is treated as a null table, but this is the simplest repro I've been able to come up with so far.

 

This occurs using both local w/Iceberg catalog or the SparkSession catalog.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu May 02 23:04:42 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1p080:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/May/24 22:39;holden;It could be because we are eliminating the CTE from in the INSERT OVERWRITE case but not in the "raw" select case?;;;, 02/May/24 23:04;holden;Ok it looks like it's coming from BatchSubstiution behaving differently:

 

```

!CTE [test_fake, test_fake2]                                   'WithCTE
!:  :- 'SubqueryAlias test_fake                                :- 'CTERelationDef 0, false
!:  :  +- 'Project [*]                                         :  +- 'SubqueryAlias test_fake
!:  :     +- 'Filter (('id = 1) AND ('id2 = 2))                :     +- 'Project [*]
!:  :        +- 'UnresolvedRelation [local, cte1], [], false   :        +- 'Filter (('id = 1) AND ('id2 = 2))
!:  +- 'SubqueryAlias test_fake2                               :           +- 'UnresolvedRelation [local, cte1], [], false
!:     +- 'Project [*]                                         :- 'CTERelationDef 1, false
!:        +- 'Filter ('id2 = 1)                                :  +- 'SubqueryAlias test_fake2
!:           +- 'UnresolvedRelation [local, cte2], [], false   :     +- 'Project [*]
!+- 'Project ['id2 AS id#0]                                    :        +- 'Filter ('id2 = 1)
!   +- 'UnresolvedRelation [test_fake2], [], false             :           +- 'UnresolvedRelation [local, cte2], [], false
!                                                              +- 'Project ['id2 AS id#0]
!                                                                 +- 'SubqueryAlias test_fake2
!                                                                    +- 'CTERelationRef 1, false, false

```

 

vs

 

```

!CTE [test_fake, test_fake2]                                                                'InsertIntoStatement 'UnresolvedRelation [local, cte3], [], false, true, false, false
!:  :- 'SubqueryAlias test_fake                                                             +- 'Project ['id2 AS id#0]
!:  :  +- 'Project [*]                                                                         +- 'SubqueryAlias test_fake2
!:  :     +- 'Filter (('id = 1) AND ('id2 = 2))                                                   +- 'Project [*]
!:  :        +- 'UnresolvedRelation [local, cte1], [], false                                         +- 'Filter ('id2 = 1)
!:  +- 'SubqueryAlias test_fake2                                                                        +- 'UnresolvedRelation [local, cte2], [], false
!:     +- 'Project [*]                                                                      
!:        +- 'Filter ('id2 = 1)                                                             
!:           +- 'UnresolvedRelation [local, cte2], [], false                                
!+- 'InsertIntoStatement 'UnresolvedRelation [local, cte3], [], false, true, false, false   
!   +- 'Project ['id2 AS id#0]                                                              
!      +- 'UnresolvedRelation [test_fake2], [], false                                       

```;;;
Affects Version/s.1: 3.4.0
Component/s.1: 
Comment.1: 02/May/24 23:04;holden;Ok it looks like it's coming from BatchSubstiution behaving differently:

 

```

!CTE [test_fake, test_fake2]                                   'WithCTE
!:  :- 'SubqueryAlias test_fake                                :- 'CTERelationDef 0, false
!:  :  +- 'Project [*]                                         :  +- 'SubqueryAlias test_fake
!:  :     +- 'Filter (('id = 1) AND ('id2 = 2))                :     +- 'Project [*]
!:  :        +- 'UnresolvedRelation [local, cte1], [], false   :        +- 'Filter (('id = 1) AND ('id2 = 2))
!:  +- 'SubqueryAlias test_fake2                               :           +- 'UnresolvedRelation [local, cte1], [], false
!:     +- 'Project [*]                                         :- 'CTERelationDef 1, false
!:        +- 'Filter ('id2 = 1)                                :  +- 'SubqueryAlias test_fake2
!:           +- 'UnresolvedRelation [local, cte2], [], false   :     +- 'Project [*]
!+- 'Project ['id2 AS id#0]                                    :        +- 'Filter ('id2 = 1)
!   +- 'UnresolvedRelation [test_fake2], [], false             :           +- 'UnresolvedRelation [local, cte2], [], false
!                                                              +- 'Project ['id2 AS id#0]
!                                                                 +- 'SubqueryAlias test_fake2
!                                                                    +- 'CTERelationRef 1, false, false

```

 

vs

 

```

!CTE [test_fake, test_fake2]                                                                'InsertIntoStatement 'UnresolvedRelation [local, cte3], [], false, true, false, false
!:  :- 'SubqueryAlias test_fake                                                             +- 'Project ['id2 AS id#0]
!:  :  +- 'Project [*]                                                                         +- 'SubqueryAlias test_fake2
!:  :     +- 'Filter (('id = 1) AND ('id2 = 2))                                                   +- 'Project [*]
!:  :        +- 'UnresolvedRelation [local, cte1], [], false                                         +- 'Filter ('id2 = 1)
!:  +- 'SubqueryAlias test_fake2                                                                        +- 'UnresolvedRelation [local, cte2], [], false
!:     +- 'Project [*]                                                                      
!:        +- 'Filter ('id2 = 1)                                                             
!:           +- 'UnresolvedRelation [local, cte2], [], false                                
!+- 'InsertIntoStatement 'UnresolvedRelation [local, cte3], [], false, true, false, false   
!   +- 'Project ['id2 AS id#0]                                                              
!      +- 'UnresolvedRelation [test_fake2], [], false                                       

```;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Hive bucketing write support
Issue key: SPARK-19256
Issue id: 13035404
Parent id: 
Issue Type: Umbrella
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: tejasp
Creator: tejasp
Created: 17/Jan/17 05:47
Updated: 06/Mar/24 03:57
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 2.1.0, 2.2.0, 2.3.0, 2.4.0, 3.0.0, 3.1.0, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 28
Labels: 
Description: Update (2020 by Cheng Su):

We use this JIRA to track progress for Hive bucketing write support in Spark. The goal is for Spark to write Hive bucketed table, to be compatible with other compute engines (Hive and Presto).

 

Current status for Hive bucketed table in Spark:

Not support for reading Hive bucketed table: read bucketed table as non-bucketed table.

Wrong behavior for writing Hive ORC and Parquet bucketed table: write orc/parquet bucketed table as non-bucketed table (code path: InsertIntoHadoopFsRelationCommand -> FileFormatWriter).

Do not allow for writing Hive non-ORC/Parquet bucketed table: throw exception by default if writing non-orc/parquet bucketed table (code path: InsertIntoHiveTable), and exception can be disabled by setting config `hive.enforce.bucketing`=false and `hive.enforce.sorting`=false, which will write as non-bucketed table.

 

Current status for Hive bucketed table in Hive:

Hive 3.0.0 and after: support writing bucketed table with Hive murmur3hash (https://issues.apache.org/jira/browse/HIVE-18910).

Hive 1.x.y and 2.x.y: support writing bucketed table with Hive hivehash.

Hive on Tez: support zero and multiple files per bucket (https://issues.apache.org/jira/browse/HIVE-14014). And more code pointer on read path - [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java#L183-L212] .

 

Current status for Hive bucketed table in Presto (take presto-sql here):

Support writing bucketed table with Hive murmur3hash and hivehash ([https://github.com/prestosql/presto/pull/1697]).

Support zero and multiple files per bucket ([https://github.com/prestosql/presto/pull/822]).

 

TLDR is to achieve Hive bucketed table compatibility across Spark, Presto and Hive. Here with this JIRA, we need to add support writing Hive bucketed table with Hive murmur3hash (for Hive 3.x.y) and hivehash (for Hive 1.x.y and 2.x.y).

 

To allow Spark efficiently read Hive bucketed table, this needs more radical change and we decide to wait until data source v2 supports bucketing, and do the read path on data source v2. Read path will not covered by this JIRA.

 

Original description (2017 by Tejas Patil):

JIRA to track design discussions and tasks related to Hive bucketing support in Spark.

Proposal : [https://docs.google.com/document/d/1a8IDh23RAkrkg9YYAeO51F4aGO8-xAlupKwdshve2fc/edit?usp=sharing]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): SPARK-21649
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 06 03:57:12 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|i38slj:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.2.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Jan/17 05:50;tejasp;cc [~cloud_fan] [~hvanhovell] for comments over the proposal;;;, 25/Jan/17 06:55;tejasp;[~cloud_fan] [~hvanhovell] : Are you guys ok with the proposal (link in jira description) or still want to discuss over any items ?;;;, 25/Jan/17 07:02;tejasp;BTW: In its current state, Spark writes data to hive bucketed tables but the outputs will not conform with hive's bucketing semantics. This can lead to data corruption if Spark outputs are used by Hive. I feel disabling writes to hive bucketing tables via Spark might be a good initial step to add. What do you think ?;;;, 28/Apr/17 23:02;tejasp;[~cloud_fan], [~sameerag] : I was looking at trunk and observed changes which would affect the plan (more from implementation perspective not the high level design). 

`InsertIntoHiveTable` is now a `RunnableCommand` (unlike earlier `UnaryExecNode`). With exec node, it was possible to set the requiredDistribution and requiredOrdering and let the planner (`EnsureRequirements`) take care of managing things. With `RunnableCommand`, the model seems to be that these requirements have to handled separately (so far there is only one place which does that: [0]). Two comments:
- I feel that this somewhat ugly as one would expect `EnsureRequirements` to be a single place for handling this
- we might miss out optimizations. eg. If I am adding an extra shuffle in `InsertIntoHiveTable` and if the previous node was shuffle as well, the code for merging these two shuffle nodes as a single shuffle would have to duplicated as well from `EnsureRequirements`.

Would it be OK to make `InsertIntoHiveTable` as a `UnaryExecNode` ?
Why was it made a `RunnableCommand` recently (https://github.com/apache/spark/pull/16517) ? cc [~smilegator]

[0] : https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala#L173;;;, 01/May/17 06:24;cloud_fan;Yea I agree that we should use {{requiredDistribution}} and {{requiredOrdering}} to group and sort the input data instead of doing it manually in `FileFormatWriter`, but at the time when we were refactoring {{InsertIntoHiveTable}}, it was better than before, because previously we group and sort the input data in {{XXXWriteContainer}}, and the code is very similar to {{FileFormatWriter}}.;;;, 22/May/17 03:09;tejasp;[~cloud_fan] : After SPARK-18243, `InsertIntoHiveTable` is a `RunnableCommand` and not a physical operator node. This hinders the ability to add constraints about partitioning and sortedness of child node of insert operator. I see SPARK-20703 is about refactoring that part of code (but it will be only restricted towards getting more metrics). Would it be ok if I just change `InsertIntoHiveTable` to have the changes I need and move on OR wait for SPARK-20703 to land (which might take time). As you guys would be reviewing my PRs, would want to get your opinion on this.;;;, 24/May/17 03:05;cloud_fan;let's wait for https://github.com/apache/spark/pull/18064 , which also refactors the insertion plan node.;;;, 13/Aug/17 23:36;tejasp;After the refactoring of the insertion plan node has been done, I revisited this and picked up where I left off. I have a unpolished but working version for both reader and writer side changes. Its too big of a patch, I am finding it hard to keep in sync with master plus self-review. I have split it into 2 parts : reader side change and writer side change (the later is needed first as one cannot write test cases for reader if there is no way to populate bucketed data). Will send a PR for write side this week after more testing. Sharing the link to my personal branch if anyone wants to see : https://github.com/tejasapatil/spark/tree/bucket_write (it still needs more work in terms of handing one corner case, test cases, refactoring + polishing but the core functionality works);;;, 16/Aug/17 04:50;tejasp;PR for writer side changes is out : https://github.com/apache/spark/pull/18954
I have documented the semantics and differences with Spark's bucketing in the jira description so that it makes it easy for anyone who wants to see what changes are done.

Reader side changes are close to completion (core meat is done, but the work is unpolished). I assume that the writer PR will have some round of comments which will buy me time to work on that. Reader side changes depend on the writer side PR to get in so won't publish it until then.;;;, 20/Aug/17 00:07;tejasp;Opened a PR which has both reader and writer side changes : https://github.com/apache/spark/pull/19001. Have explained the semantics in the PR description.;;;, 28/Aug/17 16:55;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/19001;;;, 09/Jan/18 16:15;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/20206;;;, 30/Jan/18 17:16;ferdonline;Thanks a lot for this great contribution to Spark.

I was just wondering, would it make sense to apply this to direct outputs (e.g. write.parquet()), so that we could keep partitioning information - and again avoid reshuffling data before a merge? I believe this is most what saveAsTable() does by default in Spark, but to my mind it would improve the DataFrame write API and make these performance benefits more accessible.

Update:
I've just noticed that it has been considered in [https://github.com/apache/spark/pull/13452.
] [~cloud_fan] [ |https://github.com/apache/spark/pull/13452.]- Is there an Issue to follow up on this feature? Eventually we could simply store a metadata json file together with the data files.;;;, 06/Feb/18 19:36;tejasp;[~ferdonline] : The feature you are referring to is not in the scope of this Jira as storing bucketing metadata outside of metastore is not something that hive does. Could you please write to dev@ and create a Jira to initiate this discussion ?;;;, 25/Apr/18 05:55;advancedxy;Hi [~tejasp] [~cloud_fan], are you still working on this? We also need this feature in our internal Spark stack, is there anything pending?

cc [~XuanYuan];;;, 09/Oct/18 17:59;shay_elbaz;+1

[~tejasp] is this still under progress?;;;, 27/Nov/18 00:48;chengsu;After discussion with [~tejasp], I think the most of un-merged pr ([https://github.com/apache/spark/pull/19001)] looks good, and is still valid on latest trunk. I am picking up this task, and starting working on this. cc [~advancedxy] and [~shay_elbaz]. Thanks.;;;, 27/Nov/18 08:48;shay_elbaz;[~chengsu] this is great! If there is anything I can do to assist please ask :);;;, 27/Nov/18 09:02;advancedxy;[~chengsu] great news. I can do some review work then.;;;, 17/Jul/19 08:13;aditya.dataengg;[~chengsu] any update on this?;;;, 17/Jul/19 18:28;chengsu;[~aditya.dataengg] - I started the work by submitting a PR ([https://github.com/apache/spark/pull/23163]) for https://issues.apache.org/jira/browse/SPARK-26164, and it is still under review. I will ping relevant reviewers to see whether we can speed it up, thanks.;;;, 05/Nov/19 15:39;vikramhegde;Hi [~tejasp] [~chengsu]

How to join two bucketed tables, when tables are bucketed on multiple columns. I tried to join, but still exchanges were getting created.

Below is the scenario:

table1 bucketed by a 10 buckets

table2 bucketed by a,b 10 bucktes

how to join table1 and table2 on column a without exchanges?;;;, 09/Mar/20 14:21;FelixKJose;[~chengsu] Any updates on this feature (23163 and 26164)? 

{color:#172b4d}*Note:*{color} Also I have seen recent Hive has introduced some changes in their Bucketing  Strategy and Big Data engines like Presto incorporated those changes. Are we considering the compatibility with these new HIVE changes?
Link to Presto/HIVE Bucketing improvements/changes: [https://prestosql.io/blog/2019/05/29/improved-hive-bucketing.html];;;, 16/Mar/20 03:31;FelixKJose;Expose a configuration parameter for Hive Hashing during BucketBy operation. This config is missing or not documented;;;, 18/Jun/20 19:12;viru;This is a much needed feature +1 vote;;;, 27/Aug/20 05:56;chengsu;Hi all, after discussion with [~cloud_fan], we decide to support Hive bucketed table write path. Specifically:

1.for Hive 3.x.y, writing Hive bucketed table with Hive murmur3hash.

2.for Hive 2.x.y and 1.x.y, writing Hive bucketed table with Hive hivehash.

3.zero or multiple files per bucket should be expected by user.

 

This is compatible with Hive and Presto (presto-sql for now), so user can be benefitted from writing Hive bucketed table correctly from Spark, and read bucketed table efficiently from Presto and Hive.

See JIRA description for details, and I will work on each sub-tasks, thanks.;;;, 27/Aug/20 06:44;viirya;> Do not allow for writing Hive non-ORC/Parquet bucketed table: throw exception by default if writing non-orc/parquet bucketed table (code path: InsertIntoHiveTable), and exception can be disabled by setting config `hive.enforce.bucketing`=false and `hive.enforce.sorting`=false, which will write as non-bucketed table.

Do you mean Hive ORC/Parquet bucketed table not converted to data source relation? InsertIntoHiveTable should also write Hive ORC/Parquet table which is not converted.;;;, 27/Aug/20 06:47;viirya;> Hive on Tez: support zero and multiple files per bucket (https://issues.apache.org/jira/browse/HIVE-13988).

Is that correct JIRA? Seems not and it was closed as "Won't Fix".;;;, 27/Aug/20 18:00;chengsu;[~viirya] -

> Do you mean Hive ORC/Parquet bucketed table not converted to data source relation? InsertIntoHiveTable should also write Hive ORC/Parquet table which is not converted.



No. Hive ORC/Parquet bucketed table converted to data source relation. Hive non-ORC/non-Parquet bucketed table is not converted to data source relation, and still use Hive code path (InsertIntoHiveTable). I agree there can be cases where we use Hive code path for Hive ORC/Parquet table (e.g. by tweaking config `spark.sql.orc.impl`, `spark.sql.hive.convertInsertingPartitionedTable`, etc).

 

> Is that correct JIRA? Seems not and it was closed as "Won't Fix".

Sorry I meant this one - https://issues.apache.org/jira/browse/HIVE-14014 . Hive on Tez allows 0 file per bucket. More code pointer - [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java#L183-L212] . Updated description.;;;, 18/Nov/20 15:34;findepi;>  1.for Hive 3.x.y, writing Hive bucketed table with Hive murmur3hash.

> 2.for Hive 2.x.y and 1.x.y, writing Hive bucketed table with Hive hivehash.

 

Should i assume this is a simplification and it will be actually controlled by {{bucketing_version}} table property?
e.g. Hive 3 and Presto 321+ support both bucketing version and user can still choose one (should they have a compatibility requirement with older systems).;;;, 18/Nov/20 19:33;chengsu;> Should i assume this is a simplification and it will be actually controlled by {{bucketing_version}} table property?

You can think this way. Only if bucketing_version is 2 (for Hive 3.x.y case), Spark will use Hive murmur3hash to write Hive bucketed table, and this is same to Hive's behavior.;;;, 22/Nov/20 07:56;asafh;Can you please update about the progress of this issue?
It's much-needed for our use-case.;;;, 30/Nov/20 14:43;pshar0;Thanks [~tejasp] [~chengsu]. - Thanks for all your work on this issue.  This will be a huge win for joining large datasets efficiently using open source. +1;;;, 01/Dec/20 06:59;chengsu;Sorry for delaying, just an update for progress. We identified a major blocker (https://issues.apache.org/jira/browse/SPARK-33298) needs to be resolved before merging hive bucketing write support. The underlying design is still in discussion, and current ETA for all features is Spark 3.2. We are already near the branch cut time for Spark 3.1, so we have to target this to the next release, thanks.;;;, 12/Mar/21 19:49;zsxwing;I read [https://docs.google.com/document/d/1a8IDh23RAkrkg9YYAeO51F4aGO8-xAlupKwdshve2fc/edit?usp=sharing] but didn't find any section about compatibility. I'm curious about both the forward and backward compatibility. For example, 
 * What happens if using an old Spark version to read this new format? Does it treat it as a non-bucketed table, or read it a a bucketed table but may return different results?
 * How does Spark know which bucket format when reading an existing table, and how does it decide whether to use the old format or the new format to read and write?;;;, 12/Mar/21 20:09;chengsu;[~zsxwing] - sorry the design doc was years old, and I can create a short new one if needed. Note here this Jira is supporting writing hive bucketed table, but not reading. The reason is we don't want to introduce hash function change to `HashPartitioning` right now, and would like to do this in a more general way with data source v2.

So

> What happens if using an old Spark version to read this new format? Does it treat it as a non-bucketed table, or read it a a bucketed table but may return different results?

The read behavior is not changed. Spark will read the Hive/Trino bucketed table as non-bucketed table. This has no problem for data correctness as bucketing is just an optimization.

> How does Spark know which bucket format when reading an existing table, and how does it decide whether to use the old format or the new format to read and write?

For new format (Hive murmur3hash), Hive is adding a table property "bucketing_version"=2 to metastore for this kind of new bucketed table, and Trino followed that. In the future if we want to read old/new Hive bucketed table with bucketing enabled, we can decide based on this property field.

For writing new format or old format, default behavior can be depending on Hive version used during runtime for Spark query. For Hive 3.x.y, writing Hive bucketed table with Hive murmur3hash, and for Hive 2.x.y and 1.x.y, writing Hive bucketed table with Hive hivehash. We can also add a Hive config for user to control this behavior if needed.

 ;;;, 12/Mar/21 20:20;zsxwing;[~chengsu] Thanks for the answers. Sounds like Metastore is a requirement to distinguish two formats. If I use Spark to write to a table using the table path directly, it will break the table. Right?;;;, 12/Mar/21 21:05;chengsu;[~zsxwing] - if users gets table path, and writes data directly to it without respecting table metadata, it will break the table layout. I am assuming this applies to Spark bucketed table as well (the one using spark murmur3hash). Shouldn't it?;;;, 23/Jun/21 12:26;pushkarcse;Hi [~chengsu] , Could you please update us here.;;;, 23/Jun/21 22:52;chengsu;[~pushkarcse] - we are currently working on https://issues.apache.org/jira/browse/SPARK-33298 . After that, I will resume the discussion of https://issues.apache.org/jira/browse/SPARK-32709 .;;;, 23/Jun/21 22:54;pushkarcse;Thank you [~chengsu]!!;;;, 19/Dec/23 15:15;eric.xiao;Hi [~tejasp], for the remaining work, would i be able to get assigned those tickets? I would like to work on them :).;;;, 06/Mar/24 03:57;mrbrahman;Don't really like spamming in the comments - but this is a much needed feature for big-data processing, and has been pending for a while now. Can this be given some love please? :D;;;
Affects Version/s.1: 2.2.0
Component/s.1: 
Comment.1: 25/Jan/17 06:55;tejasp;[~cloud_fan] [~hvanhovell] : Are you guys ok with the proposal (link in jira description) or still want to discuss over any items ?;;;, 28/Aug/17 16:55;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/19001;;;, 09/Jan/18 16:15;apachespark;User 'tejasapatil' has created a pull request for this issue:
https://github.com/apache/spark/pull/20206;;;, 30/Jan/18 17:16;ferdonline;Thanks a lot for this great contribution to Spark.

I was just wondering, would it make sense to apply this to direct outputs (e.g. write.parquet()), so that we could keep partitioning information - and again avoid reshuffling data before a merge? I believe this is most what saveAsTable() does by default in Spark, but to my mind it would improve the DataFrame write API and make these performance benefits more accessible.

Update:
I've just noticed that it has been considered in [https://github.com/apache/spark/pull/13452.
] [~cloud_fan] [ |https://github.com/apache/spark/pull/13452.]- Is there an Issue to follow up on this feature? Eventually we could simply store a metadata json file together with the data files.;;;, 06/Feb/18 19:36;tejasp;[~ferdonline] : The feature you are referring to is not in the scope of this Jira as storing bucketing metadata outside of metastore is not something that hive does. Could you please write to dev@ and create a Jira to initiate this discussion ?;;;, 25/Apr/18 05:55;advancedxy;Hi [~tejasp] [~cloud_fan], are you still working on this? We also need this feature in our internal Spark stack, is there anything pending?

cc [~XuanYuan];;;, 09/Oct/18 17:59;shay_elbaz;+1

[~tejasp] is this still under progress?;;;, 27/Nov/18 00:48;chengsu;After discussion with [~tejasp], I think the most of un-merged pr ([https://github.com/apache/spark/pull/19001)] looks good, and is still valid on latest trunk. I am picking up this task, and starting working on this. cc [~advancedxy] and [~shay_elbaz]. Thanks.;;;, 27/Nov/18 08:48;shay_elbaz;[~chengsu] this is great! If there is anything I can do to assist please ask :);;;, 27/Nov/18 09:02;advancedxy;[~chengsu] great news. I can do some review work then.;;;, 17/Jul/19 08:13;aditya.dataengg;[~chengsu] any update on this?;;;
Comment.2: 25/Jan/17 07:02;tejasp;BTW: In its current state, Spark writes data to hive bucketed tables but the outputs will not conform with hive's bucketing semantics. This can lead to data corruption if Spark outputs are used by Hive. I feel disabling writes to hive bucketing tables via Spark might be a good initial step to add. What do you think ?;;;, 17/Jul/19 18:28;chengsu;[~aditya.dataengg] - I started the work by submitting a PR ([https://github.com/apache/spark/pull/23163]) for https://issues.apache.org/jira/browse/SPARK-26164, and it is still under review. I will ping relevant reviewers to see whether we can speed it up, thanks.;;;, 05/Nov/19 15:39;vikramhegde;Hi [~tejasp] [~chengsu]

How to join two bucketed tables, when tables are bucketed on multiple columns. I tried to join, but still exchanges were getting created.

Below is the scenario:

table1 bucketed by a 10 buckets

table2 bucketed by a,b 10 bucktes

how to join table1 and table2 on column a without exchanges?;;;, 09/Mar/20 14:21;FelixKJose;[~chengsu] Any updates on this feature (23163 and 26164)? 

{color:#172b4d}*Note:*{color} Also I have seen recent Hive has introduced some changes in their Bucketing  Strategy and Big Data engines like Presto incorporated those changes. Are we considering the compatibility with these new HIVE changes?
Link to Presto/HIVE Bucketing improvements/changes: [https://prestosql.io/blog/2019/05/29/improved-hive-bucketing.html];;;, 16/Mar/20 03:31;FelixKJose;Expose a configuration parameter for Hive Hashing during BucketBy operation. This config is missing or not documented;;;, 18/Jun/20 19:12;viru;This is a much needed feature +1 vote;;;, 27/Aug/20 05:56;chengsu;Hi all, after discussion with [~cloud_fan], we decide to support Hive bucketed table write path. Specifically:

1.for Hive 3.x.y, writing Hive bucketed table with Hive murmur3hash.

2.for Hive 2.x.y and 1.x.y, writing Hive bucketed table with Hive hivehash.

3.zero or multiple files per bucket should be expected by user.

 

This is compatible with Hive and Presto (presto-sql for now), so user can be benefitted from writing Hive bucketed table correctly from Spark, and read bucketed table efficiently from Presto and Hive.

See JIRA description for details, and I will work on each sub-tasks, thanks.;;;, 27/Aug/20 06:44;viirya;> Do not allow for writing Hive non-ORC/Parquet bucketed table: throw exception by default if writing non-orc/parquet bucketed table (code path: InsertIntoHiveTable), and exception can be disabled by setting config `hive.enforce.bucketing`=false and `hive.enforce.sorting`=false, which will write as non-bucketed table.

Do you mean Hive ORC/Parquet bucketed table not converted to data source relation? InsertIntoHiveTable should also write Hive ORC/Parquet table which is not converted.;;;, 27/Aug/20 06:47;viirya;> Hive on Tez: support zero and multiple files per bucket (https://issues.apache.org/jira/browse/HIVE-13988).

Is that correct JIRA? Seems not and it was closed as "Won't Fix".;;;, 27/Aug/20 18:00;chengsu;[~viirya] -

> Do you mean Hive ORC/Parquet bucketed table not converted to data source relation? InsertIntoHiveTable should also write Hive ORC/Parquet table which is not converted.



No. Hive ORC/Parquet bucketed table converted to data source relation. Hive non-ORC/non-Parquet bucketed table is not converted to data source relation, and still use Hive code path (InsertIntoHiveTable). I agree there can be cases where we use Hive code path for Hive ORC/Parquet table (e.g. by tweaking config `spark.sql.orc.impl`, `spark.sql.hive.convertInsertingPartitionedTable`, etc).

 

> Is that correct JIRA? Seems not and it was closed as "Won't Fix".

Sorry I meant this one - https://issues.apache.org/jira/browse/HIVE-14014 . Hive on Tez allows 0 file per bucket. More code pointer - [https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java#L183-L212] . Updated description.;;;, 18/Nov/20 15:34;findepi;>  1.for Hive 3.x.y, writing Hive bucketed table with Hive murmur3hash.

> 2.for Hive 2.x.y and 1.x.y, writing Hive bucketed table with Hive hivehash.

 

Should i assume this is a simplification and it will be actually controlled by {{bucketing_version}} table property?
e.g. Hive 3 and Presto 321+ support both bucketing version and user can still choose one (should they have a compatibility requirement with older systems).;;;
Comment.3: 28/Apr/17 23:02;tejasp;[~cloud_fan], [~sameerag] : I was looking at trunk and observed changes which would affect the plan (more from implementation perspective not the high level design). 

`InsertIntoHiveTable` is now a `RunnableCommand` (unlike earlier `UnaryExecNode`). With exec node, it was possible to set the requiredDistribution and requiredOrdering and let the planner (`EnsureRequirements`) take care of managing things. With `RunnableCommand`, the model seems to be that these requirements have to handled separately (so far there is only one place which does that: [0]). Two comments:
- I feel that this somewhat ugly as one would expect `EnsureRequirements` to be a single place for handling this
- we might miss out optimizations. eg. If I am adding an extra shuffle in `InsertIntoHiveTable` and if the previous node was shuffle as well, the code for merging these two shuffle nodes as a single shuffle would have to duplicated as well from `EnsureRequirements`.

Would it be OK to make `InsertIntoHiveTable` as a `UnaryExecNode` ?
Why was it made a `RunnableCommand` recently (https://github.com/apache/spark/pull/16517) ? cc [~smilegator]

[0] : https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileFormatWriter.scala#L173;;;, 18/Nov/20 19:33;chengsu;> Should i assume this is a simplification and it will be actually controlled by {{bucketing_version}} table property?

You can think this way. Only if bucketing_version is 2 (for Hive 3.x.y case), Spark will use Hive murmur3hash to write Hive bucketed table, and this is same to Hive's behavior.;;;, 22/Nov/20 07:56;asafh;Can you please update about the progress of this issue?
It's much-needed for our use-case.;;;, 30/Nov/20 14:43;pshar0;Thanks [~tejasp] [~chengsu]. - Thanks for all your work on this issue.  This will be a huge win for joining large datasets efficiently using open source. +1;;;, 01/Dec/20 06:59;chengsu;Sorry for delaying, just an update for progress. We identified a major blocker (https://issues.apache.org/jira/browse/SPARK-33298) needs to be resolved before merging hive bucketing write support. The underlying design is still in discussion, and current ETA for all features is Spark 3.2. We are already near the branch cut time for Spark 3.1, so we have to target this to the next release, thanks.;;;, 12/Mar/21 19:49;zsxwing;I read [https://docs.google.com/document/d/1a8IDh23RAkrkg9YYAeO51F4aGO8-xAlupKwdshve2fc/edit?usp=sharing] but didn't find any section about compatibility. I'm curious about both the forward and backward compatibility. For example, 
 * What happens if using an old Spark version to read this new format? Does it treat it as a non-bucketed table, or read it a a bucketed table but may return different results?
 * How does Spark know which bucket format when reading an existing table, and how does it decide whether to use the old format or the new format to read and write?;;;, 12/Mar/21 20:09;chengsu;[~zsxwing] - sorry the design doc was years old, and I can create a short new one if needed. Note here this Jira is supporting writing hive bucketed table, but not reading. The reason is we don't want to introduce hash function change to `HashPartitioning` right now, and would like to do this in a more general way with data source v2.

So

> What happens if using an old Spark version to read this new format? Does it treat it as a non-bucketed table, or read it a a bucketed table but may return different results?

The read behavior is not changed. Spark will read the Hive/Trino bucketed table as non-bucketed table. This has no problem for data correctness as bucketing is just an optimization.

> How does Spark know which bucket format when reading an existing table, and how does it decide whether to use the old format or the new format to read and write?

For new format (Hive murmur3hash), Hive is adding a table property "bucketing_version"=2 to metastore for this kind of new bucketed table, and Trino followed that. In the future if we want to read old/new Hive bucketed table with bucketing enabled, we can decide based on this property field.

For writing new format or old format, default behavior can be depending on Hive version used during runtime for Spark query. For Hive 3.x.y, writing Hive bucketed table with Hive murmur3hash, and for Hive 2.x.y and 1.x.y, writing Hive bucketed table with Hive hivehash. We can also add a Hive config for user to control this behavior if needed.

 ;;;, 12/Mar/21 20:20;zsxwing;[~chengsu] Thanks for the answers. Sounds like Metastore is a requirement to distinguish two formats. If I use Spark to write to a table using the table path directly, it will break the table. Right?;;;, 12/Mar/21 21:05;chengsu;[~zsxwing] - if users gets table path, and writes data directly to it without respecting table metadata, it will break the table layout. I am assuming this applies to Spark bucketed table as well (the one using spark murmur3hash). Shouldn't it?;;;, 23/Jun/21 12:26;pushkarcse;Hi [~chengsu] , Could you please update us here.;;;, 23/Jun/21 22:52;chengsu;[~pushkarcse] - we are currently working on https://issues.apache.org/jira/browse/SPARK-33298 . After that, I will resume the discussion of https://issues.apache.org/jira/browse/SPARK-32709 .;;;
Comment.4: 01/May/17 06:24;cloud_fan;Yea I agree that we should use {{requiredDistribution}} and {{requiredOrdering}} to group and sort the input data instead of doing it manually in `FileFormatWriter`, but at the time when we were refactoring {{InsertIntoHiveTable}}, it was better than before, because previously we group and sort the input data in {{XXXWriteContainer}}, and the code is very similar to {{FileFormatWriter}}.;;;, 23/Jun/21 22:54;pushkarcse;Thank you [~chengsu]!!;;;, 19/Dec/23 15:15;eric.xiao;Hi [~tejasp], for the remaining work, would i be able to get assigned those tickets? I would like to work on them :).;;;, 06/Mar/24 03:57;mrbrahman;Don't really like spamming in the comments - but this is a much needed feature for big-data processing, and has been pending for a while now. Can this be given some love please? :D;;;
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Should expose driver service name to config for user features
Issue key: SPARK-40763
Issue id: 13485776
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: zWangSheng
Creator: zWangSheng
Created: 12/Oct/22 02:24
Updated: 06/Mar/24 02:49
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Current on kubernetes, user's feature step, which build user's kubernetes resource during spark submit spark pod, can't percept some spark resource info, such as spark driver service name.

 

User may want to expose some spark pod info to build their custom resource, such as ingress, etc.

 

We want the way expose now spark driver service name, which is now generated by clock and uuid.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 12 02:25:32 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19a2g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Oct/22 02:25;apachespark;User 'zwangsheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/38202;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: K8s will not allocate more execs if there are any pending execs until next snapshot
Issue key: SPARK-42261
Issue id: 13522285
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 31/Jan/23 18:51
Updated: 03/Feb/24 00:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1, 3.5.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: pull-request-available
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Feb 01 03:42:30 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1fiog:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 4.0.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/Jan/23 19:00;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/39825;;;, 01/Feb/23 03:42;dongjoon;I'm -1 because this is a feature to prevent pending resource (pod and dependent resources like PVCs) explosions which results EKS control plane congestion and a waste of money.;;;
Affects Version/s.1: 3.3.1
Component/s.1: 
Comment.1: 01/Feb/23 03:42;dongjoon;I'm -1 because this is a feature to prevent pending resource (pod and dependent resources like PVCs) explosions which results EKS control plane congestion and a waste of money.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.12.0, EMR-6.13.0, EMR-6.14.0, EMR-6.15.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0

Summary: ConfigMap has the same name when launching two pods on the same namespace
Issue key: SPARK-41006
Issue id: 13494573
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: ejblanco
Creator: ejblanco
Created: 03/Nov/22 12:10
Updated: 10/Dec/23 00:20
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.0, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: pull-request-available
Description: If we use the Spark Launcher to launch our spark apps in k8s:
{code:java}
val sparkLauncher = new InProcessLauncher()
 .setMaster(k8sMaster)
 .setDeployMode(deployMode)
 .setAppName(appName)
 .setVerbose(true)

sparkLauncher.startApplication(new SparkAppHandle.Listener { ...{code}
We have an issue when we launch another spark driver in the same namespace where other spark app was running:
{code:java}
kp -n audit-exporter-eee5073aac -w
NAME                                     READY   STATUS        RESTARTS   AGE
audit-exporter-71489e843d8085c0-driver   1/1     Running       0          9m54s
audit-exporter-7e6b8b843d80b9e6-exec-1   1/1     Running       0          9m40s
data-io-120204843d899567-driver          0/1     Terminating   0          1s
data-io-120204843d899567-driver          0/1     Terminating   0          2s
data-io-120204843d899567-driver          0/1     Terminating   0          3s
data-io-120204843d899567-driver          0/1     Terminating   0          3s{code}
The error is:
{code:java}
{"time":"2022-11-03T12:49:45.626Z","lvl":"WARN","logger":"o.a.s.l.InProcessAppHandle","thread":"spark-app-38: 'data-io'","msg":"Application failed with exception.","stack_trace":"io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: PUT at: https://kubernetes.default/api/v1/namespaces/audit-exporter-eee5073aac/configmaps/spark-drv-d19c37843d80350c-conf-map. Message: ConfigMap \"spark-drv-d19c37843d80350c-conf-map\" is invalid: data: Forbidden: field is immutable when `immutable` is set. Received status: Status(apiVersion=v1, code=422, details=StatusDetails(causes=[StatusCause(field=data, message=Forbidden: field is immutable when `immutable` is set, reason=FieldValueForbidden, additionalProperties={})], group=null, kind=ConfigMap, name=spark-drv-d19c37843d80350c-conf-map, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=ConfigMap \"spark-drv-d19c37843d80350c-conf-map\" is invalid: data: Forbidden: field is immutable when `immutable` is set, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=Invalid, status=Failure, additionalProperties={}).\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:682)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:661)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:612)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:555)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:518)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleUpdate(OperationSupport.java:342)\n\tat io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleUpdate(OperationSupport.java:322)\n\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleUpdate(BaseOperation.java:649)\n\tat io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.lambda$replace$1(HasMetadataOperation.java:195)\n\tat io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation$$Lambda$5360/000000000000000000.apply(Unknown Source)\n\tat io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.replace(HasMetadataOperation.java:200)\n\tat io.fabric8.kubernetes.client.dsl.base.HasMetadataOperation.replace(HasMetadataOperation.java:141)\n\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation$$Lambda$4618/000000000000000000.apply(Unknown Source)\n\tat io.fabric8.kubernetes.client.utils.CreateOrReplaceHelper.replace(CreateOrReplaceHelper.java:69)\n\tat io.fabric8.kubernetes.client.utils.CreateOrReplaceHelper.createOrReplace(CreateOrReplaceHelper.java:61)\n\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.createOrReplace(BaseOperation.java:318)\n\tat io.fabric8.kubernetes.client.dsl.base.BaseOperation.createOrReplace(BaseOperation.java:83)\n\tat io.fabric8.kubernetes.client.dsl.internal.NamespaceVisitFromServerGetWatchDeleteRecreateWaitApplicableImpl.createOrReplace(NamespaceVisitFromServerGetWatchDeleteRecreateWaitApplicableImpl.java:105)\n\tat io.fabric8.kubernetes.client.dsl.internal.NamespaceVisitFromServerGetWatchDeleteRecreateWaitApplicableListImpl.lambda$createOrReplace$7(NamespaceVisitFromServerGetWatchDeleteRecreateWaitApplicableListImpl.java:174)\n\tat io.fabric8.kubernetes.client.dsl.internal.NamespaceVisitFromServerGetWatchDeleteRecreateWaitApplicableListImpl$$Lambda$5012/000000000000000000.apply(Unknown Source)\n\tat java.base/java.util.stream.ReferencePipeline$3$1.accept(Unknown Source)\n\tat java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(Unknown Source)\n\tat java.base/java.util.stream.AbstractPipeline.copyInto(Unknown Source)\n\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(Unknown Source)\n\tat java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(Unknown Source)\n\tat java.base/java.util.stream.AbstractPipeline.evaluate(Unknown Source)\n\tat java.base/java.util.stream.ReferencePipeline.collect(Unknown Source)\n\tat io.fabric8.kubernetes.client.dsl.internal.NamespaceVisitFromServerGetWatchDeleteRecreateWaitApplicableListImpl.createOrReplace(NamespaceVisitFromServerGetWatchDeleteRecreateWaitApplicableListImpl.java:176)\n\tat io.fabric8.kubernetes.client.dsl.internal.NamespaceVisitFromServerGetWatchDeleteRecreateWaitApplicableListImpl.createOrReplace(NamespaceVisitFromServerGetWatchDeleteRecreateWaitApplicableListImpl.java:54)\n\tat org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:175)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5(KubernetesClientApplication.scala:248)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5$adapted(KubernetesClientApplication.scala:242)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication$$Lambda$4885/000000000000000000.apply(Unknown Source)\n\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2764)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:242)\n\tat org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:214)\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n\tat org.apache.spark.deploy.InProcessSparkSubmit$.main(SparkSubmit.scala:987)\n\tat org.apache.spark.deploy.InProcessSparkSubmit.main(SparkSubmit.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor432.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n\tat org.apache.spark.launcher.InProcessAppHandle.lambda$start$0(InProcessAppHandle.java:72)\n\tat org.apache.spark.launcher.InProcessAppHandle$$Lambda$4658/000000000000000000.run(Unknown Source)\n\tat java.base/java.lang.Thread.run(Unknown Source)\n"  {code}
When launching the second algorithm (data-io) in the same namespace, the name of the configmap is the same as the one created for the previous running algorithm (spark-drv-d19c37843d80350c-conf-map), and since it is immutable, it fails.

The solution is pretty straightforward, simply change here:
[https://github.com/apache/spark/blob/master/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/submit/KubernetesClientUtils.scala#L46]

From val to def, so the uniqueID is always new when we call to configMapNameDriver:
{code:java}
def configMapNameDriver: String = configMapName(s"spark-drv-${KubernetesUtils.uniqueID()}") {code}
We have tested and it works in our case. I could do the pull request if you think it is a bug
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 28 12:19:47 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1as6o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Mar/23 13:43;apachespark;User 'DHKold' has created a pull request for this issue:
https://github.com/apache/spark/pull/40491;;;, 21/Mar/23 11:03;dhkold;As mentionned above, I created a Pull Request with the correction. I updated the Unit Tests accordingly.;;;, 28/Mar/23 09:48;dhkold;Anyone to check this? [~dongjoon]  (Sorry if tagging the wrong person, I don't know who I can tag);;;, 28/Mar/23 12:19;dongjoon;Thank you for reporting and pinging me, [~dhkold]. Let me take a look at your PR.;;;
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 21/Mar/23 11:03;dhkold;As mentionned above, I created a Pull Request with the correction. I updated the Unit Tests accordingly.;;;
Comment.2: 28/Mar/23 09:48;dhkold;Anyone to check this? [~dongjoon]  (Sorry if tagging the wrong person, I don't know who I can tag);;;
Comment.3: 28/Mar/23 12:19;dongjoon;Thank you for reporting and pinging me, [~dhkold]. Let me take a look at your PR.;;;
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: WholeStageCodeGen pipelineTime metric incorrect result.
Issue key: SPARK-44793
Issue id: 13547061
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: abmodi
Creator: abmodi
Created: 13/Aug/23 05:27
Updated: 01/Dec/23 00:21
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: PipelineTime metric of wholestagecodegen is added multiple times if hasNext is called multiple times after there are no rows left.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-08-13 05:27:02.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1jqzk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Rename trait Unevaluable
Issue key: SPARK-39769
Issue id: 13471457
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: yuzhihong@gmail.com
Creator: yuzhihong@gmail.com
Created: 13/Jul/22 17:56
Updated: 27/Nov/23 02:26
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: pull-request-available
Description: I came upon `trait Unevaluable` which is defined in sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala

Unevaluable is not a word.

There are `valuable`, `invaluable` but I have never seen Unevaluable.

This issue renames the trait to Unevaluatable
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-07-13 17:56:05.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16ub4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: append to parquet file with column type changed corrupts fie
Issue key: SPARK-46093
Issue id: 13559344
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: rich84t
Creator: rich84t
Created: 24/Nov/23 15:06
Updated: 24/Nov/23 15:06
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Input/Output
Due Date: 
Votes: 0
Labels: 
Description: from pyspark.sql.functions import *
from pyspark.sql.types import *

fnBad = "dbfs:/tmp/richard.gooding@os.uk/test_bad_parquet/f1"
df = spark.createDataFrame( [ ["aaaa" ] ] ).select( col("_1").alias("aa") )
df.printSchema()

fmt = "parquet"
# fmt = "delta"
df.write.mode("overwrite").format( fmt ) .save( fnBad )
df.show()

df = df.withColumn( "aa", struct( col("aa")) ) # change type of column - error on load
df.printSchema()
df.show()
df.write.mode("append").format( fmt).save( fnBad ) # format = delta :   "AnalysisException: Failed to merge fields 'aa' and 'aa'. Failed to merge incompatible data types StringType and StructType(StructField(aa,StringType,true))"
# df.write.mode("append").option("mergeSchema", "true").format(fmt).save( fnBad ) # gives a different error, but only when dataframe read

print(" --- at df 2 --- ")
df2 = spark.read.format(fmt).load( fnBad )
# df2 = spark.read.option("mergeSchema", "true").format(fmt).load( fnBad )
df2.show()  # this will error - 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-11-24 15:06:10.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1lu1s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Make improvement to LocalKMeans
Issue key: SPARK-43297
Issue id: 13534192
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: leo wen
Creator: leo wen
Created: 26/Apr/23 17:00
Updated: 22/Sep/23 00:17
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: MLlib
Due Date: 
Votes: 0
Labels: pull-request-available
Description: There are two initializationMode in Kmeans, random mode and parallel mode.

The ParallelMode is using kmeansPlusPlus to generate the centers point, but the kMeansPlusPlus is a local method which runs in the driver.

If the scale of points is huge, the kMeansPlusPlus will run for a long time, because it is a single thread method running in the driiver.

We can make this method run in parallel to make it faster, such as using Parallel collections. 

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jun 02 04:08:15 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1hjyg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/May/23 14:47;srowen;Can you make a pull request to show what you mean?;;;, 02/Jun/23 04:08;snoot;User 'wenwj0' has created a pull request for this issue:
https://github.com/apache/spark/pull/41384;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 02/Jun/23 04:08;snoot;User 'wenwj0' has created a pull request for this issue:
https://github.com/apache/spark/pull/41384;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add Spark History Server Links for Kubernetes & other CMs
Issue key: SPARK-37562
Issue id: 13415668
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: holden
Reporter: holden
Creator: holden
Created: 06/Dec/21 19:00
Updated: 26/Jul/23 22:39
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.1, 3.3.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: In YARN we have the Spark history server configured with `spark.yarn.historyServer.address` which allows us to print out useful links on startup for eventual debugging. More than just YARN can have the history server. We should either add `spark.kubernetes.historyServer.address` or move it to `spark.historyServer.address` w/a fall back to the old YARN specific config.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 26 22:39:24 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xego:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Dec/21 19:00;holden;cc [~dongjoon] any thoughts on the param name we should use?;;;, 07/Dec/21 00:32;dongjoon;Does this JIRA simply aim to advertise some random URLs (from the setting) by embedding it into the log? When you say `K8s and other CMs`, do you have any other goal to achieve in this JIRA?
1. K8s
2. Mesos
3. Standalone cluster
4. Notebook (like Zeppelin)

I'm curious about the benefit of this JIRA issue.;;;, 26/Jul/23 22:39;holden;So (in theory) the cluster administrator has some base config, they set it up. They also configure a history server location. When we run on YARN they can configure that location the the log URL will be printed with the correct location (e.g. [historyserver]/[app] ) for someone investigating after the case.

 

This just proposes to generalize the YARN config.;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 07/Dec/21 00:32;dongjoon;Does this JIRA simply aim to advertise some random URLs (from the setting) by embedding it into the log? When you say `K8s and other CMs`, do you have any other goal to achieve in this JIRA?
1. K8s
2. Mesos
3. Standalone cluster
4. Notebook (like Zeppelin)

I'm curious about the benefit of this JIRA issue.;;;
Comment.2: 26/Jul/23 22:39;holden;So (in theory) the cluster administrator has some base config, they set it up. They also configure a history server location. When we run on YARN they can configure that location the the log URL will be printed with the correct location (e.g. [historyserver]/[app] ) for someone investigating after the case.

 

This just proposes to generalize the YARN config.;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Setting the topKSortFallbackThreshold value may lead to inaccurate results
Issue key: SPARK-44240
Issue id: 13541884
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: dzcxzl
Creator: dzcxzl
Created: 29/Jun/23 05:13
Updated: 29/Jun/23 16:16
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 2.4.0, 3.0.0, 3.1.0, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description:  
{code:java}
set spark.sql.execution.topKSortFallbackThreshold=10000;
SELECT min(id) FROM ( SELECT id FROM range(999999999) ORDER BY id LIMIT 10000) a; {code}
 

If GlobalLimitExec is not the final operator and has a sort operator, shuffle read does not guarantee the order, which leads to the limit read data that may be random.

TakeOrderedAndProjectExec has ordering, so there is no such problem.

 

!topKSortFallbackThreshold.png!
{code:java}
set spark.sql.execution.topKSortFallbackThreshold=10000;
select min(id) from (select  id  from range(999999999) order by id desc limit 10000) a; {code}
!topKSortFallbackThresholdDesc.png!

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 29/Jun/23 05:14;dzcxzl;topKSortFallbackThreshold.png;https://issues.apache.org/jira/secure/attachment/13060953/topKSortFallbackThreshold.png, 29/Jun/23 16:14;dzcxzl;topKSortFallbackThresholdDesc.png;https://issues.apache.org/jira/secure/attachment/13060975/topKSortFallbackThresholdDesc.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-06-29 05:13:46.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1iv4g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.0.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.12.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: EXPODE function selects outer column
Issue key: SPARK-40974
Issue id: 13493690
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: ismailo
Creator: ismailo
Created: 31/Oct/22 09:02
Updated: 16/Feb/23 22:14
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Im trying to determine if indirectly selecting an outer column is a bug or an intended feature of the EXPLODE function. 

 

If I run the following SQL statement:

```

SELECT

      (SELECT FIRST(name_element_)

        FROM LATERAL VIEW EXPLODE(name) AS name_element_

       *)*

FROM patient

```

 

it fails with:

```

Accessing outer query column is not allowed in:

Generate explode(outer(name#9628))

```

 

However, if I do a "cheeky select" (bolded below), the SQL query is valid and runs:

```

SELECT(

    SELECT FIRST(name_element_)

    FROM (SELECT EXPLODE(name_element_) AS name_element_ 

          \{*}FROM ({*}{*}SELECT{*} *name AS name_element_)*

        **        )

         )

FROM patient

```

From the viewpoint of the EXPLODE function, it seems like the column name_element_ does not come from an outer column. Is this an intended feature or a bug?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Feb 16 22:14:29 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1amrs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/Oct/22 09:09;apachespark;User 'clairezhuang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38446;;;, 31/Oct/22 15:20;apachespark;User 'clairezhuang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38446;;;, 31/Oct/22 15:21;apachespark;User 'clairezhuang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38446;;;, 16/Feb/23 22:14;ismailo;Any update on this?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 31/Oct/22 15:20;apachespark;User 'clairezhuang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38446;;;
Comment.2: 31/Oct/22 15:21;apachespark;User 'clairezhuang' has created a pull request for this issue:
https://github.com/apache/spark/pull/38446;;;
Comment.3: 16/Feb/23 22:14;ismailo;Any update on this?;;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Inconsistent data produced by `FlatMapCoGroupsInPandas`
Issue key: SPARK-42397
Issue id: 13524199
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: tedjenks
Creator: tedjenks
Created: 10/Feb/23 10:26
Updated: 13/Feb/23 08:23
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0, 3.3.1
Fix Version/s: 
Component/s: Pandas API on Spark, SQL
Due Date: 
Votes: 0
Labels: 
Description: We are seeing inconsistent data returned when using `FlatMapCoGroupsInPandas`. In the PySpark example from the comments, when we call `grouped_df.collect()` we get:

 

{{[Row(left_colms="Index(['cluster', 'event', 'abc'], dtype='object')", right_colms="Index(['cluster', 'event', 'def'], dtype='object')")] }}

 

When we call `grouped_df.show(5, truncate=False)` we get:

 

{{[Row(left_colms="Index(['cluster', 'abc'], dtype='object')", right_colms="Index(['cluster', 'event', 'def'], dtype='object')", xyz='1234')] }}

 

When we call `grouped_df_1.collect()` we get:

 

{{[Row(left_colms="Index(['cluster', 'abc'], dtype='object')", right_colms="Index(['cluster', 'event', 'def'], dtype='object')", xyz='1234')] }}

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Feb 13 08:23:58 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1fufk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Feb/23 10:28;tedjenks;{{    test_df = spark.createDataFrame(}}
{{        [}}
{{            ["1", "23", "abc", "blah", "def", "1"],}}
{{            ["1", "23", "abc", "blah", "def", "1"],}}
{{            ["1", "23", "abc", "blah", "def", "2"],}}
{{            ["1", "23", "abc", "blah", "def", "2"],}}
{{        ],}}
{{        ["cluster", "partition", "event", "abc", "def", "one_or_two"]}}
{{    )}}
{{    df1 = test_df.filter(}}
{{        F.col("one_or_two") == "1"}}
{{    ).select(}}
{{        "cluster", "event", "abc"}}
{{    )}}{{    df2 = test_df.filter(}}
{{        F.col("one_or_two") == "2"}}
{{    ).select(}}
{{        "cluster", "event", "def"}}
{{    )}}
{{    def get_schema(l, r):}}
{{            return pd.DataFrame(}}
{{                [(str(l.columns), str(r.columns))],}}
{{                columns=["left_colms", "right_colms"]}}
{{            )}}{{   grouped_df = df1.groupBy("cluster").cogroup(df2.groupBy("cluster")).applyInPandas(}}
{{        get_schema, "left_colms string, right_colms string"}}
{{    )}}
{{    grouped_df_1 = grouped_df.withColumn(}}
{{       "xyz", F.lit("1234")}}
{{     )}};;;, 13/Feb/23 06:53;gurwls223;It's probably related the order which Spark doesn't guarantee. Is the actual value different?;;;, 13/Feb/23 08:23;tedjenks;Is it ever expected for df.show() and df.collect() to give different results? That is what struck me as odd in this case and yes those two give different values.;;;
Affects Version/s.1: 3.3.1
Component/s.1: SQL
Comment.1: 13/Feb/23 06:53;gurwls223;It's probably related the order which Spark doesn't guarantee. Is the actual value different?;;;
Comment.2: 13/Feb/23 08:23;tedjenks;Is it ever expected for df.show() and df.collect() to give different results? That is what struck me as odd in this case and yes those two give different values.;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Missing typing for pandas_udf
Issue key: SPARK-42235
Issue id: 13521983
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: donggu
Creator: donggu
Created: 30/Jan/23 05:45
Updated: 11/Feb/23 13:47
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: The typing stub {{site-packages/pyspark/sql/pandas/functions.pyi}} has a list of possible signatures of {{pandas_udf}}. It is missing a case

{code:python}
import pyspark.sql.functions as F

# PySpark3's typing stub error
@F.pandas_udf(
    returnType=LongType()
)
def my_udf(dummy_col: pd.Series) -> pd.Series:
    ...
{code}

The stub defined {{pandas_udf(f, returnType, functionType)}} but not {{pandas_udf(f, returnType)}}. The official documentation recommends using a return type hint instead of {{returnType}}.
 
{code:python}
# defined
@overload
def pandas_udf( f: PandasScalarToScalarFunction, returnType: Union[AtomicDataTypeOrString, ArrayType], functionType: PandasScalarUDFType, ) -> UserDefinedFunctionLike: ...
 
# not defined
@overload
def pandas_udf( f: PandasScalarToScalarFunction, returnType: Union[AtomicDataTypeOrString, ArrayType]) -> UserDefinedFunctionLike: ...
{code}
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Feb 11 13:47:18 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1fgu0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Feb/23 13:47;apachespark;User 'wayneguow' has created a pull request for this issue:
https://github.com/apache/spark/pull/39974;;;, 11/Feb/23 13:47;apachespark;User 'wayneguow' has created a pull request for this issue:
https://github.com/apache/spark/pull/39974;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 11/Feb/23 13:47;apachespark;User 'wayneguow' has created a pull request for this issue:
https://github.com/apache/spark/pull/39974;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Multi sparkSession should share single SQLAppStatusStore
Issue key: SPARK-41555
Issue id: 13514287
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: lijiahong
Creator: lijiahong
Created: 17/Dec/22 00:49
Updated: 17/Dec/22 03:03
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.1, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: In spark , if we create multi sparkSession in the program, we will get multi-SQLTab in UI, 

At the same time, we will get muti-SQLAppStatusListener object, it is waste of memory.

code like this:

 
{code:java}
// code placeholder
def main(args: Array[String]): Unit = {
val sparkConf = new SparkConf()
.setAppName("demo")
.setMaster("local[*]")

val spark = SparkSession.builder()
.config(sparkConf)
.getOrCreate()

setDefaultSession(null)
setActiveSession(null)

val spark2 = SparkSession.builder()
.config(sparkConf)
.getOrCreate()

import spark.implicits._
val testData = spark.sparkContext
.parallelize((1 to 3).map(i => TestData(i, i.toString))).toDF()
testData.createOrReplaceTempView("testTable")
val testData2 = spark.sparkContext.parallelize(
TestData2(1, "1") ::
TestData2(1, "2") ::
TestData2(2, "1") ::
TestData2(2, "2") ::
TestData2(3, "1") ::
TestData2(3, "2") ::
Nil, 2).toDF()

testData2.createOrReplaceTempView("testTable2")
val query = "select ind2,count(*) from ( select * from testTable2 join testTable on testTable.ind = testTable2.ind2 where testTable.name <> '1') group by ind2"
spark.sql(query).collect()

Thread.sleep(500000)
spark.stop()
}
case class TestData(ind: Int, name: String)
case class TestData2(ind2: Int, name: String) {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 17/Dec/22 00:50;lijiahong;muti-SQLStore.png;https://issues.apache.org/jira/secure/attachment/13053934/muti-SQLStore.png, 17/Dec/22 00:50;lijiahong;muti-sqltab.png;https://issues.apache.org/jira/secure/attachment/13053933/muti-sqltab.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Dec 17 01:16:30 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1e5qw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Dec/22 01:08;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39101;;;, 17/Dec/22 01:09;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39101;;;, 17/Dec/22 01:16;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39102;;;
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 17/Dec/22 01:09;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39101;;;
Comment.2: 17/Dec/22 01:16;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39102;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: pyspark.pandas .apply() changing rows ordering
Issue key: SPARK-40063
Issue id: 13476616
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: marcelorossini
Creator: marcelorossini
Created: 12/Aug/22 21:10
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Pandas API on Spark
Due Date: 
Votes: 0
Labels: Pandas, PySpark
Description: When using the apply function to apply a function to a DataFrame column, it ends up mixing the column's rows ordering.

A command like this:
{code:java}
def example_func(df_col):
  return df_col ** 2 

df['col_to_apply_function'] = df.apply(lambda row: example_func(row['col_to_apply_function']), axis=1) {code}
A workaround is to assign the results to a new column instead of the same one, but if the old column is dropped, the same error is produced.

Setting one column as index also didn't work.
Environment: Databricks Runtime 11.1
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python
Custom field (Last public comment date): Wed Aug 17 19:36:12 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17q14:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Aug/22 05:39;gurwls223;{quote}
 it ends up mixing the column's rows ordering.
{quote}

Can you show the expected/actual output? What's column's rows ordering?;;;, 14/Aug/22 22:04;marcelorossini;I can't really print it because it contains sensitive information, but assume that the DataFrame df is defined as:
||Col_1||Col_2||col_to_apply_function||
|1|Name1|10|
|2|Name2|15|
|3|Name3|20|
|4|Name4|25|

After applying the function, the results are placed in the wrong lines, like this:
||Col_1||Col_2||col_to_apply_function||
|1|Name1|400|
|2|Name2|625|
|3|Name3|225|
|4|Name4|100|

This error does not happen when I use pandas, only with pyspark.pandas. 
But pandas is impossible to use on a DataFrame with millions of rows.;;;, 16/Aug/22 06:20;gurwls223;Just to clarify, does that only happen in single column? or happen in other columns too? Spark itself doesn't guarantee the order of rows. If you need to keep the natural order, you could try to enable {{compute.ordered_head}} and see what you get.;;;, 16/Aug/22 20:31;marcelorossini;Yes, it doesn't change the other ones.

When I do this on the same column, replacing the data, the order on this column changes.
But if I assign the results to a new column, I get the right order, but if I drop the old one, I get the same problem again on the new one.

About the compute.ordered_head, I actually tried it, but it didn't help.;;;, 17/Aug/22 01:25;gurwls223;[~marcelorossini] what's your "Default Index type"? {{compute.default_index_type}} configuration;;;, 17/Aug/22 19:36;marcelorossini;Normally I use the default, {{{}distributed-sequence{}}}, but I already tried {{sequence}} too and I get the same error.
So, I tried it again, combining with {{compute.ordered_head}} enabled.

This operation requires me to use {{compute.ops_on_diff_frames}} enabled, I think it's worth mentioning.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 14/Aug/22 22:04;marcelorossini;I can't really print it because it contains sensitive information, but assume that the DataFrame df is defined as:
||Col_1||Col_2||col_to_apply_function||
|1|Name1|10|
|2|Name2|15|
|3|Name3|20|
|4|Name4|25|

After applying the function, the results are placed in the wrong lines, like this:
||Col_1||Col_2||col_to_apply_function||
|1|Name1|400|
|2|Name2|625|
|3|Name3|225|
|4|Name4|100|

This error does not happen when I use pandas, only with pyspark.pandas. 
But pandas is impossible to use on a DataFrame with millions of rows.;;;
Comment.2: 16/Aug/22 06:20;gurwls223;Just to clarify, does that only happen in single column? or happen in other columns too? Spark itself doesn't guarantee the order of rows. If you need to keep the natural order, you could try to enable {{compute.ordered_head}} and see what you get.;;;
Comment.3: 16/Aug/22 20:31;marcelorossini;Yes, it doesn't change the other ones.

When I do this on the same column, replacing the data, the order on this column changes.
But if I assign the results to a new column, I get the right order, but if I drop the old one, I get the same problem again on the new one.

About the compute.ordered_head, I actually tried it, but it didn't help.;;;
Comment.4: 17/Aug/22 01:25;gurwls223;[~marcelorossini] what's your "Default Index type"? {{compute.default_index_type}} configuration;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark 3.1.1 Internet Explorer 11 compatibility issues
Issue key: SPARK-35821
Issue id: 13384636
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jobitmathew
Creator: jobitmathew
Created: 19/Jun/21 08:20
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.0.3, 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Web UI
Due Date: 
Votes: 0
Labels: 
Description: Spark UI-Executor tab is empty in IE11

Spark UI-Stages DAG visualization is empty in IE11

other tabs looks Ok.

Spark job history shows completed and incomplete applications list .But when we go inside each application same issue may be there.

Attaching some screenshots
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 21/Jun/21 10:43;jobitmathew;Executortab_Chrome.png;https://issues.apache.org/jira/secure/attachment/13027111/Executortab_Chrome.png, 21/Jun/21 10:43;jobitmathew;Executortab_IE.PNG;https://issues.apache.org/jira/secure/attachment/13027110/Executortab_IE.PNG, 21/Jun/21 10:42;jobitmathew;dag_IE.PNG;https://issues.apache.org/jira/secure/attachment/13027109/dag_IE.PNG, 21/Jun/21 10:42;jobitmathew;dag_chrome.png;https://issues.apache.org/jira/secure/attachment/13027108/dag_chrome.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 4.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 21 10:46:24 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s3a0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Jun/21 03:18;gurwls223;[~jobitmathew] can you fill the JIRA description? ;;;, 20/Jun/21 03:18;gurwls223;cc [~sarutak] FYI;;;, 20/Jun/21 03:19;gurwls223;It would also be great to show some screenshots;;;, 20/Jun/21 04:30;sarutak;[~tsudukim] I remember you have hit this issue a few years ago. Do you know the cause of this issue?;;;, 21/Jun/21 07:54;tsudukim;Yes, our project met the problems like not showing the DAG area in the history page on IE11 several years ago, but we thought that those are just because of IE which has not enough compatibility with HTML5 or something, so we just avoided them by using Firefox. We didn't investigate the cause.;;;, 21/Jun/21 10:46;jobitmathew;[~hyukjin.kwon] I attached some screen shots .Could you please have a look;;;
Affects Version/s.1: 3.1.2
Component/s.1: 
Comment.1: 20/Jun/21 03:18;gurwls223;cc [~sarutak] FYI;;;
Comment.2: 20/Jun/21 03:19;gurwls223;It would also be great to show some screenshots;;;
Comment.3: 20/Jun/21 04:30;sarutak;[~tsudukim] I remember you have hit this issue a few years ago. Do you know the cause of this issue?;;;
Comment.4: 21/Jun/21 07:54;tsudukim;Yes, our project met the problems like not showing the DAG area in the history page on IE11 several years ago, but we thought that those are just because of IE which has not enough compatibility with HTML5 or something, so we just avoided them by using Firefox. We didn't investigate the cause.;;;
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary:  Argument 1 to "rename" of "DataFrame" has incompatible type in pandas.LocIndexerLike.__getitem__
Issue key: SPARK-37551
Issue id: 13415467
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: zero323
Creator: zero323
Created: 05/Dec/21 15:07
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: When I run dev mypy ({{mypy @ git+git://github.com/python/mypy.git@f1eb04ad379fa7492b88794d2c575461079b478a}}) against [{{feba5ac32f2598f6ca8a274850934106be0db64d}}|https://github.com/apache/spark/commit/feba5ac32f2598f6ca8a274850934106be0db64d] I get 

{code:python}
python/pyspark/pandas/indexing.py:558: error: Argument 1 to "rename" of "DataFrame" has incompatible type "Union[Any, Tuple[Any, ...]]"; expected "Union[Dict[Any, Any], Callable[[Any], Any]]"  [arg-type]
{code}

I didn't investigate if further yet, so it is not clear if it is a problem with annotations or bug in mypy.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Dec 08 06:02:18 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xd80:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Dec/21 15:08;zero323;cc [~itholic] [~ueshin], since you're both active in this module.;;;, 08/Dec/21 06:02;gurwls223;cc [~XinrongM] too FYI;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 08/Dec/21 06:02;gurwls223;cc [~XinrongM] too FYI;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add data frame equality check for testing purposes.
Issue key: SPARK-41370
Issue id: 13509267
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: oscard
Creator: oscard
Created: 02/Dec/22 15:07
Updated: 02/Dec/22 15:07
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: We woud like to have the functionality as suggested in https://issues.apache.org/jira/browse/SPARK-28172 . It got closed by an unrelated story. The comment on the story 

> Wouldn't this require to execute both DataFrames and collect the data into driver side? When the datasets are large, it's very easy for users to shoot them in the foot. I won't do that without an explicit plan and design doc for all other operators.

does not make sense since this will be used for unit testing purposes and the dataframes compared will be small.

We are currently using Pandas `assert_frame_equal` https://github.com/pandas-dev/pandas/blob/v1.5.2/pandas/_testing/asserters.py#L1135-L1358 functionality which works very well. However, we are having issues with pandas since it only supports only a subset of the timestamps supported by Spark [https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-timestamp-limits.] This is the first time this became a blocker for us so therefore we would like to have the functionality to validate equality and get feedback on what is different similar to that in Spark. As far as I could see there is nothing in PySpark which currently supports does this.

Please let me know any feedback you have, thanks!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-12-02 15:07:03.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1dasg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Test case for insert partition should verify value 
Issue key: SPARK-40988
Issue id: 13494058
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: rangareddy.avula@gmail.com
Creator: rangareddy.avula@gmail.com
Created: 01/Nov/22 14:48
Updated: 02/Dec/22 08:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.0.0, 3.1.0, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Spark3 has not validated the Partition Column type while inserting the data but on the Hive side exception is thrown while inserting different type values.

*Spark Code:*

 
{code:java}
scala> val tableName="test_partition_table"
tableName: String = test_partition_table

scala>scala> spark.sql(s"DROP TABLE IF EXISTS $tableName")
res0: org.apache.spark.sql.DataFrame = []

scala> spark.sql(s"CREATE EXTERNAL TABLE $tableName ( id INT, name STRING ) PARTITIONED BY (age INT) LOCATION 'file:/tmp/spark-warehouse/$tableName'")
res1: org.apache.spark.sql.DataFrame = []

scala> spark.sql("SHOW tables").show(truncate=false)
+---------+---------------------+-----------+
|namespace|tableName            |isTemporary|
+---------+---------------------+-----------+
|default  |test_partition_table |false      |
+---------+---------------------+-----------+

scala> spark.sql("SET spark.sql.sources.validatePartitionColumns").show(50, false)
+------------------------------------------+-----+
|key                                       |value|
+------------------------------------------+-----+
|spark.sql.sources.validatePartitionColumns|true |
+------------------------------------------+-----+

scala> spark.sql(s"""INSERT INTO $tableName partition (age=25) VALUES (1, 'Ranga')""")
res4: org.apache.spark.sql.DataFrame = []scala> spark.sql(s"show partitions $tableName").show(50, false)
+---------+
|partition|
+---------+
|age=25   |
+---------+

scala> spark.sql(s"select * from $tableName").show(50, false)
+---+-----+---+
|id |name |age|
+---+-----+---+
|1  |Ranga|25 |
+---+-----+---+

scala> spark.sql(s"""INSERT INTO $tableName partition (age=\"test_age\") VALUES (2, 'Nishanth')""")
res7: org.apache.spark.sql.DataFrame = []scala> spark.sql(s"show partitions $tableName").show(50, false)
+------------+
|partition   |
+------------+
|age=25      |
|age=test_age|
+------------+

scala> spark.sql(s"select * from $tableName").show(50, false)
+---+--------+----+
|id |name    |age |
+---+--------+----+
|1  |Ranga   |25  |
|2  |Nishanth|null|
+---+--------+----+ {code}
*Hive Code:*

 

 
{code:java}
> INSERT INTO test_partition_table partition (age="test_age2") VALUES (3, 'Nishanth');
Error: Error while compiling statement: FAILED: SemanticException [Error 10248]: Cannot add partition column age of type string as it cannot be converted to type int (state=42000,code=10248){code}
 

*Expected Result:*

When *spark.sql.sources.validatePartitionColumns=true* it needs to be validated the datatype value and exception needs to be thrown if we provide wrong data type value.

*Reference:*

[https://spark.apache.org/docs/3.3.1/sql-migration-guide.html#data-sources]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Dec 02 08:10:07 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ap0g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Nov/22 14:49;rangareddy.avula@gmail.com;I will work on this issue.;;;, 11/Nov/22 04:55;rangareddy.avula@gmail.com;In Spark 3.4, if we run the following code we can see the *CAST_INVALID_INPUT* exception.
{code:java}
spark.sql(s"""INSERT INTO $tableName partition (age=\"test_age\") VALUES (2, 'Nishanth')"""){code}
*Exception:*
{code:java}
[CAST_INVALID_INPUT] The value 'AGE_34' of the type "STRING" cannot be cast to "INT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "spark.sql.ansi.enabled" to "false" to bypass this error.
== SQL(line 1, position 1) ==
INSERT INTO TABLE partition_table PARTITION(age="AGE_34") VALUES (1, 'ABC')
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'AGE_34' of the type "STRING" cannot be cast to "INT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "spark.sql.ansi.enabled" to "false" to bypass this error.
== SQL(line 1, position 1) ==
INSERT INTO TABLE partition_table PARTITION(age="AGE_34") VALUES (1, 'ABC')
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:161)
    at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)
    at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)
    at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToInt$2(Cast.scala:927)
    at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToInt$2$adapted(Cast.scala:927)
    at org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:588)
    at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToInt$1(Cast.scala:927)
    at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1285)
    at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:526)
    at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:522)
    at org.apache.spark.sql.util.PartitioningUtils$.normalizePartitionStringValue(PartitioningUtils.scala:56)
    at org.apache.spark.sql.util.PartitioningUtils$.$anonfun$normalizePartitionSpec$1(PartitioningUtils.scala:100)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableLike.map(TraversableLike.scala:286)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
    at scala.collection.AbstractTraversable.map(Traversable.scala:108)
    at org.apache.spark.sql.util.PartitioningUtils$.normalizePartitionSpec(PartitioningUtils.scala:76)
    at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$.org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(rules.scala:382)
    at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:426)
    at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:420)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:30)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:30)
    at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$.apply(rules.scala:420)
    at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$.apply(rules.scala:374)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
    at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    at scala.collection.immutable.List.foldLeft(List.scala:91)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)
    at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
    at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
    at org.apache.spark.sql.hive.test.TestHiveQueryExecution.$anonfun$analyzed$1(TestHive.scala:624)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed$lzycompute(TestHive.scala:600)
    at org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed(TestHive.scala:600)
    at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
    at org.apache.spark.sql.hive.test.TestHiveSparkSession.$anonfun$sql$1(TestHive.scala:240)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.hive.test.TestHiveSparkSession.sql(TestHive.scala:238)
    at org.apache.spark.sql.test.SQLTestUtilsBase.$anonfun$sql$1(SQLTestUtils.scala:232)
    at org.apache.spark.sql.hive.InsertSuite.$anonfun$new$146(InsertSuite.scala:930)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:306)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:304)
    at org.apache.spark.sql.hive.InsertSuite.withTable(InsertSuite.scala:41)
    at org.apache.spark.sql.hive.InsertSuite.$anonfun$new$145(InsertSuite.scala:921)
    at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54)
    at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38){code};;;, 11/Nov/22 06:36;rangareddy.avula@gmail.com;The following Jira will resolve the issue by throwing the *CAST_INVALID_INPUT* error.

https://issues.apache.org/jira/browse/SPARK-40798;;;, 22/Nov/22 13:14;apachespark;User 'rangareddy' has created a pull request for this issue:
https://github.com/apache/spark/pull/38761;;;, 22/Nov/22 13:15;apachespark;User 'rangareddy' has created a pull request for this issue:
https://github.com/apache/spark/pull/38761;;;, 02/Dec/22 08:09;apachespark;User 'rangareddy' has created a pull request for this issue:
https://github.com/apache/spark/pull/38875;;;, 02/Dec/22 08:10;apachespark;User 'rangareddy' has created a pull request for this issue:
https://github.com/apache/spark/pull/38875;;;
Affects Version/s.1: 3.1.0
Component/s.1: 
Comment.1: 11/Nov/22 04:55;rangareddy.avula@gmail.com;In Spark 3.4, if we run the following code we can see the *CAST_INVALID_INPUT* exception.
{code:java}
spark.sql(s"""INSERT INTO $tableName partition (age=\"test_age\") VALUES (2, 'Nishanth')"""){code}
*Exception:*
{code:java}
[CAST_INVALID_INPUT] The value 'AGE_34' of the type "STRING" cannot be cast to "INT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "spark.sql.ansi.enabled" to "false" to bypass this error.
== SQL(line 1, position 1) ==
INSERT INTO TABLE partition_table PARTITION(age="AGE_34") VALUES (1, 'ABC')
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^org.apache.spark.SparkNumberFormatException: [CAST_INVALID_INPUT] The value 'AGE_34' of the type "STRING" cannot be cast to "INT" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set "spark.sql.ansi.enabled" to "false" to bypass this error.
== SQL(line 1, position 1) ==
INSERT INTO TABLE partition_table PARTITION(age="AGE_34") VALUES (1, 'ABC')
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    at org.apache.spark.sql.errors.QueryExecutionErrors$.invalidInputInCastToNumberError(QueryExecutionErrors.scala:161)
    at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.withException(UTF8StringUtils.scala:51)
    at org.apache.spark.sql.catalyst.util.UTF8StringUtils$.toIntExact(UTF8StringUtils.scala:34)
    at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToInt$2(Cast.scala:927)
    at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToInt$2$adapted(Cast.scala:927)
    at org.apache.spark.sql.catalyst.expressions.Cast.buildCast(Cast.scala:588)
    at org.apache.spark.sql.catalyst.expressions.Cast.$anonfun$castToInt$1(Cast.scala:927)
    at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:1285)
    at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:526)
    at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:522)
    at org.apache.spark.sql.util.PartitioningUtils$.normalizePartitionStringValue(PartitioningUtils.scala:56)
    at org.apache.spark.sql.util.PartitioningUtils$.$anonfun$normalizePartitionSpec$1(PartitioningUtils.scala:100)
    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableLike.map(TraversableLike.scala:286)
    at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
    at scala.collection.AbstractTraversable.map(Traversable.scala:108)
    at org.apache.spark.sql.util.PartitioningUtils$.normalizePartitionSpec(PartitioningUtils.scala:76)
    at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$.org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(rules.scala:382)
    at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:426)
    at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3.applyOrElse(rules.scala:420)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDownWithPruning(LogicalPlan.scala:30)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning(AnalysisHelper.scala:99)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsWithPruning$(AnalysisHelper.scala:96)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsWithPruning(LogicalPlan.scala:30)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:76)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:75)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:30)
    at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$.apply(rules.scala:420)
    at org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$.apply(rules.scala:374)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
    at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
    at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
    at scala.collection.immutable.List.foldLeft(List.scala:91)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:228)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:224)
    at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:224)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
    at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
    at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)
    at org.apache.spark.sql.hive.test.TestHiveQueryExecution.$anonfun$analyzed$1(TestHive.scala:624)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed$lzycompute(TestHive.scala:600)
    at org.apache.spark.sql.hive.test.TestHiveQueryExecution.analyzed(TestHive.scala:600)
    at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)
    at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
    at org.apache.spark.sql.hive.test.TestHiveSparkSession.$anonfun$sql$1(TestHive.scala:240)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.hive.test.TestHiveSparkSession.sql(TestHive.scala:238)
    at org.apache.spark.sql.test.SQLTestUtilsBase.$anonfun$sql$1(SQLTestUtils.scala:232)
    at org.apache.spark.sql.hive.InsertSuite.$anonfun$new$146(InsertSuite.scala:930)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:306)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:304)
    at org.apache.spark.sql.hive.InsertSuite.withTable(InsertSuite.scala:41)
    at org.apache.spark.sql.hive.InsertSuite.$anonfun$new$145(InsertSuite.scala:921)
    at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54)
    at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38){code};;;
Comment.2: 11/Nov/22 06:36;rangareddy.avula@gmail.com;The following Jira will resolve the issue by throwing the *CAST_INVALID_INPUT* error.

https://issues.apache.org/jira/browse/SPARK-40798;;;
Comment.3: 22/Nov/22 13:14;apachespark;User 'rangareddy' has created a pull request for this issue:
https://github.com/apache/spark/pull/38761;;;
Comment.4: 22/Nov/22 13:15;apachespark;User 'rangareddy' has created a pull request for this issue:
https://github.com/apache/spark/pull/38761;;;
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Optimized query plan cost/statistics overview
Issue key: SPARK-41322
Issue id: 13507281
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: gerbenvdhuizen
Creator: gerbenvdhuizen
Created: 29/Nov/22 15:10
Updated: 29/Nov/22 15:13
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: GraphX, SQL
Due Date: 
Votes: 0
Labels: 
Description: *Motivation*

Spark SQL supports running the `EXPLAIN COST` statement on a query to show the optimized logical plan and its data costs per stage (i.e. statistics) [https://spark.apache.org/docs/latest/sql-ref-syntax-qry-explain.html]. However, it can currently be difficult to determine what the total data read cost will be for a complex query with many stages. Other query engines such as Trino/Presto attempt to provide a general estimate of resource costs of a query when running the `EXPLAIN` statement, which includes CPU, memory, row count, and data size [https://trino.io/docs/current/optimizer/cost-in-explain.html.]

*Proposal*

We suggested adding an overview/estimation of the total resources that will be used within the optimized logical plan of a Spark query, or maybe as an alternative, provide this overview/estimation when the `EXPLAIN COST` statement is called on a query. As a first version, it would already be beneficial if this general cost estimation would include anything that is available within the statistics of the optimized query plan, such as:
 * The amount of data the will be read in bytes
 * The total amount of rows 
 * etc.

Given that the optimized logical plan is divided in stages it would already be sufficient to show these parameters per stage so they can be aggregated for the entire job later on if needed.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-11-29 15:10:59.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1cyjc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: SQL
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Simplify fetchBlocks w/o retry
Issue key: SPARK-37574
Issue id: 13415953
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chengpan
Creator: 
Created: 08/Dec/21 04:34
Updated: 24/Nov/22 00:29
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description:  
{code:java}
if (maxRetries > 0) {
  // Note this Fetcher will correctly handle maxRetries == 0; we avoid it just in case there's
  // a bug in this code. We should remove the if statement once we're sure of the stability.
  new RetryingBlockTransferor(transportConf, blockFetchStarter, blockIds, listener).start()
} else {
  blockFetchStarter.createAndStart(blockIds, listener)
}{code}
 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Dec 08 04:45:34 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xg7s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Dec/21 04:44;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/34831;;;, 08/Dec/21 04:45;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/34831;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 08/Dec/21 04:45;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/34831;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add SPARK_SUBMIT_OPTS in spark-env.sh.template
Issue key: SPARK-39403
Issue id: 13448839
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chengpan
Creator: 
Created: 07/Jun/22 16:08
Updated: 24/Nov/22 00:29
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Deploy
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jun 07 16:14:44 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z130y8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Jun/22 16:14;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/36789;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: BytesToBytesMap's longArray size can be up to MAX_CAPACITY
Issue key: SPARK-41200
Issue id: 13503725
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: EdisonWang
Creator: EdisonWang
Created: 19/Nov/22 05:19
Updated: 19/Nov/22 05:25
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Nov 19 05:25:56 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ccm8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Nov/22 05:25;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38722;;;, 19/Nov/22 05:25;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38722;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 19/Nov/22 05:25;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/38722;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Update gson transitive dependency to 2.8.9 or later
Issue key: SPARK-40681
Issue id: 13484866
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: apurtell
Creator: apurtell
Created: 06/Oct/22 20:36
Updated: 03/Nov/22 22:34
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark 3.3 currently ships with GSON 2.8.6 and this should be managed up to 2.8.9 or later.

Versions of GSON prior to 2.8.9 are subject to [gson#1991|https://github.com/google/gson/pull/1991] , detected and reported by several flavors of static vulnerability assessment tools, at a fairly high score because it is a deserialization of untrusted data problem.

This issue is not meant to imply any particular security problem in Spark itself.

{noformat}
[INFO] org.apache.spark:spark-network-common_2.12:jar:3.3.2-SNAPSHOT
[INFO] +- com.google.crypto.tink:tink:jar:1.6.1:compile
[INFO] |  \- com.google.code.gson:gson:jar:2.8.6:compile
{noformat}

{noformat}
[INFO] org.apache.spark:spark-hive_2.12:jar:3.3.2-SNAPSHOT
[INFO] +- org.apache.hive:hive-exec:jar:core:2.3.9:compile
[INFO] |  +- com.google.code.gson:gson:jar:2.2.4:compile
{noformat}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Nov 03 22:34:50 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z194ig:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Oct/22 17:45;srowen;Go ahead and try updating it in a PR;;;, 03/Nov/22 22:34;michael.deleon@salesforce.com;Is there any update on when we might we this in a spark release ?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 03/Nov/22 22:34;michael.deleon@salesforce.com;Is there any update on when we might we this in a spark release ?;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Exception when handling timestamp data in PySpark Structured Streaming
Issue key: SPARK-40952
Issue id: 13493450
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: KaiRoesner
Creator: KaiRoesner
Created: 28/Oct/22 10:23
Updated: 28/Oct/22 12:34
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark, Structured Streaming, Windows
Due Date: 
Votes: 0
Labels: 
Description: I'm trying to process data that contains timestamps in PySpark "Structured Streaming" using the [foreach|https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreach] option. When I run the job I get a {{OSError: [Errno 22] Invalid argument}} exception in {{\pyspark\sql\types.py}} at the
{noformat}
return datetime.datetime.fromtimestamp(ts // 1000000).replace(microsecond=ts % 1000000)
{noformat}
statement.

I have boiled down my Spark job to the essentials:
{noformat}
from pyspark.sql import SparkSession

def handle_row(row):
  print(f'Processing: \{row}')

spark = (SparkSession.builder
  .appName('test.stream.tstmp.byrow')
  .getOrCreate())

data = (spark.readStream
  .option('delimiter', ',')
  .option('header', True)
  .schema('a integer, b string, c timestamp')
  .csv('data/test'))

query = (data.writeStream
  .foreach(handle_row)
  .start())

query.awaitTermination()
{noformat}
In the {{data/test}} folder I have one csv file:
{noformat}
a,b,c
1,x,1970-01-01 00:59:59.999
2,y,1999-12-31 23:59:59.999
3,z,2022-10-18 15:53:12.345
{noformat}
If I change the csv schema to {{'a integer, b string, c string'}} everything works fine and I get the expected output of
{noformat}
Processing: Row(a=1, b='x', c='1970-01-01 00:59:59.999')
Processing: Row(a=2, b='y', c='1999-12-31 23:59:59.999')
Processing: Row(a=3, b='z', c='2022-10-18 15:53:12.345')
{noformat}
Also, if I change the stream handling to [micro-batches|https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#foreachbatch] like so:
{noformat}
...
def handle_batch(df, epoch_id):
  print(f'Processing: \{df} - Epoch: \{epoch_id}')
...
query = (data.writeStream
  .foreachBatch(handle_batch)
  .start())
{noformat}
I get the expected output of
{noformat}
Processing: DataFrame[a: int, b: string, c: timestamp] - Epoch: 0
{noformat}

But "by row" handling should work with the row having the correct column data type of {{timestamp}}.

This issue also affects using the {{foreach}} sink in Structured Streaming with the Kafka data souce since Kafka events contain a timestamp
Environment: OS: Windows 10 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-10-28 10:23:39.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1alao:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: Structured Streaming
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark session does not update number of files for partition
Issue key: SPARK-40430
Issue id: 13481575
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: ffcms
Creator: ffcms
Created: 14/Sep/22 20:30
Updated: 25/Oct/22 19:49
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When a spark session has already queried data from a table and partition and new files are inserted into the partition externally, the spark session keeps the outdated number of files and does not return the new records.
If the data is inserted into a new partition, the problem will not occur.

Steps to reproduce the behavior:

Open a Spark session
Query a count in a table
Open another spark session
insert data into an existing partition
Check the count again in the first session


I expect to see the inserted records.
Environment: I'm using spark 3.1.2 on AWS EMR and AWS Glue as catalog.
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 14/Sep/22 20:31;ffcms;session 1.png;https://issues.apache.org/jira/secure/attachment/13049304/session+1.png, 14/Sep/22 20:31;ffcms;session 2.png;https://issues.apache.org/jira/secure/attachment/13049303/session+2.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Oct 25 19:49:56 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18ke0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Oct/22 03:47;ivan.sadikov;Can you try FSCK REPAIR TABLE command on your table if you use metastore or run REFRESH (https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-refresh.html)?

Metadata is cached so it is likely you need to refresh the table to get the updates.;;;, 25/Oct/22 19:49;ffcms;It works with MSCK REPAIR TABLE.
Is there a way to force always update metadata in spark session, or do i have to know that the metadata is out of date and run MSCK REPAIR TABLE?

Thanks [~ivan.sadikov];;;
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 25/Oct/22 19:49;ffcms;It works with MSCK REPAIR TABLE.
Is there a way to force always update metadata in spark session, or do i have to know that the metadata is out of date and run MSCK REPAIR TABLE?

Thanks [~ivan.sadikov];;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark doesn't see some Parquet columns written from r-arrow
Issue key: SPARK-40873
Issue id: 13488640
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: darabos
Creator: darabos
Created: 21/Oct/22 13:11
Updated: 21/Oct/22 14:30
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I have a Parquet file that was created in R with the r-arrow package version 9.0.0 from Conda Forge with the write_dataset() function. It has four columns, but Spark 3.3.0 only sees two of them.

{{>>> df = spark.read.parquet('part-0.parquet')}}
{{()}}
{{>>> df.head()}}
{{Row(name='Adam', age=20.0)}}
{{>>> df.columns}}
{{['name', 'age']}}
{{>>> import pandas as pd}}
{{>>> pd.read_parquet('part-0.parquet')}}
{{           name   age   age_2      age_4}}
{{0          Adam  20.0   400.0   160000.0}}
{{1           Eve  18.0   324.0   104976.0}}
{{2           Bob  50.0  2500.0  6250000.0}}
{{3  Isolated Joe   2.0     4.0       16.0}}
{{>>> import pyarrow as pa}}
{{>>> import pyarrow.parquet as pq}}
{{>>> t = pq.read_table('part-0.parquet')}}
{{>>> t}}
{{pyarrow.Table}}
{{name: string}}
{{age: double}}
{{age_2: double}}
{{age_4: double}}
{{----}}
{{name: [["Adam","Eve","Bob","Isolated Joe"]]}}
{{age: [[20,18,50,2]]}}
{{age_2: [[400,324,2500,4]]}}
{{age_4: [[160000,104976,6250000,16]]}}
{{>>> pq.read_metadata('part-0.parquet')}}
{{<pyarrow._parquet.FileMetaData object at 0x7f13e9dee5e0>}}
{{  created_by: parquet-cpp-arrow version 9.0.0}}
{{  num_columns: 4}}
{{  num_rows: 4}}
{{  num_row_groups: 1}}
{{  format_version: 2.6}}
{{  serialized_size: 1510}}
{{>>> pq.read_metadata('part-0.parquet').schema}}
{{<pyarrow._parquet.ParquetSchema object at 0x7f13e9dc46c0>}}
{{required group field_id=-1 schema {}}
{{  optional binary field_id=-1 name (String);}}
{{  optional double field_id=-1 age;}}
{{  optional double field_id=-1 age_2;}}
{{  optional double field_id=-1 age_4;}}
{{}}}

"age_2" and "age_4" look no different from "age" based on the schema. I tried changing the names (just letters) but I still get the same behavior.

Is something wrong with my file? Is something wrong with Spark?

(I'll attach the file in a minute, I just need to figure out how.)
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 21/Oct/22 13:11;darabos;part-0.parquet;https://issues.apache.org/jira/secure/attachment/13051279/part-0.parquet
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Oct 21 14:30:47 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19rns:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Oct/22 13:17;darabos;Oh, I think I got it! With debug logging Spark prints a lot of stuff, including the metadata:

{{"keyValueMetaData" : {}}
{{      "ARROW:schema" : "/////zACAAAQAAAAAAAKAA4ABgAFAAgACgAAAAABBAAQAAAAAAAKAAwAAAAEAAgACgAAACwBAAAEAAAAAgAAAOgAAAAEAAAAKP///wgAAAA0AAAAKQAAAG9yZy5hcGFjaGUuc3Bhcmsuc3FsLnBhcnF1ZXQucm93Lm1ldGFkYXRhAAAAlwAAAHsidHlwZSI6InN0cnVjdCIsImZpZWxkcyI6W3sibmFtZSI6Im5hbWUiLCJ0eXBlIjoic3RyaW5nIiwibnVsbGFibGUiOnRydWUsIm1ldGFkYXRhIjp7fX0seyJuYW1lIjoiYWdlIiwidHlwZSI6ImRvdWJsZSIsIm51bGxhYmxlIjp0cnVlLCJtZXRhZGF0YSI6e319XX0ACAAMAAQACAAIAAAACAAAACQAAAAYAAAAb3JnLmFwYWNoZS5zcGFyay52ZXJzaW9uAAAAAAUAAAAzLjMuMAAAAAQAAACoAAAAZAAAADQAAAAEAAAAeP///wAAAQMQAAAAGAAAAAQAAAAAAAAABQAAAGFnZV80AAAAqv///wAAAgCk////AAABAxAAAAAYAAAABAAAAAAAAAAFAAAAYWdlXzIAAADW////AAACAND///8AAAEDEAAAABwAAAAEAAAAAAAAAAMAAABhZ2UAAAAGAAgABgAGAAAAAAACABAAFAAIAAYABwAMAAAAEAAQAAAAAAABBRAAAAAcAAAABAAAAAAAAAAEAAAAbmFtZQAAAAAEAAQABAAAAA==",}}
{{      "org.apache.spark.version" : "3.3.0",}}
{{      "org.apache.spark.sql.parquet.row.metadata" : "\{\"type\":\"struct\",\"fields\":[{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},\{\"name\":\"age\",\"type\":\"double\",\"nullable\":true,\"metadata\":{}}]}"}}
{{    },}}

This file is based on a Parquet file written by Spark. The "age_2" and "age_4" columns were added in R. Looks like r-arrow managed to carry over the metadata from the original file, so we have "{{{}org.apache.spark.sql.parquet.row.metadata{}}}" with only "name" and "age".

I'll see if I can drop the metadata in r-arrow.;;;, 21/Oct/22 14:30;darabos;This works on the R side for dropping the metadata:

{{df$schema$metadata <- NULL}}

After that Spark sees my new columns!

I don't know if this is a Spark bug or an Arrow bug or a bug at all. Hopefully the next person to hit this problem finds this issue.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 21/Oct/22 14:30;darabos;This works on the R side for dropping the metadata:

{{df$schema$metadata <- NULL}}

After that Spark sees my new columns!

I don't know if this is a Spark bug or an Arrow bug or a bug at all. Hopefully the next person to hit this problem finds this issue.;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: "RocksDB: commit - pause bg time total" metric always 0
Issue key: SPARK-40807
Issue id: 13486461
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jlaskowski
Creator: jlaskowski
Created: 15/Oct/22 18:23
Updated: 15/Oct/22 18:25
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: {{RocksDBStateStore}} uses [pauseBg|https://github.com/apache/spark/blob/v3.3.0/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDBStateStoreProvider.scala#L131] key to report "RocksDB: commit - pause bg time" metric while {{RocksDB}} uses [pause|https://github.com/apache/spark/blob/v3.3.0/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala#L308] to provide a value every commit. That leads to a name mismatch and 0 reported (as the default value for no metrics).
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 15/Oct/22 18:24;jlaskowski;spark-streams-commit-pause-bg-time.png;https://issues.apache.org/jira/secure/attachment/13050976/spark-streams-commit-pause-bg-time.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-10-15 18:23:23.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19ea8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Explain exit code 137 as killed due to OOM
Issue key: SPARK-40781
Issue id: 13486021
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: warrenzhu25
Creator: warrenzhu25
Created: 13/Oct/22 04:28
Updated: 13/Oct/22 04:31
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Explain exit code 137 as killed due to OOM to reduce the efforts of users to search the meaning of exit code.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Oct 13 04:31:38 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19bkg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Oct/22 04:31;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/38233;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Update kryo transitive dependency to 5.2.0 or later
Issue key: SPARK-40685
Issue id: 13484875
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: apurtell
Creator: apurtell
Created: 06/Oct/22 21:04
Updated: 12/Oct/22 17:45
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark 3.3 currently ships with kryo-shaded-4.0.2.jar, subject to [kryo#829|[https://github.com/EsotericSoftware/kryo/issues/829],] detected by several flavors of static vulnerability assessment tools, as a medium scored problem. 

Kryo versions 5.2.0 or later have a fix for this issue.

{noformat}
[INFO] org.apache.spark:spark-unsafe_2.12:jar:3.3.2-SNAPSHOT
[INFO] +- com.twitter:chill_2.12:jar:0.10.0:compile
[INFO] |  \- com.esotericsoftware:kryo-shaded:jar:4.0.2:compile
{noformat}

{noformat}
[INFO] org.apache.spark:spark-core_2.12:jar:3.3.2-SNAPSHOT
[INFO] +- com.twitter:chill_2.12:jar:0.10.0:compile
[INFO] |  \- com.esotericsoftware:kryo-shaded:jar:4.0.2:compile
{noformat}

This issue is not meant to imply a security problem in Spark itself.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 12 17:45:45 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z194k8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Oct/22 17:45;srowen;Does kryo 5 work with Spark? are there updated 4.x versions that resolve this too?;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Executor ID sorted as lexicographical order in Task Table of Stage Tab
Issue key: SPARK-40572
Issue id: 13483295
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: dcoliversun
Creator: dcoliversun
Created: 27/Sep/22 01:27
Updated: 27/Sep/22 03:25
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Web UI
Due Date: 
Votes: 0
Labels: 
Description: As figure shows, Executor ID sorted as lexicographical order in UI Stages Tab. Better sort as number order
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 27/Sep/22 01:28;dcoliversun;Executor_ID_IN_STAGES_TAB.png;https://issues.apache.org/jira/secure/attachment/13049777/Executor_ID_IN_STAGES_TAB.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Sep 27 02:32:12 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18uv4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Sep/22 02:32;dcoliversun;I think the root cause is that [executorId is string in TaskDataWrapper|https://github.com/apache/spark/blob/072575c9e6fc304f09e01ad0ee180c8f309ede91/core/src/main/scala/org/apache/spark/status/storeTypes.scala#L174-L175]. Executor ID is string in apache spark and there are tons of changes that will be introduced into apache spark if modify the type.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: SharedState to redact secrets when propagating them to HadoopConf
Issue key: SPARK-40567
Issue id: 13483209
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: stevel@apache.org
Creator: stevel@apache.org
Created: 26/Sep/22 13:29
Updated: 26/Sep/22 13:29
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
When SharedState propagates (key, value) pairs from initialConfigs to HadoopConf, it logs the values at debug.

If the config contained secrets (cloud credentials, etc) the log will contain them.

The org.apache.hadoop.conf.ConfigRedactor class will redact values of all keys matching a patten in "hadoop.security.sensitive-config-keys"; this is configured by default to be


{code}
  "secret$",
  "password$",
  "ssl.keystore.pass$",
  "fs.s3.*[Ss]ecret.?[Kk]ey",
  "fs.s3a.*.server-side-encryption.key",
  "fs.s3a.encryption.algorithm",
  "fs.s3a.encryption.key",
  "fs.azure\\.account.key.*",
  "credential$",
  "oauth.*secret",
  "oauth.*password",
  "oauth.*token",
	"hadoop.security.sensitive-config-keys"
{code}

...And it may be extended in site configs/future hadoop releases

Spark should be using the redactor for log hygiene/security


Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-09-26 13:29:07.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18ucw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Extend the partitioning options of the JDBC data source
Issue key: SPARK-40485
Issue id: 13482124
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: lucacanali
Creator: lucacanali
Created: 19/Sep/22 08:00
Updated: 19/Sep/22 08:28
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: This proposes to extend the available partitioning options for the JDBC data source.

Partitioning options allow to read data using multiple workers connected to the target RDBMS. This can improve the performance of data extraction, under the right circumstances.

Currently the only available partitioning and parallelization option for reading from databases is to specify lowerBound, upperBound, together with numPartitions and partitionColumn. The Spark JDBC data source will then use multiple partitions, and thus workers, to read from the RDBMS.

This proposes to add a similar, however complementary, mechanism for partitioning, where a user-provided list of values is used to compute the target partitions.

This provides a way to split the data extraction work among workers that could be aligned with the database physical (partitioned and/or indexed) structure, as in the following example:
{code:java}
option("partitionColumn", "region").
option("numPartitions", 3).
option("partitionColValues", "'eastern', 'central', 'western'").  {code}
This feature is motivated for performance reasons, to scale and speed up data extraction from:

 - list partitioned tables, available in Oracle and PostgreSQL

 - this is also applicable to tables stored in B*Tree indexes, such as in Oracle's IOTs (Index Organized Tables) and SQL Server's Clustered Indexes.

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Sep 19 08:28:54 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18nr4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Sep/22 08:28;apachespark;User 'LucaCanali' has created a pull request for this issue:
https://github.com/apache/spark/pull/37928;;;, 19/Sep/22 08:28;apachespark;User 'LucaCanali' has created a pull request for this issue:
https://github.com/apache/spark/pull/37928;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 19/Sep/22 08:28;apachespark;User 'LucaCanali' has created a pull request for this issue:
https://github.com/apache/spark/pull/37928;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support standalone worker recommission
Issue key: SPARK-40381
Issue id: 13480529
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: warrenzhu25
Creator: warrenzhu25
Created: 07/Sep/22 18:26
Updated: 07/Sep/22 18:32
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Deploy
Due Date: 
Votes: 0
Labels: 
Description: Currently, spark standalone only support kill workers. We may want to recommission some workers. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 07 18:32:30 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18dzk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Sep/22 18:32;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37822;;;, 07/Sep/22 18:32;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37822;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 07/Sep/22 18:32;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37822;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Examples can't run in IDE directly
Issue key: SPARK-36422
Issue id: 13393618
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 05/Aug/21 05:29
Updated: 07/Sep/22 08:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Examples
Due Date: 
Votes: 0
Labels: 
Description: I found the examples can't run in IDE(such as Intellij).

For example,  if run `org.apache.spark.examples.sql.JavaUserDefinedScalar` in IDE, the error message as follows:
{code:java}
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/spark/sql/SparkSession
  at org.apache.spark.examples.sql.JavaUserDefinedScalar.main(JavaUserDefinedScalar.java:33)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.SparkSession
  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
  ... 1 more
{code}
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 07 08:00:09 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tmm8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Sep/22 08:00;726575153@qq.com;you can try set idea run option
{code:java}
Include dependencies with "Provided" scope. {code};;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: After `RemoveRedundantAggregates`, `PullOutGroupingExpressions` should applied to avoid attribute missing when use complex expression.
Issue key: SPARK-40288
Issue id: 13479432
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: hgs19921112
Creator: hgs19921112
Created: 31/Aug/22 07:06
Updated: 04/Sep/22 08:58
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: {{--table}}
{{create}}  {{table}} {{miss_expr(id }}{{{}int{}}}{{{},{}}}{{{}name{}}} {{string,age }}{{{}double{}}}{{{}) stored {}}}{{as}} {{textfile}}
{{--data}}
{{insert}} {{overwrite }}{{table}} {{miss_expr }}{{{}values{}}}{{{}(1,{}}}{{{}'ox'{}}}{{{},1.0),(1,{}}}{{{}'oox'{}}}{{{},2.0),(2,{}}}{{{}'ox'{}}}{{{},3.0),(2,{}}}{{{}'xxo'{}}}{{{},4.0){}}}
{{--failure sql}}

{{select}} {{{}id,{}}}{{{}name{}}}{{{},nage {}}}{{as}} {{nnnnn }}{{{}from{}}}{{{}({}}}
{{select}} {{{}id,{}}}{{{}name{}}}{{{},if(age>3,100,200) {}}}{{as}} {{nage }}{{from}} {{miss_expr }}{{group}} {{by}} {{{}id,{}}}{{{}name{}}}{{{},age{}}}
{{) }}{{group}} {{by}} {{{}id,{}}}{{{}name{}}}{{{},nage{}}}

--error stack
{{Caused by: java.lang.IllegalStateException: Couldn't find age#4 in [id#2,name#3,if ((age#4 > 3.0)) 100 else 200#12|#2,name#3,if ((age#4 > 3.0)) 100 else 200#12]}}
{{at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)}}
{{at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)}}
{{at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)}}
Environment: spark 3.2.0 spark 3.2.2 spark 3.3.0
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Sep 04 08:58:38 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z187bc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Sep/22 03:40;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37765;;;, 02/Sep/22 03:40;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37765;;;, 02/Sep/22 04:36;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37766;;;, 02/Sep/22 04:36;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37766;;;, 03/Sep/22 12:08;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37782;;;, 03/Sep/22 12:08;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37782;;;, 03/Sep/22 17:38;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37784;;;, 03/Sep/22 17:39;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37784;;;, 03/Sep/22 17:47;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37785;;;, 03/Sep/22 17:48;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37785;;;, 04/Sep/22 08:35;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37788;;;, 04/Sep/22 08:36;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37788;;;, 04/Sep/22 08:58;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37790;;;, 04/Sep/22 08:58;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37790;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 02/Sep/22 03:40;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37765;;;, 04/Sep/22 08:35;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37788;;;, 04/Sep/22 08:36;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37788;;;, 04/Sep/22 08:58;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37790;;;, 04/Sep/22 08:58;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37790;;;
Comment.2: 02/Sep/22 04:36;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37766;;;
Comment.3: 02/Sep/22 04:36;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37766;;;
Comment.4: 03/Sep/22 12:08;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/37782;;;
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Can't get JDBC type for map<string,string> in Spark 3.3.0 and PostgreSQL
Issue key: SPARK-40237
Issue id: 13478886
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: igor.suhorukov
Creator: igor.suhorukov
Created: 26/Aug/22 21:51
Updated: 31/Aug/22 17:21
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description:  

Exception 'Can't get JDBC type for map<string,string>' happens when I try to save dataset into PostgreSQL via JDBC
{code:java}
Dataset<Row> ds = ...;
ds.write().mode(SaveMode.Overwrite)
.option("truncate", true).format("jdbc")
.option("url", "jdbc:postgresql://127.0.0.1:5432/???")
.option("dbtable", "t1")
.option("isolationLevel", "NONE")
.option("user", "???")
.option("password", "???")
.save();
{code}
 

This Issue related to unimplemented PostgresDialect#getJDBCType  and it is strange as PostgreSQL supports hstore/json/jsonb types sutable to store map. PostgreSql JDBC driver support hstore write by using statement.setObject(parameterIndex, map) for hstore and statement.setString(parameterIndex,map) with  cast(? as JSON). 

 

 

 
{code:java}
java.lang.IllegalArgumentException: Can't get JDBC type for map<string,string>
    at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotGetJdbcTypeError(QueryExecutionErrors.scala:810)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getJdbcType$2(JdbcUtils.scala:162)
    at scala.Option.getOrElse(Option.scala:201)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getJdbcType(JdbcUtils.scala:162)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$schemaString$4(JdbcUtils.scala:782)
    at scala.collection.immutable.Map$EmptyMap$.getOrElse(Map.scala:228)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$schemaString$3(JdbcUtils.scala:782)
    at scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.schemaString(JdbcUtils.scala:779)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.createTable(JdbcUtils.scala:883)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:81)
    at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
    at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
    at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
    at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
    at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
    at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
    at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
    at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
    at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
    at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)
    at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
    at com.github.igorsuhorukov.arrow.spark.SparkSessionTest.initSession(SparkSessionTest.java:75)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
    at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.util.ArrayList.forEach(ArrayList.java:1259)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.util.ArrayList.forEach(ArrayList.java:1259)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
    at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
    at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
    at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
    at com.intellij.junit5.JUnit5IdeaTestRunner.startRunnerWithArgs(JUnit5IdeaTestRunner.java:71)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:221)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
{code}
 
Environment: Linux 5.15.0-46-generic #49~20.04.1-Ubuntu SMP Thu Aug 4 19:15:44 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

22/08/27 00:30:01 INFO SparkContext: Running Spark version 3.3.0
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-08-26 21:51:06.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z183z4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add description for ExecutorAllocationManager metrics
Issue key: SPARK-40267
Issue id: 13479207
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: warrenzhu25
Creator: warrenzhu25
Created: 30/Aug/22 02:49
Updated: 30/Aug/22 17:47
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Documentation
Due Date: 
Votes: 0
Labels: 
Description: Some ExecutorAllocationManager metrics are hard to know what stands for just from metric name.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Aug 30 17:46:59 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z185y0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Aug/22 17:46;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37733;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: The partitionSpec should be distinct keys after filter one row of row_number
Issue key: SPARK-40164
Issue id: 13477816
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: wankun
Creator: wankun
Created: 21/Aug/22 09:35
Updated: 21/Aug/22 15:01
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: For query
{code:sql}
SELECT *
  FROM (
    SELECT *, row_number() over(partition by key order by value) rn
    FROM testData t
  ) t1
  WHERE rn=1
{code}
column *key* will be distinct 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Aug 21 15:01:53 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17xew:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Aug/22 15:01;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/37602;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Optionally use a serialized storage level for DataFrame.localCheckpoint()
Issue key: SPARK-40155
Issue id: 13477749
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: paulstaab
Creator: paulstaab
Created: 20/Aug/22 14:12
Updated: 20/Aug/22 14:12
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In PySpark 3.3.0 `DataFrame.localCheckpoint()` stores the RDD checkpoints using the "Disk Memory *Deserialized* 1x Replicated" storage level. Looking through the Python code and the documentation, I haven't found any possibility to change this.

As serialized RDDs are often a lot smaller than deserialized ones - I have seen examples where a 40GB deserialized RDD shrank to 200MB when serialized - I would usually like to create local checkpoints that are stored in serialized instead of deserialized format.

To make this possible, we could e.g. add an optional `storage_level` argument to `DataFrame.localCheckpoint()` similar to `DataFrame.persist()` or add a global configuration option similar to `spark.checkpoint.compress`.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-08-20 14:12:32.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17x08:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add reason for cancelJobGroup 
Issue key: SPARK-40119
Issue id: 13477190
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: santosh.pingale
Creator: santosh.pingale
Created: 17/Aug/22 08:19
Updated: 17/Aug/22 15:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Currently, `cancelJob` supports passing the reason for failure. We use `cancelJobGroup` in a few cases of async actions. It would be great to pass reason of cancellation to the job group.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 17 15:11:39 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17tkg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Aug/22 15:11;apachespark;User 'santosh-d3vpl3x' has created a pull request for this issue:
https://github.com/apache/spark/pull/37555;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: MillisToTimestamp overflows on Long.MAX_VALUE
Issue key: SPARK-40104
Issue id: 13477022
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: mtelling
Creator: mtelling
Created: 16/Aug/22 09:23
Updated: 16/Aug/22 09:23
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.2.1, 3.2.2, 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: MillisToTimestamp throws an ArithmeticOverflowException on Long.MAX_VALUE even though Instant.ofEpochMillis(Long.MAX_VALUE) is valid.

The reason this happens is that MillisToTimestamp delegates to MicrosToTimestamp and so it multiplies the input with a factor and tries to fit that into a long. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-08-16 09:23:31.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17sj4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.1
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Simplify conflict binary comparison
Issue key: SPARK-39841
Issue id: 13472887
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: kaifeiYi
Creator: kaifeiYi
Created: 22/Jul/22 11:23
Updated: 22/Jul/22 12:09
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, We do not handle conflicting binary comparison expressions, Let's say have a query, it filter `a < 0 and a > 1`, this condition should be a `FalseLiteral`, we can fold ahead of time to avoid unnecessary calculations.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 22/Jul/22 11:23;kaifeiYi;image-2022-07-22-19-23-55-452.png;https://issues.apache.org/jira/secure/attachment/13047118/image-2022-07-22-19-23-55-452.png, 22/Jul/22 11:24;kaifeiYi;image-2022-07-22-19-24-46-068.png;https://issues.apache.org/jira/secure/attachment/13047119/image-2022-07-22-19-24-46-068.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 22 12:09:23 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17348:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Jul/22 12:08;apachespark;User 'yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/37254;;;, 22/Jul/22 12:09;apachespark;User 'yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/37254;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 22/Jul/22 12:09;apachespark;User 'yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/37254;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add the Support for Custom Authentication for Thrift Server using Http Servlet Request input Param
Issue key: SPARK-39820
Issue id: 13472375
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: saurabhc100
Creator: saurabhc100
Created: 20/Jul/22 05:53
Updated: 20/Jul/22 06:05
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: As per the CustomAuthentication(hive.server2.transport.mode=HTTP) in Thrift Server only String username and String password is supported in the authenticate method of PasswdAuthenticationProvider([https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/java/org/apache/hive/service/auth/PasswdAuthenticationProvider.java#L37]). There is no support for the getting Header param of Request, input param of request, request URI, getMethod if these param is required by the Custom Authentication Service(third party sdk, In House Authentication Service).

In order to support CustomAuthentication for Custom Authentication Service(third party sdk, In House Authentication Service) there is need to add the code change in Spark Code , than only user can add the Custom Authentication.

For Example
{code:java}
public class SampleAuthenticator implements PasswdAuthenticationProvider {
  static Logger logger = Logger.getLogger(SampleAuthenticator.class.getName());

  public SampleAuthenticator() {
    logger.log(Level.INFO, "initialize SampleAuthenticator");
  }
  @Override
  public void Authenticate(String user, String  password)
      throws AuthenticationException {
  }

 @Override
  public void AuthenticateRequest(HttpServletRequest request)
      throws AuthenticationException {
        Map<String, List<String>> requestHeaders = new HashMap(); 
        // derrive the request headerValue from the HttpServletRequest request
        logger.log(Level.INFO, "SampleAuthenticator AuthenticateRequest");
        dummyAuthenticate(request.getMethod, requestHeaders, request.);
      }
  public void dummyAuthenticate(String httpMethod, Map<String, List<String>> requestHeaders, Uri uri) {
     // Code added for custom Authentication
  }    
}
{code}
So there is need to support void AuthenticateRequest(HttpServletRequest request) in the Spark Code base, to integrate the custom Authentication in apache spark
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 20 06:05:15 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16zyg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Jul/22 06:04;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/37231;;;, 20/Jul/22 06:05;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/37231;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 20/Jul/22 06:05;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/37231;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support Avro schema evolution
Issue key: SPARK-39770
Issue id: 13471475
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: koert
Creator: koert
Created: 13/Jul/22 21:27
Updated: 13/Jul/22 21:36
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: avro
Description: currently the avro source in connector/avro does not yet support schema evolution.
from source code of AvroUtils:
{code:java}
// Schema evolution is not supported yet. Here we only pick first random readable sample file to
// figure out the schema of the whole dataset.
 {code}
i added schema evolution for our inhouse spark version. if there is interest in this i could contribute it.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 13 21:36:43 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16uf4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Jul/22 21:36;apachespark;User 'koertkuipers' has created a pull request for this issue:
https://github.com/apache/spark/pull/37183;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Strip any CRLF character if lineSep is not set in CSV data source
Issue key: SPARK-39768
Issue id: 13471451
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: yaohua
Creator: yaohua
Created: 13/Jul/22 16:52
Updated: 13/Jul/22 18:09
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: If `lineSep` is not set, the line separator is automatically detected. To be safe, we should strip any _CRLF_ character at the suffix in the column names.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 13 18:09:21 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16u9s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Jul/22 16:53;yaohua;cc @[~hyukjin.kwon] ;;;, 13/Jul/22 18:08;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/37180;;;, 13/Jul/22 18:09;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/37180;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 13/Jul/22 18:08;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/37180;;;
Comment.2: 13/Jul/22 18:09;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/37180;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve stats estimation for v2 tables
Issue key: SPARK-39678
Issue id: 13470073
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: singhpk234
Creator: singhpk234
Created: 05/Jul/22 08:21
Updated: 05/Jul/22 08:35
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Optimizer
Due Date: 
Votes: 0
Labels: 
Description: In case of v2 tables, connectors can bubble up both [sizeInBytes and rowCount |https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/connector/read/Statistics.java].

Presently, SizeInBytesOnlyStatsPlanVisitor, ommits propagating / estimating rowCount stats, some places like :
 * [CodePointer1|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/SizeInBytesOnlyStatsPlanVisitor.scala#L54-L58]
 * [CodePointer2 |https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/SizeInBytesOnlyStatsPlanVisitor.scala#L46-L47]

For the [non-cbo|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statsEstimation/LogicalPlanStats.scala#L34-L39] flow, as per my understanding, this can improve the stats estimation, since rowcount is indirectly used in places to estimate the size as well. 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 05 08:35:01 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16lvk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Jul/22 08:21;singhpk234;will add a pr for it shortly.;;;, 05/Jul/22 08:35;apachespark;User 'singhpk234' has created a pull request for this issue:
https://github.com/apache/spark/pull/37083;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 05/Jul/22 08:35;apachespark;User 'singhpk234' has created a pull request for this issue:
https://github.com/apache/spark/pull/37083;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: parameters quotechar and escapechar needs to limite to a char in pyspark pandas read_csv function
Issue key: SPARK-39654
Issue id: 13469592
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: bzhaoop
Creator: bzhaoop
Created: 01/Jul/22 08:12
Updated: 01/Jul/22 08:32
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Different behavior between pyspark and pandas with the single blank string. We need to keep the same behavior like pandas, even the backend DataFrame support this input.

 

test case(test3.csv):
{code:java}
"column1","column2", "column3", "column4", "column5", "column6" "AM", 7, "1", "SD", "SD", "CR" "AM", 8, "1,2 ,3", "PR, SD,SD", "PR ; , SD,SD", "PR , ,, SD ,SD" "AM", 1, "2", "SD", "SD", "SD" {code}
 

For quotechar

pandas:
{code:java}
>>> pd.read_csv('/home/spark/test3.csv', quotechar=' ')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/util/_decorators.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 680, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 581, in _read
    return parser.read(nrows)
  File "/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1254, in read
    index, columns, col_dict = self._engine.read(nrows)
  File "/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py", line 225, in read
    chunks = self._reader.read_low_memory(nrows)
  File "pandas/_libs/parsers.pyx", line 805, in pandas._libs.parsers.TextReader.read_low_memory
  File "pandas/_libs/parsers.pyx", line 861, in pandas._libs.parsers.TextReader._read_rows
  File "pandas/_libs/parsers.pyx", line 847, in pandas._libs.parsers.TextReader._tokenize_rows
  File "pandas/_libs/parsers.pyx", line 1960, in pandas._libs.parsers.raise_parser_error
pandas.errors.ParserError: Error tokenizing data. C error: EOF inside string starting at row 2
>>> 
 {code}
pyspark:
{code:java}
>>> sp.read_csv('/home/spark/test3.csv', quotechar=' ')
/home/spark/spark/python/pyspark/pandas/utils.py:976: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.
  warnings.warn(message, PandasAPIOnSparkAdviceWarning)
  "column1" "column2"  "column3", "column4"  "column5", "column6"
0      "AM"    7, "1"            "SD", "SD"                  "CR"
1      "AM"     8, "1                    2                     3"
2      "AM"    1, "2"            "SD", "SD"                  "SD"
 {code}
 

For escapechar

pandas:
{code:java}
>>> pd.read_csv('/home/spark/test3.csv', escapechar=' ')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/util/_decorators.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 680, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 581, in _read
    return parser.read(nrows)
  File "/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/io/parsers/readers.py", line 1254, in read
    index, columns, col_dict = self._engine.read(nrows)
  File "/home/spark/.pyenv/versions/3.8.13/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py", line 225, in read
    chunks = self._reader.read_low_memory(nrows)
  File "pandas/_libs/parsers.pyx", line 805, in pandas._libs.parsers.TextReader.read_low_memory
  File "pandas/_libs/parsers.pyx", line 861, in pandas._libs.parsers.TextReader._read_rows
  File "pandas/_libs/parsers.pyx", line 847, in pandas._libs.parsers.TextReader._tokenize_rows
  File "pandas/_libs/parsers.pyx", line 1960, in pandas._libs.parsers.raise_parser_error
pandas.errors.ParserError: Error tokenizing data. C error: Expected 6 fields in line 3, saw 11{code}
pyspark:
{code:java}
>>> sp.read_csv('/home/spark/test3.csv', escapechar=' ')
/home/spark/spark/python/pyspark/pandas/utils.py:976: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.
  warnings.warn(message, PandasAPIOnSparkAdviceWarning)
  column1  column2  "column3"  "column4"  "column5"  "column6"
0      AM      7.0        "1"       "SD"       "SD"       "CR"
1      AM      8.0         "1         2          3"        "PR
2      AM      1.0        "2"       "SD"       "SD"       "SD"
 {code}
Environment: pyspark pandas: master

OS: Ubuntu 1804

Python version: 3.8.14

pandas version: 1.4.2
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 01 08:32:16 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16iwo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.3.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Jul/22 08:31;apachespark;User 'bzhaoopenstack' has created a pull request for this issue:
https://github.com/apache/spark/pull/37044;;;, 01/Jul/22 08:32;apachespark;User 'bzhaoopenstack' has created a pull request for this issue:
https://github.com/apache/spark/pull/37044;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 01/Jul/22 08:32;apachespark;User 'bzhaoopenstack' has created a pull request for this issue:
https://github.com/apache/spark/pull/37044;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: add state utils to StreamTest to check states during streaming queries
Issue key: SPARK-39632
Issue id: 13469114
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: matar993
Creator: matar993
Created: 29/Jun/22 13:40
Updated: 29/Jun/22 16:19
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL, Tests
Due Date: 
Votes: 0
Labels: 
Description: The framework for implementing tests for streaming queries ([StreamTest|https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/streaming/StreamTest.scala]) doesn't have any StreamActions that allows to check the state for a specific key during a stateful streaming query. 

For this purpose, two new Stream Actions need to be implemented:
{code:java}
case class CheckEmptyState[K](expectedKey: K) extends StreamAction
case class CheckNonEmptyState[K, V](expectedKey: K, expectedState: V) extends StreamAction{code}
The CheckEmptyState StreamAction will check if doesn't exists a state for the specified key.

The CheckNonEmptyState StreamAction will check if there is a state of type V, for the specified key, which is equals to the specified state.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jun 29 16:19:12 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16gu8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Jun/22 16:19;apachespark;User 'matar993' has created a pull request for this issue:
https://github.com/apache/spark/pull/37026;;;
Affects Version/s.1: 
Component/s.1: Tests
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Allow all Reader or Writer settings to be provided as options
Issue key: SPARK-39630
Issue id: 13468980
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: nchammas
Creator: nchammas
Created: 28/Jun/22 21:03
Updated: 28/Jun/22 21:03
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Almost all Reader or Writer settings can be provided via individual calls to `.option()` or by providing a map to `.options()`.

There are notable exceptions, though, like:
 * read/write format
 * write mode
 * write partitionBy, bucketBy, and sortBy

These settings must be provided via dedicated method calls.

Why not make it so that _all_ settings can be provided as options? Is there a design reason not to do this?

Any given DataFrameReader or DataFrameWriter (along with the streaming equivalents) should be able to "export" all of its settings as a map of options, and then in turn be reconstituted entirely from that map of options.
{code:python}
reader1 = spark.read.option("format", "parquet").option("path", "/data")
options = reader.getOptions()
reader2 = spark.read.options(options)

# reader1 and reader2 are configured identically
data1 = reader1.load()
data2 = reader2.load()
data1.collect() == data2.collect(){code}
Some of these special directives like `bucketBy` will need to be broken up to fit the structure of an option.
{code:python}
# instead of this
data.write.bucketBy(42, "name")
# someone would be able to do this
data.write.option("bucketByCount", "42").option("bucketByColumn", "name"){code}
Complex read or write options aren't new to Spark. We already use them to configure [detailed JDBC connections|https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html], for example.

So why would someone care about this? Mainly, because they want to capture read or write settings declaratively somewhere, like in a YAML file, and use them to drive a Spark job's behavior.

Right now we are 80% of the way there. Unless there is a design obstacle in the way, it seems good to go the last 20% and just make it standard for every and any read or write directive to be supported via the option interface.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-06-28 21:03:17.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16g0g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Fix race condition when handling IdleStateEvent again
Issue key: SPARK-39628
Issue id: 13468853
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: dzcxzl
Creator: dzcxzl
Created: 28/Jun/22 10:26
Updated: 28/Jun/22 10:37
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In SPARK-27073, fix a race condition when handling of IdleStateEvent, but in SPARK-37462 the call order is modified, which leads to a possible regression.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jun 28 10:37:56 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16f88:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Jun/22 10:37;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37015;;;, 28/Jun/22 10:37;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37015;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 28/Jun/22 10:37;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/37015;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support coalesce partition through cartesianProduct
Issue key: SPARK-39624
Issue id: 13468777
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: mushui
Creator: mushui
Created: 28/Jun/22 03:42
Updated: 28/Jun/22 10:14
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: `CoalesceShufflePartitions` can not optimize CartesianProductExec and the result partition would be `left partition * right partition` which can be quite lagre.

 

It's better to support partial optimize with `CartesianProduct`.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 28/Jun/22 03:43;mushui;屏幕截图 2022-06-28 114256.jpg;https://issues.apache.org/jira/secure/attachment/13045774/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE+2022-06-28+114256.jpg
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jun 28 10:14:53 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16erc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Jun/22 10:14;apachespark;User 'lsm1' has created a pull request for this issue:
https://github.com/apache/spark/pull/37014;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Write event logs in a continuous manner
Issue key: SPARK-39580
Issue id: 13468309
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: kanoute
Creator: kanoute
Created: 24/Jun/22 13:49
Updated: 24/Jun/22 13:49
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: We are using Spark Thrift Server as a service to run Spark SQL queries.

Currently Spark writes *eventlogs* once the job is done/ gracefully killed.

This means that in case of Spark thrift Server when it runs as one job in case of unexpected error ( i.e. OOM ) we cannot access the logs after, as it doesn't write them due to the unexpected shut down.

We would like to suggest as an improvement the functionality for sparks *to write the event logs to a cloud storage in a continuous manner.*

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Patch
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-06-24 13:49:57.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16c9s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: “SPARK_HOME” usage in bin/docker-image-tool.sh might be explained
Issue key: SPARK-39565
Issue id: 13459575
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: Sookyung Park
Creator: Sookyung Park
Created: 23/Jun/22 04:16
Updated: 23/Jun/22 05:49
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 0
Labels: 
Description: Hello, I am using docker-image-tool.sh on building docker images for k8s, as described in here:
[https://spark.apache.org/docs/latest/running-on-kubernetes.html#user-identity] 

 

I found out the “SPARK_HOME” env variable is utilized on build step, like this:
{code:java}
// https://github.com/apache/spark/blob/master/bin/docker-image-tool.sh#L177-L179

(cd $(img_ctx_dir base) && docker build $NOCACHEARG "${BUILD_ARGS[@]}" \
-t $(image_ref spark) \
-f "$BASEDOCKERFILE" .){code}
 

It means that if the user set the SPARK_HOME to other path on the local, and runs this script in another spark directory,

it would perform build on the directory where SPARK_HOME is set and would not work as intended.

 

Since this might become hard to debug, and this script usage is suggested to common users in public doc,

I thought further explanation on “SPARK_HOME” or “is_dev_build” usage in this script might be needed, like this:
{code:java}
// docker-image-tool.sh
 
Usage: $0 [options] [command]
Builds or pushes the built-in Spark Docker image.
…
Examples:
- Build image in current directory
  env SPARK_HOME=. $0 build {code}
Thank you.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-06-23 04:16:36.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z14ucw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Remove no-null conditional statements in SparkSubmitCommandBuilder#isThriftServer
Issue key: SPARK-39526
Issue id: 13450895
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: dcoliversun
Creator: dcoliversun
Created: 20/Jun/22 03:45
Updated: 20/Jun/22 03:45
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: {code:java}
private boolean isThriftServer(String mainClass) {
  return (mainClass != null &&
   mainClass.equals("org.apache.spark.sql.hive.thriftserver.HiveThriftServer2")); 
} {code}
No scenario that *mainClass* is null, because already have defensive code.
{code:java}
if (isExample && !isSpecialCommand) {
  checkArgument(mainClass != null, "Missing example class name.");
}

if (mainClass != null) {
  args.add(parser.CLASS);
  args.add(mainClass);
} {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-06-20 03:45:22.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z13dko:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Only disable bucketing when autoBucketedScan is enabled if bucket columns are not in scan output
Issue key: SPARK-39344
Issue id: 13447608
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: mauzhang
Creator: mauzhang
Created: 31/May/22 05:11
Updated: 31/May/22 09:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, bucketing was disabled when bucket columns are not in scan output after https://github.com/apache/spark/pull/27924. It break existing applications whose input size is huge by creating too many FilePartitions and causing driver hang. And it cannot be switched off. This is to propose merging the rule into DisableUnnecessaryBucketedScan.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue May 31 09:00:11 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12tcw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/May/22 08:59;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36733;;;, 31/May/22 09:00;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36733;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 31/May/22 09:00;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36733;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Alternative configs of Hadoop Filesystems to access break backward compatibility
Issue key: SPARK-39278
Issue id: 13446659
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: mauzhang
Creator: mauzhang
Created: 25/May/22 01:37
Updated: 25/May/22 02:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Before [https://github.com/apache/spark/pull/23698,]

The precedence of configuring Hadoop Filesystems to access is
{code:java}
spark.yarn.access.hadoopFileSystems -> spark.yarn.access.namenodes{code}
Afterwards, it's
{code:java}
spark.kerberos.access.hadoopFileSystems -> spark.yarn.access.namenodes -> spark.yarn.access.hadoopFileSystems{code}
When both spark.yarn.access.hadoopFileSystems and spark.yarn.access.namenodes are configured with different values, the PR will break backward compatibility and cause application failure.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed May 25 02:00:21 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12nk8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/May/22 01:59;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36658;;;, 25/May/22 02:00;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36658;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 25/May/22 02:00;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36658;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Update user-facing catalog to adapt CatalogPlugin
Issue key: SPARK-36790
Issue id: 13401787
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: xiaopenglei
Creator: xiaopenglei
Created: 17/Sep/21 06:21
Updated: 06/May/22 10:08
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: At now the SparkSession.catalog always retuan a CatalogImpl with a SessionCatalog that is SparkSession.sessionState.catalog
{code:java}
@transient lazy val catalog: Catalog = new CatalogImpl(self)
{code}
{code:java}
private def sessionCatalog: SessionCatalog = sparkSession.sessionState.catalog
{code}
So we can do the action is just based the SessionCatalog, we could not do action based user-defined CatalogPlugin.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Sep 17 06:51:02 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v10o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Sep/21 06:51;apachespark;User 'Peng-Lei' has created a pull request for this issue:
https://github.com/apache/spark/pull/34030;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Optimize shuffle error handler
Issue key: SPARK-39080
Issue id: 13442641
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: wankun
Creator: wankun
Created: 30/Apr/22 15:22
Updated: 30/Apr/22 15:28
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: Shuffle ErrorHandler only has two stateless method, so we do not need create a ErrorHandler instance for each shuffle fetcher or shuffle pusher.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Apr 30 15:28:28 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11yw0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Apr/22 15:28;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/36419;;;, 30/Apr/22 15:28;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/36419;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 30/Apr/22 15:28;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/36419;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Fast Fail the remaining push blocks if shuffle stage finalized
Issue key: SPARK-39072
Issue id: 13442514
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: wankun
Creator: wankun
Created: 29/Apr/22 13:09
Updated: 29/Apr/22 13:21
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: Map task will try to push all map outputs to external shuffle service now.

After the shuffle stage is finalized, the reduce fetch blocks RPC will be blocked if there are still many map output blocks in flight.

We could stop pushing the remaining blocks if the shuffle stage is finalized.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Apr 29 13:21:34 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11y48:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Apr/22 13:20;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/36411;;;, 29/Apr/22 13:21;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/36411;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 29/Apr/22 13:21;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/36411;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Optimize RemoteBlockPushResolver with a memory pool
Issue key: SPARK-38965
Issue id: 13440612
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: wankun
Creator: wankun
Created: 20/Apr/22 08:24
Updated: 25/Apr/22 10:22
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: For push-based shuffle service, there are many {{BLOCK_APPEND_COLLISION_DETECTED}} when there are many small map tasks outputs. In {{{}RemoteBlockPushResolver{}}}, if one map task pushed blocks is writing, the others map tasks pushed blocks will failed in {{onComplete()}} method.
And {{RemoteBlockPushResolver}} has no memory limit , so many executors will OOM when there are many small pushed blocks waiting to be written to the final data file.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 20 08:38:40 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11mgw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Apr/22 08:38;apachespark;User 'wankunde' has created a pull request for this issue:
https://github.com/apache/spark/pull/36279;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Make stage navigable from max metrics displayed in UI
Issue key: SPARK-38963
Issue id: 13440579
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: pavithraramachandran
Creator: pavithraramachandran
Created: 20/Apr/22 06:10
Updated: 20/Apr/22 06:33
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.1.1, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Web UI
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 20 06:33:27 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11m9k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Apr/22 06:32;pavithraramachandran;working on it;;;, 20/Apr/22 06:33;apachespark;User 'PavithraRamachandran' has created a pull request for this issue:
https://github.com/apache/spark/pull/36278;;;
Affects Version/s.1: 3.2.0
Component/s.1: 
Comment.1: 20/Apr/22 06:33;apachespark;User 'PavithraRamachandran' has created a pull request for this issue:
https://github.com/apache/spark/pull/36278;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve the exception type and message of casting string to numbers
Issue key: SPARK-38935
Issue id: 13440169
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: apachespark
Reporter: Gengliang.Wang
Creator: Gengliang.Wang
Created: 18/Apr/22 11:59
Updated: 18/Apr/22 14:20
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: # change the exception type from "java.lang.NumberFormatException" to SparkNumberFormatException

       2. Show the exact target data type in the error message
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Apr 18 12:07:03 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11k3s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.3.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Apr/22 12:07;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36244;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Replace all the ArithmeticException with SparkArithmeticException
Issue key: SPARK-38842
Issue id: 13438705
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: Gengliang.Wang
Creator: Gengliang.Wang
Created: 09/Apr/22 03:52
Updated: 09/Apr/22 03:52
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Similar to [https://github.com/apache/spark/pull/36022,] we should replace all the ArithmeticException with SparkArithmeticException
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-09 03:52:03.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11b48:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use more generic Channel type to initialize Netty BootStrap
Issue key: SPARK-38547
Issue id: 13433644
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 14/Mar/22 12:07
Updated: 14/Mar/22 12:37
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark use {{SocketChannel}} type to initialize {{BootStrap}} now, but there is no need to limit the type to {{{}Socket{}}}, so this pr change the related type definitions from {{SocketChannel}} to {{{}Channel{}}}.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Mar 14 12:26:08 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10ghc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Mar/22 12:25;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35847;;;, 14/Mar/22 12:26;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35847;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 14/Mar/22 12:26;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35847;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Migrate legacy.keepCommandOutputSchema related to KeepLegacyOutputs
Issue key: SPARK-38482
Issue id: 13432861
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: kaifeiYi
Creator: kaifeiYi
Created: 09/Mar/22 12:33
Updated: 09/Mar/22 12:48
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Migrate legacy.keepCommandOutputSchema related to KeepLegacyOutputs
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 09 12:48:01 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10bo8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Mar/22 12:48;apachespark;User 'Yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/35788;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Clean up duplicate codes in get method of ColumnarBatchRow/ColumnarRow/MutableColumnarRow
Issue key: SPARK-38460
Issue id: 13432750
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 09/Mar/22 04:46
Updated: 09/Mar/22 04:55
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The logic of the `ColumnarBatchRow.get` is very similar to `ColumnarRow.get` and `MutableColumnarRow.get`
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 09 04:55:25 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10azk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Mar/22 04:54;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35781;;;, 09/Mar/22 04:55;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35781;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 09/Mar/22 04:55;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35781;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Combine compatible scalar subqueries
Issue key: SPARK-38386
Issue id: 13431426
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: sperlingxx
Creator: sperlingxx
Created: 02/Mar/22 06:15
Updated: 02/Mar/22 06:27
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Optimizer
Due Date: 
Votes: 0
Labels: 
Description: The idea of this issue is originated from [https://github.com/NVIDIA/spark-rapids/issues/4186]

Currently, Spark SQL executes each incorrelated scalar subquery as an independent spark job. It generates a lot of spark jobs when we run a query with a lot of incorrelated scalar subqueries. Scenarios like this can be optimized in terms of logcial plan. We can combine subquery plans of compatible scalar subqueries into fused subquery plans. And let them shared by multiple scalar subqueries. With combining compatible scalar subqueries, we can cut off the cost of subquery jobs, because common parts of compatible subquery plans (scans/filters) will be reused.

 

Here is an example to demonstrate the basic idea of combining compatible scalar subqueries:
{code:java}
SELECT SUM(i)
FROM t
WHERE l > (SELECT MIN(l2) FROM t)
AND l2 < (SELECT MAX(l) FROM t)
AND i2 <> (SELECT MAX(i2) FROM t)
AND i2 <> (SELECT MIN(i2) FROM t) {code}
Optimized logicial plan of above query looks like:
{code:java}
Aggregate [sum(i)]
+- Project [i]
  +- Filter (((l > scalar-subquery#1) AND (l2 < scalar-subquery#2)) AND (NOT (i2 = scalar-subquery#3) AND NOT (i2 = scalar-subquery#4)))
     :  :- Aggregate [min(l2)]
     :  :  +- Project [l2]
     :  :     +- Relation [l,l2,i,i2]
     :  +- Aggregate [max(l)]
     :     +- Project [l]
     :        +- Relation [l,l2,i,i2]
     :  +- Aggregate [max(i2)]
     :     +- Project [l]
     :        +- Relation [l,l2,i,i2]
     :  +- Aggregate [min(i2)]
     :     +- Project [l]
     :        +- Relation [l,l2,i,i2]
     +- Relation [l,l2,i,i2] {code}
After the combination of compatible scalar subqueries, the logicial plan becomes:
{code:java}
 Aggregate [sum(i)]
 +- Project [i]
   +- Filter (((l > shared-scalar-subquery#1) AND (l2 < shared-scalar-subquery#2)) AND (NOT (i2 = shared-scalar-subquery#3) AND NOT (i2 = shared-scalar-subquery#4)))
      :  :- Aggregate [min(l2),max(l),max(i2),min(i2)]
      :  :  +- Project [l2,l,i2]
      :  :     +- Relation [l,l2,i,i2]
      :  :- Aggregate [min(l2),max(l),max(i2),min(i2)]
      :  :  +- Project [l2,l,i2]
      :        +- Relation [l,l2,i,i2]
      :  :- Aggregate [min(l2),max(l),max(i2),min(i2)]
      :  :  +- Project [l2,l,i2]
      :        +- Relation [l,l2,i,i2]
      :  :- Aggregate [min(l2),max(l),max(i2),min(i2)]
      :  :  +- Project [l2,l,i2]
      :        +- Relation [l,l2,i,i2]
      +- Relation [l,l2,i,i2] {code}
 

There are 4 scalar subqueries within this query. Although they are semantically unequal, they are based on the same relation. Therefore, we can merge all of them into an unified Aggregate to resue the common scan(relation).

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Patch
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 02 06:27:41 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z102uw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Mar/22 06:27;apachespark;User 'sperlingxx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35708;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Print resolved versions of platform JARs used by Spark in Yarn client
Issue key: SPARK-38356
Issue id: 13431101
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: akpatnam25
Creator: akpatnam25
Created: 28/Feb/22 22:32
Updated: 28/Feb/22 23:24
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: YARN
Due Date: 
Votes: 0
Labels: 
Description: It would be nice if we could print the resolved versions. I think a sensible approach would be to resolve the symlinks in yarn.Client before uploading. This would also make the localized file name include the version number, so we could see it easily on the Environment SHS tab.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Feb 28 23:24:02 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z100v4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Feb/22 22:54;akpatnam25;working on this;;;, 28/Feb/22 23:24;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/35688;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 28/Feb/22 23:24;apachespark;User 'akpatnam25' has created a pull request for this issue:
https://github.com/apache/spark/pull/35688;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Change of unionByName parameter
Issue key: SPARK-38193
Issue id: 13428166
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: ddavies1
Creator: ddavies1
Created: 12/Feb/22 23:07
Updated: 13/Feb/22 00:39
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Hello,

I had a quick question about the unionByName function. This function currently seems to accept a parameter- "allowMissingColumns"- that allows some tolerance to merging datasets with different schemas [here|[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2170]]; but the implementation is currently a bit restrictive, i.e., with the second parameter being a boolean, it is only possible to make unionByName add all columns from both dataframes at the moment. We have other use cases in our workflows- for example, to take only column names that are in both dataframes (and I'm assuming that other users will have different merge strategies in mind also). Does it seem reasonable to extend the parameter from "allowMissingColumns" to a "mode" string-type parameter natively in Spark? If so, I'm happy to make a PR to achieve this (the change would involve amending the [ResolveUnion.scala|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveUnion.scala] utility to make it more flexible in merging columns; to a user it would look a lot more like the 'join' operator, where a join strategy is selected). 

I've posted this question on the dev mailing list also; happy to continue the conversation there if that is preferable.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-02-12 23:07:37.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ziuo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Shuffle by rand could lead to incorrect answers when ShuffleFetchFailed happend
Issue key: SPARK-38160
Issue id: 13427423
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: EdisonWang
Creator: EdisonWang
Created: 09/Feb/22 08:48
Updated: 09/Feb/22 10:26
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When we do shuffle on indeterminate expressions such as rand, and ShuffleFetchFailed happend, we may get incorrent result since it only retries failed map tasks.

We try to fix this by retry all upstream map tasks in this situation.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Feb 09 10:26:56 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zea0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.3.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Feb/22 10:26;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/35460;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve test suite ExternalCatalogSuite
Issue key: SPARK-38025
Issue id: 13424743
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: khalidmammadov9@gmail.com
Creator: khalidmammadov9@gmail.com
Created: 25/Jan/22 17:14
Updated: 09/Feb/22 01:34
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Tests
Due Date: 
Votes: 0
Labels: 
Description: Test suite *ExternalCatalogSuite.scala* can be optimized by removing repetitive code by replacing them with already available utility function with some minor changes. This will reduce redundant code, simplify the suite and improve readability.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jan 25 17:44:22 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yxvk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Jan/22 17:44;apachespark;User 'khalidmammadov' has created a pull request for this issue:
https://github.com/apache/spark/pull/35323;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Fix bug of EnumTypeSetBenchmark and update benchmark result
Issue key: SPARK-38127
Issue id: 13426867
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 07/Feb/22 08:26
Updated: 07/Feb/22 08:30
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The iteration times of the comparison case in the benchmark is not same
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Feb 07 08:30:27 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zawg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Feb/22 08:30;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35418;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use loops to refactor methods UnsafeRow#isFixedLength and UnsafeRow#isMutable
Issue key: SPARK-38052
Issue id: 13425320
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 28/Jan/22 05:53
Updated: 28/Jan/22 06:00
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Methods UnsafeRow#isFixedLength and UnsafeRow#isMutable use tail recursion now , we can rewrite them with looping, which will be considerably faster
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jan 28 06:00:18 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z1fc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Jan/22 05:59;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35350;;;, 28/Jan/22 06:00;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35350;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 28/Jan/22 06:00;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35350;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support SchemaRegistry in from_avro method
Issue key: SPARK-34652
Issue id: 13362857
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: blcksrx
Creator: blcksrx
Created: 07/Mar/21 18:23
Updated: 25/Dec/21 11:40
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: Confluent Schema Registry provides a serving layer for your metadata. It provides a RESTful interface for storing and retrieving your Avro®, JSON Schema, and Protobuf schemas. 
It would be nice to implement a new method that uses SchemaRegistry instead of the raw schema.
In addition, the DataBricks has already implemented this function. just check this link out:
https://docs.databricks.com/spark/latest/structured-streaming/avro-dataframe.html#example-with-schema-registry



Maybe it would be a simple and short method but really usefull
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Dec 25 11:40:26 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0odvk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Mar/21 18:38;apachespark;User 'blcksrx' has created a pull request for this issue:
https://github.com/apache/spark/pull/31771;;;, 25/Dec/21 11:39;apachespark;User 'blcksrx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35019;;;, 25/Dec/21 11:40;apachespark;User 'blcksrx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35019;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 25/Dec/21 11:39;apachespark;User 'blcksrx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35019;;;
Comment.2: 25/Dec/21 11:40;apachespark;User 'blcksrx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35019;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Allow nondeterministic expression in aggregate function
Issue key: SPARK-37387
Issue id: 13412714
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: yoda-mon
Creator: yoda-mon
Created: 19/Nov/21 07:06
Updated: 19/Nov/21 11:23
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Nondeterministic expression in aggregate function is not allow in spark, so we cannot execute query like
{code:java}
SELECT COUNT(RANDOM());
{code}
and raise \{{nondeterministic expression ... should not appear in the arguments of an aggregate function. }}error message.
[related code section|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/CheckAnalysis.scala#L298]

 

Hence other DB like PostgreSQL, we can call the SQL.
{code:java}
postgres=# SELECT COUNT(RANDOM());
 count
-------
     1
(1 row) {code}
 

I tried to remove the error message section, then I found spark could execute the query. 
{code:java}
scala> spark.sql("SELECT COUNT(RANDOM())").show()
+-------------+
|count(rand())|
+-------------+
|            1|
+-------------+ {code}
 

It could be useful for spark users to be able to execute those kinds of queries because they can simply call
{code:java}
spark.sql("SELECT COUNT(DISTINCT(INPUT_FILE_NAME())) FROM table WHERE ...") {code}
to find target files, for example.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Nov 19 11:23:55 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ww94:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Nov/21 07:14;yoda-mon;[~cloud_fan] I would like you to ask your opinion, I couldn't find the reason to set the rule, but there might be some reasons to prohibit this.;;;, 19/Nov/21 08:40;zhenw;[~yoda-mon] what is the use case behind, random doesn't seem justify your usage;;;, 19/Nov/21 11:23;yoda-mon;[~zhenw] I found this behavior when I tried to find the filenames in a S3 bucket, which contained a specific value.

Both {{RANDOM()}} and {{INPUT_FILE_NAME()}} are nondeterministic function. I referred to RANDOM() because it is widely implemented by RDBs and is easy to compare.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 19/Nov/21 08:40;zhenw;[~yoda-mon] what is the use case behind, random doesn't seem justify your usage;;;
Comment.2: 19/Nov/21 11:23;yoda-mon;[~zhenw] I found this behavior when I tried to find the filenames in a S3 bucket, which contained a specific value.

Both {{RANDOM()}} and {{INPUT_FILE_NAME()}} are nondeterministic function. I referred to RANDOM() because it is widely implemented by RDBs and is easy to compare.;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add benchmark for Z-order
Issue key: SPARK-37366
Issue id: 13412406
Parent id: 13412397.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chengsu
Creator: chengsu
Created: 18/Nov/21 02:35
Updated: 18/Nov/21 02:35
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: We should add a benchmark of Z-order for Parquet and ORC after other sub-tasks have finished.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-18 02:35:58.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wuco:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Introduce Z-order for efficient data skipping
Issue key: SPARK-37361
Issue id: 13412397
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chengsu
Creator: chengsu
Created: 18/Nov/21 02:06
Updated: 18/Nov/21 02:33
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: This is the umbrella Jira to track the progress of introducing Z-order in Spark. Z-order enables to sort tuples in a way, to allow efficiently data skipping for columnar file format (Parquet and ORC).

For query with filter on combination of multiple columns, example:
{code:java}
SELECT *
FROM table
WHERE x = 0 OR y = 0
{code}
Parquet/ORC cannot skip file/row-groups efficiently when reading, even though the table is sorted (locally or globally) on any columns. However when table is Z-order sorted on multiple columns, Parquet/ORC can skip file/row-groups efficiently when reading.

We should add the feature in Spark to allow OSS Spark users benefitted in running these queries.

 

Reference for other systems support Z-order:
 * Databricks Delta Lake:
 ** [https://databricks.com/blog/2018/07/31/processing-petabytes-of-data-in-seconds-with-databricks-delta.html]
 ** [https://docs.databricks.com/spark/latest/spark-sql/language-manual/delta-optimize.html]
 * Presto:
 ** [https://github.com/prestodb/presto/blob/master/presto-hive-common/src/main/java/com/facebook/presto/hive/zorder/ZOrder.java]
 * Impala:
 ** https://issues.apache.org/jira/browse/IMPALA-8755 
 * AWS:
 ** [https://aws.amazon.com/blogs/database/z-order-indexing-for-multifaceted-queries-in-amazon-dynamodb-part-1/]
 ** [https://aws.amazon.com/blogs/database/z-order-indexing-for-multifaceted-queries-in-amazon-dynamodb-part-2/]
 ** [https://cdn2.hubspot.net/hubfs/392937/Whitepaper/WP/interleaved_keys_v12_1.pdf]

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Nov 18 02:14:10 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wuao:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Nov/21 02:14;chengsu;Just FYI, I am working on each sub-task now. Thanks.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add ZORDER BY syntax and plan rule
Issue key: SPARK-37365
Issue id: 13412402
Parent id: 13412397.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chengsu
Creator: chengsu
Created: 18/Nov/21 02:13
Updated: 18/Nov/21 02:13
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: We can introduce a special syntax in Spark syntax `ZORDER BY (x, y, ...)` to allow a better interface to users.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-18 02:13:43.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wubs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add code-gen evaluation for Z-order expression
Issue key: SPARK-37364
Issue id: 13412400
Parent id: 13412397.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chengsu
Creator: chengsu
Created: 18/Nov/21 02:11
Updated: 18/Nov/21 02:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: This Jira is to add code-gen support for Z-order expression.
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-18 02:11:57.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wubc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support string type in Z-order expression
Issue key: SPARK-37363
Issue id: 13412399
Parent id: 13412397.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chengsu
Creator: chengsu
Created: 18/Nov/21 02:10
Updated: 18/Nov/21 02:10
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: This Jira is to support string type in Z-order expression.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-18 02:10:12.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wub4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support float type in Z-order expression
Issue key: SPARK-37362
Issue id: 13412398
Parent id: 13412397.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chengsu
Creator: chengsu
Created: 18/Nov/21 02:09
Updated: 18/Nov/21 02:09
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: This Jira is to support float type in Z-order expression.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-18 02:09:13.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wuaw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add benchmark for aggregate push down
Issue key: SPARK-37167
Issue id: 13409202
Parent id: 13112874.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chengsu
Creator: chengsu
Created: 29/Oct/21 20:33
Updated: 06/Nov/21 01:42
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: As we added aggregate push down for Parquet and ORC, let's also add a micro benchmark for both file formats, similar to filter push down and nested schema pruning.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Nov 06 01:42:54 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0waqo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Nov/21 01:42;chengsu;Just FYI, I am working on it.;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add the Hive macro functionality to SparkSQL
Issue key: SPARK-37216
Issue id: 13410217
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: hgs19921112
Creator: hgs19921112
Created: 05/Nov/21 10:04
Updated: 05/Nov/21 10:34
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Should add macro to SparkSQL? We have did this in Spark that we use.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): scala
Custom field (Last public comment date): Fri Nov 05 10:34:13 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wgzs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Nov/21 10:34;apachespark;User 'hgs19921112' has created a pull request for this issue:
https://github.com/apache/spark/pull/34492;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Allow sequences (tuples and lists) as pivot values argument in PySpark
Issue key: SPARK-37116
Issue id: 13408346
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: dchvn
Creator: dchvn
Created: 26/Oct/21 04:56
Updated: 26/Oct/21 08:16
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Both tuples and lists are accepted by PySpark on runtime.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Oct 26 08:16:21 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0w5gg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Oct/21 04:57;dchvn;work on this :v

 ;;;, 26/Oct/21 05:03;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34392;;;, 26/Oct/21 05:04;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34392;;;, 26/Oct/21 08:16;zero323;Sadly, this is not going to work. For example, this will typecheck, although incorrect. {{Tuple | List}} might work, but this probably more general problem in how we interact with JVM, not limited to typing issues.

 
{code:python}
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

(spark.read
    .csv("foo.csv")
    .groupBy("foo")
    .pivot("bar", "baz")
    .sum()) {code};;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 26/Oct/21 05:03;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34392;;;
Comment.2: 26/Oct/21 05:04;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34392;;;
Comment.3: 26/Oct/21 08:16;zero323;Sadly, this is not going to work. For example, this will typecheck, although incorrect. {{Tuple | List}} might work, but this probably more general problem in how we interact with JVM, not limited to typing issues.

 
{code:python}
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

(spark.read
    .csv("foo.csv")
    .groupBy("foo")
    .pivot("bar", "baz")
    .sum()) {code};;;
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Use java.util.Objects API instead of Guava API
Issue key: SPARK-36750
Issue id: 13400923
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 14/Sep/21 06:07
Updated: 14/Sep/21 06:38
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: MLlib, Spark Core, SQL
Due Date: 
Votes: 0
Labels: 
Description: Java 8 provides the java.util.Objects, we can use it to replace some guava API usages with the same semantics.

 
 * Preconditions.checkNotNull -> j.u.Objects.requireNonNull
 * Objects.hashCode -> j.u.Objects.hash
 * Objects.equal -> j.u.Objects.equals
 * 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Sep 14 06:38:27 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uvow:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Sep/21 06:38;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33991;;;
Affects Version/s.1: 
Component/s.1: Spark Core
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark UI-Stages DAG visualization is empty in IE11
Issue key: SPARK-35823
Issue id: 13384638
Parent id: 13384636.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jobitmathew
Creator: jobitmathew
Created: 19/Jun/21 08:22
Updated: 10/Sep/21 06:55
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.0.3, 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Web UI
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 08 16:55:14 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s3ag:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Sep/21 16:55;dc-heros;I'm working on this.;;;
Affects Version/s.1: 3.1.2
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Allow expand 'quotedRegexColumnNames' in all expressions when it’s expanded to one named expression
Issue key: SPARK-36698
Issue id: 13400101
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: planga82
Creator: planga82
Created: 08/Sep/21 22:51
Updated: 09/Sep/21 23:35
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: With
{code:java}
spark.sql.parser.quotedRegexColumnNames=true{code}
regular expressions are not allowed in expressions like
{code:java}
SELECT `col_.*`/exp FROM (SELECT 3 AS col_a, 1 as exp){code}
 I propose to improve this behavior and allow this regular expression when it expands to only one named expression. It’s the same behavior in Hive.

*Example 1:* 
{code:java}
SELECT `col_.*`/exp FROM (SELECT 3 AS col_a, 1 as exp){code}
 col_.* expands to col_a: OK

*Example 2:*
{code:java}
SELECT `col_.*`/col_b FROM (SELECT 3 AS col_a, 1 as col_b){code}
col_.* expands to (col_a, col_b): Fail like now

 

Related with SPARK-36488
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 09 23:35:29 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uqm8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Sep/21 22:51;planga82;I'm working on it;;;, 09/Sep/21 23:35;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/33950;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 09/Sep/21 23:35;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/33950;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: The Cache use weight eviction mechanism has risk of memory leak
Issue key: SPARK-36598
Issue id: 13397453
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 26/Aug/21 07:51
Updated: 26/Aug/21 08:44
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core, SQL
Due Date: 
Votes: 0
Labels: 
Description: In spark, we define Guava Cache in 3 places and use the weight eviction mechanism:
 # ExternalShuffleBlockResolver#shuffleIndexCache
 # RemoteBlockPushResolver#indexCache
 # SharedInMemoryCache#cache

These 3 Guava Cache has risk of memory leakage if configured `maximumWeight` >= 8589934592 (8g) because LocalCache weight eviction does not work when maxSegmentWeight is >= Int.MAX_VALUE( [https://github.com/google/guava/issues/1761|https://github.com/google/guava/issues/1761).])

The UT that can be reproduced this issue is as follows:
{code:java}
@Test
public void testShuffleIndexCacheEvictionBehavior() throws IOException, ExecutionException {
  Map<String, String> config = new HashMap<>();
  String indexCacheSize = "8192m";
  config.put("spark.shuffle.service.index.cache.size", indexCacheSize);
  TransportConf transportConf = new TransportConf("shuffle", new MapConfigProvider(config));
  ExternalShuffleBlockResolver resolver = new ExternalShuffleBlockResolver(transportConf, null);
  resolver.registerExecutor("app0", "exec0", dataContext.createExecutorInfo(SORT_MANAGER));

  LoadingCache<File, ShuffleIndexInformation> shuffleIndexCache = resolver.shuffleIndexCache; // need change access scope of ExternalShuffleBlockResolver.shuffleIndexCache

  // 8g -> 8589934592 bytes
  long maximumWeight = JavaUtils.byteStringAsBytes(indexCacheSize);
  int unitSize = 1048575;
  // CacheBuilder.DEFAULT_CONCURRENCY_LEVEL
  int concurrencyLevel = 4;
  int totalGetCount = 16384;
  // maxCacheCount is 8192
  long maxCacheCount = maximumWeight / concurrencyLevel / unitSize * concurrencyLevel;
  for (int i = 0; i < totalGetCount; i++) {
    File indexFile = new File("shuffle_" + 0 + "_" + i + "_0.index");
    ShuffleIndexInformation indexInfo = Mockito.mock(ShuffleIndexInformation.class);
    Mockito.when(indexInfo.getSize()).thenReturn(unitSize);
    shuffleIndexCache.get(indexFile, () -> indexInfo);
  }

  long totalWeight =
    shuffleIndexCache.asMap().values().stream().mapToLong(ShuffleIndexInformation::getSize).sum();
  long size = shuffleIndexCache.size();
  try{
    Assert.assertTrue(size <= maxCacheCount);
    Assert.assertTrue(totalWeight < maximumWeight);
    fail("The tests code should not enter this line now.");
  } catch (AssertionError error) {
    // The code will enter this branch because LocalCache weight eviction does not work
    // when maxSegmentWeight is >= Int.MAX_VALUE.
    Assert.assertTrue(size > maxCacheCount && size <= totalGetCount);
    Assert.assertTrue(totalWeight > maximumWeight);
  }
}
{code}
 

and from the debug view  we found that there are 2 segment.totalWeight is a negative value

 

!image-2021-08-26-15-56-09-028.png!  

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 26/Aug/21 07:56;LuciferYang;image-2021-08-26-15-56-09-028.png;https://issues.apache.org/jira/secure/attachment/13032499/image-2021-08-26-15-56-09-028.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 26 08:44:19 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uaa8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Aug/21 08:12;LuciferYang;cc [~hyukjin.kwon] [~dongjoon]
  
 # Upgrade the Guava version can fix this bug
 # Or we can avoid this issue by limiting the configuration related to `maximumWeight` to the security range

Do you have any better suggestions ?;;;, 26/Aug/21 08:43;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33848;;;, 26/Aug/21 08:44;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33848;;;
Affects Version/s.1: 
Component/s.1: SQL
Comment.1: 26/Aug/21 08:43;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33848;;;
Comment.2: 26/Aug/21 08:44;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33848;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Improve consistency processing floating point special literals
Issue key: SPARK-36453
Issue id: 13394105
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: planga82
Creator: planga82
Created: 08/Aug/21 22:35
Updated: 15/Aug/21 23:13
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Special literal in floating point are not consistent between cast and json expressions

 
{code:java}
scala> spark.sql("SELECT CAST('+Inf' as Double)").show
+--------------------+                                                        
|CAST(+Inf AS DOUBLE)|
+--------------------+
|            Infinity|
+--------------------+
{code}
 
{code:java}
scala> val schema =  StructType(StructField("a", DoubleType) :: Nil)

scala> Seq("""{"a" : "+Inf"}""").toDF("col1").select(from_json(col("col1"),schema)).show

+---------------+
|from_json(col1)|
+---------------+
|         {null}|
+---------------+

scala> Seq("""{"a" : "+Inf"}""").toDF("col").withColumn("col", from_json(col("col"), StructType.fromDDL("a DOUBLE"))).write.json("/tmp/jsontests12345")
scala> spark.read.schema(StructType(Seq(StructField("col",schema)))).json("/tmp/jsontests12345").show

+------+
|   col|
+------+
|{null}|
+------+

{code}
 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Aug 15 23:12:56 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tpmg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Aug/21 22:36;planga82;I'm working on it;;;, 15/Aug/21 23:12;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/33747;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 15/Aug/21 23:12;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/33747;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: support to set the meta conf in HiveMetastoreClient
Issue key: SPARK-36514
Issue id: 13395309
Parent id: 
Issue Type: Task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: KevinPis
Creator: KevinPis
Created: 15/Aug/21 09:42
Updated: 15/Aug/21 10:11
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: _currently, we can't set the meta conf in HiveMetastoreClient, so add this feature._
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Aug 15 10:11:47 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tx20:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Aug/21 09:42;KevinPis;I'm working on.;;;, 15/Aug/21 10:11;apachespark;User 'kevincmchen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33746;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 15/Aug/21 10:11;apachespark;User 'kevincmchen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33746;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add end-to-end tests for ANSI implicit cast rules
Issue key: SPARK-34247
Issue id: 13354702
Parent id: 
Issue Type: Task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: Gengliang.Wang
Creator: Gengliang.Wang
Created: 26/Jan/21 15:51
Updated: 13/Aug/21 09:24
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Add end-to-end tests for ANSI implicit cast rules:
https://github.com/apache/spark/pull/31349
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-01-26 15:51:11.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0n08o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Upgrade json4s to 4.0.3
Issue key: SPARK-36408
Issue id: 13393378
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: sarutak
Reporter: sarutak
Creator: sarutak
Created: 04/Aug/21 04:52
Updated: 04/Aug/21 04:57
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 0
Labels: 
Description: json4s v4 was released and 173 bug fixes and improvements are applied after v3.7.0-M11 which Spark currently depends on.
Some improvements seem to be about Scala 2.13 and Scala 3.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 04 04:57:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tl4w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Aug/21 04:57;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33630;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Replace revertPartialWritesAndClose with close in ExternalSorter.spill and ExternalAppendOnlyMap.spill
Issue key: SPARK-36324
Issue id: 13392267
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 28/Jul/21 07:26
Updated: 28/Jul/21 07:43
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: ExternalAppendOnlyMap.spill method call  `revertPartialWritesAndClose` method when `objectsWritten == 0` as follows:
{code:java}
try {
  while (inMemoryIterator.hasNext) {
    ...
    if (objectsWritten == serializerBatchSize) {
      flush()
    }
  }
  if (objectsWritten > 0) {
    flush()
    writer.close()
  } else {
    writer.revertPartialWritesAndClose()
  }
  success = true
} finally {
  ...
}{code}
 

writer.revertPartialWritesAndClose() can replace with writer.close to reduce a set of file operations includes open, truncate and close.

 

A similar situation exists for ExternalSorter

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 28 07:43:26 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0teag:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Jul/21 07:42;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33556;;;, 28/Jul/21 07:43;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33556;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 28/Jul/21 07:43;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33556;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Unify write exception and delete abnormal disk block object file process
Issue key: SPARK-36053
Issue id: 13388379
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 08/Jul/21 12:03
Updated: 08/Jul/21 12:32
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: There are some duplicate codes related to cleaning up failed files after DiskBlockObjectWriter writes data abnormally in `BypassMergeSortShuffleWriter`, `ExternalAppendOnlyMap` and `ExternalSorter`, the duplicate codes as follows:
{code:java}
        writer.revertPartialWritesAndClose()
        if (file.exists()) {
          if (!file.delete()) {
            logWarning(s"Error deleting ${file}")
          }
        }
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jul 08 12:32:40 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sqbs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Jul/21 12:32;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33267;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Sort entries in the pyspark API reference docs alphabetically
Issue key: SPARK-43236
Issue id: 13533634
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: lukasz.mentel
Creator: lukasz.mentel
Created: 22/Apr/23 11:26
Updated: 22/Apr/23 11:26
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0
Fix Version/s: 
Component/s: Documentation
Due Date: 
Votes: 0
Labels: 
Description: Currently the methods on a lot of api reference docs are nor sorted which makes navigation quite hard and unintuitive, e.g. [https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/series.html#reindexing-selection-label-manipulation]

has the following order:
|[{{Series.equals}}|https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.Series.equals.html#pyspark.pandas.Series.equals](other)|Compare if the current value is equal to the other.|
|[{{Series.add_prefix}}|https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.Series.add_prefix.html#pyspark.pandas.Series.add_prefix](prefix)|Prefix labels with string ??prefix??.|
|[{{Series.add_suffix}}|https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.Series.add_suffix.html#pyspark.pandas.Series.add_suffix](suffix)|Suffix labels with string suffix.|
|[{{Series.first}}|https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.Series.first.html#pyspark.pandas.Series.first](offset)|Select first periods of time series data based on a date offset.|
|[{{Series.head}}|https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.Series.head.html#pyspark.pandas.Series.head]([n])|

 

Ideally the methods should be sorted alphabetically to make navigating the docs predictable.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python
Custom field (Last public comment date): 2023-04-22 11:26:33.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1hgjc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.12.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Make Dataset.showString() public
Issue key: SPARK-39722
Issue id: 13470680
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: jatinsharma
Creator: jatinsharma
Created: 08/Jul/22 11:04
Updated: 27/Mar/23 09:18
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 2.4.8, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, we have {{.show}} APIs on a Dataset, but they print directly to stdout.

But there are a lot of cases where we might need to get a String representation of the show output. For example
 * We have a logging framework to which we need to push the representation of a df
 * We have to send the string over a REST call from the driver
 * We want to send the string to stderr instead of stdout

For such cases, currently one needs to do a hack by changing the Console.out temporarily and catching the representation in a ByteArrayOutputStream or similar, then extracting the string from it.

Strictly only printing to stdout seems like a limiting choice. 

 

Solution:

We expose APIs to return the String representation back. We already have the .{{{}showString{}}} method internally.

 

We could mirror the current {{.show}} APIS with a corresponding {{.showString}} (and rename the internal private function to something else if required)
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Mar 27 09:18:33 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16pj4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Jul/22 23:59;xkrogen;General +1 from me. We have some internal code that does exactly the {{Console.out}} redirection hack you described.;;;, 19/Mar/23 05:05;vindhyag;I would like  to work on this. 
I think new set of APIs should be something like GetString since it doesnt technically show but return string ;;;, 27/Mar/23 09:18;githubbot;User 'VindhyaG' has created a pull request for this issue:
https://github.com/apache/spark/pull/40553;;;
Affects Version/s.1: 3.3.0
Component/s.1: 
Comment.1: 19/Mar/23 05:05;vindhyag;I would like  to work on this. 
I think new set of APIs should be something like GetString since it doesnt technically show but return string ;;;
Comment.2: 27/Mar/23 09:18;githubbot;User 'VindhyaG' has created a pull request for this issue:
https://github.com/apache/spark/pull/40553;;;
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Correct the property name of data source option for csv
Issue key: SPARK-40244
Issue id: 13478918
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: diamondsky
Creator: diamondsky
Created: 27/Aug/22 07:58
Updated: 27/Aug/22 08:40
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: Documentation
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Aug 27 08:40:16 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18468:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Aug/22 08:03;apachespark;User 'mukever' has created a pull request for this issue:
https://github.com/apache/spark/pull/37689;;;, 27/Aug/22 08:20;apachespark;User 'mukever' has created a pull request for this issue:
https://github.com/apache/spark/pull/37690;;;, 27/Aug/22 08:20;apachespark;User 'mukever' has created a pull request for this issue:
https://github.com/apache/spark/pull/37690;;;, 27/Aug/22 08:25;apachespark;User 'mukever' has created a pull request for this issue:
https://github.com/apache/spark/pull/37691;;;, 27/Aug/22 08:39;apachespark;User 'mukever' has created a pull request for this issue:
https://github.com/apache/spark/pull/37692;;;, 27/Aug/22 08:40;apachespark;User 'mukever' has created a pull request for this issue:
https://github.com/apache/spark/pull/37692;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 27/Aug/22 08:20;apachespark;User 'mukever' has created a pull request for this issue:
https://github.com/apache/spark/pull/37690;;;
Comment.2: 27/Aug/22 08:20;apachespark;User 'mukever' has created a pull request for this issue:
https://github.com/apache/spark/pull/37690;;;
Comment.3: 27/Aug/22 08:25;apachespark;User 'mukever' has created a pull request for this issue:
https://github.com/apache/spark/pull/37691;;;
Comment.4: 27/Aug/22 08:39;apachespark;User 'mukever' has created a pull request for this issue:
https://github.com/apache/spark/pull/37692;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Logging the exception of detect jdbc table exist
Issue key: SPARK-39765
Issue id: 13471419
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: kaifeiYi
Creator: kaifeiYi
Created: 13/Jul/22 14:15
Updated: 13/Jul/22 14:22
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Logging the exception of detect jdbc table exist in `JdbcUtils.tableExists`
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 13 14:22:17 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16u2o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Jul/22 14:21;apachespark;User 'yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/37177;;;, 13/Jul/22 14:22;apachespark;User 'yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/37177;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 13/Jul/22 14:22;apachespark;User 'yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/37177;;;
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add hash probes metrics for shuffled hash join
Issue key: SPARK-38354
Issue id: 13431048
Parent id: 13319611.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: chengsu
Creator: chengsu
Created: 28/Feb/22 17:17
Updated: 26/Apr/22 23:57
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: For `HashAggregate` there's a SQL metrics to track number of hash probes per looked-up key. It would be better to add a similar metrics for shuffled hash join as well, to get some idea of hash probing performance.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Apr 26 23:13:22 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z100jc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Feb/22 17:25;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/35686;;;, 09/Mar/22 15:53;cloud_fan;Issue resolved by pull request 35686
[https://github.com/apache/spark/pull/35686];;;, 25/Apr/22 05:48;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/36338;;;, 25/Apr/22 05:49;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/36338;;;, 26/Apr/22 23:13;dongjoon;This is reverted due to the regression via https://github.com/apache/spark/pull/36338;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 09/Mar/22 15:53;cloud_fan;Issue resolved by pull request 35686
[https://github.com/apache/spark/pull/35686];;;
Comment.2: 25/Apr/22 05:48;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/36338;;;
Comment.3: 25/Apr/22 05:49;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/36338;;;
Comment.4: 26/Apr/22 23:13;dongjoon;This is reverted due to the regression via https://github.com/apache/spark/pull/36338;;;
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Rename `MEMORY_MODE` in `VectorizedParquetRecordReader` to `memoryMode` 
Issue key: SPARK-38012
Issue id: 13424567
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: LuciferYang
Creator: LuciferYang
Created: 25/Jan/22 03:49
Updated: 25/Jan/22 03:54
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: `MEMORY_MODE` is a `final` field, not a `static final` field,  the naming should be CamelCase

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jan 25 03:54:35 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ywsg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Jan/22 03:54;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35309;;;
Affects Version/s.1: 
Component/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Add support for running make-distribution without a "clean"
Issue key: SPARK-36250
Issue id: 13391222
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 22/Jul/21 00:04
Updated: 22/Jul/21 00:04
Last Viewed: 26/Jul/24 00:23
Resolved: 
Affects Version/s: 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Build, Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: Running the K8s integration tests requires* building a distribution, but clean builds are really really slow. We could make the param BUILD_COMMAND set if unset or add a --skip-clean to our shell script to allow for folks to more quickly test there k8s related changes.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Blocker): 
Outward issue link (Blocker): 
Outward issue link (Child-Issue): 
Inward issue link (Cloners): 
Outward issue link (Cloners): 
Inward issue link (Container): 
Outward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (Supercedes): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-22 00:04:19.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0t7u8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.3.0
Component/s.1: Kubernetes
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
EMR Versions: EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

