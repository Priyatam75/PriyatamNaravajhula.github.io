Summary: Data duplication and loss occur after executing 'insert overwrite...' in Spark 3.1.1
Issue key: SPARK-42694
Issue id: 13527390
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: bigdata_feng
Creator: bigdata_feng
Created: 07/Mar/23 07:51
Updated: 14/May/24 12:32
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: shuffle, spark
Description: We are currently using Spark version 3.1.1 in our production environment. We have noticed that occasionally, after executing 'insert overwrite ... select', the resulting data is inconsistent, with some data being duplicated or lost. This issue does not occur all the time and seems to be more prevalent on large tables with tens of millions of records.

We compared the execution plans for two runs of the same SQL and found that they were identical. In the case where the SQL was executed successfully, the amount of data being written and read during the shuffle stage was the same. However, in the case where the problem occurred, the amount of data being written and read during the shuffle stage was different. Please see the attached screenshots for the write/read data during shuffle stage.
 
Normal SQL:
!image-2023-03-07-15-59-08-818.png!


SQL with issues:
!image-2023-03-07-15-59-27-665.png!
 
Is this problem caused by a bug in version 3.1.1, specifically (SPARK-34534): 'New protocol FetchShuffleBlocks in OneForOneBlockFetcher lead to data loss or correctness'? Or is it caused by something else? What could be the root cause of this problem?
Environment: Spark 3.1.1

Hadoop 3.2.1

Hive 3.1.2
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 07/Mar/23 07:59;bigdata_feng;image-2023-03-07-15-59-08-818.png;https://issues.apache.org/jira/secure/attachment/13056090/image-2023-03-07-15-59-08-818.png, 07/Mar/23 07:59;bigdata_feng;image-2023-03-07-15-59-27-665.png;https://issues.apache.org/jira/secure/attachment/13056091/image-2023-03-07-15-59-27-665.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue May 14 12:32:43 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ge34:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Mar/23 09:10;yumwang;Could you upgrade to Spark 3.1.3 or Spark 3.3.2?;;;, 07/Mar/23 19:52;bjornjorgensen;Spark 3.1 [is EOL|https://github.com/apache/spark-website/commit/40f58f884bd258d6a332d583dc91c717b6b461f0 ] 
Try Spark 3.3.2 or 3.2.3 ;;;, 08/Mar/23 02:09;bigdata_feng;[~yumwang] 
This version has been running in production environment for over a year, and upgrading now would have a significant impact. Upgrading to 3.3.2 requires retesting and validation of the associated Ranger and Spark permission plugins. Therefore, the only option for a short-term upgrade is to choose 3.1.3. However, it's unclear whether upgrading to 3.1.3 will solve the problem since the cause of the issue is unknown, as it only occurs occasionally, which is confusing us.;;;, 08/Mar/23 02:09;bigdata_feng;[~bjornjorgensen] 
As the current upgrade would have a significant impact, is there any other faster way to locate and solve the problem besides upgrading?;;;, 17/Aug/23 02:32;bigboy001;Are there any exeptions when the data-loss occur?;;;, 17/Nov/23 07:52;bigdata_feng;No. Everything is OK, all the tasks are successful.;;;, 07/Mar/24 07:07;KK_3740;I also encountered this situation in spark3.1.1. Setting the parameter spark.shuffle.useOldFetchProtocol to true can avoid this situation, but I have not found the specific cause.;;;, 14/May/24 12:32;gaoyajun02;Have you enabled push-based shuffle?;;;
Affects Version/s.1: 
Comment.1: 07/Mar/23 19:52;bjornjorgensen;Spark 3.1 [is EOL|https://github.com/apache/spark-website/commit/40f58f884bd258d6a332d583dc91c717b6b461f0 ] 
Try Spark 3.3.2 or 3.2.3 ;;;
Comment.2: 08/Mar/23 02:09;bigdata_feng;[~yumwang] 
This version has been running in production environment for over a year, and upgrading now would have a significant impact. Upgrading to 3.3.2 requires retesting and validation of the associated Ranger and Spark permission plugins. Therefore, the only option for a short-term upgrade is to choose 3.1.3. However, it's unclear whether upgrading to 3.1.3 will solve the problem since the cause of the issue is unknown, as it only occurs occasionally, which is confusing us.;;;
Comment.3: 08/Mar/23 02:09;bigdata_feng;[~bjornjorgensen] 
As the current upgrade would have a significant impact, is there any other faster way to locate and solve the problem besides upgrading?;;;
Comment.4: 17/Aug/23 02:32;bigboy001;Are there any exeptions when the data-loss occur?;;;
Comment.5: 17/Nov/23 07:52;bigdata_feng;No. Everything is OK, all the tasks are successful.;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Optimize hive patition filter when the comparision dataType not match
Issue key: SPARK-45387
Issue id: 13552522
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: tianyima
Creator: tianyima
Created: 30/Sep/23 09:47
Updated: 16/Apr/24 07:44
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.3.0, 3.4.0, 3.5.0, 3.5.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Suppose we have a partitioned table `table_pt` with partition colum `dt` which is StringType and the table metadata is managed by Hive Metastore, if we filter partition by dt = '123', this filter can be pushed down to data source directly, but if the filter condition is number, e.g. dt = 123, Spark will not known which partition should be pushed down. Thus in the process of physical plan optimization, Spark will pull all of that table's partition meta data to client side, to decide which partition filter should be push down to the data source. This is poor of performance if the table has thousands of partitions and increasing the risk of hive metastore oom.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 13/Oct/23 15:40;tianyima;PruneFileSourcePartitions.diff;https://issues.apache.org/jira/secure/attachment/13063546/PruneFileSourcePartitions.diff
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 05 09:57:20 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1kobk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Feb/24 03:43;doki;I can't reproduce it in spark 3.5.0.

I try to create a partitioned csv table on hdfs like follow:
{code:java}
create external table noaa (column0 string, column1 int, column2 string, column3 int, column4 string, column5 string, column6 string, column7 string) PARTITIONED BY (year string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' LOCATION '/tmp/noaa'; 

alter table noaa add partition (year = '2019') LOCATION '/tmp/noaa/year=2019';

alter table noaa add partition (year = '2020') LOCATION '/tmp/noaa/year=2020';{code}
and the spark plan is 
{code:java}
scala> spark.sql("select * from noaa where year=2019 limit 10").explain(true)
== Parsed Logical Plan ==
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project [*]
      +- 'Filter ('year = 2019)
         +- 'UnresolvedRelation [noaa], [], false== Analyzed Logical Plan ==
column0: string, column1: string, column2: string, column3: string, column4: string, column5: string, column6: string, column7: string, year: string
GlobalLimit 10
+- LocalLimit 10
   +- Project [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62, year#63]
      +- Filter (cast(year#63 as int) = 2019)
         +- SubqueryAlias spark_catalog.default.noaa
            +- HiveTableRelation [`spark_catalog`.`default`.`noaa`, org.apache.hadoop.hive.serde2.OpenCSVSerde, Data Cols: [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62], Partition Cols: [year#63]]== Optimized Logical Plan ==
GlobalLimit 10
+- LocalLimit 10
   +- Filter (isnotnull(year#63) AND (cast(year#63 as int) = 2019))
      +- HiveTableRelation [`spark_catalog`.`default`.`noaa`, org.apache.hadoop.hive.serde2.OpenCSVSerde, Data Cols: [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62], Partition Cols: [year#63], Pruned Partitions: [(year=2019)]]== Physical Plan ==
CollectLimit 10
+- Scan hive spark_catalog.default.noaa [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62, year#63], HiveTableRelation [`spark_catalog`.`default`.`noaa`, org.apache.hadoop.hive.serde2.OpenCSVSerde, Data Cols: [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62], Partition Cols: [year#63], Pruned Partitions: [(year=2019)]], [isnotnull(year#63), (cast(year#63 as int) = 2019)]{code}
The filter has been pushed down.;;;, 05/Mar/24 09:57;tianyima;[~doki] the output execution plan is the final result, but the problem lies in the optimize process.

In your example, the partition key is stringType, but was cast to int to filter partitions. The driver will get all the partition to do this filter. If you have a hive table with thousands of partitions, this process will very slow and costly.;;;
Affects Version/s.1: 3.1.2
Comment.1: 05/Mar/24 09:57;tianyima;[~doki] the output execution plan is the final result, but the problem lies in the optimize process.

In your example, the partition key is stringType, but was cast to int to filter partitions. The driver will get all the partition to do this filter. If you have a hive table with thousands of partitions, this process will very slow and costly.;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.12.0, EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0, Unknown

Summary: spark driver process hangs due to "unable to create new native thread"
Issue key: SPARK-47279
Issue id: 13570765
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tianyima
Creator: tianyima
Created: 05/Mar/24 03:12
Updated: 26/Jul/24 00:20
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.5.0
Fix Version/s: 
Component/s: Scheduler, Spark Core
Due Date: 
Votes: 0
Labels: pull-request-available
Description: we encounter that spark driver hangs for about 11 hours,  and finall killed by user. In the driver log there is an error log: 
{quote}16:42:40 151 ERROR (org.apache.spark.rpc.netty.Inbox:94) - An error happened while processing message in the inbox for CoarseGrainedScheduler
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:719)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:957)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1367)
        at org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)
        at org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)
        at org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:144)
        at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
        at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
        at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
        at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
        at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
{quote}
 

After detailed analysis, we found that, the driver submitted task 0.0 at "16:40:50" to executor 4, and executor 4 finished the task 0.0 at "16:42:39", then executor 4 sent results to the driver. But in the same time, there is not sufficient memory in the the server that running the driver, the driver "unable to create new native thread" to handle the successful result of task 0.0, then the driver think task 0.0 has not finished and waiting for the "missed result" forever.

 

driver submit task 0.0

!driver_submit_task.png!

 

executor 4 task 0.0

!executor_4.png!

 

oom-killer:

!oom-killer.png!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 05/Mar/24 03:25;tianyima;driver_submit_task.png;https://issues.apache.org/jira/secure/attachment/13067264/driver_submit_task.png, 05/Mar/24 03:36;tianyima;executor_4.png;https://issues.apache.org/jira/secure/attachment/13067265/executor_4.png, 05/Mar/24 12:29;tianyima;oom-killer.png;https://issues.apache.org/jira/secure/attachment/13067272/oom-killer.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 3.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2024-03-05 03:12:27.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ns0g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.5.0
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-7.0.0, EMR-7.1.0

Summary: toJSON produces wrong values if DecimalType information is lost in as[Product]
Issue key: SPARK-48965
Issue id: 13586566
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: LDVSoft
Creator: LDVSoft
Created: 22/Jul/24 18:39
Updated: 22/Jul/24 19:34
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.5.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Consider this example:
{code:scala}
package com.jetbrains.jetstat.etl

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.DecimalType

object A {
  case class Example(x: BigDecimal)

  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .master("local[1]")
      .getOrCreate()

    import spark.implicits._

    val originalRaw = BigDecimal("123.456")
    val original = Example(originalRaw)

    val ds1 = spark.createDataset(Seq(original))
    val ds2 = ds1
      .withColumn("x", $"x" cast DecimalType(12, 6))

    val ds3 = ds2
      .as[Example]

    println(s"DS1: schema=${ds1.schema}, encoder.schema=${ds1.encoder.schema}")
    println(s"DS2: schema=${ds1.schema}, encoder.schema=${ds2.encoder.schema}")
    println(s"DS3: schema=${ds1.schema}, encoder.schema=${ds3.encoder.schema}")

    val json1 = ds1.toJSON.collect().head
    val json2 = ds2.toJSON.collect().head
    val json3 = ds3.toJSON.collect().head

    val collect1 = ds1.collect().head
    val collect2_ = ds2.collect().head
    val collect2 = collect2_.getDecimal(collect2_.fieldIndex("x"))
    val collect3 = ds3.collect().head

    println(s"Original: $original (scale = ${original.x.scale}, precision = ${original.x.precision})")
    println(s"Collect1: $collect1 (scale = ${collect1.x.scale}, precision = ${collect1.x.precision})")
    println(s"Collect2: $collect2 (scale = ${collect2.scale}, precision = ${collect2.precision})")
    println(s"Collect3: $collect3 (scale = ${collect3.x.scale}, precision = ${collect3.x.precision})")
    println(s"json1: $json1")
    println(s"json2: $json2")
    println(s"json3: $json3")
  }
}
{code}
Running it you'd see that json3 contains very much wrong data. After a bit of debugging, and sorry since I'm bad with Spark internals, I've found that:
 * In-memory representation of the data in this example used {{UnsafeRow}}, whose {{.getDecimal}} uses compression to store small Decimal values as longs, but doesn't remember decimal sizing parameters,
 * However, there are at least two sources for precision & scale to pass to that method: {{Dataset.schema}} (which is based on query execution, always contains 38,18 for me) and {{Dataset.encoder.schema}} (that gets updated in `ds2` to 12,6 but then is reset in `ds3`). Also, there is a {{Dataset.deserializer}} that seems to be combining those two non-trivially.
 * This doesn't seem to affect {{Dataset.collect()}} methods since they use {{deserializer}}, but {{Dataset.toJSON}} only uses the first schema.

Seems to me that either {{.toJSON}} should be more aware of what's going on or {{.as[]}} should be doing something else.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jul 22 19:34:22 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1qgy0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Jul/24 19:34;LDVSoft;I might've overlooked some internals, but it's definitely a conflict between {{encoder}}/{{exprEnc}}/{{resolvedEnc}} schema and {{queryExecution}} schema.;;;
Affects Version/s.1: 3.5.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: LZ4 failed to decompress a stream of shuffled data
Issue key: SPARK-18105
Issue id: 13015272
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: davies
Creator: davies
Created: 25/Oct/16 23:52
Updated: 05/Jul/24 16:36
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 14
Labels: 
Description: When lz4 is used to compress the shuffle files, it may fail to decompress it as "stream is corrupt"

{code}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 92 in stage 5.0 failed 4 times, most recent failure: Lost task 92.3 in stage 5.0 (TID 16616, 10.0.27.18): java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:220)
	at org.apache.spark.io.LZ4BlockInputStream.available(LZ4BlockInputStream.java:109)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:353)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at com.google.common.io.ByteStreams.read(ByteStreams.java:828)
	at com.google.common.io.ByteStreams.readFully(ByteStreams.java:695)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:127)
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:110)
	at scala.collection.Iterator$$anon$13.next(Iterator.scala:372)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:30)
	at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.sort_addToSorter$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:397)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(InsertIntoHadoopFsRelationCommand.scala:143)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

https://github.com/jpountz/lz4-java/issues/89
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-30225
Outward issue link (Reference): 
Attachment: 04/Aug/21 10:03;cameron.todd;TestWeightedGraph.java;https://issues.apache.org/jira/secure/attachment/13031431/TestWeightedGraph.java
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 05 16:35:42 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|i35e7b:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Oct/16 23:56;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/15632;;;, 27/Oct/16 18:18;davies;It turned out that the bug in LZ4 is a false alarm, so close the upstream issue.

Can't reproduce the behavior now.;;;, 30/Jan/17 02:44;Tagar;Ran into the same issue 

Looks like it's a "floating" issue - happens more over time. Restarting a spark context makes it harder to reproduce, but then it starts happening again.
Also, seems a precursor for this issue to show up is running executors tight on memory. I had stages failed many times, like :

{noformat}
ExecutorLostFailure (executor 49 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 9.4 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
{noformat}
(many executors but not all were killed like this) And then after several job restart attempts, this shows up:
{noformat}
java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)
	at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
{noformat}
once this problem showed up, I don't see executors failed because of memory, stage starts failing just because of this one problem -
java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)

Here's my job/spark sql query (running from pyspark if this matters) that at some point start reproducing this problem :
{code}
create table marketview.cb_filt_ratings2
stored as parquet
as
select psn, merchant_id
     , PERCENT_RANK() OVER (PARTITION BY merchant_id ORDER BY records_count ASC)        record_count_rating
     , PERCENT_RANK() OVER (PARTITION BY merchant_id ORDER BY transaction_count ASC)    transaction_count_rating
     , PERCENT_RANK() OVER (PARTITION BY merchant_id ORDER BY sum_spend ASC)            sum_spend_rating
from marketview.cb_filt_ratings1
{code}
table has ~6B rows.

ps. Also, the same problem was reported on SO a couple of months ago - http://stackoverflow.com/questions/40289464/spark-job-failing-in-yarn-mode
;;;, 30/Jan/17 16:40;Tagar;It's also worth to mention that the above query runs with spark.sql.shuffle.partitions= 36000 on a highly skewed data.
So it's possible that some of the partitions are highly populated; while others might be empty.
This problem happens here
https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/io/LZ4BlockInputStream.java#L163
I thought if this magic header check is broken when lz4 streams got merged and it does not insert that magic header for empty partitions 
(or inserts it oncorrectly for empty partitions?). That's just a hypothesis, as I couldn't find code that merges compressed files.;;;, 30/Jan/17 19:52;davies;There is a workaround merged into Spark 2.1 for these type of failures (decompress it and try again), can you try that?;;;, 30/Jan/17 21:14;Tagar;Thanks for the follow up [~davies]. 
We'd have to wait for a cdh parcel of Spark 2.1 to be released to try it out. ;;;, 20/Feb/17 22:46;jasonmoore2k;I've hit the same using a very recent build from branch-2.1 (b083ec5115f53a79ac54b85024c358510a03a459).;;;, 30/Mar/17 11:10;ouyangxc.zte;[~Tagar] Hi, I met this issue occasionally in Spark1.4.1 and Spark2.0.2. So, I wonder that do you have resolved this issue? Thanks!;;;, 05/May/17 20:19;rupeshmane;I'm facing this issue with Spark 2.1.0 but not with Spark 2.0.2. I'm using AWS EMR 5.2.0 which has Spark 2.0.2 and jobs run successfully. With everything same (code, files to process, settings, etc.) when I use EMR 5.5.0 which has Spark 2.1.0 I run in this issue. Stack trace is slightly different (see below), similar to this one: https://github.com/lz4/lz4-java/issues/13 and was fixed in 2013. Comparing LZO binary dependency Spark 2.0.2 and Spark 2.1.0 both use LZ4 1.3.0. So I'm confused why it is working on older version of Spark. Only difference in directory structure I see is Spark 2.0.2 has LZ4 libraries in lib but not under python/lib folder. While Spark 2.1.0 has these libraries in both lib and python/lib folder.


2017-05-05 01:15:50,681 [ERROR  ] schema: Exception raised during Operation: An error occurred while calling o104.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, ip-172-31-26-105.ec2.internal, executor 1): java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)
	at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
	at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2606)
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2622)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3099)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:853)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:349)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:284)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:221)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1269)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1493)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1492)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1492)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1720)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1675)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1664)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127)
	... 30 more
Caused by: java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)
	at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
	at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2606)
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2622)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3099)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:853)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:349)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:284)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:221)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1269)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more
Traceback (most recent call last):
  File "/usr/local/lib/python2.7/site-packages/cl_dataflow/alchemy/schema.py", line 149, in manage
    yield self
  File "/mnt/log-processor/src/job.py", line 126, in _run_single
    self._run_single_internal(processor, op, data_set)
  File "/mnt/log-processor/src/enricher.py", line 129, in _run_single_internal
    input_df, self._get_run_ts(processor, '%Y-%m-%d %H:%M:%S.%f')[:-3], output_location, output_location_tsv
  File "/mnt/log-processor/src/enricher.py", line 102, in _enrich
    self._write_avro(enriched, output_location)
  File "/mnt/log-processor/src/enricher.py", line 85, in _write_avro
    output_df.write.format("com.databricks.spark.avro").save(output_location)
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 550, in save
    self._jwrite.save(path)
  File "/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling o104.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, ip-172-31-26-105.ec2.internal, executor 1): java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)
	at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
	at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2606)
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2622)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3099)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:853)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:349)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:284)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:221)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1269)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1493)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1492)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1492)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1720)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1675)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1664)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127)
	... 30 more
Caused by: java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)
	at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
	at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2606)
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2622)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3099)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:853)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:349)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:284)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:221)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1269)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

2017-05-05 01:15:50,766 [INFO   ] dataflow: Changing Status for Operation 198972 from processing to failed: An error occurred while calling o104.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, ip-172-31-26-105.ec2.internal, executor 1): java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)
	at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
	at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2606)
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2622)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3099)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:853)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:349)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:284)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:221)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1269)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1493)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1492)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1492)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1720)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1675)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1664)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127)
	... 30 more
Caused by: java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)
	at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
	at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2606)
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2622)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3099)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:853)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:349)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:284)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:221)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1269)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more

2017-05-05 01:15:50,766 [INFO   ] schema: Committing Operation transaction
2017-05-05 01:15:50,774 [INFO   ] schema: Committing Operation transaction
2017-05-05 01:15:50,946 [INFO   ] schema: Committing Operation transaction
Traceback (most recent call last):
  File "/mnt/log-processor/src/enricher.py", line 168, in <module>
    cl_cli.run_main()
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 56, in run_main
    return CLI.run('__main__', argv, **main_kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 52, in run
    return CLI.COMMANDS[command](parse_args, **command_kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 34, in command_wrapper
    return func(args, extra_args=extra_args, **command_kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 44, in command_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 44, in command_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 44, in command_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 44, in command_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 44, in command_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 44, in command_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 44, in command_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 44, in command_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 44, in command_wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 44, in command_wrapper
    return func(*args, **kwargs)
  File "/mnt/log-processor/src/enricher.py", line 163, in main
    cl_cli.cli.run(args.action, extra_args, runner=runner)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 52, in run
    return CLI.COMMANDS[command](parse_args, **command_kwargs)
  File "/usr/local/lib/python2.7/site-packages/cl_cli/cli.py", line 34, in command_wrapper
    return func(args, extra_args=extra_args, **command_kwargs)
  File "/mnt/log-processor/src/enricher.py", line 141, in enrich
    runner.run()
  File "/mnt/log-processor/src/job.py", line 145, in run
    if self._run_single():
  File "/mnt/log-processor/src/job.py", line 126, in _run_single
    self._run_single_internal(processor, op, data_set)
  File "/mnt/log-processor/src/enricher.py", line 129, in _run_single_internal
    input_df, self._get_run_ts(processor, '%Y-%m-%d %H:%M:%S.%f')[:-3], output_location, output_location_tsv
  File "/mnt/log-processor/src/enricher.py", line 102, in _enrich
    self._write_avro(enriched, output_location)
  File "/mnt/log-processor/src/enricher.py", line 85, in _write_avro
    output_df.write.format("com.databricks.spark.avro").save(output_location)
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 550, in save
  File "/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
  File "/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o104.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:147)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:121)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:101)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:87)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:492)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, ip-172-31-26-105.ec2.internal, executor 1): java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)
	at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
	at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2606)
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2622)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3099)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:853)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:349)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:284)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:221)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1269)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1505)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1493)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1492)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1492)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:803)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:803)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1720)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1675)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1664)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:629)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:127)
	... 30 more
Caused by: java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)
	at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
	at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2606)
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2622)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:3099)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:853)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:349)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:284)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:221)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1269)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more
;;;, 22/May/17 10:23;labud;I met the same issue in spark 1.5.2, and it throw "Stream is corrupted" Exception when checking the MAGIC number (at the beginning of function net.jpountz.lz4.LZ4BlockInputStream.refill ).

Here is the log:
{quote}
java.io.IOException: Stream is corrupted
	at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:153)
	at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:117)
	at java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2310)
	at java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2323)
	at java.io.ObjectInputStream$BlockDataInputStream.readShort(ObjectInputStream.java:2794)
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:801)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:299)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:64)
	at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:64)
	at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:106)
	at org.apache.spark.shuffle.hash.HashShuffleReader$$anonfun$4.apply(HashShuffleReader.scala:64)
{quote}
Thanks for help
;;;, 24/May/17 15:12;cloud_fan;can you try to set `spark.file.transferTo` to false and try again? After a closer look, I can't find any problems in `LZ4BlockInputStream`, so I look back, and seems there can be a problem when we merge a lot of large shuffle files into one using `transferTo`;;;, 25/May/17 01:10;rupeshmane;For the stack provided earlier, I found the root cause: Issue is in Executor while getting compressed Broadcast variable. I'm specifying *spark.io.compression.codec* as *snappy*. So both driver and executors should be using this codec to compress and uncompress broadcast variable. But it seems like executor is defaulting to LZ4 instead of Snappy.

I'm not seeing this on my dev environment which is on Spark 2.1.1. While I am seeing this problem on AWS EMR 5.5.0 which has Spark 2.1.0. Not sure if this is related to AWS or Spark.

Thanks - Rupesh

;;;, 24/Oct/17 17:35;ashwinshankar77;Hi [~davies] [~cloud_fan]
We hit the same issue. What is the aforementioned workaround that went into 2.1? Any other workaround?;;;, 05/Mar/19 06:59;F7753;Still hit the same issue in Spark 2.3.1：

 
{code:java}
 

org.apache.spark.shuffle.FetchFailedException: Stream is corrupted at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523) at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:439) at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61) at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_1$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Stream is corrupted at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:202) at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157) at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:170) at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:348) at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:335) at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:335) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1380) at org.apache.spark.util.Utils$.copyStream(Utils.scala:356) at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:431) ... 21 more{code}
 
 ;;;, 03/Jun/19 06:17;mlebihan;I have also a problem involving a corrupted stream by LZ4, using Spark 2.4.3.

As extra info, it is sometimes replaced by an alternative issue :
{code:java}
2019-06-03 08:12:20.673  INFO 20023 --- [ver-heartbeater] o.a.spark.storage.BlockManagerMaster     : Registered BlockManager BlockManagerId(driver, 192.168.0.12, 34105, None)
2019-06-03 08:12:20.673  INFO 20023 --- [ver-heartbeater] org.apache.spark.storage.BlockManager    : Reporting 7 blocks to the master.
2019-06-03 08:12:20.676  INFO 20023 --- [er-event-loop-1] o.apache.spark.storage.BlockManagerInfo  : Added broadcast_3_piece0 in memory on 192.168.0.12:34105 (size: 21.0 KB, free: 8.2 GB)
2019-06-03 08:12:20.676  INFO 20023 --- [er-event-loop-0] o.apache.spark.storage.BlockManagerInfo  : Added broadcast_7_piece0 in memory on 192.168.0.12:34105 (size: 21.0 KB, free: 8.2 GB)
2019-06-03 08:12:20.677  INFO 20023 --- [er-event-loop-1] o.apache.spark.storage.BlockManagerInfo  : Added broadcast_10_piece1 in memory on 192.168.0.12:34105 (size: 1415.9 KB, free: 8.2 GB)
2019-06-03 08:12:20.678  INFO 20023 --- [er-event-loop-0] o.apache.spark.storage.BlockManagerInfo  : Added broadcast_10_piece0 in memory on 192.168.0.12:34105 (size: 4.0 MB, free: 8.2 GB)
2019-06-03 08:12:20.731 ERROR 20023 --- [ker for task 73] org.apache.spark.MapOutputTracker        : Missing an output location for shuffle 2
2019-06-03 08:12:20.732  WARN 20023 --- [result-getter-1] o.apache.spark.scheduler.TaskSetManager  : Lost task 29.0 in stage 3.0 (TID 73, localhost, executor driver): FetchFailed(null, shuffleId=2, mapId=-1, reduceId=29, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 2
    at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
    at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
    at org.apache.spark.MapOutputTrackerMaster$$anonfun$getMapSizesByExecutorId$2.apply(MapOutputTracker.scala:655)
    at org.apache.spark.MapOutputTrackerMaster$$anonfun$getMapSizesByExecutorId$2.apply(MapOutputTracker.scala:654)
    at org.apache.spark.ShuffleStatus.withMapStatuses(MapOutputTracker.scala:192)
    at org.apache.spark.MapOutputTrackerMaster.getMapSizesByExecutorId(MapOutputTracker.scala:654)
    at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
    at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
    at org.apache.spark.scheduler.Task.run(Task.scala:121)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748){code}
But sometimes, everything works well too.
But not this morning...;;;, 05/Jun/19 09:58;Piotr Chowaniec;I have a similar issue with Spark 2.3.2.

Here is a stack trace:
{code:java}
org.apache.spark.scheduler.DAGScheduler  : ShuffleMapStage 647 (count at Step.java:20) failed in 1.908 s due to org.apache.spark.shuffle.FetchFailedException: Stream is corrupted
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:528)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:444)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:62)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithKeys_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithKeys_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
        at org.apache.spark.scheduler.Task.run(Task.scala:109)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Stream is corrupted
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:252)
        at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)
        at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:170)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:349)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:336)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:336)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1381)
        at org.apache.spark.util.Utils$.copyStream(Utils.scala:357)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:436)
        ... 21 more
Caused by: net.jpountz.lz4.LZ4Exception: Error decoding offset 2010 of input buffer
        at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:39)
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:247)
        ... 29 more
{code}
It happens during ETL process that has about 200 steps. It looks like it depends on the input data because we have exceptions only on the production environment (on test and dev machines same process with different data is running without problems). Unfortunately there is no way to use production data on other environment, so we cannot find differences.

Changing compression codec to Snappy gives:
{code:java}
o.apache.spark.scheduler.TaskSetManager  : Lost task 0.0 in stage 852.3 (TID 308
36, localhost, executor driver): FetchFailed(BlockManagerId(driver, DNS.domena, 33588, None), shuffleId=298, mapId=2, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:528)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:444)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:62)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:109)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
        at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
        at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
        at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:474)
        at org.xerial.snappy.Snappy.uncompress(Snappy.java:513)
        at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:439)
        at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:167)
        at java.io.InputStream.read(InputStream.java:101)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:349)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:336)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:336)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1381)
        at org.apache.spark.util.Utils$.copyStream(Utils.scala:357)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:436)
        ... 20 more
{code}
Any ideas how to debug this issue? Any chance to have a workaround or solution?

 
 *UPDATE*
 Changing the number of partitions helps in our case, but still this is only a workaround, not a real solution. Default number of partitions is 200, we've changed to 20:
 {{spark.sql.shuffle.partitions=20}}

Docs: https://spark.apache.org/docs/latest/sql-performance-tuning.html;;;, 12/Jun/19 06:48;mlebihan;It _seems_ that exchanging from org.lz4:lz4-java:1.4.0 to org.lz4:lz4-java:1.6.0 helps.

 
{code:java}
        <!-- Dépendances Spark : en provided pour la compilation classique : il sera disponible ainsi pour les tests d'intégration -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>${spark.version}</version>
    
            <!-- Exclusion de ses loggers -->
            <exclusions>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
    
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                
                <exclusion>
                    <groupId>org.lz4</groupId>
                    <artifactId>lz4-java</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        
        <dependency>
            <groupId>org.lz4</groupId>
            <artifactId>lz4-java</artifactId>
            <version>1.6.0</version>
        </dependency>
        
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>log4j-over-slf4j</artifactId>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>${spark.version}</version>
        </dependency>
    </dependencies>{code}
If it's right, the cause _could be_ a bug they corrected on 1.4.1.

Didn't succeed.;;;, 18/Jun/19 19:03;mlebihan;My trick eventually didn't succeed. And I fall back into the bug again.

I've apttemted to upgrade from spark-xxx_2.11 to spark_xxx.2.12 for scala but received this kind of stacktrace :

{code:log}
2019-06-18 20:43:54.747  INFO 1539 --- [er for task 547] o.a.s.s.ShuffleBlockFetcherIterator      : Started 0 remote fetches in 0 ms
2019-06-18 20:43:59.015 ERROR 1539 --- [er for task 547] org.apache.spark.executor.Executor       : Exception in task 93.0 in stage 4.2 (TID 547)

java.lang.NullPointerException: null
    at org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:757) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at scala.collection.Iterator$$anon$10.next(Iterator.scala:459) ~[scala-library-2.12.8.jar!/:na]
    at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) ~[scala-library-2.12.8.jar!/:na]
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) ~[scala-library-2.12.8.jar!/:na]
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) ~[scala-library-2.12.8.jar!/:na]
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) ~[scala-library-2.12.8.jar!/:na]
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) ~[scala-library-2.12.8.jar!/:na]
    at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.scheduler.Task.run(Task.scala:121) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_212]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_212]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_212]

{code}

This issue 18105 prevent from using spark at all. 

Spark with 300 opened and in progress (but stalling) issues is become less and less reliable each day.
I'm about to send a message on dev forum to ask if developers can stop implementing new features until they have corrected the issues on the features they once written.;;;, 25/Nov/19 11:08;dyptan;You can recreate the error consistently by forcing disk spill on shuffle:
 # Run spark-shell with minimum memory
 `./bin/spark-shell --master yarn  --executor-memory 1g --num-executors 1`
 # Create two large tables, at least ~10M records
 # Join them together

{code:java}
// code placeholder
val people0 = spark.read.orc("/tmp/people/people0.orc").select("ID", "ln").sortWithinPartitions("ID")
val people1 = spark.read.orc("/tmp/people/people1.orc").select("ID", "fn").sortWithinPartitions("ID")
people0.join(people1, Seq("ID")).write.parquet("/tmp/people/joined")

{code}
Tested with Spark 2.4.0;;;, 25/Nov/19 11:26;moliinyk;I was able to reproduce with the code as shown below:
{code:java}
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions._
val dfx = spark.range(0, 10000000000L).withColumn("scope", (rand() * 100).cast(IntegerType)).withColumn("value", rand()).repartition(400)
dfx.groupBy("id").pivot("scope").agg(first(col("value"))).show(100, false){code}
on spark-2.3.2.

Here's our settings (used same settings for all versions):
{code:java}
/opt/mapr/spark/spark-2.3.2/bin/spark-shell --master yarn --num-executors 20 --executor-cores 5 --executor-memory=10G --driver-memory=15G
{code};;;, 11/Dec/19 19:09;mkempanna;If you are facing this in spark 2.4.0 , then as a temporary measure disable unsafe spill readaheads with setting
{code:java}
  --conf spark.unsafe.sorter.spill.read.ahead.enabled=false{code}
 ;;;, 09/Dec/20 14:44;cloud_fan;Is this still an issue in Spark 3.x? cc [~dongjoon];;;, 09/Dec/20 18:01;dongjoon;Apache Spark 3.x is using lz4-java-1.7.1.jar and this seems to be fixed by upgrading dependency, [~cloud_fan]. I'm not aware of any new incident about this.

cc [~viirya] since he is working on the codec issue in Hadoop community recently.
cc [~sunchao], too
;;;, 20/Mar/21 07:06;devaraj;We are still seeing the issue with Spark 3.0.1 as well,
{code:java}
org.apache.spark.shuffle.FetchFailedException: Stream is corrupted
 at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748)
 at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:823)
 at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
 at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
 at java.io.DataInputStream.readInt(DataInputStream.java:387)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.<init>(UnsafeRowSerializer.scala:120)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2.asKeyValueIterator(UnsafeRowSerializer.scala:110)
 at org.apache.spark.shuffle.BlockStoreShuffleReader.$anonfun$read$2(BlockStoreShuffleReader.scala:92)
 at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
 at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
 at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.sort_addToSorter_0$(Unknown Source)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
 at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
 at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.findNextInnerJoinRows$(Unknown Source)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
 at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
 at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:748)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:177)
 at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
 at org.apache.spark.scheduler.Task.run(Task.scala:127)
 at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
 Caused by: java.io.IOException: Stream is corrupted
 at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:200)
 at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)
 at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:819)
 ... 33 more{code}

  

 ;;;, 23/Mar/21 03:51;dongjoon;[~devaraj]. Do you have a reproducer? BTW, there is a know issue, SPARK-34790, with the latest Apache Spark with IO Encryption + AQE configuration.

 ;;;, 23/Mar/21 22:40;devaraj;[~dongjoon] We are seeing this error while running workloads with ~10TB shuffle data, we are not setting spark.io.encryption.enabled=true. ;;;, 24/Mar/21 04:21;dongjoon;Got it. Thank you for the details, [~devaraj]. Let's keep this open for a while because this issue needs more information.;;;, 27/Apr/21 07:25;jangryeo;We are seeing similar issues with Spark 3.0.1 as well. Not exactly reproducible but often seen
{code:java}
org.apache.spark.shuffle.FetchFailedException: Stream is corrupted
 at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748)
 at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:823)
 at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
 at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
 at java.io.DataInputStream.readInt(DataInputStream.java:387)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.<init>(UnsafeRowSerializer.scala:120)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2.asKeyValueIterator(UnsafeRowSerializer.scala:110)
 at org.apache.spark.shuffle.BlockStoreShuffleReader.$anonfun$read$2(BlockStoreShuffleReader.scala:92)
 at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
 at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
 at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage72.sort_addToSorter_0$(Unknown Source)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage72.processNext(Unknown Source)
 at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
 at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
 at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
 at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:860)
 at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:730)
 at org.apache.spark.sql.execution.joins.SortMergeJoinExec.$anonfun$doExecute$1(SortMergeJoinExec.scala:254)
 at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
 at org.apache.spark.scheduler.Task.run(Task.scala:127)
 at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
 Caused by: java.io.IOException: Stream is corrupted
 at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:250)
 at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)
 at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:819)
 ... 35 more
 Caused by: net.jpountz.lz4.LZ4Exception: Error decoding offset 10122 of input buffer
 at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:39)
 at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:245)
 ... 37 more
{code};;;, 28/Jul/21 13:53;arghya18;I am also facing the same error, I have raised a duplicate [JIRA|https://issues.apache.org/jira/browse/SPARK-36196], did not notice this before. Do we have any further info or workaround on it? I am running Spark 3.1.1;;;, 28/Jul/21 14:00;arghya18;[~dongjoon] Can we please address this before next release, this is one of the blocker for many.

For example, we have actually migrated our entire ETL running thousands of job everyday on 2PB data warehouse  from EMR to open source Spark on K8S and is running stable except this issue. I can provide any further info required for debugging. The only issue of the error is its not reproducible. So out of 1000 of spark job only few fails and those few are different in different days. ;;;, 29/Jul/21 00:24;viirya;Looked at lz4 codebase and the reported failures. I suspect this is not related to lz4 codec. For example, [https://github.com/Mellanox/SparkRDMA/issues/34,] even disabling shuffle compression, there is still "{{java.io.EOFException: reached end of stream"}} error.

Previous comment suggested to disable "spark.unsafe.sorter.spill.read.ahead.enabled". If it is a temporary workaround, it sounds more like an issue on unsafe spill?;;;, 29/Jul/21 19:23;dongjoon;I agree with [~viirya] that this might be not a LZ4 codec issue.

To [~arghya18], more info would be definitely helpful of course.
> I can provide any further info required for debugging. 

Yep. That's the reason why this issue is still open. If we have a deterministic reproducible example, it will be fixed already.
> The only issue of the error is its not reproducible. So out of 1000 of spark job only few fails and those few are different in different days. 

FYI, LZ4-java become 1.7.1 at Apache Spark 3.0.0. So, your report on 3.1.1 is invaluable and [~devaraj]'s 3.0.1 report is too. For Apache Spark 3.3, we will use LZ4-java 1.8. (SPARK-36256);;;, 03/Aug/21 09:42;cameron.todd;I'm also facing this same error when scaling up my project to a larger dataset. It's consistently crashing in the same spot (I can reproduce it) and I'm pretty sure it's coming down to using a repartition() after import as below:
{code:java}
Dataset<Row> relevantPivots = spark.read().parquet(jobConf.pathToDataDedup)
        .select(VadisSparkUtils.scalaSeqConverterColumn(allColsNeeded))
        .na().drop()
        .withColumn("pivot_hash", hash(VadisSparkUtils.scalaSeqConverterColumn(pivot)))
        .drop(VadisSparkUtils.scalaSeqConverterString(pivot))
        .repartition(5000)
        .cache();
{code}
I have already tried to disable  "spark.unsafe.sorter.spill.read.ahead.enabled" but did not work. My cluster environment is spark 3.0.1 standalone with Kubernetes and using lz4-java-1.7.1.jar

Not entirely sure how to debug this further and give you guys anymore info because I can't get the logs on each slave node

 ;;;, 03/Aug/21 20:51;dongjoon;It's a good news because you make it consistently. Could you provide a way for us to follow your path? Your example has proprietary parts.;;;, 04/Aug/21 10:03;cameron.todd;I'll attach a portion of the code that is not proprietary but it's mostly all there. Anyway the code is one small step in our entity resolution process that processes 1billion person records so lots of pairwise comparisons and skewed joins before aggregate and output staging results.[^TestWeightedGraph.java];;;, 04/Aug/21 10:08;cameron.todd;Let me know if that's enough info. From my tests if I remove the  .repartition after import of file, I get no corrupted stream error but I get skewed joins and memory problems later. Then if I add the repartition back, the job does not complete because multiple tasks crash and are retried but still crash with the stream corrupted error before completely crashing the job;;;, 07/Aug/21 17:39;dongjoon;[~cameron.todd]. Thank you. The code itself looks nice. The only problem to me is that we cannot run it due to `pathToDataDedup`. I don't think you can share 1 billion person records here. Is there a way for me to generate some random data for your app?
{code}
Dataset<Row> relevantPivots = spark.read().parquet(pathToDataDedup)
{code};;;, 09/Aug/21 10:23;cameron.todd;Yep I understand. I have hashed my data keeping the same distribution and the full_name hashed column is weak but string distance functions still work on it. Do you have any recommendations where I can upload this data, it's only 2gb?
{code:java}
//So this line of code:
Dataset<Row> relevantPivots = spark.read().parquet(pathToDataDedup)
				.select("id", "full_name", "last_name","birthdate")
				.na().drop()
				.withColumn("pivot_hash", hash(col("last_name"),col("birthdate")))
				.drop("last_name","birthdate")
				.repartition(5000)
				.cache();
// can be replaced with this
Dataset<Row> relevantPivots = spark.read().parquet(pathToDataDedup).repartition(5000).cache();{code}
I have also run the same code on the same hashed data and getting the same corrupted stream error. Also in case it wasn't clear my data normally sits on an s3 bucket.;;;, 10/Aug/21 18:12;dongjoon;Ya, this is a nice simplification really. Thanks.
{code}
Dataset<Row> relevantPivots = spark.read().parquet(pathToDataDedup).repartition(5000).cache();
{code}

For the data, is it Parquet with snappy compression? If you use Gzip compression, it will be reduced a little more. For the location, could you share it as a pre-signed S3 URL or upload/share it via Box.com?;;;, 11/Aug/21 12:11;cameron.todd;Ok I added the zip file on this public S3 bucket, it holds a parquet file with snappy compression, it's 1.75gb. You can retrieve the data as such
{code:java}
wget https://storage.gra.cloud.ovh.net/v1/AUTH_147b880980f148f5ad1af09542e6f37a/public_data/SPARK18105/hashed_data.zip
{code};;;, 11/Aug/21 19:37;dongjoon;Thank you, [~cameron.todd]. I downloaded and ran the test code in my laptop. It works well to me. Do you have some other configurations with you?
{code}
SPARK-18105:$ du -h hash_import.parquet
2.4G	hash_import.parquet
{code}

{code}
scala> val df = spark.read.parquet("/Users/dongjoon/data/SPARK-18105/hash_import.parquet").repartition(5000).cache()
df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, pivot_hash: int ... 1 more field]

scala> df.count()
res3: Long = 136935074

scala> spark.version
res4: String = 3.1.2
{code};;;, 11/Aug/21 19:40;dongjoon;BTW, I checked that a single zip file contains 157 snappy parquet files which is generated by Apache Spark 3.0.1.

{code}
$ ls -al /Users/dongjoon/data/SPARK-18105/hash_import.parquet | grep parquet | wc -l
     157

$ ls -al /Users/dongjoon/data/SPARK-18105/hash_import.parquet
total 5040768
drwxr-xr-x  160 dongjoon  staff      5120 Aug  9 02:55 .
drwxr-xr-x    4 dongjoon  staff       128 Aug 11 12:32 ..
-rw-r--r--    1 dongjoon  staff         0 Aug  9 02:21 _SUCCESS
-rw-r--r--    1 dongjoon  staff   7098577 Aug  9 02:19 part-00000-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9310705 Aug  9 02:19 part-00001-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9286598 Aug  9 02:19 part-00002-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9066660 Aug  9 02:19 part-00003-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7174124 Aug  9 02:19 part-00004-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7269784 Aug  9 02:19 part-00005-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9045563 Aug  9 02:19 part-00006-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8874048 Aug  9 02:19 part-00007-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9088055 Aug  9 02:19 part-00008-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7921907 Aug  9 02:19 part-00009-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9734530 Aug  9 02:19 part-00010-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11430364 Aug  9 02:19 part-00011-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10106472 Aug  9 02:19 part-00012-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11672118 Aug  9 02:19 part-00013-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11077873 Aug  9 02:19 part-00014-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  17459766 Aug  9 02:19 part-00015-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18000141 Aug  9 02:19 part-00016-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10688008 Aug  9 02:19 part-00017-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10876962 Aug  9 02:19 part-00018-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   6001747 Aug  9 02:19 part-00019-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13051685 Aug  9 02:19 part-00020-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  40607547 Aug  9 02:19 part-00021-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  16122066 Aug  9 02:19 part-00022-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7858387 Aug  9 02:19 part-00023-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7953870 Aug  9 02:19 part-00024-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20777892 Aug  9 02:19 part-00025-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7785559 Aug  9 02:19 part-00026-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   6101494 Aug  9 02:19 part-00027-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14878775 Aug  9 02:19 part-00028-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8807412 Aug  9 02:19 part-00029-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff    664925 Aug  9 02:19 part-00030-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  36711578 Aug  9 02:19 part-00031-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   6376692 Aug  9 02:19 part-00032-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9121175 Aug  9 02:19 part-00033-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  12318653 Aug  9 02:19 part-00034-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18944542 Aug  9 02:20 part-00035-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   5108518 Aug  9 02:19 part-00036-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10884610 Aug  9 02:19 part-00037-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7678782 Aug  9 02:19 part-00038-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9950415 Aug  9 02:19 part-00039-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22648978 Aug  9 02:19 part-00040-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9918156 Aug  9 02:19 part-00041-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20537613 Aug  9 02:20 part-00042-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20121006 Aug  9 02:20 part-00043-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10919311 Aug  9 02:19 part-00044-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13743611 Aug  9 02:20 part-00045-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20744467 Aug  9 02:20 part-00046-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8362612 Aug  9 02:20 part-00047-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  17083828 Aug  9 02:20 part-00048-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8325658 Aug  9 02:20 part-00049-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  28616403 Aug  9 02:20 part-00050-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  27766601 Aug  9 02:20 part-00051-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7274627 Aug  9 02:20 part-00052-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18133083 Aug  9 02:20 part-00053-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20962549 Aug  9 02:20 part-00054-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10766021 Aug  9 02:20 part-00055-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10987925 Aug  9 02:20 part-00056-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  24120006 Aug  9 02:20 part-00057-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7940774 Aug  9 02:20 part-00058-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15471824 Aug  9 02:20 part-00059-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  17041126 Aug  9 02:20 part-00060-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   4558582 Aug  9 02:20 part-00061-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  28812083 Aug  9 02:20 part-00062-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18382491 Aug  9 02:20 part-00063-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11464446 Aug  9 02:20 part-00064-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13522906 Aug  9 02:20 part-00065-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  12881794 Aug  9 02:20 part-00066-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14688841 Aug  9 02:20 part-00067-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13630922 Aug  9 02:20 part-00068-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15015507 Aug  9 02:20 part-00069-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21820733 Aug  9 02:20 part-00070-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9051687 Aug  9 02:20 part-00071-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14009387 Aug  9 02:20 part-00072-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8043754 Aug  9 02:20 part-00073-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15522853 Aug  9 02:20 part-00074-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22376010 Aug  9 02:20 part-00075-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22600841 Aug  9 02:20 part-00076-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  12973790 Aug  9 02:20 part-00077-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13724406 Aug  9 02:20 part-00078-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9837992 Aug  9 02:20 part-00079-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  24946732 Aug  9 02:20 part-00081-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21250848 Aug  9 02:20 part-00082-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22465612 Aug  9 02:20 part-00083-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15383161 Aug  9 02:20 part-00084-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10725376 Aug  9 02:20 part-00085-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  19325378 Aug  9 02:20 part-00086-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8895845 Aug  9 02:20 part-00087-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  19790382 Aug  9 02:20 part-00088-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8773449 Aug  9 02:20 part-00089-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  19144012 Aug  9 02:20 part-00090-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  23290537 Aug  9 02:20 part-00091-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  39088655 Aug  9 02:20 part-00092-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8602985 Aug  9 02:20 part-00093-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15345549 Aug  9 02:20 part-00094-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13559709 Aug  9 02:20 part-00095-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   6852728 Aug  9 02:20 part-00096-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  29831129 Aug  9 02:20 part-00097-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11356100 Aug  9 02:20 part-00098-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21956322 Aug  9 02:20 part-00099-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  16915694 Aug  9 02:20 part-00100-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21229399 Aug  9 02:20 part-00101-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   3039283 Aug  9 02:20 part-00102-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  31255403 Aug  9 02:20 part-00103-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13639185 Aug  9 02:20 part-00104-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22379210 Aug  9 02:20 part-00105-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20522598 Aug  9 02:20 part-00106-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21562145 Aug  9 02:20 part-00107-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21461668 Aug  9 02:20 part-00108-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  27882056 Aug  9 02:20 part-00109-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14075767 Aug  9 02:20 part-00110-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  17072086 Aug  9 02:20 part-00111-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   2745029 Aug  9 02:20 part-00112-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18697825 Aug  9 02:20 part-00113-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  42039504 Aug  9 02:20 part-00114-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7403373 Aug  9 02:20 part-00115-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10516182 Aug  9 02:20 part-00116-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  16325101 Aug  9 02:20 part-00117-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14799829 Aug  9 02:20 part-00118-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14694122 Aug  9 02:20 part-00119-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  24990548 Aug  9 02:20 part-00120-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11079775 Aug  9 02:20 part-00121-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  33703475 Aug  9 02:20 part-00122-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14656154 Aug  9 02:20 part-00123-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   2180907 Aug  9 02:20 part-00124-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22524866 Aug  9 02:20 part-00125-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  23664835 Aug  9 02:20 part-00126-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   3718511 Aug  9 02:20 part-00127-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15386780 Aug  9 02:20 part-00128-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15793443 Aug  9 02:21 part-00129-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  25739876 Aug  9 02:21 part-00130-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22014066 Aug  9 02:21 part-00131-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  26034951 Aug  9 02:21 part-00132-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18732517 Aug  9 02:21 part-00133-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15581656 Aug  9 02:21 part-00134-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  24107039 Aug  9 02:21 part-00135-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff    617756 Aug  9 02:21 part-00136-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  17878862 Aug  9 02:21 part-00137-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  37368882 Aug  9 02:21 part-00138-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14727470 Aug  9 02:21 part-00139-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  36085625 Aug  9 02:21 part-00140-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21564288 Aug  9 02:21 part-00141-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   4207926 Aug  9 02:21 part-00142-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  26885735 Aug  9 02:21 part-00143-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff     47222 Aug  9 02:21 part-00145-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff      3031 Aug  9 02:21 part-00146-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff    101888 Aug  9 02:21 part-00147-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18902521 Aug  9 02:21 part-00148-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff      3428 Aug  9 02:21 part-00149-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  23022202 Aug  9 02:21 part-00150-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff      2675 Aug  9 02:21 part-00155-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21980843 Aug  9 02:21 part-00157-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  37397239 Aug  9 02:21 part-00158-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  53319037 Aug  9 02:21 part-00159-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  53852766 Aug  9 02:21 part-00160-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  52634756 Aug  9 02:21 part-00161-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  58968977 Aug  9 02:21 part-00162-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14955676 Aug  9 02:21 part-00163-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
{code}

{code}
$ parquet-tools meta /Users/dongjoon/data/SPARK-18105/hash_import.parquet/part-00163-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
file:        file:/Users/dongjoon/data/SPARK-18105/hash_import.parquet/part-00163-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
creator:     parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)
extra:       org.apache.spark.version = 3.0.1
extra:       org.apache.spark.sql.parquet.row.metadata = {"type":"struct","fields":[{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"pivot_hash","type":"integer","nullable":false,"metadata":{}},{"name":"full_name","type":"string","nullable":true,"metadata":{}}]}
{code};;;, 11/Aug/21 20:03;cameron.todd;Good to hear. Also the count of 136,935,074 is right. 

When you say "I downloaded and ran the test code in my laptop. It works well to me". You mean the java code I attached or do you mean the read file and count() that you just attached above. Because the error is only raised after the first few aggregations and joins in my code and I also doubt your laptop is big enough to process that ;) .

I can attach key config variables on your request but I'm using a cloud Kubernetes spark cluster managed by OVH (spark 3.0.1) so it's out of box solution managed by OVH not really me.;;;, 12/Aug/21 02:16;dongjoon;I did only the above code I posted because you wrote like this. What did you mean by `can be replaced with this` then?
{code:java}
//So this line of code:
...
// can be replaced with this
Dataset<Row> relevantPivots = spark.read().parquet(pathToDataDedup).repartition(5000).cache(); {code};;;, 12/Aug/21 07:33;cameron.todd;Oh sorry, I meant just a portion of the code can be replaced or only the import of the parquet part because I hashed the data file so the 'pivot_hash' column already exists. The real test begins with the following joins and aggregations per the .java file I uploaded.;;;, 16/Aug/21 03:24;dragonlong;[~cameron.todd] Hi, is any news? I issue this problem both in spark 2.3 and spark 3.0. Thanks.

 

Here is the spark3.0 trace:
{code:java}
// code placeholder
FetchFailed(BlockManagerId(19, xxxxx, 26065, None), shuffleId=0, mapId=841, reduceId=1024, message=FetchFailed(BlockManagerId(19, xxxxx, 26065, None), shuffleId=0, mapId=841, reduceId=1024, message=org.apache.spark.shuffle.FetchFailedException: Stream is corrupted at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:750) at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:823) at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) at java.io.BufferedInputStream.read(BufferedInputStream.java:265) at java.io.DataInputStream.readInt(DataInputStream.java:387) at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113) at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:129) at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110) at scala.collection.Iterator$$anon$11.next(Iterator.scala:494) at scala.collection.Iterator$$anon$10.next(Iterator.scala:459) at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40) at scala.collection.Iterator$$anon$10.next(Iterator.scala:459) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729) at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225) at org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349) at org.apache.spark.rdd.RDD.iterator(RDD.scala:313) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:127) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$runWithUgi$3(Executor.scala:462) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377) at org.apache.spark.executor.Executor$TaskRunner.runWithUgi(Executor.scala:465) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:394) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)

Caused by: java.io.IOException: Stream is corrupted at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:250) at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:226) at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157) at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:819) ... 30 more

Caused by: net.jpountz.lz4.LZ4Exception: Error decoding offset 94821 of input buffer at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:39) at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:245) ... 33 more
{code}
 ;;;, 30/Aug/21 08:30;Ngone51;FYI, for users who hit the "Stream is corrupted" error in 3.0.0, please try to apply the fix of SPARK-32658 first as it's a known bug that can cause the error.;;;, 29/Sep/21 06:51;vladimir.prus;FYI, we recently started to get a lot of such errors; they appear to be correlated with increased load and increased spot termination in AWS. So as an experiment, I've disabled executor decommission, and the errors all disappeared. Specifically, I set these options:
{noformat}
storage.decommission.enabled: false
storage.decommission.rddBlocks.enabled: false
storage.decommission.shuffleBlocks.enabled: false{noformat}
It is of course not a perfect set of options for production, but maybe will be a hint at the problem. I am using a recent build from branch-3.1, specifically from commit e1fc62de8e05.;;;, 29/Sep/21 07:05;Ngone51;[~vladimir.prus] Hi, could you also file a sub-task under https://issues.apache.org/jira/browse/SPARK-20624? That sounds like an issue in decommission.;;;, 16/Nov/21 09:18;zhangweilst;In our case, it is strongly related with `spark.file.transferTo`. Set `spark.file.transferTo` to false has saved our life.

We have been tortured by this problem for almost about a year, and we got about hundreds of tasks  with this `FetchFailedException: Decompression error: Corrupted block detected` error everyday. Recently the heat is burning up and got us kind of on fire since we are using Spark in more scenarios.

One of the stack I got is: 
{code:java}
org.apache.spark.shuffle.FetchFailedException: Decompression error: Corrupted block detected	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:569)	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:474)	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:66)	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)	at org.apache.spark.scheduler.Task.run(Task.scala:123)	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748) 

Caused by: java.io.IOException: Decompression error: Corrupted block detected	at com.github.luben.zstd.ZstdInputStream.read(ZstdInputStream.java:111)	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)	at org.apache.spark.util.Utils$$anonfun$1.apply$mcZ$sp(Utils.scala:402)	at org.apache.spark.util.Utils$$anonfun$1.apply(Utils.scala:397)	at org.apache.spark.util.Utils$$anonfun$1.apply(Utils.scala:397)	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)	at org.apache.spark.util.Utils$.copyStreamUpTo(Utils.scala:409)	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:467)	... 32 more{code}
We have Spark version 2.2.2, 2.4.7, 3.0.1 and 3.1.2，and all have this issue. I tried methods in this post and others, nothing can make the non-repeatable error disappear. After weeks' struggle and going through the code with one of our almost repeatable case, we are able to find that `spark.file.transferTo` is the one. We turned off this flag in our cluster, and this error literally disappears – None of this error any more.

 

Normally we won't suspect flags like `spark.file.transferTo`, in that the only difference is you are using `FileStream` or `FileChannel#transferTo` in shuffle spill merge phase. And if that's the problem, it probably relates to JVM or kernel bug. `transferTo` is more efficient than `FileStream` in that it copies file segment with zero-copy, but empirically from our experience, this error definitely IS caused by this flag.

 

So for those who are stilled bothered by this problem, `spark.file.transferTo` is one thing to try. And if [~dongjoon] or somebody else is interested in this, I can offer some more info if I can. Currently, we are using linux kernel 4.19.95 and JDK 1.8.0_171-b11.

 ;;;, 17/Nov/21 22:27;sidk;Hi, I saw a similar failure just as [~vladimir.prus]. In my experiment, I enabled node decommissioning along with a decommission fallback storage and then terminated an executor while the shuffle blocks are being fetched. The node decommissioning begins for the lost executor and migrates all the shuffle blocks to peer executors. Post migration, when the shuffle blocks are being fetched, I see the "FetchFailedException: Stream is corrupted" and "Error decoding offset 19258 of input buffer" message as seen in this thread. The error goes away when I do not add the fallback storage option.

These were the options I set in my experiment
{code:java}
spark.decommission.enabled: true,
spark.storage.decommission.shuffleBlocks.enabled : true,
spark.storage.decommission.enabled: true,
spark.storage.decommission.fallbackStorage.path : s3://<bucket>/    # Stopped seeing errors after removing this{code};;;, 03/Dec/21 03:57;yumwang;Workaround this issue by set spark.io.compression.codec=zstd.;;;, 24/Mar/22 11:46;sleep1661;It's working in my case by setting spark.file.transferTo=false. Thanks to [~zhangweilst] . And my spark version was 3.1.2.;;;, 14/Feb/24 07:43;jwozniak;With Spark 3.5.0 running on Yarn Hadoop we had this: 
{code}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 2551.0 failed 20 times, most recent failure: Lost task 13.19 in stage 2551.0 (TID 205408) (ithdp-nxcals5001.cern.ch executor 31): org.apache.spark.SparkException: [INTERNAL_ERROR_STORAGE] Unexpected type of BlockId, shuffle_713_203394_130_140
        at org.apache.spark.SparkException$.internalError(SparkException.scala:92)
        at org.apache.spark.SparkException$.internalError(SparkException.scala:100)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.diagnoseCorruption(ShuffleBlockFetcherIterator.scala:1147)
        at org.apache.spark.storage.BufferReleasingInputStream.$anonfun$tryOrFetchFailedException$1(ShuffleBlockFetcherIterator.scala:1385)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.storage.BufferReleasingInputStream.tryOrFetchFailedException(ShuffleBlockFetcherIterator.scala:1384)
        at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:1370)
        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113)
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:129)
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:496)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
        at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
        at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)
        at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
{code}

After setting 
{code}
spark.shuffle.detectCorrupt: false
spark.shuffle.checksum.enabled: false
{code}

we started to get this one (similar to other exception from this issue)
{code}
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.15.jar:?]
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.15.jar:?]
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.0.jar:3.5.0]
Caused by: java.io.IOException: Stream is corrupted
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:252) ~[lz4-java-1.8.0.jar:?]
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:228) ~[lz4-java-1.8.0.jar:?]
        at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:159) ~[lz4-java-1.8.0.jar:?]
        at org.apache.spark.storage.BufferReleasingInputStream.$anonfun$read$3(ShuffleBlockFetcherIterator.scala:1370) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.storage.BufferReleasingInputStream.tryOrFetchFailedException(ShuffleBlockFetcherIterator.scala:1381) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:1370) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252) ~[?:?]
        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271) ~[?:?]
        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392) ~[?:?]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:129) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:496) ~[scala-library-2.12.15.jar:?]
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.base/java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: net.jpountz.lz4.LZ4Exception: Error decoding offset 14570 of input buffer
        at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:41) ~[lz4-java-1.8.0.jar:?]
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:247) ~[lz4-java-1.8.0.jar:?]
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:228) ~[lz4-java-1.8.0.jar:?]
        at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:159) ~[lz4-java-1.8.0.jar:?]
        at org.apache.spark.storage.BufferReleasingInputStream.$anonfun$read$3(ShuffleBlockFetcherIterator.scala:1370) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.storage.BufferReleasingInputStream.tryOrFetchFailedException(ShuffleBlockFetcherIterator.scala:1381) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:1370) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252) ~[?:?]
        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271) ~[?:?]
        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392) ~[?:?]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:129) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:496) ~[scala-library-2.12.15.jar:?]
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.base/java.lang.Thread.run(Thread.java:829) ~[?:?]
{code}


;;;, 05/Jul/24 16:35;chengpan;there is an upstream XFS kernel bug identified by the Bilibili team that affects the spark shuffle use case, see details at [https://lore.kernel.org/linux-xfs/20220817093627.GZ3600936@dread.disaster.area/], disabling `spark.file.transferTo` could also workaround this issue.;;;
Affects Version/s.1: 3.1.1
Comment.1: 27/Oct/16 18:18;davies;It turned out that the bug in LZ4 is a false alarm, so close the upstream issue.

Can't reproduce the behavior now.;;;, 24/May/17 15:12;cloud_fan;can you try to set `spark.file.transferTo` to false and try again? After a closer look, I can't find any problems in `LZ4BlockInputStream`, so I look back, and seems there can be a problem when we merge a lot of large shuffle files into one using `transferTo`;;;, 25/May/17 01:10;rupeshmane;For the stack provided earlier, I found the root cause: Issue is in Executor while getting compressed Broadcast variable. I'm specifying *spark.io.compression.codec* as *snappy*. So both driver and executors should be using this codec to compress and uncompress broadcast variable. But it seems like executor is defaulting to LZ4 instead of Snappy.

I'm not seeing this on my dev environment which is on Spark 2.1.1. While I am seeing this problem on AWS EMR 5.5.0 which has Spark 2.1.0. Not sure if this is related to AWS or Spark.

Thanks - Rupesh

;;;, 24/Oct/17 17:35;ashwinshankar77;Hi [~davies] [~cloud_fan]
We hit the same issue. What is the aforementioned workaround that went into 2.1? Any other workaround?;;;, 05/Mar/19 06:59;F7753;Still hit the same issue in Spark 2.3.1：

 
{code:java}
 

org.apache.spark.shuffle.FetchFailedException: Stream is corrupted at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:523) at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:439) at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:61) at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_1$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeys_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: java.io.IOException: Stream is corrupted at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:202) at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157) at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:170) at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:348) at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:335) at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:335) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1380) at org.apache.spark.util.Utils$.copyStream(Utils.scala:356) at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:431) ... 21 more{code}
 
 ;;;, 03/Jun/19 06:17;mlebihan;I have also a problem involving a corrupted stream by LZ4, using Spark 2.4.3.

As extra info, it is sometimes replaced by an alternative issue :
{code:java}
2019-06-03 08:12:20.673  INFO 20023 --- [ver-heartbeater] o.a.spark.storage.BlockManagerMaster     : Registered BlockManager BlockManagerId(driver, 192.168.0.12, 34105, None)
2019-06-03 08:12:20.673  INFO 20023 --- [ver-heartbeater] org.apache.spark.storage.BlockManager    : Reporting 7 blocks to the master.
2019-06-03 08:12:20.676  INFO 20023 --- [er-event-loop-1] o.apache.spark.storage.BlockManagerInfo  : Added broadcast_3_piece0 in memory on 192.168.0.12:34105 (size: 21.0 KB, free: 8.2 GB)
2019-06-03 08:12:20.676  INFO 20023 --- [er-event-loop-0] o.apache.spark.storage.BlockManagerInfo  : Added broadcast_7_piece0 in memory on 192.168.0.12:34105 (size: 21.0 KB, free: 8.2 GB)
2019-06-03 08:12:20.677  INFO 20023 --- [er-event-loop-1] o.apache.spark.storage.BlockManagerInfo  : Added broadcast_10_piece1 in memory on 192.168.0.12:34105 (size: 1415.9 KB, free: 8.2 GB)
2019-06-03 08:12:20.678  INFO 20023 --- [er-event-loop-0] o.apache.spark.storage.BlockManagerInfo  : Added broadcast_10_piece0 in memory on 192.168.0.12:34105 (size: 4.0 MB, free: 8.2 GB)
2019-06-03 08:12:20.731 ERROR 20023 --- [ker for task 73] org.apache.spark.MapOutputTracker        : Missing an output location for shuffle 2
2019-06-03 08:12:20.732  WARN 20023 --- [result-getter-1] o.apache.spark.scheduler.TaskSetManager  : Lost task 29.0 in stage 3.0 (TID 73, localhost, executor driver): FetchFailed(null, shuffleId=2, mapId=-1, reduceId=29, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 2
    at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)
    at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)
    at scala.collection.Iterator$class.foreach(Iterator.scala:891)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
    at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)
    at org.apache.spark.MapOutputTrackerMaster$$anonfun$getMapSizesByExecutorId$2.apply(MapOutputTracker.scala:655)
    at org.apache.spark.MapOutputTrackerMaster$$anonfun$getMapSizesByExecutorId$2.apply(MapOutputTracker.scala:654)
    at org.apache.spark.ShuffleStatus.withMapStatuses(MapOutputTracker.scala:192)
    at org.apache.spark.MapOutputTrackerMaster.getMapSizesByExecutorId(MapOutputTracker.scala:654)
    at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)
    at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
    at org.apache.spark.scheduler.Task.run(Task.scala:121)
    at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748){code}
But sometimes, everything works well too.
But not this morning...;;;, 05/Jun/19 09:58;Piotr Chowaniec;I have a similar issue with Spark 2.3.2.

Here is a stack trace:
{code:java}
org.apache.spark.scheduler.DAGScheduler  : ShuffleMapStage 647 (count at Step.java:20) failed in 1.908 s due to org.apache.spark.shuffle.FetchFailedException: Stream is corrupted
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:528)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:444)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:62)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithKeys_1$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.agg_doAggregateWithKeys_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
        at org.apache.spark.scheduler.Task.run(Task.scala:109)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Stream is corrupted
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:252)
        at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)
        at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:170)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:349)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:336)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:336)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1381)
        at org.apache.spark.util.Utils$.copyStream(Utils.scala:357)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:436)
        ... 21 more
Caused by: net.jpountz.lz4.LZ4Exception: Error decoding offset 2010 of input buffer
        at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:39)
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:247)
        ... 29 more
{code}
It happens during ETL process that has about 200 steps. It looks like it depends on the input data because we have exceptions only on the production environment (on test and dev machines same process with different data is running without problems). Unfortunately there is no way to use production data on other environment, so we cannot find differences.

Changing compression codec to Snappy gives:
{code:java}
o.apache.spark.scheduler.TaskSetManager  : Lost task 0.0 in stage 852.3 (TID 308
36, localhost, executor driver): FetchFailed(BlockManagerId(driver, DNS.domena, 33588, None), shuffleId=298, mapId=2, reduceId=3, message=
org.apache.spark.shuffle.FetchFailedException: FAILED_TO_UNCOMPRESS(5)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:528)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:444)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:62)
        at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
        at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
        at org.apache.spark.scheduler.Task.run(Task.scala:109)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
        at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
        at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
        at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:474)
        at org.xerial.snappy.Snappy.uncompress(Snappy.java:513)
        at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:439)
        at org.xerial.snappy.SnappyInputStream.read(SnappyInputStream.java:167)
        at java.io.InputStream.read(InputStream.java:101)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply$mcJ$sp(Utils.scala:349)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:336)
        at org.apache.spark.util.Utils$$anonfun$copyStream$1.apply(Utils.scala:336)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1381)
        at org.apache.spark.util.Utils$.copyStream(Utils.scala:357)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:436)
        ... 20 more
{code}
Any ideas how to debug this issue? Any chance to have a workaround or solution?

 
 *UPDATE*
 Changing the number of partitions helps in our case, but still this is only a workaround, not a real solution. Default number of partitions is 200, we've changed to 20:
 {{spark.sql.shuffle.partitions=20}}

Docs: https://spark.apache.org/docs/latest/sql-performance-tuning.html;;;, 12/Jun/19 06:48;mlebihan;It _seems_ that exchanging from org.lz4:lz4-java:1.4.0 to org.lz4:lz4-java:1.6.0 helps.

 
{code:java}
        <!-- Dépendances Spark : en provided pour la compilation classique : il sera disponible ainsi pour les tests d'intégration -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>${spark.version}</version>
    
            <!-- Exclusion de ses loggers -->
            <exclusions>
                <exclusion>
                    <groupId>org.slf4j</groupId>
                    <artifactId>slf4j-log4j12</artifactId>
                </exclusion>
    
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
                
                <exclusion>
                    <groupId>org.lz4</groupId>
                    <artifactId>lz4-java</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        
        <dependency>
            <groupId>org.lz4</groupId>
            <artifactId>lz4-java</artifactId>
            <version>1.6.0</version>
        </dependency>
        
        <dependency>
            <groupId>org.slf4j</groupId>
            <artifactId>log4j-over-slf4j</artifactId>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>${spark.version}</version>
        </dependency>
    </dependencies>{code}
If it's right, the cause _could be_ a bug they corrected on 1.4.1.

Didn't succeed.;;;, 18/Jun/19 19:03;mlebihan;My trick eventually didn't succeed. And I fall back into the bug again.

I've apttemted to upgrade from spark-xxx_2.11 to spark_xxx.2.12 for scala but received this kind of stacktrace :

{code:log}
2019-06-18 20:43:54.747  INFO 1539 --- [er for task 547] o.a.s.s.ShuffleBlockFetcherIterator      : Started 0 remote fetches in 0 ms
2019-06-18 20:43:59.015 ERROR 1539 --- [er for task 547] org.apache.spark.executor.Executor       : Exception in task 93.0 in stage 4.2 (TID 547)

java.lang.NullPointerException: null
    at org.apache.spark.rdd.PairRDDFunctions.$anonfun$mapValues$3(PairRDDFunctions.scala:757) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at scala.collection.Iterator$$anon$10.next(Iterator.scala:459) ~[scala-library-2.12.8.jar!/:na]
    at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484) ~[scala-library-2.12.8.jar!/:na]
    at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490) ~[scala-library-2.12.8.jar!/:na]
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) ~[scala-library-2.12.8.jar!/:na]
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) ~[scala-library-2.12.8.jar!/:na]
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458) ~[scala-library-2.12.8.jar!/:na]
    at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.scheduler.Task.run(Task.scala:121) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) ~[spark-core_2.12-2.4.3.jar!/:2.4.3]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_212]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_212]
    at java.lang.Thread.run(Thread.java:748) [na:1.8.0_212]

{code}

This issue 18105 prevent from using spark at all. 

Spark with 300 opened and in progress (but stalling) issues is become less and less reliable each day.
I'm about to send a message on dev forum to ask if developers can stop implementing new features until they have corrected the issues on the features they once written.;;;, 25/Nov/19 11:08;dyptan;You can recreate the error consistently by forcing disk spill on shuffle:
 # Run spark-shell with minimum memory
 `./bin/spark-shell --master yarn  --executor-memory 1g --num-executors 1`
 # Create two large tables, at least ~10M records
 # Join them together

{code:java}
// code placeholder
val people0 = spark.read.orc("/tmp/people/people0.orc").select("ID", "ln").sortWithinPartitions("ID")
val people1 = spark.read.orc("/tmp/people/people1.orc").select("ID", "fn").sortWithinPartitions("ID")
people0.join(people1, Seq("ID")).write.parquet("/tmp/people/joined")

{code}
Tested with Spark 2.4.0;;;, 25/Nov/19 11:26;moliinyk;I was able to reproduce with the code as shown below:
{code:java}
import org.apache.spark.sql.types.IntegerType
import org.apache.spark.sql.functions._
val dfx = spark.range(0, 10000000000L).withColumn("scope", (rand() * 100).cast(IntegerType)).withColumn("value", rand()).repartition(400)
dfx.groupBy("id").pivot("scope").agg(first(col("value"))).show(100, false){code}
on spark-2.3.2.

Here's our settings (used same settings for all versions):
{code:java}
/opt/mapr/spark/spark-2.3.2/bin/spark-shell --master yarn --num-executors 20 --executor-cores 5 --executor-memory=10G --driver-memory=15G
{code};;;
Comment.2: 30/Jan/17 02:44;Tagar;Ran into the same issue 

Looks like it's a "floating" issue - happens more over time. Restarting a spark context makes it harder to reproduce, but then it starts happening again.
Also, seems a precursor for this issue to show up is running executors tight on memory. I had stages failed many times, like :

{noformat}
ExecutorLostFailure (executor 49 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 9.4 GB of 9 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.
{noformat}
(many executors but not all were killed like this) And then after several job restart attempts, this shows up:
{noformat}
java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)
	at org.apache.spark.io.LZ4BlockInputStream.read(LZ4BlockInputStream.java:125)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:254)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
{noformat}
once this problem showed up, I don't see executors failed because of memory, stage starts failing just because of this one problem -
java.io.IOException: Stream is corrupted
	at org.apache.spark.io.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:163)

Here's my job/spark sql query (running from pyspark if this matters) that at some point start reproducing this problem :
{code}
create table marketview.cb_filt_ratings2
stored as parquet
as
select psn, merchant_id
     , PERCENT_RANK() OVER (PARTITION BY merchant_id ORDER BY records_count ASC)        record_count_rating
     , PERCENT_RANK() OVER (PARTITION BY merchant_id ORDER BY transaction_count ASC)    transaction_count_rating
     , PERCENT_RANK() OVER (PARTITION BY merchant_id ORDER BY sum_spend ASC)            sum_spend_rating
from marketview.cb_filt_ratings1
{code}
table has ~6B rows.

ps. Also, the same problem was reported on SO a couple of months ago - http://stackoverflow.com/questions/40289464/spark-job-failing-in-yarn-mode
;;;, 11/Dec/19 19:09;mkempanna;If you are facing this in spark 2.4.0 , then as a temporary measure disable unsafe spill readaheads with setting
{code:java}
  --conf spark.unsafe.sorter.spill.read.ahead.enabled=false{code}
 ;;;, 09/Dec/20 14:44;cloud_fan;Is this still an issue in Spark 3.x? cc [~dongjoon];;;, 09/Dec/20 18:01;dongjoon;Apache Spark 3.x is using lz4-java-1.7.1.jar and this seems to be fixed by upgrading dependency, [~cloud_fan]. I'm not aware of any new incident about this.

cc [~viirya] since he is working on the codec issue in Hadoop community recently.
cc [~sunchao], too
;;;, 20/Mar/21 07:06;devaraj;We are still seeing the issue with Spark 3.0.1 as well,
{code:java}
org.apache.spark.shuffle.FetchFailedException: Stream is corrupted
 at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748)
 at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:823)
 at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
 at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
 at java.io.DataInputStream.readInt(DataInputStream.java:387)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.<init>(UnsafeRowSerializer.scala:120)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2.asKeyValueIterator(UnsafeRowSerializer.scala:110)
 at org.apache.spark.shuffle.BlockStoreShuffleReader.$anonfun$read$2(BlockStoreShuffleReader.scala:92)
 at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
 at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
 at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.sort_addToSorter_0$(Unknown Source)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)
 at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
 at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.findNextInnerJoinRows$(Unknown Source)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage18.processNext(Unknown Source)
 at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
 at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:748)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:177)
 at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
 at org.apache.spark.scheduler.Task.run(Task.scala:127)
 at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
 Caused by: java.io.IOException: Stream is corrupted
 at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:200)
 at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)
 at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:819)
 ... 33 more{code}

  

 ;;;, 23/Mar/21 03:51;dongjoon;[~devaraj]. Do you have a reproducer? BTW, there is a know issue, SPARK-34790, with the latest Apache Spark with IO Encryption + AQE configuration.

 ;;;, 23/Mar/21 22:40;devaraj;[~dongjoon] We are seeing this error while running workloads with ~10TB shuffle data, we are not setting spark.io.encryption.enabled=true. ;;;, 24/Mar/21 04:21;dongjoon;Got it. Thank you for the details, [~devaraj]. Let's keep this open for a while because this issue needs more information.;;;, 27/Apr/21 07:25;jangryeo;We are seeing similar issues with Spark 3.0.1 as well. Not exactly reproducible but often seen
{code:java}
org.apache.spark.shuffle.FetchFailedException: Stream is corrupted
 at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:748)
 at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:823)
 at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
 at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
 at java.io.DataInputStream.readInt(DataInputStream.java:387)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.<init>(UnsafeRowSerializer.scala:120)
 at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2.asKeyValueIterator(UnsafeRowSerializer.scala:110)
 at org.apache.spark.shuffle.BlockStoreShuffleReader.$anonfun$read$2(BlockStoreShuffleReader.scala:92)
 at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
 at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
 at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
 at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage72.sort_addToSorter_0$(Unknown Source)
 at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage72.processNext(Unknown Source)
 at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
 at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
 at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83)
 at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:860)
 at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:730)
 at org.apache.spark.sql.execution.joins.SortMergeJoinExec.$anonfun$doExecute$1(SortMergeJoinExec.scala:254)
 at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
 at org.apache.spark.scheduler.Task.run(Task.scala:127)
 at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
 at java.lang.Thread.run(Thread.java:748)
 Caused by: java.io.IOException: Stream is corrupted
 at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:250)
 at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157)
 at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:819)
 ... 35 more
 Caused by: net.jpountz.lz4.LZ4Exception: Error decoding offset 10122 of input buffer
 at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:39)
 at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:245)
 ... 37 more
{code};;;, 28/Jul/21 13:53;arghya18;I am also facing the same error, I have raised a duplicate [JIRA|https://issues.apache.org/jira/browse/SPARK-36196], did not notice this before. Do we have any further info or workaround on it? I am running Spark 3.1.1;;;, 28/Jul/21 14:00;arghya18;[~dongjoon] Can we please address this before next release, this is one of the blocker for many.

For example, we have actually migrated our entire ETL running thousands of job everyday on 2PB data warehouse  from EMR to open source Spark on K8S and is running stable except this issue. I can provide any further info required for debugging. The only issue of the error is its not reproducible. So out of 1000 of spark job only few fails and those few are different in different days. ;;;
Comment.3: 30/Jan/17 16:40;Tagar;It's also worth to mention that the above query runs with spark.sql.shuffle.partitions= 36000 on a highly skewed data.
So it's possible that some of the partitions are highly populated; while others might be empty.
This problem happens here
https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/io/LZ4BlockInputStream.java#L163
I thought if this magic header check is broken when lz4 streams got merged and it does not insert that magic header for empty partitions 
(or inserts it oncorrectly for empty partitions?). That's just a hypothesis, as I couldn't find code that merges compressed files.;;;, 29/Jul/21 00:24;viirya;Looked at lz4 codebase and the reported failures. I suspect this is not related to lz4 codec. For example, [https://github.com/Mellanox/SparkRDMA/issues/34,] even disabling shuffle compression, there is still "{{java.io.EOFException: reached end of stream"}} error.

Previous comment suggested to disable "spark.unsafe.sorter.spill.read.ahead.enabled". If it is a temporary workaround, it sounds more like an issue on unsafe spill?;;;, 29/Jul/21 19:23;dongjoon;I agree with [~viirya] that this might be not a LZ4 codec issue.

To [~arghya18], more info would be definitely helpful of course.
> I can provide any further info required for debugging. 

Yep. That's the reason why this issue is still open. If we have a deterministic reproducible example, it will be fixed already.
> The only issue of the error is its not reproducible. So out of 1000 of spark job only few fails and those few are different in different days. 

FYI, LZ4-java become 1.7.1 at Apache Spark 3.0.0. So, your report on 3.1.1 is invaluable and [~devaraj]'s 3.0.1 report is too. For Apache Spark 3.3, we will use LZ4-java 1.8. (SPARK-36256);;;, 03/Aug/21 09:42;cameron.todd;I'm also facing this same error when scaling up my project to a larger dataset. It's consistently crashing in the same spot (I can reproduce it) and I'm pretty sure it's coming down to using a repartition() after import as below:
{code:java}
Dataset<Row> relevantPivots = spark.read().parquet(jobConf.pathToDataDedup)
        .select(VadisSparkUtils.scalaSeqConverterColumn(allColsNeeded))
        .na().drop()
        .withColumn("pivot_hash", hash(VadisSparkUtils.scalaSeqConverterColumn(pivot)))
        .drop(VadisSparkUtils.scalaSeqConverterString(pivot))
        .repartition(5000)
        .cache();
{code}
I have already tried to disable  "spark.unsafe.sorter.spill.read.ahead.enabled" but did not work. My cluster environment is spark 3.0.1 standalone with Kubernetes and using lz4-java-1.7.1.jar

Not entirely sure how to debug this further and give you guys anymore info because I can't get the logs on each slave node

 ;;;, 03/Aug/21 20:51;dongjoon;It's a good news because you make it consistently. Could you provide a way for us to follow your path? Your example has proprietary parts.;;;, 04/Aug/21 10:03;cameron.todd;I'll attach a portion of the code that is not proprietary but it's mostly all there. Anyway the code is one small step in our entity resolution process that processes 1billion person records so lots of pairwise comparisons and skewed joins before aggregate and output staging results.[^TestWeightedGraph.java];;;, 04/Aug/21 10:08;cameron.todd;Let me know if that's enough info. From my tests if I remove the  .repartition after import of file, I get no corrupted stream error but I get skewed joins and memory problems later. Then if I add the repartition back, the job does not complete because multiple tasks crash and are retried but still crash with the stream corrupted error before completely crashing the job;;;, 07/Aug/21 17:39;dongjoon;[~cameron.todd]. Thank you. The code itself looks nice. The only problem to me is that we cannot run it due to `pathToDataDedup`. I don't think you can share 1 billion person records here. Is there a way for me to generate some random data for your app?
{code}
Dataset<Row> relevantPivots = spark.read().parquet(pathToDataDedup)
{code};;;, 09/Aug/21 10:23;cameron.todd;Yep I understand. I have hashed my data keeping the same distribution and the full_name hashed column is weak but string distance functions still work on it. Do you have any recommendations where I can upload this data, it's only 2gb?
{code:java}
//So this line of code:
Dataset<Row> relevantPivots = spark.read().parquet(pathToDataDedup)
				.select("id", "full_name", "last_name","birthdate")
				.na().drop()
				.withColumn("pivot_hash", hash(col("last_name"),col("birthdate")))
				.drop("last_name","birthdate")
				.repartition(5000)
				.cache();
// can be replaced with this
Dataset<Row> relevantPivots = spark.read().parquet(pathToDataDedup).repartition(5000).cache();{code}
I have also run the same code on the same hashed data and getting the same corrupted stream error. Also in case it wasn't clear my data normally sits on an s3 bucket.;;;, 10/Aug/21 18:12;dongjoon;Ya, this is a nice simplification really. Thanks.
{code}
Dataset<Row> relevantPivots = spark.read().parquet(pathToDataDedup).repartition(5000).cache();
{code}

For the data, is it Parquet with snappy compression? If you use Gzip compression, it will be reduced a little more. For the location, could you share it as a pre-signed S3 URL or upload/share it via Box.com?;;;, 11/Aug/21 12:11;cameron.todd;Ok I added the zip file on this public S3 bucket, it holds a parquet file with snappy compression, it's 1.75gb. You can retrieve the data as such
{code:java}
wget https://storage.gra.cloud.ovh.net/v1/AUTH_147b880980f148f5ad1af09542e6f37a/public_data/SPARK18105/hashed_data.zip
{code};;;
Comment.4: 30/Jan/17 19:52;davies;There is a workaround merged into Spark 2.1 for these type of failures (decompress it and try again), can you try that?;;;, 11/Aug/21 19:37;dongjoon;Thank you, [~cameron.todd]. I downloaded and ran the test code in my laptop. It works well to me. Do you have some other configurations with you?
{code}
SPARK-18105:$ du -h hash_import.parquet
2.4G	hash_import.parquet
{code}

{code}
scala> val df = spark.read.parquet("/Users/dongjoon/data/SPARK-18105/hash_import.parquet").repartition(5000).cache()
df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: bigint, pivot_hash: int ... 1 more field]

scala> df.count()
res3: Long = 136935074

scala> spark.version
res4: String = 3.1.2
{code};;;, 11/Aug/21 19:40;dongjoon;BTW, I checked that a single zip file contains 157 snappy parquet files which is generated by Apache Spark 3.0.1.

{code}
$ ls -al /Users/dongjoon/data/SPARK-18105/hash_import.parquet | grep parquet | wc -l
     157

$ ls -al /Users/dongjoon/data/SPARK-18105/hash_import.parquet
total 5040768
drwxr-xr-x  160 dongjoon  staff      5120 Aug  9 02:55 .
drwxr-xr-x    4 dongjoon  staff       128 Aug 11 12:32 ..
-rw-r--r--    1 dongjoon  staff         0 Aug  9 02:21 _SUCCESS
-rw-r--r--    1 dongjoon  staff   7098577 Aug  9 02:19 part-00000-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9310705 Aug  9 02:19 part-00001-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9286598 Aug  9 02:19 part-00002-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9066660 Aug  9 02:19 part-00003-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7174124 Aug  9 02:19 part-00004-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7269784 Aug  9 02:19 part-00005-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9045563 Aug  9 02:19 part-00006-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8874048 Aug  9 02:19 part-00007-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9088055 Aug  9 02:19 part-00008-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7921907 Aug  9 02:19 part-00009-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9734530 Aug  9 02:19 part-00010-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11430364 Aug  9 02:19 part-00011-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10106472 Aug  9 02:19 part-00012-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11672118 Aug  9 02:19 part-00013-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11077873 Aug  9 02:19 part-00014-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  17459766 Aug  9 02:19 part-00015-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18000141 Aug  9 02:19 part-00016-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10688008 Aug  9 02:19 part-00017-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10876962 Aug  9 02:19 part-00018-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   6001747 Aug  9 02:19 part-00019-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13051685 Aug  9 02:19 part-00020-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  40607547 Aug  9 02:19 part-00021-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  16122066 Aug  9 02:19 part-00022-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7858387 Aug  9 02:19 part-00023-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7953870 Aug  9 02:19 part-00024-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20777892 Aug  9 02:19 part-00025-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7785559 Aug  9 02:19 part-00026-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   6101494 Aug  9 02:19 part-00027-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14878775 Aug  9 02:19 part-00028-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8807412 Aug  9 02:19 part-00029-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff    664925 Aug  9 02:19 part-00030-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  36711578 Aug  9 02:19 part-00031-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   6376692 Aug  9 02:19 part-00032-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9121175 Aug  9 02:19 part-00033-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  12318653 Aug  9 02:19 part-00034-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18944542 Aug  9 02:20 part-00035-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   5108518 Aug  9 02:19 part-00036-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10884610 Aug  9 02:19 part-00037-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7678782 Aug  9 02:19 part-00038-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9950415 Aug  9 02:19 part-00039-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22648978 Aug  9 02:19 part-00040-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9918156 Aug  9 02:19 part-00041-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20537613 Aug  9 02:20 part-00042-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20121006 Aug  9 02:20 part-00043-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10919311 Aug  9 02:19 part-00044-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13743611 Aug  9 02:20 part-00045-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20744467 Aug  9 02:20 part-00046-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8362612 Aug  9 02:20 part-00047-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  17083828 Aug  9 02:20 part-00048-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8325658 Aug  9 02:20 part-00049-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  28616403 Aug  9 02:20 part-00050-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  27766601 Aug  9 02:20 part-00051-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7274627 Aug  9 02:20 part-00052-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18133083 Aug  9 02:20 part-00053-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20962549 Aug  9 02:20 part-00054-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10766021 Aug  9 02:20 part-00055-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10987925 Aug  9 02:20 part-00056-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  24120006 Aug  9 02:20 part-00057-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7940774 Aug  9 02:20 part-00058-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15471824 Aug  9 02:20 part-00059-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  17041126 Aug  9 02:20 part-00060-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   4558582 Aug  9 02:20 part-00061-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  28812083 Aug  9 02:20 part-00062-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18382491 Aug  9 02:20 part-00063-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11464446 Aug  9 02:20 part-00064-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13522906 Aug  9 02:20 part-00065-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  12881794 Aug  9 02:20 part-00066-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14688841 Aug  9 02:20 part-00067-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13630922 Aug  9 02:20 part-00068-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15015507 Aug  9 02:20 part-00069-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21820733 Aug  9 02:20 part-00070-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9051687 Aug  9 02:20 part-00071-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14009387 Aug  9 02:20 part-00072-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8043754 Aug  9 02:20 part-00073-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15522853 Aug  9 02:20 part-00074-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22376010 Aug  9 02:20 part-00075-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22600841 Aug  9 02:20 part-00076-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  12973790 Aug  9 02:20 part-00077-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13724406 Aug  9 02:20 part-00078-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   9837992 Aug  9 02:20 part-00079-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  24946732 Aug  9 02:20 part-00081-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21250848 Aug  9 02:20 part-00082-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22465612 Aug  9 02:20 part-00083-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15383161 Aug  9 02:20 part-00084-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10725376 Aug  9 02:20 part-00085-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  19325378 Aug  9 02:20 part-00086-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8895845 Aug  9 02:20 part-00087-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  19790382 Aug  9 02:20 part-00088-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8773449 Aug  9 02:20 part-00089-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  19144012 Aug  9 02:20 part-00090-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  23290537 Aug  9 02:20 part-00091-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  39088655 Aug  9 02:20 part-00092-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   8602985 Aug  9 02:20 part-00093-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15345549 Aug  9 02:20 part-00094-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13559709 Aug  9 02:20 part-00095-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   6852728 Aug  9 02:20 part-00096-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  29831129 Aug  9 02:20 part-00097-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11356100 Aug  9 02:20 part-00098-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21956322 Aug  9 02:20 part-00099-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  16915694 Aug  9 02:20 part-00100-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21229399 Aug  9 02:20 part-00101-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   3039283 Aug  9 02:20 part-00102-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  31255403 Aug  9 02:20 part-00103-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  13639185 Aug  9 02:20 part-00104-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22379210 Aug  9 02:20 part-00105-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  20522598 Aug  9 02:20 part-00106-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21562145 Aug  9 02:20 part-00107-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21461668 Aug  9 02:20 part-00108-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  27882056 Aug  9 02:20 part-00109-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14075767 Aug  9 02:20 part-00110-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  17072086 Aug  9 02:20 part-00111-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   2745029 Aug  9 02:20 part-00112-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18697825 Aug  9 02:20 part-00113-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  42039504 Aug  9 02:20 part-00114-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   7403373 Aug  9 02:20 part-00115-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  10516182 Aug  9 02:20 part-00116-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  16325101 Aug  9 02:20 part-00117-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14799829 Aug  9 02:20 part-00118-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14694122 Aug  9 02:20 part-00119-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  24990548 Aug  9 02:20 part-00120-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  11079775 Aug  9 02:20 part-00121-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  33703475 Aug  9 02:20 part-00122-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14656154 Aug  9 02:20 part-00123-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   2180907 Aug  9 02:20 part-00124-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22524866 Aug  9 02:20 part-00125-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  23664835 Aug  9 02:20 part-00126-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   3718511 Aug  9 02:20 part-00127-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15386780 Aug  9 02:20 part-00128-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15793443 Aug  9 02:21 part-00129-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  25739876 Aug  9 02:21 part-00130-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  22014066 Aug  9 02:21 part-00131-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  26034951 Aug  9 02:21 part-00132-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18732517 Aug  9 02:21 part-00133-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  15581656 Aug  9 02:21 part-00134-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  24107039 Aug  9 02:21 part-00135-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff    617756 Aug  9 02:21 part-00136-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  17878862 Aug  9 02:21 part-00137-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  37368882 Aug  9 02:21 part-00138-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14727470 Aug  9 02:21 part-00139-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  36085625 Aug  9 02:21 part-00140-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21564288 Aug  9 02:21 part-00141-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff   4207926 Aug  9 02:21 part-00142-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  26885735 Aug  9 02:21 part-00143-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff     47222 Aug  9 02:21 part-00145-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff      3031 Aug  9 02:21 part-00146-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff    101888 Aug  9 02:21 part-00147-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  18902521 Aug  9 02:21 part-00148-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff      3428 Aug  9 02:21 part-00149-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  23022202 Aug  9 02:21 part-00150-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff      2675 Aug  9 02:21 part-00155-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  21980843 Aug  9 02:21 part-00157-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  37397239 Aug  9 02:21 part-00158-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  53319037 Aug  9 02:21 part-00159-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  53852766 Aug  9 02:21 part-00160-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  52634756 Aug  9 02:21 part-00161-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  58968977 Aug  9 02:21 part-00162-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
-rw-r--r--    1 dongjoon  staff  14955676 Aug  9 02:21 part-00163-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
{code}

{code}
$ parquet-tools meta /Users/dongjoon/data/SPARK-18105/hash_import.parquet/part-00163-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
file:        file:/Users/dongjoon/data/SPARK-18105/hash_import.parquet/part-00163-51e43836-6ce9-419d-b632-2a8ca8185b9b-c000.snappy.parquet
creator:     parquet-mr version 1.10.1 (build a89df8f9932b6ef6633d06069e50c9b7970bebd1)
extra:       org.apache.spark.version = 3.0.1
extra:       org.apache.spark.sql.parquet.row.metadata = {"type":"struct","fields":[{"name":"id","type":"long","nullable":true,"metadata":{}},{"name":"pivot_hash","type":"integer","nullable":false,"metadata":{}},{"name":"full_name","type":"string","nullable":true,"metadata":{}}]}
{code};;;, 11/Aug/21 20:03;cameron.todd;Good to hear. Also the count of 136,935,074 is right. 

When you say "I downloaded and ran the test code in my laptop. It works well to me". You mean the java code I attached or do you mean the read file and count() that you just attached above. Because the error is only raised after the first few aggregations and joins in my code and I also doubt your laptop is big enough to process that ;) .

I can attach key config variables on your request but I'm using a cloud Kubernetes spark cluster managed by OVH (spark 3.0.1) so it's out of box solution managed by OVH not really me.;;;, 12/Aug/21 02:16;dongjoon;I did only the above code I posted because you wrote like this. What did you mean by `can be replaced with this` then?
{code:java}
//So this line of code:
...
// can be replaced with this
Dataset<Row> relevantPivots = spark.read().parquet(pathToDataDedup).repartition(5000).cache(); {code};;;, 12/Aug/21 07:33;cameron.todd;Oh sorry, I meant just a portion of the code can be replaced or only the import of the parquet part because I hashed the data file so the 'pivot_hash' column already exists. The real test begins with the following joins and aggregations per the .java file I uploaded.;;;, 16/Aug/21 03:24;dragonlong;[~cameron.todd] Hi, is any news? I issue this problem both in spark 2.3 and spark 3.0. Thanks.

 

Here is the spark3.0 trace:
{code:java}
// code placeholder
FetchFailed(BlockManagerId(19, xxxxx, 26065, None), shuffleId=0, mapId=841, reduceId=1024, message=FetchFailed(BlockManagerId(19, xxxxx, 26065, None), shuffleId=0, mapId=841, reduceId=1024, message=org.apache.spark.shuffle.FetchFailedException: Stream is corrupted at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:750) at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:823) at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) at java.io.BufferedInputStream.read(BufferedInputStream.java:265) at java.io.DataInputStream.readInt(DataInputStream.java:387) at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113) at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:129) at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110) at scala.collection.Iterator$$anon$11.next(Iterator.scala:494) at scala.collection.Iterator$$anon$10.next(Iterator.scala:459) at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40) at scala.collection.Iterator$$anon$10.next(Iterator.scala:459) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage6.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729) at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225) at org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349) at org.apache.spark.rdd.RDD.iterator(RDD.scala:313) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:127) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$runWithUgi$3(Executor.scala:462) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377) at org.apache.spark.executor.Executor$TaskRunner.runWithUgi(Executor.scala:465) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:394) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)

Caused by: java.io.IOException: Stream is corrupted at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:250) at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:226) at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157) at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:819) ... 30 more

Caused by: net.jpountz.lz4.LZ4Exception: Error decoding offset 94821 of input buffer at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:39) at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:245) ... 33 more
{code}
 ;;;, 30/Aug/21 08:30;Ngone51;FYI, for users who hit the "Stream is corrupted" error in 3.0.0, please try to apply the fix of SPARK-32658 first as it's a known bug that can cause the error.;;;, 29/Sep/21 06:51;vladimir.prus;FYI, we recently started to get a lot of such errors; they appear to be correlated with increased load and increased spot termination in AWS. So as an experiment, I've disabled executor decommission, and the errors all disappeared. Specifically, I set these options:
{noformat}
storage.decommission.enabled: false
storage.decommission.rddBlocks.enabled: false
storage.decommission.shuffleBlocks.enabled: false{noformat}
It is of course not a perfect set of options for production, but maybe will be a hint at the problem. I am using a recent build from branch-3.1, specifically from commit e1fc62de8e05.;;;, 29/Sep/21 07:05;Ngone51;[~vladimir.prus] Hi, could you also file a sub-task under https://issues.apache.org/jira/browse/SPARK-20624? That sounds like an issue in decommission.;;;, 16/Nov/21 09:18;zhangweilst;In our case, it is strongly related with `spark.file.transferTo`. Set `spark.file.transferTo` to false has saved our life.

We have been tortured by this problem for almost about a year, and we got about hundreds of tasks  with this `FetchFailedException: Decompression error: Corrupted block detected` error everyday. Recently the heat is burning up and got us kind of on fire since we are using Spark in more scenarios.

One of the stack I got is: 
{code:java}
org.apache.spark.shuffle.FetchFailedException: Decompression error: Corrupted block detected	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:569)	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:474)	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:66)	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1165)	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:357)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:308)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)	at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)	at org.apache.spark.scheduler.Task.run(Task.scala:123)	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)	at java.lang.Thread.run(Thread.java:748) 

Caused by: java.io.IOException: Decompression error: Corrupted block detected	at com.github.luben.zstd.ZstdInputStream.read(ZstdInputStream.java:111)	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286)	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)	at org.apache.spark.util.Utils$$anonfun$1.apply$mcZ$sp(Utils.scala:402)	at org.apache.spark.util.Utils$$anonfun$1.apply(Utils.scala:397)	at org.apache.spark.util.Utils$$anonfun$1.apply(Utils.scala:397)	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)	at org.apache.spark.util.Utils$.copyStreamUpTo(Utils.scala:409)	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:467)	... 32 more{code}
We have Spark version 2.2.2, 2.4.7, 3.0.1 and 3.1.2，and all have this issue. I tried methods in this post and others, nothing can make the non-repeatable error disappear. After weeks' struggle and going through the code with one of our almost repeatable case, we are able to find that `spark.file.transferTo` is the one. We turned off this flag in our cluster, and this error literally disappears – None of this error any more.

 

Normally we won't suspect flags like `spark.file.transferTo`, in that the only difference is you are using `FileStream` or `FileChannel#transferTo` in shuffle spill merge phase. And if that's the problem, it probably relates to JVM or kernel bug. `transferTo` is more efficient than `FileStream` in that it copies file segment with zero-copy, but empirically from our experience, this error definitely IS caused by this flag.

 

So for those who are stilled bothered by this problem, `spark.file.transferTo` is one thing to try. And if [~dongjoon] or somebody else is interested in this, I can offer some more info if I can. Currently, we are using linux kernel 4.19.95 and JDK 1.8.0_171-b11.

 ;;;
Comment.5: 30/Jan/17 21:14;Tagar;Thanks for the follow up [~davies]. 
We'd have to wait for a cdh parcel of Spark 2.1 to be released to try it out. ;;;, 17/Nov/21 22:27;sidk;Hi, I saw a similar failure just as [~vladimir.prus]. In my experiment, I enabled node decommissioning along with a decommission fallback storage and then terminated an executor while the shuffle blocks are being fetched. The node decommissioning begins for the lost executor and migrates all the shuffle blocks to peer executors. Post migration, when the shuffle blocks are being fetched, I see the "FetchFailedException: Stream is corrupted" and "Error decoding offset 19258 of input buffer" message as seen in this thread. The error goes away when I do not add the fallback storage option.

These were the options I set in my experiment
{code:java}
spark.decommission.enabled: true,
spark.storage.decommission.shuffleBlocks.enabled : true,
spark.storage.decommission.enabled: true,
spark.storage.decommission.fallbackStorage.path : s3://<bucket>/    # Stopped seeing errors after removing this{code};;;, 03/Dec/21 03:57;yumwang;Workaround this issue by set spark.io.compression.codec=zstd.;;;, 24/Mar/22 11:46;sleep1661;It's working in my case by setting spark.file.transferTo=false. Thanks to [~zhangweilst] . And my spark version was 3.1.2.;;;, 14/Feb/24 07:43;jwozniak;With Spark 3.5.0 running on Yarn Hadoop we had this: 
{code}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 2551.0 failed 20 times, most recent failure: Lost task 13.19 in stage 2551.0 (TID 205408) (ithdp-nxcals5001.cern.ch executor 31): org.apache.spark.SparkException: [INTERNAL_ERROR_STORAGE] Unexpected type of BlockId, shuffle_713_203394_130_140
        at org.apache.spark.SparkException$.internalError(SparkException.scala:92)
        at org.apache.spark.SparkException$.internalError(SparkException.scala:100)
        at org.apache.spark.storage.ShuffleBlockFetcherIterator.diagnoseCorruption(ShuffleBlockFetcherIterator.scala:1147)
        at org.apache.spark.storage.BufferReleasingInputStream.$anonfun$tryOrFetchFailedException$1(ShuffleBlockFetcherIterator.scala:1385)
        at scala.Option.map(Option.scala:230)
        at org.apache.spark.storage.BufferReleasingInputStream.tryOrFetchFailedException(ShuffleBlockFetcherIterator.scala:1384)
        at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:1370)
        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)
        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)
        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113)
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:129)
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:496)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
        at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
        at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:62)
        at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1$adapted(SortAggregateExec.scala:59)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:877)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:877)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
{code}

After setting 
{code}
spark.shuffle.detectCorrupt: false
spark.shuffle.checksum.enabled: false
{code}

we started to get this one (similar to other exception from this issue)
{code}
Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.15.jar:?]
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.15.jar:?]
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.Option.foreach(Option.scala:407) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[spark-core_2.12-3.5.0.jar:3.5.0]
Caused by: java.io.IOException: Stream is corrupted
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:252) ~[lz4-java-1.8.0.jar:?]
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:228) ~[lz4-java-1.8.0.jar:?]
        at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:159) ~[lz4-java-1.8.0.jar:?]
        at org.apache.spark.storage.BufferReleasingInputStream.$anonfun$read$3(ShuffleBlockFetcherIterator.scala:1370) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.storage.BufferReleasingInputStream.tryOrFetchFailedException(ShuffleBlockFetcherIterator.scala:1381) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:1370) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252) ~[?:?]
        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271) ~[?:?]
        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392) ~[?:?]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:129) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:496) ~[scala-library-2.12.15.jar:?]
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.base/java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: net.jpountz.lz4.LZ4Exception: Error decoding offset 14570 of input buffer
        at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:41) ~[lz4-java-1.8.0.jar:?]
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:247) ~[lz4-java-1.8.0.jar:?]
        at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:228) ~[lz4-java-1.8.0.jar:?]
        at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:159) ~[lz4-java-1.8.0.jar:?]
        at org.apache.spark.storage.BufferReleasingInputStream.$anonfun$read$3(ShuffleBlockFetcherIterator.scala:1370) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.storage.BufferReleasingInputStream.tryOrFetchFailedException(ShuffleBlockFetcherIterator.scala:1381) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:1370) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252) ~[?:?]
        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271) ~[?:?]
        at java.base/java.io.DataInputStream.readInt(DataInputStream.java:392) ~[?:?]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.readSize(UnsafeRowSerializer.scala:113) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:129) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:496) ~[scala-library-2.12.15.jar:?]
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:461) ~[scala-library-2.12.15.jar:?]
        at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:226) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.sql.execution.SortExec.$anonfun$doExecute$1(SortExec.scala:119) ~[spark-sql_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:328) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) ~[spark-core_2.12-3.5.0.jar:3.5.0]
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
        at java.base/java.lang.Thread.run(Thread.java:829) ~[?:?]
{code}


;;;, 05/Jul/24 16:35;chengpan;there is an upstream XFS kernel bug identified by the Bilibili team that affects the spark shuffle use case, see details at [https://lore.kernel.org/linux-xfs/20220817093627.GZ3600936@dread.disaster.area/], disabling `spark.file.transferTo` could also workaround this issue.;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Memory leak when dataset is being persisted
Issue key: SPARK-35262
Issue id: 13375680
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: iamelin
Creator: iamelin
Created: 28/Apr/21 20:02
Updated: 15/Apr/24 19:39
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: If a Java- or Scala-application with SparkSession runs a long time and persists a lot of datasets, it can crash because of a memory leak.
 I've noticed the following. When we have a dataset and persist it, the SparkSession used to load that dataset is cloned in CacheManager, and this clone is added as a listener to `listenersPlusTimers` in `ListenerBus`. But this clone isn't removed from the list of listeners after that, e.g. unpersisting the dataset. If we persist a lot of datasets, the SparkSession is cloned and added to `ListenerBus` many times. This leads to a memory leak since the `listenersPlusTimers` list become very large.

I've found out that the SparkSession is cloned is CacheManager when the parameters `spark.sql.sources.bucketing.autoBucketedScan.enabled` and `spark.sql.adaptive.enabled` are true. The first one is true by default, and this default behavior leads to the problem. When auto bucketed scan is disabled, the SparkSession isn't cloned, and there are no duplicates in ListenerBus, so the memory leak doesn't occur.

Here is a small Java application to reproduce the memory leak: [https://github.com/iamelin/spark-memory-leak]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): SPARK-47859
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Apr 15 19:35:15 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qk4w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Apr/21 10:40;fchen;[~iamelin] This is should be a duplicate bug with SPARK-34087 and has been fixed by PR-31919. Spark 3.1.1 has a memory leak when we clone the SparkSession.

When you disabled `spark.sql.sources.bucketing.autoBucketedScan.enabled` and `spark.sql.adaptive.enabled`, the CacheManager cache query using original SparkSession (means spark not clone session).;;;, 10/Jan/22 19:20;dnskrv;[~iamelin] Could you please check/confirm the issue still exists in 3.2.0?;;;, 15/Apr/24 19:35;leotim;Hello [~dnskrv] [~iamelin]
I can confirm that the issue is still existing in 3.5.0;;;
Affects Version/s.1: 
Comment.1: 10/Jan/22 19:20;dnskrv;[~iamelin] Could you please check/confirm the issue still exists in 3.2.0?;;;
Comment.2: 15/Apr/24 19:35;leotim;Hello [~dnskrv] [~iamelin]
I can confirm that the issue is still existing in 3.5.0;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Support subexpression elimination for non-common branches of conditional expressions
Issue key: SPARK-35564
Issue id: 13381130
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kimahriman
Creator: kimahriman
Created: 29/May/21 21:34
Updated: 24/Mar/24 00:20
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: https://issues.apache.org/jira/browse/SPARK-33337 added support for pulling subexpressions out of branches of conditional expressions for expressions present in all branches. We should be able to take this a step further and pull out subexpressions for any branch, as long as that expression will definitely be evaluated at least once.

Consider a common data validation example:
{code:java}
from pyspark.sql.functions import *
df = spark.createDataFrame([['word'], ['1234']])
col = regexp_replace('_1', r'\d', '')
df = df.withColumn('numbers_removed', when(length(col) > 0, col)){code}
We only want to keep the value if it's non-empty with numbers removed, otherwise we want it to be null. 

Because we have no otherwise value, `col` is not a candidate for subexpression elimination (you can see two regular expression replacements in the codegen). But whenever the length is greater than 0, we will have to execute the regular expression replacement twice. Since we know we will always calculate `col` at least once, it makes sense to consider that as a subexpression since we might need it again in the branch value. So we can update the logic from:

Create a subexpression if an expression will always be evaluated at least twice

To:

Create a subexpression if an expression will always be evaluated at least once AND will either always or conditionally be evaluated at least twice.

The trade off is potentially another subexpression function call (for split subexpressions) if the second evaluation doesn't happen, but this seems like it would be worth it for when it is evaluated the second time.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 29 16:52:47 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rhps:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/May/21 02:23;gurwls223;cc [~viirya] FYI;;;, 31/May/21 03:54;viirya;Thanks [~hyukjin.kwon] for the ping.

> Create a subexpression if an expression will always be evaluated at least once AND will either always or conditionally be evaluated at least twice.

Do you mean "Create a subexpression if an expression will always be evaluated at least once AND will be evaluated at least once in conditional expression"?

I.e., an expression will be evaluated at least 1 + (optional) 1?

> The trade off is potentially another subexpression function call (for split subexpressions) if the second evaluation doesn't happen, but this seems like it would be worth it for when it is evaluated the second time.

Yea, if the second evaluation doesn't happen, there is redundant function call and redundant function added to the generated class (for the split case).

I think here there is no correct answer but a trade-off. I also worry about if we will too many optional subexpressions for worst case.

And this looks like a corner case, so I'm not sure if it is worth to do this.








;;;, 31/May/21 13:00;kimahriman;>Do you mean "Create a subexpression if an expression will always be evaluated at least once AND will be evaluated at least once in conditional expression"?

Yeah you can think of it that way in terms of adding to existing functionality. I was trying to word it in a way that encompassed existing functionality as well.

>And this looks like a corner case, so I'm not sure if it is worth to do this.

I don't really think this is much of a corner case, but a common case of using a when expression for data validation. Most of our ETL process comes down to normalizing, cleaning, and validating strings, which at the end of the day usually looks like:
{code:java}
column = normalize_value(col('my_raw_value'))
result = when(column != '', column){code}
where "normalize_value" usually involves some combination of regexp_repace's, lower/upper, and trim.

And things get worse when you are dealing with arrays of strings and want to minimize your data:
{code:java}
column = filter(transform(col('my_raw_array_value'), lambda x: normalize_value(x)), lambda x: x != '')
result = when(size(column) > 0, column){code}
though currently higher order functions are always semantically different so they don't get subexpressions regardless I think. That's something I plan to look into as a follow up.

It's natural for users to think that these expressions only get evaluated once, and not that they are doubling their runtime trying to clean their data. To me the edge case is creating a subexpression in this case decreasing throughput. It would require a very large percentage of the rows to not pass the conditional check, since the additional calculation is much more expensive than the additional function call. I'm playing around with an implementation so we'll see how far I can get with it.

 ;;;, 31/May/21 17:35;viirya;> I don't really think this is much of a corner case, but a common case of using a when expression for data validation. Most of our ETL process comes down to normalizing, cleaning, and validating strings, which at the end of the day usually looks like:

This is a corner case because it simplifies other possible cases, although you might actually use this pattern in your ETL process.

For example, when we treat an always-evaluate-at-least-once and optionally-evaluate-at-least-once expression as subexpression, there are many expressions qualified for this. A child expression of the first predicate of when, if it is also part of any conditional predicate/value, might also be treated as subexpression. Finally we might end with tons of subexpressions like that to flood generated code.

On the other hand, how much gain we can get from this case? In the example, for the worst case we evaluate it twice, not 5 or 10 times. It may be just small piece of the entire ETL process. I feel it's not worth because we might pay a lot cost including making the code more complicated and creating tons of subexpressions, but in the end we only get a little bit from it and it is also only for a worst case.

> though currently higher order functions are always semantically different so they don't get subexpressions regardless I think. That's something I plan to look into as a follow up.

Oh, this is another issue. I noticed it last time when I worked on another PR recently, but don't have time to look at it yet.

;;;, 31/May/21 19:09;kimahriman;A 2x gain would be pretty significant to us, I don't know about others. I'm planning to implement this in our fork and if I get good results I'll put up a PR for further discussion. Could optionally add a config for this if it's workload dependent. Also, the only thing it could likely do to the generated code is reduce the overall size, albeit with more functional calls in worst cases. Whether smaller code size adds any value, I don't know enough about Java to know.

>Oh, this is another issue. I noticed it last time when I worked on another PR recently, but don't have time to look at it yet.

I created https://issues.apache.org/jira/browse/SPARK-35580 to track what I've figured out so far. Not sure what the right fix is.;;;, 06/Jun/21 13:54;kimahriman;Turns out this is already happening for certain when and coalesce expressions. For example:
{code:java}
spark.range(2).select(myUdf($"id"), coalesce($"id", myUdf($"id")))
{code}
myUdf gets pulled out as a subexpression even though it might only be executed once per row. This can be a correctness issue for very specific edge cases similar to https://issues.apache.org/jira/browse/SPARK-35449 where myUdf could get executed for a row even though it doesn't pass certain conditional checks;;;, 06/Jun/21 17:05;viirya;{{select(myUdf($"id"), coalesce($"id", myUdf($"id")))}} => Doesn't {{myUdf($"id")}} always run at lease once?;;;, 06/Jun/21 19:25;kimahriman;Yes that was an example of "will run at least once and maybe more than once" that I'm proposing to add more support for in this issue.

An example of current behavior that would be considered a bug is:
{code:java}
spark.range(2).select(coalesce($"id", myUdf($"id")), coalesce($"id" + 1, myUdf($"id"))).show()
{code}
myUdf will be pulled out into a subexpression even though it is never executed.;;;, 06/Jun/21 19:49;viirya;For the case {{spark.range(2).select(coalesce($"id", myUdf($"id")), coalesce($"id" + 1, myUdf($"id"))).show()}}, looks like it can possibly be performance issue by pulling a subexpr that might not be executed for a row but not a bug. Different to elsevalue in when, coalesce is not a condition expression, it supposes all arguments can be executed without problem.;;;, 06/Jun/21 19:52;kimahriman;You can construct a similar CaseWhen that could lead to a similar problem, the coalesce was just simpler to demonstrate;;;, 06/Jun/21 20:05;viirya;Do you mean {{CaseWhen(($"id", myUdf($"id") :: ($"id" + 1, myUdf($"id") :: Nil, Some(myUdf($"id")))}}?

{{myUdf($"id")}} always runs for all rows, no?
;;;, 06/Jun/21 22:49;kimahriman;No the values are fine, it's the tail conditions that cause the issue.
{code:java}
spark.range(2).select(when($"id" >= 0, lit(1)).when(myUdf($"id") > 0, lit(2)), when($"id" > -1, lit(1)).when(myUdf($"id") > 0, lit(2))).show(){code}
Here myUdf($"id") gets pulled out as a subexpression even though it never should be evaluated.;;;, 07/Jun/21 00:33;viirya;If you mean a common expr in tail conditions other than the first one, it is similar as coalesce example above as I think it supposes all conditions can be executed without problem. It is still performance consideration here.;;;, 07/Jun/21 00:52;kimahriman;Is that documented somewhere? I know Boolean expressions aren't guaranteed to short circuit, but I think most spark users would assume multiple when clauses would short circuit;;;, 20/Jun/21 13:36;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/32987;;;, 29/Jun/23 16:52;ignitetcbot;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/41677;;;
Affects Version/s.1: 
Comment.1: 31/May/21 03:54;viirya;Thanks [~hyukjin.kwon] for the ping.

> Create a subexpression if an expression will always be evaluated at least once AND will either always or conditionally be evaluated at least twice.

Do you mean "Create a subexpression if an expression will always be evaluated at least once AND will be evaluated at least once in conditional expression"?

I.e., an expression will be evaluated at least 1 + (optional) 1?

> The trade off is potentially another subexpression function call (for split subexpressions) if the second evaluation doesn't happen, but this seems like it would be worth it for when it is evaluated the second time.

Yea, if the second evaluation doesn't happen, there is redundant function call and redundant function added to the generated class (for the split case).

I think here there is no correct answer but a trade-off. I also worry about if we will too many optional subexpressions for worst case.

And this looks like a corner case, so I'm not sure if it is worth to do this.








;;;, 06/Jun/21 20:05;viirya;Do you mean {{CaseWhen(($"id", myUdf($"id") :: ($"id" + 1, myUdf($"id") :: Nil, Some(myUdf($"id")))}}?

{{myUdf($"id")}} always runs for all rows, no?
;;;, 06/Jun/21 22:49;kimahriman;No the values are fine, it's the tail conditions that cause the issue.
{code:java}
spark.range(2).select(when($"id" >= 0, lit(1)).when(myUdf($"id") > 0, lit(2)), when($"id" > -1, lit(1)).when(myUdf($"id") > 0, lit(2))).show(){code}
Here myUdf($"id") gets pulled out as a subexpression even though it never should be evaluated.;;;, 07/Jun/21 00:33;viirya;If you mean a common expr in tail conditions other than the first one, it is similar as coalesce example above as I think it supposes all conditions can be executed without problem. It is still performance consideration here.;;;, 07/Jun/21 00:52;kimahriman;Is that documented somewhere? I know Boolean expressions aren't guaranteed to short circuit, but I think most spark users would assume multiple when clauses would short circuit;;;, 20/Jun/21 13:36;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/32987;;;, 29/Jun/23 16:52;ignitetcbot;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/41677;;;
Comment.2: 31/May/21 13:00;kimahriman;>Do you mean "Create a subexpression if an expression will always be evaluated at least once AND will be evaluated at least once in conditional expression"?

Yeah you can think of it that way in terms of adding to existing functionality. I was trying to word it in a way that encompassed existing functionality as well.

>And this looks like a corner case, so I'm not sure if it is worth to do this.

I don't really think this is much of a corner case, but a common case of using a when expression for data validation. Most of our ETL process comes down to normalizing, cleaning, and validating strings, which at the end of the day usually looks like:
{code:java}
column = normalize_value(col('my_raw_value'))
result = when(column != '', column){code}
where "normalize_value" usually involves some combination of regexp_repace's, lower/upper, and trim.

And things get worse when you are dealing with arrays of strings and want to minimize your data:
{code:java}
column = filter(transform(col('my_raw_array_value'), lambda x: normalize_value(x)), lambda x: x != '')
result = when(size(column) > 0, column){code}
though currently higher order functions are always semantically different so they don't get subexpressions regardless I think. That's something I plan to look into as a follow up.

It's natural for users to think that these expressions only get evaluated once, and not that they are doubling their runtime trying to clean their data. To me the edge case is creating a subexpression in this case decreasing throughput. It would require a very large percentage of the rows to not pass the conditional check, since the additional calculation is much more expensive than the additional function call. I'm playing around with an implementation so we'll see how far I can get with it.

 ;;;
Comment.3: 31/May/21 17:35;viirya;> I don't really think this is much of a corner case, but a common case of using a when expression for data validation. Most of our ETL process comes down to normalizing, cleaning, and validating strings, which at the end of the day usually looks like:

This is a corner case because it simplifies other possible cases, although you might actually use this pattern in your ETL process.

For example, when we treat an always-evaluate-at-least-once and optionally-evaluate-at-least-once expression as subexpression, there are many expressions qualified for this. A child expression of the first predicate of when, if it is also part of any conditional predicate/value, might also be treated as subexpression. Finally we might end with tons of subexpressions like that to flood generated code.

On the other hand, how much gain we can get from this case? In the example, for the worst case we evaluate it twice, not 5 or 10 times. It may be just small piece of the entire ETL process. I feel it's not worth because we might pay a lot cost including making the code more complicated and creating tons of subexpressions, but in the end we only get a little bit from it and it is also only for a worst case.

> though currently higher order functions are always semantically different so they don't get subexpressions regardless I think. That's something I plan to look into as a follow up.

Oh, this is another issue. I noticed it last time when I worked on another PR recently, but don't have time to look at it yet.

;;;
Comment.4: 31/May/21 19:09;kimahriman;A 2x gain would be pretty significant to us, I don't know about others. I'm planning to implement this in our fork and if I get good results I'll put up a PR for further discussion. Could optionally add a config for this if it's workload dependent. Also, the only thing it could likely do to the generated code is reduce the overall size, albeit with more functional calls in worst cases. Whether smaller code size adds any value, I don't know enough about Java to know.

>Oh, this is another issue. I noticed it last time when I worked on another PR recently, but don't have time to look at it yet.

I created https://issues.apache.org/jira/browse/SPARK-35580 to track what I've figured out so far. Not sure what the right fix is.;;;
Comment.5: 06/Jun/21 13:54;kimahriman;Turns out this is already happening for certain when and coalesce expressions. For example:
{code:java}
spark.range(2).select(myUdf($"id"), coalesce($"id", myUdf($"id")))
{code}
myUdf gets pulled out as a subexpression even though it might only be executed once per row. This can be a correctness issue for very specific edge cases similar to https://issues.apache.org/jira/browse/SPARK-35449 where myUdf could get executed for a row even though it doesn't pass certain conditional checks;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Executor obtained error information 
Issue key: SPARK-43221
Issue id: 13533426
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yorksity
Creator: yorksity
Created: 20/Apr/23 15:56
Updated: 17/Jan/24 00:19
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Block Manager
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Spark on Yarn Cluster

When multiple executors exist on a node, and the same block exists on both executors, with some in memory and some on disk.

Probabilistically, the executor failed to obtain the block,throw Exception:

java.lang.ArrayIndexOutofBoundsException: 0

    at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:183)

 

Next, I will replay the process of the problem occurring: 

step 1:

The executor requests the driver to obtain block information(locationsAndStatusOption). The input parameters are BlockId and the host of its own node. Please note that it does not carry port information

line:1092

!image-2023-04-21-00-24-22-059.png!

step 2:

On the driver side, the driver obtains all blockManagers holding the block based on the BlockId. For non remote shuffle scenarios, the driver will retrieve the first one with the blockId and blockManager from the locations

Assuming that there are two BlockManagers holding the BlockId on this node, BM-1 holds the Block and stores it in memory, and BM-2 holds the Block and stores it in disk

Assuming the returned status is of type memory and its disksize is 0

line: 852, 856

!image-2023-04-21-00-30-41-851.png!

step 3:

This method will return a BlockLocationsAndStatus object. If there are BMs using disk, the disk's path information will be stored in localDirs

!image-2023-04-21-00-50-10-918.png!

step 4:

When the executor obtains locationsAndStatusOption, localDirs is not empty, but status.diskSize is 0

line: 1102

!image-2023-04-21-00-54-11-968.png!

step 5:

The readDiskBlockFromSameHostExecutor only determines whether the Block file exists, and then directly uses the incoming blocksize to read the byte array. If the blocksize is 0, it returns an empty byte array

Only checked if the file exists

line: 1234, 1240

!image-2023-04-21-00-57-29-140.png!

Taking values from an empty array, causing an out of bounds problem
Environment: 
Original Estimate: 86400.0
Remaining Estimate: 86400.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 86400.0
Σ Remaining Estimate: 86400.0
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 20/Apr/23 16:20;yorksity;image-2023-04-21-00-19-58-021.png;https://issues.apache.org/jira/secure/attachment/13057452/image-2023-04-21-00-19-58-021.png, 20/Apr/23 16:24;yorksity;image-2023-04-21-00-24-22-059.png;https://issues.apache.org/jira/secure/attachment/13057453/image-2023-04-21-00-24-22-059.png, 20/Apr/23 16:30;yorksity;image-2023-04-21-00-30-41-851.png;https://issues.apache.org/jira/secure/attachment/13057454/image-2023-04-21-00-30-41-851.png, 20/Apr/23 16:50;yorksity;image-2023-04-21-00-50-10-918.png;https://issues.apache.org/jira/secure/attachment/13057455/image-2023-04-21-00-50-10-918.png, 20/Apr/23 16:53;yorksity;image-2023-04-21-00-53-20-720.png;https://issues.apache.org/jira/secure/attachment/13057456/image-2023-04-21-00-53-20-720.png, 20/Apr/23 16:54;yorksity;image-2023-04-21-00-54-11-968.png;https://issues.apache.org/jira/secure/attachment/13057457/image-2023-04-21-00-54-11-968.png, 20/Apr/23 16:57;yorksity;image-2023-04-21-00-57-29-140.png;https://issues.apache.org/jira/secure/attachment/13057458/image-2023-04-21-00-57-29-140.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 7.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-04-20 15:56:59.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1hf9c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: autoBroadcastJoinThreshold compared to project of a plan not a relation size
Issue key: SPARK-46516
Issue id: 13562947
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gsavinov
Creator: gsavinov
Created: 26/Dec/23 17:07
Updated: 29/Dec/23 15:05
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: From the docs: spark.sql.autoBroadcastJoinThreshold - Configures the maximum size in bytes for a table that will be broadcasted to all worker nodes when performing a join.

[https://spark.apache.org/docs/3.5.0/configuration.html#runtime-sql-configuration]

In fact Spark compares plan.statistics.sizeInBytes  of a project (columns selected in join), not a relation size.

[https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala#L368]

Join can select only a few columns and sizeInBytes will be lesser than autoBroadcastJoinThreshold, but broadcasted table can be huge and it is loaded entirely into drivers memory which can lead to OOM.

spark.sql.autoBroadcastJoinThreshold parameter compared to projection instead of broadcasted table size seems quite risky feature: there will be more broadcasted relations but more chances to get OOM on the driver too.

The solution is to disable spark.sql.autoBroadcastJoinThreshold and set hints on really small relations, but in that case autoBroadcastJoinThreshold seems useless.  It would be more usefull to have autoBroadcastJoinThreshold which campres to relations size and have predicted memory usage on the driver.

 

Original task and test when autobroadcast compared to relation totalSize:

https://issues.apache.org/jira/browse/SPARK-2393

[https://github.com/apache/spark/pull/1238/files#diff-00485e6cae519f81adca5ceee66227c6eae35db709619d505468f8765175ac18R39]

 

Task and PR where autoBroadcastJoinThreshold started to be compared to project of a plan instead of relations:

https://issues.apache.org/jira/browse/SPARK-13329

[https://github.com/apache/spark/pull/11210]

 

Related topic on SO: [https://stackoverflow.com/questions/74435020/how-dataframe-count-selects-broadcasthashjoin-while-dataframe-show-selects-s]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-39667, SPARK-2393
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-12-26 17:07:46.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1mg9k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Repartition + Stage retries could lead to incorrect data 
Issue key: SPARK-38388
Issue id: 13431450
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kings129
Creator: kings129
Created: 02/Mar/22 08:32
Updated: 27/Dec/23 03:25
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.4.0, 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 5
Labels: correctness, data-loss
Description: Spark repartition uses RoundRobinPartitioning, the generated results is non-deterministic when data has some randomness and stage/task retries happen.

The bug can be triggered when upstream data has some randomness, a repartition is called on them, then followed by result stage (could be more stages).
As the pattern shows below:
upstream stage (data with randomness) -> (repartition shuffle) -> result stage

When one executor goes down at result stage, some tasks of that stage might have finished, others would fail, shuffle files on that executor also get lost, some tasks from previous stage (upstream data generation, repartition) will need to rerun to generate dependent shuffle data files.
Because data has some randomness, regenerated data in upstream retried tasks is slightly different, repartition then generates inconsistent ordering, then tasks at result stage will be retried generating different data.

This is similar but different to https://issues.apache.org/jira/browse/SPARK-23207, fix for it uses extra local sort to make the row ordering deterministic, the sorting algorithm it uses simply compares row/record hash. But in this case, upstream data has some randomness, the sorting algorithm doesn't help keep the order, thus RoundRobinPartitioning introduced non-deterministic result.

The following code returns 986415, instead of 1000000:
{code:java}
import scala.sys.process._
import org.apache.spark.TaskContext

case class TestObject(id: Long, value: Double)

val ds = spark.range(0, 1000 * 1000, 1).repartition(100, $"id").withColumn("val", rand()).repartition(100).map { 
  row => if (TaskContext.get.stageAttemptNumber == 0 && TaskContext.get.attemptNumber == 0 && TaskContext.get.partitionId > 97) {
    throw new Exception("pkill -f java".!!)
  }
  TestObject(row.getLong(0), row.getDouble(1))
}

ds.toDF("id", "value").write.mode("overwrite").saveAsTable("tmp.test_table")

spark.sql("select count(distinct id) from tmp.test_table").show{code}
Command: 
{code:java}
spark-shell --num-executors 10 (--conf spark.dynamicAllocation.enabled=false --conf spark.shuffle.service.enabled=false){code}
To simulate the issue, disable external shuffle service is needed (if it's also enabled by default in your environment),  this is to trigger shuffle file loss and previous stage retries.
In our production, we have external shuffle service enabled, this data correctness issue happened when there were node losses.

Although there's some non-deterministic factor in upstream data, user wouldn't expect  to see incorrect result.
Environment: Spark 2.4 and 3.x
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Dec 27 03:25:13 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10308:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Mar/22 08:44;kings129;I think one potential solution is do not allow partial retry of the stage after repartition, even succeeded tasks need to rerun.;;;, 03/Mar/22 19:12;kings129;Hi [~jiangxb1987], would you have any suggestion on how to fix this issue? Thanks!;;;, 03/Mar/22 19:27;jiangxb1987;Have you tried override the `getOutputDeterministicLevel` function in your customized RDD that generated the upstream data? If the data could be non-deterministic, you can return DeterministicLevel.INDETERMINATE, thus every time a children stage need to retry, it restarts tasks for all the partitions (not just the partitions that are lost/failed).;;;, 06/Mar/22 08:16;kings129;[~jiangxb1987] thanks for reply, above repo example uses DataFrame APIs and doesn't use RDD directly, I don't follow how to override the `getOutputDeterministicLevel` function in this case. If I missed something, could you help suggest how to modify above repo code?
I see `getOutputDeterministicLevel` function in RDD is introduced in [https://github.com/apache/spark/pull/22112|https://github.com/apache/spark/pull/22112,] ,does it only help when user create a customized RDD?;;;, 07/Mar/22 18:19;jiangxb1987;> upstream stage (data with randomness)

I assume you are using some customized data source that generated the random data, but didn't correctly set the outputDeterministicLevel. A proper fix is to modify the data source to reflect the nature of data randomness.;;;, 07/Mar/22 18:22;jiangxb1987;I see in your example you used `rand()` function between Repartition operators, I don't think this would be a valid use case, do you really need to do it in production?;;;, 08/Mar/22 18:11;kings129; [~jiangxb1987] using `rand()` in the reproduce example is to simulate the non-deterministic data in our production pipeline, we don't use rand() in production. We have seen two cases got into this incorrect data issue: 1. using .groupByKey(..).mapGroups(..) to pick one of the signals meet certain criteria 2. using row_number() over a window function, then pick first row.
We are not using customized data source, the pipelines read in dataframe/dataset, then use public APIs for transformation.;;;, 08/Mar/22 18:24;kings129;[~cloud_fan] [~mridul] have lots of insight on this issue in [https://github.com/apache/spark/pull/20393] and [https://github.com/apache/spark/pull/22112], could you also help take a look? Appreciate!;;;, 15/Mar/22 22:56;mridulm80;Agree with [~jiangxb1987], either the computation should be repeatable (specify order'ing for example) or it should be marked as nondeterminate (if input source changing order of tuples or computation not being repeatable, etc).;;;, 21/Mar/22 23:56;kings129;Thank you [~mridulm80] ! Wenchen also suggested to propagate the deterministic level in dev email thread: [https://lists.apache.org/thread/z5b8qssg51024nmtvk6gr2skxctl6xcm. |https://lists.apache.org/thread/z5b8qssg51024nmtvk6gr2skxctl6xcm]. I'm looking into it.;;;, 27/Apr/22 21:41;kings129;Hi [~cloud_fan], could you assign this ticket to me? I have bandwidth to work on it in May.

Another possible solution:
Since the root cause is related to non-deterministic data in shuffling, is it possible to let driver to keep checksums of all shuffle blocks, if a map task re-attempt generates shuffle block with different checksum, Spark can detect on-the-fly and rerun all reduce tasks to avoid correctness issue.
I feel this could be a better solution because this is transparent to users, it doesn't require users to explicitly mark their data as nondeterminate. There are challenges for the other solution: 1. It wouldn't be easy to educate regular Spark users about the issue, they might not see the advice or they don't understand the importance of marking DeterministicLevel. 2. Even if they understand, it's hard for users to always remember to mark nondeterminate of their data.

Would do you think?;;;, 19/Jun/22 10:17;tomsisso;Hi
We want to share that we experience a variant of this issue as well (at Taboola, using Spark 3.1.3), and stress that this issue can occur in any scenario where the recalculation won't produce exactly the same rows, not only randomness related or the ones mentioned here.
In our case we suffered from it in multiple jobs that perform simple aggregation over some double/float type column and then repartition it to control the number of output files, for example:
{code:java}
sqlContext.sql(
" SELECT integerColumn, SUM(someDoubleTypeValue) AS value
  FROM data
  GROUP BY integerColumn "
).repartition(3)
.write()
…{code}
In cases where we perform such calculation, we might get slightly different value on a retry due to precision issues.
Similar to what was already explained here, such case will again lead to different hashcode for this row, which will later cause multiple rows to be mapped to different partitions, which results in incorrect data that contains duplicate & missing rows.

It can be reproduced in a similar manner - add failure after some parts already succeeded (& disable external shuffle service, speculation and dynamic allocation):
{code:java}
sqlContext.sql(
" SELECT integerColumn, SUM(someDoubleTypeValue) AS value
  FROM data
  GROUP BY integerColumn "
).repartition(3)
.map((MapFunction<Row, Row>) row -> {
                    if (TaskContext.get().stageAttemptNumber() == 0 &&
                            TaskContext.get().attemptNumber() == 0 &&
                            TaskContext.get().partitionId() > 1) {
                        Thread.sleep(60_000);
                        System.exit(1);
                    }

                    return row;
                }, RowEncoder.apply(schema))
.write()
… {code}
 

We will appreciate if it can get higher priority and think that it should be communicated to users until a proper solution will be implemented, maybe with a warning on the repartition(numPartitions) api, we stopped using it.
Thanks;;;, 23/Jun/22 09:25;loicd38;Hi,

Does this bug occur if you don't specify the number of partitions manually?

For example if you do
spark.range(0, 1000 * 1000, 1).repartition( $"id")

instead of
spark.range(0, 1000 * 1000, 1).repartition(100, $"id")

 

It seems easy to avoid repartition(n ) but repartionning by column seems a very important feature of spark that may be hard to avoid;;;, 27/Dec/23 03:25;alw2018;We had the same problem(using Spark 3.2.1)，is there any plan to fix the problem？;;;
Affects Version/s.1: 3.1.1
Comment.1: 03/Mar/22 19:12;kings129;Hi [~jiangxb1987], would you have any suggestion on how to fix this issue? Thanks!;;;, 27/Apr/22 21:41;kings129;Hi [~cloud_fan], could you assign this ticket to me? I have bandwidth to work on it in May.

Another possible solution:
Since the root cause is related to non-deterministic data in shuffling, is it possible to let driver to keep checksums of all shuffle blocks, if a map task re-attempt generates shuffle block with different checksum, Spark can detect on-the-fly and rerun all reduce tasks to avoid correctness issue.
I feel this could be a better solution because this is transparent to users, it doesn't require users to explicitly mark their data as nondeterminate. There are challenges for the other solution: 1. It wouldn't be easy to educate regular Spark users about the issue, they might not see the advice or they don't understand the importance of marking DeterministicLevel. 2. Even if they understand, it's hard for users to always remember to mark nondeterminate of their data.

Would do you think?;;;, 19/Jun/22 10:17;tomsisso;Hi
We want to share that we experience a variant of this issue as well (at Taboola, using Spark 3.1.3), and stress that this issue can occur in any scenario where the recalculation won't produce exactly the same rows, not only randomness related or the ones mentioned here.
In our case we suffered from it in multiple jobs that perform simple aggregation over some double/float type column and then repartition it to control the number of output files, for example:
{code:java}
sqlContext.sql(
" SELECT integerColumn, SUM(someDoubleTypeValue) AS value
  FROM data
  GROUP BY integerColumn "
).repartition(3)
.write()
…{code}
In cases where we perform such calculation, we might get slightly different value on a retry due to precision issues.
Similar to what was already explained here, such case will again lead to different hashcode for this row, which will later cause multiple rows to be mapped to different partitions, which results in incorrect data that contains duplicate & missing rows.

It can be reproduced in a similar manner - add failure after some parts already succeeded (& disable external shuffle service, speculation and dynamic allocation):
{code:java}
sqlContext.sql(
" SELECT integerColumn, SUM(someDoubleTypeValue) AS value
  FROM data
  GROUP BY integerColumn "
).repartition(3)
.map((MapFunction<Row, Row>) row -> {
                    if (TaskContext.get().stageAttemptNumber() == 0 &&
                            TaskContext.get().attemptNumber() == 0 &&
                            TaskContext.get().partitionId() > 1) {
                        Thread.sleep(60_000);
                        System.exit(1);
                    }

                    return row;
                }, RowEncoder.apply(schema))
.write()
… {code}
 

We will appreciate if it can get higher priority and think that it should be communicated to users until a proper solution will be implemented, maybe with a warning on the repartition(numPartitions) api, we stopped using it.
Thanks;;;, 23/Jun/22 09:25;loicd38;Hi,

Does this bug occur if you don't specify the number of partitions manually?

For example if you do
spark.range(0, 1000 * 1000, 1).repartition( $"id")

instead of
spark.range(0, 1000 * 1000, 1).repartition(100, $"id")

 

It seems easy to avoid repartition(n ) but repartionning by column seems a very important feature of spark that may be hard to avoid;;;, 27/Dec/23 03:25;alw2018;We had the same problem(using Spark 3.2.1)，is there any plan to fix the problem？;;;
Comment.2: 03/Mar/22 19:27;jiangxb1987;Have you tried override the `getOutputDeterministicLevel` function in your customized RDD that generated the upstream data? If the data could be non-deterministic, you can return DeterministicLevel.INDETERMINATE, thus every time a children stage need to retry, it restarts tasks for all the partitions (not just the partitions that are lost/failed).;;;
Comment.3: 06/Mar/22 08:16;kings129;[~jiangxb1987] thanks for reply, above repo example uses DataFrame APIs and doesn't use RDD directly, I don't follow how to override the `getOutputDeterministicLevel` function in this case. If I missed something, could you help suggest how to modify above repo code?
I see `getOutputDeterministicLevel` function in RDD is introduced in [https://github.com/apache/spark/pull/22112|https://github.com/apache/spark/pull/22112,] ,does it only help when user create a customized RDD?;;;
Comment.4: 07/Mar/22 18:19;jiangxb1987;> upstream stage (data with randomness)

I assume you are using some customized data source that generated the random data, but didn't correctly set the outputDeterministicLevel. A proper fix is to modify the data source to reflect the nature of data randomness.;;;
Comment.5: 07/Mar/22 18:22;jiangxb1987;I see in your example you used `rand()` function between Repartition operators, I don't think this would be a valid use case, do you really need to do it in production?;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Pyspark undertakes pruning of decision trees and random forests outside the control of the user, leading to undesirable and unexpected outcomes that are challenging to diagnose and impossible to correct
Issue key: SPARK-34591
Issue id: 13361782
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: alpha137
Creator: alpha137
Created: 02/Mar/21 08:09
Updated: 03/Oct/23 11:12
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.4.0, 2.4.4, 3.1.1
Fix Version/s: 
Component/s: ML
Due Date: 
Votes: 1
Labels: pull-request-available, pyspark
Description: *History of the issue*

SPARK-3159 implemented a method designed to reduce the computational burden for predictions from decision trees and random forests by pruning the tree after fitting. This is done in such a way that branches where child leaves all produce the same classification prediction are merged.

This was implemented via a PR: [https://github.com/apache/spark/pull/20632]

This feature is controllable by a "prune" parameter in the Scala version of the code, which is set to True as the default behaviour. However, this parameter is not exposed in the Pyspark API, resulting in the behaviour above:
 * Occurring always (despite the user may not wanting it to occur)
 * Not being documented in the ML documentation, leading to decision tree behavoiur that may be in conflict with what the user expects to happen

*Why is this a problem?*

+Problem 1: Inaccurate probabilities+

Because the decision to prune is based on the classification prediction from the tree (not the probability prediction from the node), this introduces additional bias compared to the situation where the pruning is not done. The impact here may be severe in some cases

+Problem 2: Leads to completely unacceptable behaviours in some circumstances and for some hyper-parameters+

My colleagues and I encountered this bug in a scenario where we could not get a decision tree classifier (or random forest classifier with a single tree) to split a single node, despite this being eminently supported by the data. This renders the decision trees and random forests complete unusable

+Problem 3: Outcomes are highly sensitive to the hyper-parameters chosen, and how they interact with the data+

Small changes in the hyper-parameters should ideally produce small changes in the built trees. However, here we have found that small changes in the hyper-parameters lead to large and unpredictable changes in the resultant trees as a result of this pruning.

In principle, this high degree of instability means that re-training the same model, with the same hyper-parameter settings, on slightly different data may lead to large variations in the tree structure simply as a result of the pruning

+Problem 4: The problems above are much worse for unbalanced data sets+

Probability estimation on unbalanced data sets using trees should be supported, but the pruning method described will make this very difficult

+Problem 5: This pruning method is a substantial variation from the description of the decision tree algorithm in the MLLib documents and is not described+

This made it extremely confusing for us in working out why we were seeing certain behaviours - we had to trace back through all of the Spark detailed release notes to identify where the problem might.

*Proposed solutions*

+Option 1 (much easier):+

The proposed solution here is:
 * Set the default pruning behaviour to False rather than True, thereby bringing the default behaviour back into alignment with the documentation whilst avoiding the issues described above

+Option 2 (more involved):+

The proposed solution here is:
 * Leave the default pruning behaviour set to False
 * Expand the pyspark API to expose the pruning behaviour as a user-controllable option
 * Document the change to the API
 * Document the change to the tree building behaviour at appropriate points in the Spark ML and Spark MLLib documentation

We recommend that the default behaviour be set to False because this approach is not the generally understood approach for building decision trees, where pruning is decided a separate and user-controllable step.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): SPARK-14045
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): SPARK-3159
Inward issue link (Reference): SPARK-14046
Outward issue link (Reference): 
Attachment: 08/Jun/21 12:37;alpha137;Reproducible example of Spark bug - no 2.pdf;https://issues.apache.org/jira/secure/attachment/13026544/Reproducible+example+of+Spark+bug+-+no+2.pdf, 08/Jun/21 00:09;alpha137;Reproducible example of Spark bug.pdf;https://issues.apache.org/jira/secure/attachment/13026511/Reproducible+example+of+Spark+bug.pdf
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 22 14:51:45 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0o794:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Mar/21 08:13;alpha137;FYI [~asolimando];;;, 24/Mar/21 14:46;srowen;(Not nearly Critical)
Yes please open a PR to add the parameter;;;, 08/Jun/21 00:11;alpha137;Here is a reproducible example of this bug which demonstrates a "maximally worst" outcome, ie the tree has no splits whatsoever. 

[^Reproducible example of Spark bug.pdf]

 

[~srowen], I disagree about this being minor. Based on this, I consider both DecisionTreeClassifier and RandomForestClassifier to be functionally broken. ;;;, 08/Jun/21 00:21;srowen;I think it's OK to at least expose the parameter in Pyspark, if that seems to address it.
Off the top of my head, in this example, there is no real signal as the features and label are random. It's perhaps just not able to find any way to split the data, given defaults, that results in progress in predicting the label?;;;, 08/Jun/21 00:26;alpha137;The fact that there's no signal isn't the issue. The issue is that Spark is undertaking modifications to the tree in a way which are not consistent with what the documentation describes it does.

This is a trivial example with no signal, but I've seen this on actual data sets that definitely have signal (that I cannot share for confidentiality reasons) with the same behaviour. 

Mechanically, decision trees can always keep splitting until there is only one data point per node. Using either the entropy or gini loss functions mathematically guarantees that you can find a split with an improved objective function in the sum of the children vs the parent. 

[~CBribiescas] has confirmed that when you disable the pruning parameter you get the desired behaviour. We will endeavour to submit a pull request soon. 

The original PR for SPARK-3159 should never have been accepted in the first place because it modifies the DT algo in a non-trivial way, and this change was not described in the Spark documentation.;;;, 08/Jun/21 04:32;apachespark;User 'CBribiescas' has created a pull request for this issue:
https://github.com/apache/spark/pull/32813;;;, 08/Jun/21 04:32;apachespark;User 'CBribiescas' has created a pull request for this issue:
https://github.com/apache/spark/pull/32813;;;, 08/Jun/21 12:37;alpha137;To address any concerns about the example above being generated with random data, here's an example where there is very definitely signal in the data. 

[^Reproducible example of Spark bug - no 2.pdf]


The a priori rules used to generate the data are:
 * If x < 0.5, p = 0.02
 * If x >= 0.5, p = 0.3

10^6 data points are generated using this rule with Bernoulli simulation used to generate 0/1 outcomes.

The resultant DecisionTreeClassifier from Spark 2.4.6 has depth 0 - it is is "unable" to "find" even very obvious structure in the data, on account of the pruning. (Disabling the pruning fixes the problem.)

 ;;;, 22/Sep/21 14:51;rafael_hernandez;I noticed this behavior of the DecisionTreeClassifier by chance in a use case when upgrading from Spark 2.2.3 to 2.4.7

I do not understand how it is possible that this issue is not critical. 

Regarding unbalanced data sets, both DecisionTreeClassifier and RandomForestClassifier are useless for many uses cases that we are working on.

I hope that that this will be fix soon, even if it is just by setting the default pruning behaviour to False.

I saw the opened PR but I'm worry that we'll have to wait a lot for the solution. ;;;
Affects Version/s.1: 2.4.4
Comment.1: 24/Mar/21 14:46;srowen;(Not nearly Critical)
Yes please open a PR to add the parameter;;;
Comment.2: 08/Jun/21 00:11;alpha137;Here is a reproducible example of this bug which demonstrates a "maximally worst" outcome, ie the tree has no splits whatsoever. 

[^Reproducible example of Spark bug.pdf]

 

[~srowen], I disagree about this being minor. Based on this, I consider both DecisionTreeClassifier and RandomForestClassifier to be functionally broken. ;;;
Comment.3: 08/Jun/21 00:21;srowen;I think it's OK to at least expose the parameter in Pyspark, if that seems to address it.
Off the top of my head, in this example, there is no real signal as the features and label are random. It's perhaps just not able to find any way to split the data, given defaults, that results in progress in predicting the label?;;;
Comment.4: 08/Jun/21 00:26;alpha137;The fact that there's no signal isn't the issue. The issue is that Spark is undertaking modifications to the tree in a way which are not consistent with what the documentation describes it does.

This is a trivial example with no signal, but I've seen this on actual data sets that definitely have signal (that I cannot share for confidentiality reasons) with the same behaviour. 

Mechanically, decision trees can always keep splitting until there is only one data point per node. Using either the entropy or gini loss functions mathematically guarantees that you can find a split with an improved objective function in the sum of the children vs the parent. 

[~CBribiescas] has confirmed that when you disable the pruning parameter you get the desired behaviour. We will endeavour to submit a pull request soon. 

The original PR for SPARK-3159 should never have been accepted in the first place because it modifies the DT algo in a non-trivial way, and this change was not described in the Spark documentation.;;;
Comment.5: 08/Jun/21 04:32;apachespark;User 'CBribiescas' has created a pull request for this issue:
https://github.com/apache/spark/pull/32813;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Do not combine multiple Generate operators in the same WholeStageCodeGen node because it can  easily cause OOM failures if arrays are relatively large
Issue key: SPARK-44759
Issue id: 13546803
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tafranky@gmail.com
Creator: tafranky@gmail.com
Created: 10/Aug/23 09:23
Updated: 13/Aug/23 06:03
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1
Fix Version/s: 
Component/s: Deploy, Optimizer, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: This is an issue since the WSCG  implementation of the generate node. 

Because WSCG compute rows in batches , the combination of WSCG and the explode operation consume a lot of the dedicated executor memory. This is even more true when the WSCG node contains multiple explode nodes. This is the case when flattening a nested array.

The generate node used to flatten array generally  produces an amount of output rows that is significantly higher than the input rows.

the number of output rows generated is even drastically higher when flattening a nested array .

When we combine more that 1 generate node in the same WholeStageCodeGen  node, we run  a high risk of running out of memory for multiple reasons. 

1- As you can see from snapshots added in the comments ,  the rows created in the nested loop are saved in a writer buffer.  In this case because the rows were big , the job failed with an Out Of Memory Exception error .

2_ The generated WholeStageCodeGen result in a nested loop that for each row  , will explode the parent array and then explode the inner array.  The rows are accumulated in the writer buffer without accounting for the row size.

Please view the attached Spark Gui and Spark Dag 

In my case the wholestagecodegen includes 2 explode nodes. 

Because the array elements are large , we end up with an Out Of Memory error. 

 

I recommend that we do not merge  multiple explode nodes in the same whole stage code gen node . Doing so leads to potential memory issues.

In our case , the job execution failed with an  OOM error because the the WSCG executed  into a nested for loop . 

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 10/Aug/23 16:27;tafranky@gmail.com;image-2023-08-10-09-27-24-124.png;https://issues.apache.org/jira/secure/attachment/13062036/image-2023-08-10-09-27-24-124.png, 10/Aug/23 16:29;tafranky@gmail.com;image-2023-08-10-09-29-24-804.png;https://issues.apache.org/jira/secure/attachment/13062037/image-2023-08-10-09-29-24-804.png, 10/Aug/23 16:32;tafranky@gmail.com;image-2023-08-10-09-32-46-163.png;https://issues.apache.org/jira/secure/attachment/13062038/image-2023-08-10-09-32-46-163.png, 10/Aug/23 16:33;tafranky@gmail.com;image-2023-08-10-09-33-47-788.png;https://issues.apache.org/jira/secure/attachment/13062039/image-2023-08-10-09-33-47-788.png, 10/Aug/23 09:25;tafranky@gmail.com;wholestagecodegen_wc1_debug_wholecodegen_passed;https://issues.apache.org/jira/secure/attachment/13062030/wholestagecodegen_wc1_debug_wholecodegen_passed
Custom field (Affects version (Component)): 
Custom field (Attachment count): 5.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 10 16:34:45 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1jpe8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Aug/23 16:27;tafranky@gmail.com;WSCG  generated code that calls  generate_doConsume_0

!image-2023-08-10-09-27-24-124.png!;;;, 10/Aug/23 16:29;tafranky@gmail.com;WSCG  generated code for first Generate node 

!image-2023-08-10-09-29-24-804.png!;;;, 10/Aug/23 16:33;tafranky@gmail.com;WSCG  generated code for second Generate node 

!image-2023-08-10-09-32-46-163.png!;;;, 10/Aug/23 16:34;tafranky@gmail.com;Spark Dag for the use case . The failure is from the execution of WholeStageCodeGen(2)

!image-2023-08-10-09-33-47-788.png!;;;
Affects Version/s.1: 3.0.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1
Comment.1: 10/Aug/23 16:29;tafranky@gmail.com;WSCG  generated code for first Generate node 

!image-2023-08-10-09-29-24-804.png!;;;
Comment.2: 10/Aug/23 16:33;tafranky@gmail.com;WSCG  generated code for second Generate node 

!image-2023-08-10-09-32-46-163.png!;;;
Comment.3: 10/Aug/23 16:34;tafranky@gmail.com;Spark Dag for the use case . The failure is from the execution of WholeStageCodeGen(2)

!image-2023-08-10-09-33-47-788.png!;;;
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.12.0, EMR-6.13.0, EMR-6.14.0, EMR-6.15.0, EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Reject disk blocks when out of disk space
Issue key: SPARK-34337
Issue id: 13356310
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 02/Feb/21 22:25
Updated: 09/Aug/23 18:34
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Now that we have the ability to store shuffle blocks on dis-aggregated storage (when configured) we should add the option to reject storing blocks locally on an executor at a certain disk pressure threshold.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Feb 04 17:35:47 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0na5c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 4.0.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Feb/21 17:35;holden;Initially we should allow the user to configure a maximum amount of shuffle blocks to be stored. In the future we can try and use underlying FS info.;;;
Affects Version/s.1: 3.1.2
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Driver can't distribute task to executor because NullPointerException
Issue key: SPARK-35914
Issue id: 13386210
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Heltman
Creator: Heltman
Created: 28/Jun/21 08:25
Updated: 26/Jul/23 06:22
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: When use spark3 submit a spark job to yarn cluster, I get a problem. Once in a while, driver can't distribute any tasks to any executors, and the stage will stuck , the total spark job will stuck. Check driver log, I found NullPointerException. It's like a netty problem, I can confirm this problem only exist in spark3, because I use spark2 never happend.

 
{code:java}
// Error message
21/06/28 14:42:43 INFO TaskSetManager: Starting task 2592.0 in stage 1.0 (TID 3494) (worker39.hadoop, executor 84, partition 2592, RACK_LOCAL, 5006 bytes) taskResourceAssignments Map()
21/06/28 14:42:43 INFO TaskSetManager: Finished task 4155.0 in stage 1.0 (TID 3367) in 36670 ms on worker39.hadoop (executor 84) (3278/4249)
21/06/28 14:42:43 INFO TaskSetManager: Finished task 2283.0 in stage 1.0 (TID 3422) in 22371 ms on worker15.hadoop (executor 109) (3279/4249)
21/06/28 14:42:43 ERROR Inbox: Ignoring error
java.lang.NullPointerException
	at java.lang.String.length(String.java:623)
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:420)
	at java.lang.StringBuilder.append(StringBuilder.java:136)
	at org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$5(TaskSetManager.scala:483)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.scheduler.TaskSetManager.logInfo(TaskSetManager.scala:54)
	at org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:484)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:444)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:581)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:576)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:576)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:547)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:547)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.$anonfun$makeOffers$5(CoarseGrainedSchedulerBackend.scala:340)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$withLock(CoarseGrainedSchedulerBackend.scala:904)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers(CoarseGrainedSchedulerBackend.scala:332)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:157)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
21/06/28 14:42:43 INFO TaskSetManager: Finished task 2255.0 in stage 1.0 (TID 3419) in 23035 ms on worker15.hadoop (executor 116) (3280/4249)
21/06/28 14:42:43 ERROR Inbox: Ignoring error
java.lang.NullPointerException
	at java.lang.String.length(String.java:623)
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:420)
	at java.lang.StringBuilder.append(StringBuilder.java:136)
	at org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$5(TaskSetManager.scala:483)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.scheduler.TaskSetManager.logInfo(TaskSetManager.scala:54)
	at org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:484)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:444)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:581)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:576)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:576)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:547)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:547)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.$anonfun$makeOffers$5(CoarseGrainedSchedulerBackend.scala:340)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$withLock(CoarseGrainedSchedulerBackend.scala:904)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers(CoarseGrainedSchedulerBackend.scala:332)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:157)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
.....

21/06/28 14:43:44 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 3486) in 64503 ms on worker02.hadoop (executor 59) (3427/4249)
21/06/28 14:43:44 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:45 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:46 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:47 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:48 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:49 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:49 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 3491) in 66320 ms on worker02.hadoop (executor 62) (3428/4249)
21/06/28 14:43:49 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:50 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:51 ERROR Inbox: Ignoring error
java.lang.NullPointerException{code}
 

 

The first time I found this problem in spark3.0.1, I checked spark source code, the NPE happended in  *org.apache.spark.scheduler.TaskSetManager.logInfo*, source code like below:
{code:java}
logInfo(s"Starting $taskName (TID $taskId, $host, executor ${info.executorId}, " +
    s"partition ${task.partitionId}, $taskLocality, ${serializedTask.limit()} bytes)")
{code}
I tried to confirm which variable is null, so I add log and repack spark-core like below:
{code:java}
// add log:
logInfo(s"zyh Starting $taskName")
logInfo(s"zyh (TID $taskId")
logInfo(s"zyh $host")
logInfo(s"zyh executor ${info.executorId}")
logInfo(s"zyh ${task.partitionId}")
logInfo(s"zyh $taskLocality,")
logInfo(s"zyh ${serializedTask.limit()} bytes)")

// repack command
./build/mvn -DskipTests -pl :spark-core_2.12 clean install{code}
Then I got *host* is null, try print:
{code:java}
println(host) // result is Some(null)
println(host.getClass.getName) // result is scala.Some
{code}
track host, add log everywhere to print it:

{{TaskSchedulerImpl.resourceOfferSingleTaskSet}} -> {{TaskSetManager.resourceOffer}}-> {{TaskSetManager.dequeueTask}} -> {{TaskSetManager.logInfo}}

Strange host is nomal ! But executorId become null... This two variable came from one case class  *WorkerOffer*

So I add log to print executorId everywhere, the problem disappered !!!
{code:java}
// TaskSchedulerImpl.resourceOfferSingleTaskSet
 private def resourceOfferSingleTaskSet(
      taskSet: TaskSetManager,
      maxLocality: TaskLocality,
      shuffledOffers: Seq[WorkerOffer],
      availableCpus: Array[Int],
      availableResources: Array[Map[String, Buffer[String]]],
      tasks: IndexedSeq[ArrayBuffer[TaskDescription]],
      addressesWithDescs: ArrayBuffer[(String, TaskDescription)]) : Boolean = {
    var launchedTask = false    // nodes and executors that are blacklisted for the entire application have already been
    // filtered out by this point
    for (i <- 0 until shuffledOffers.size) {
      val execId = shuffledOffers(i).executorId
      val host = shuffledOffers(i).host
      if (availableCpus(i) >= CPUS_PER_TASK &&
        resourcesMeetTaskRequirements(availableResources(i))) {
        try {
          val time = System.currentTimeMillis()
          // the first print
          // scalastyle:off println
          println(s"==zyh1== $time" + host.getClass.getName)
          println(s"==zyh1== $time" + execId.getClass.getName)
          for (task <- taskSet.resourceOffer(execId, host, maxLocality, availableResources(i))) {// TaskSetManager.resourceOffer
  def resourceOffer(
      execId: String,
      host: String,
      maxLocality: TaskLocality.TaskLocality,
      availableResources: Map[String, Seq[String]] = Map.empty)
    : Option[TaskDescription] =
  {
    val time = System.currentTimeMillis()
    // the second print
    // scalastyle:off println
    println(s"==zyh2== $time" + host.getClass.getName)
    println(s"==zyh2== $time" + execId.getClass.getName)
      
    val offerBlacklisted = taskSetBlacklistHelperOpt.exists { blacklist =>
      blacklist.isNodeBlacklistedForTaskSet(host) ||
        blacklist.isExecutorBlacklistedForTaskSet(execId)
    }
    if (!isZombie && !offerBlacklisted) {
      val curTime = clock.getTimeMillis()      var allowedLocality = maxLocality      if (maxLocality != TaskLocality.NO_PREF) {
        allowedLocality = getAllowedLocalityLevel(curTime)
        if (allowedLocality > maxLocality) {
          // We're not allowed to search for farther-away tasks
          allowedLocality = maxLocality
        }
      }      dequeueTask(execId, host, allowedLocality).map { case ((index, taskLocality, speculative)) =>
          
// TaskSetManager.dequeueTask
private def dequeueTask(
      execId: String,
      host: String,
      maxLocality: TaskLocality.Value): Option[(Int, TaskLocality.Value, Boolean)] = {
    // Tries to schedule a regular task first; if it returns None, then schedules
    // a speculative task
    val time = System.currentTimeMillis()
    // the third print
    // scalastyle:off println
    println(s"==zyh3== $time" + host.getClass.getName)
    println(s"==zyh3== $time" + execId.getClass.getName)
    dequeueTaskHelper(execId, host, maxLocality, false).orElse(
      dequeueTaskHelper(execId, host, maxLocality, true))
  }
          
// TaskSetManager.resourceOffer.dequeueTask，上面返回后进入map时再次打印
dequeueTask(execId, host, allowedLocality).map { case ((index, taskLocality, speculative)) =>
        // Found a task; do some bookkeeping and return a task description
        val time = System.currentTimeMillis()
        // // the fourth print
        // scalastyle:off println
        println(s"==zyh4== $time" + host.getClass.getName)
        println(s"==zyh4== $time" + execId.getClass.getName)
    
// TaskSetManager.logInfo
    // try catch NPE
        val taskName = s"task ${info.id} in stage ${taskSet.id}"
        try {
          logInfo(s"Starting $taskName (TID $taskId, $host, executor ${info.executorId}, " +
            s"partition ${task.partitionId}, $taskLocality, ${serializedTask.limit()} bytes)")
        } catch {
          case e: NullPointerException =>
            val time = System.currentTimeMillis()
            e.printStackTrace()
            // scalastyle:off println
            println("+++zyh+++" + host.getClass.getName + "  " + taskName.getClass.getName)
            println("+++zyh+++" + taskId.getClass.getName + "  " + info.executorId.getClass.getName)
            println("+++zyh+++" + task.partitionId.getClass.getName)
            println("+++zyh+++" + taskLocality.getClass.getName)
            println("+++zyh+++" + serializedTask.limit().getClass.getName)
        }
{code}
This problem make it's difficult to trust a spark job is running or not, only when I check spark web ui and found it's not any running task, I can kill it by hand. I test on spark3.0.1 3.1.1 3.1.2, it's same problem.
Environment: hadoop 2.6.0-cdh5.7.1

Spark 3.0.1, 3.1.1, 3.1.2
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 28/Jun/21 09:07;Heltman;stuck log.png;https://issues.apache.org/jira/secure/attachment/13027349/stuck+log.png, 28/Jun/21 09:07;Heltman;webui stuck.png;https://issues.apache.org/jira/secure/attachment/13027348/webui+stuck.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 26 06:22:05 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sczk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Jun/21 03:02;Heltman;I guess this problem is related to hadoop-version, I use CDH-5.7.1:hadoop-2.6.5, spark3 use hadoop-2.7. Because the other problem I found in spark about webui, it's caused by the version. I will try higher hadoop-version to confirm it.

[SPARK-35802] Error loading the stages/stage/<id> page in spark UI - ASF JIRA (apache.org);;;, 29/Jun/21 14:12;code_kr_dev_s;[~Heltman] Hi, I want to work upon this issue. If you are not working can I work upon it?;;;, 30/Jun/21 01:21;Heltman;Do what you wanna do, [~code_kr_dev_s], but this problem is difficult to recurrent, it's random occurrence. If you need any help, tell me!;;;, 26/Jul/23 06:22;bhargwa;Hey,

We are facing similar issue and we are using spark3.1.1 with hadoop3.2.
Is this issue resolved in further versions ? If yes, can you guys let me know the fixed version.
OR If there is any workaround to solve this issue, Please let me know.;;;
Affects Version/s.1: 3.1.1
Comment.1: 29/Jun/21 14:12;code_kr_dev_s;[~Heltman] Hi, I want to work upon this issue. If you are not working can I work upon it?;;;
Comment.2: 30/Jun/21 01:21;Heltman;Do what you wanna do, [~code_kr_dev_s], but this problem is difficult to recurrent, it's random occurrence. If you need any help, tell me!;;;
Comment.3: 26/Jul/23 06:22;bhargwa;Hey,

We are facing similar issue and we are using spark3.1.1 with hadoop3.2.
Is this issue resolved in further versions ? If yes, can you guys let me know the fixed version.
OR If there is any workaround to solve this issue, Please let me know.;;;
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Wrong size estimation leads to "Cannot broadcast the table that is larger than 8GB: 8 GB"
Issue key: SPARK-37321
Issue id: 13411668
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: igreenfi
Creator: igreenfi
Created: 14/Nov/21 14:34
Updated: 19/May/23 06:48
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.0
Fix Version/s: 
Component/s: Optimizer, SQL
Due Date: 
Votes: 0
Labels: 
Description: When CBO is enabled then a situation occurs where spark tries to broadcast very large DataFrame due to wrong output size estimation.

 

In `EstimationUtils.getSizePerRow`, if there is no statistics then spark will use `DataType.defaultSize`.

In the case where the output contains `functions.concat_ws`, the `getSizePerRow` function will estimate the size to be 20 bytes, while in our case the actual size can be a lot larger.

As a result, we in some cases end up with an estimated size of < 300K while the actual size can be > 8GB, thus leading to exceptions as spark thinks the tables may be broadcast but later realizes the data size is too large.

 

Code sample to reproduce:

for running that I used `-Xmx45G`
{code:scala}
import spark.implicits._

(1 to 100000).toDF("index").withColumn("index", col("index").cast("string")).write.parquet("/tmp/a")
(1 to 1000).toDF("index_b").withColumn("index_b", col("index_b").cast("string")).write.parquet("/tmp/b")

val a = spark.read
   .parquet("/tmp/a")
   .withColumn("b", col("index"))
   .withColumn("l1", functions.concat_ws("/", col("index"), functions.current_date(), functions.current_date(), functions.current_date(), functions.current_date()))
   .withColumn("l2", functions.concat_ws("/", col("index"), functions.current_date(), functions.current_date(), functions.current_date(), functions.current_date()))
   .withColumn("l3", functions.concat_ws("/", col("index"), functions.current_date(), functions.current_date(), functions.current_date(), functions.current_date()))
   .withColumn("l4", functions.concat_ws("/", col("index"), functions.current_date(), functions.current_date(), functions.current_date(), functions.current_date()))
   .withColumn("l5", functions.concat_ws("/", col("index"), functions.current_date(), functions.current_date(), functions.current_date(), functions.current_date()))

val r = Random.alphanumeric
val l = 220
val i = 2800

val b = spark.read
   .parquet("/tmp/b")
   .withColumn("l1", functions.concat_ws("/", (0 to i).flatMap(a => List(col("index_b"), lit(r.take(l).mkString), lit(r.take(l).mkString))): _*))
   .withColumn("l2", functions.concat_ws("/", (0 to i).flatMap(a => List(col("index_b"), lit(r.take(l).mkString), lit(r.take(l).mkString))): _*))
   .withColumn("l3", functions.concat_ws("/", (0 to i).flatMap(a => List(col("index_b"), lit(r.take(l).mkString), lit(r.take(l).mkString))): _*))
   .withColumn("l4", functions.concat_ws("/", (0 to i).flatMap(a => List(col("index_b"), lit(r.take(l).mkString), lit(r.take(l).mkString))): _*))
   .withColumn("l5", functions.concat_ws("/", (0 to i).flatMap(a => List(col("index_b"), lit(r.take(l).mkString), lit(r.take(l).mkString))): _*))
   .withColumn("l6", functions.concat_ws("/", (0 to i).flatMap(a => List(col("index_b"), lit(r.take(l).mkString), lit(r.take(l).mkString))): _*))
   .withColumn("l7", functions.concat_ws("/", (0 to i).flatMap(a => List(col("index_b"), lit(r.take(l).mkString), lit(r.take(l).mkString))): _*))
 
a.join(b, col("index") === col("index_b")).show(2000)
{code}
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-43589
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 04 15:04:16 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wptc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Aug/22 15:04;cabral1888;I'm facing the same situation too. You can see more details about my case here: [https://stackoverflow.com/questions/72793116/migration-from-spark-2-4-0-to-spark-3-1-1-caused-sortmergejoin-to-change-to-broa.|https://stackoverflow.com/questions/72793116/migration-from-spark-2-4-0-to-spark-3-1-1-caused-sortmergejoin-to-change-to-broa] In my case, the dataset is not reaching the upper value of 8G yet, but we are facing OOM and BHJ timeout issues. After some research, I found this issue https://issues.apache.org/jira/browse/HIVE-20079. Apparently, Hive has a bug that was solved in v4.0.0 regarding statistics calculation for parquet files.;;;
Affects Version/s.1: 3.2.0
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0

Summary: Fix remote table location based on database location
Issue key: SPARK-39203
Issue id: 13445307
Parent id: 
Issue Type: Bug
Status: Reopened
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yumwang
Creator: yumwang
Created: 17/May/22 03:25
Updated: 21/Apr/23 01:30
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.2.0, 2.3.0, 2.4.0, 3.0.0, 3.1.0, 3.1.1, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: We have HDFS and Hive on cluster A. We have Spark on cluster B and need to read data from cluster A. The table location is incorrect:
{noformat}
spark-sql> desc formatted  default.test_table;
fas_acct_id         	decimal(18,0)
fas_acct_cd         	string
cmpny_cd            	string
entity_id           	string
cre_date            	date
cre_user            	string
upd_date            	timestamp
upd_user            	string

# Detailed Table Information
Database             default
Table               	test_table
Type                	EXTERNAL
Provider            	parquet
Statistics          	25310025737 bytes
Location            	/user/hive/warehouse/test_table
Serde Library       	org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe
InputFormat         	org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat
OutputFormat        	org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat
Storage Properties  	[compression=snappy]

spark-sql> desc database default;
Namespace Name	default
Comment
Location	viewfs://clusterA/user/hive/warehouse/
Owner     hive_dba
{noformat}

The correct table location should be viewfs://clusterA/user/hive/warehouse/test_table.
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Apr 21 01:30:10 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12f88:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/May/22 10:23;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36625;;;, 31/May/22 10:09;yumwang;Issue resolved by pull request 36625
[https://github.com/apache/spark/pull/36625];;;, 20/Oct/22 12:48;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/38321;;;, 20/Apr/23 09:11;githubbot;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40871;;;, 21/Apr/23 01:29;gurwls223;Reverted in https://github.com/apache/spark/pull/40871;;;, 21/Apr/23 01:30;gurwls223;But to be clear, this change exists in Spark 3.4.0.
It was taken out from 3.4.1 and 3.5.0.;;;
Affects Version/s.1: 2.3.0
Comment.1: 31/May/22 10:09;yumwang;Issue resolved by pull request 36625
[https://github.com/apache/spark/pull/36625];;;
Comment.2: 20/Oct/22 12:48;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/38321;;;
Comment.3: 20/Apr/23 09:11;githubbot;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40871;;;
Comment.4: 21/Apr/23 01:29;gurwls223;Reverted in https://github.com/apache/spark/pull/40871;;;
Comment.5: 21/Apr/23 01:30;gurwls223;But to be clear, this change exists in Spark 3.4.0.
It was taken out from 3.4.1 and 3.5.0.;;;
EMR Versions: EMR-6.12.0, EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Spark SQL configs not respected in RDDs
Issue key: SPARK-35324
Issue id: 13376886
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: rshkv
Creator: rshkv
Created: 05/May/21 23:12
Updated: 06/Apr/23 12:00
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.2, 3.1.1
Fix Version/s: 
Component/s: Input/Output, SQL
Due Date: 
Votes: 0
Labels: 
Description: When reading a CSV file, {{spark.sql.timeParserPolicy}} is respected in actions on the resulting dataframe. But it's ignored in actions on dataframe's RDD.

E.g. say to parse dates in a CSV you need {{spark.sql.timeParserPolicy}} to be set to {{LEGACY}}. If you set the config, {{df.collect}} will work as you'd expect. However, {{df.collect.rdd}} will fail because it'll ignore the override and read the config value as {{EXCEPTION}}.

For instance:
{code:java|title=test.csv}
date
2/6/18
{code}
{code:java}
scala> spark.conf.set("spark.sql.legacy.timeParserPolicy", "legacy")

scala> val df = {
     |   spark.read
     |     .option("header", "true")
     |     .option("dateFormat", "MM/dd/yy")
     |     .schema("date date")
     |     .csv("/Users/wraschkowski/Downloads/test.csv")
     | }
df: org.apache.spark.sql.DataFrame = [date: date]

scala> df.show
+----------+                                                                    
|      date|
+----------+
|2018-02-06|
+----------+


scala> df.count
res3: Long = 1

scala> df.rdd.count
21/05/06 00:06:18 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)
org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse '2/6/18' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601DateFormatter.$anonfun$parse$1(DateFormatter.scala:61)
	at scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.util.Iso8601DateFormatter.parse(DateFormatter.scala:58)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$21(UnivocityParser.scala:202)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:238)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$20(UnivocityParser.scala:200)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:291)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:254)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:396)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)
	at org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:400)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1866)
	at org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)
	at org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.time.format.DateTimeParseException: Text '2/6/18' could not be parsed at index 0
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)
	at org.apache.spark.sql.catalyst.util.Iso8601DateFormatter.$anonfun$parse$1(DateFormatter.scala:59)
	... 32 more
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-38328
Attachment: 05/May/21 23:49;rshkv;Screen Shot 2021-05-06 at 00.33.10.png;https://issues.apache.org/jira/secure/attachment/13025063/Screen+Shot+2021-05-06+at+00.33.10.png, 05/May/21 23:49;rshkv;Screen Shot 2021-05-06 at 00.35.10.png;https://issues.apache.org/jira/secure/attachment/13025064/Screen+Shot+2021-05-06+at+00.35.10.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Apr 06 12:00:21 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qrk0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/May/21 23:24;rshkv;Same reading JSON:

{code:title=test.json}
{"date": "2/6/18"}
{code}

{code}
scala> spark.conf.set("spark.sql.legacy.timeParserPolicy", "legacy")

scala> spark.read.option("dateFormat", "dd/MM/yy").schema("date date").json("/Users/wraschkowski/Downloads/test.json").collect
res5: Array[org.apache.spark.sql.Row] = Array([2018-06-02])

scala> spark.read.option("dateFormat", "dd/MM/yy").schema("date date").json("/Users/wraschkowski/Downloads/test.json").rdd.collect
21/05/06 00:22:27 ERROR Executor: Exception in task 0.0 in stage 4.0 (TID 4)
org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse '2/6/18' in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)
	at org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.util.Iso8601DateFormatter.$anonfun$parse$1(DateFormatter.scala:61)
	at scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.util.Iso8601DateFormatter.parse(DateFormatter.scala:58)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$10$1.applyOrElse(JacksonParser.scala:260)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$10$1.applyOrElse(JacksonParser.scala:257)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:343)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$10(JacksonParser.scala:257)
	at org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject(JacksonParser.scala:397)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:96)
	at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:95)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:343)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:95)
	at org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:467)
	at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2611)
	at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:462)
	at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$7(JsonDataSource.scala:140)
	at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)
	at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$readFile$9(JsonDataSource.scala:144)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at scala.collection.AbstractIterator.to(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1429)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.time.format.DateTimeParseException: Text '2/6/18' could not be parsed at index 0
	at java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)
	at java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)
	at org.apache.spark.sql.catalyst.util.Iso8601DateFormatter.$anonfun$parse$1(DateFormatter.scala:59)
	... 53 more
{code};;;, 05/May/21 23:25;rshkv;For what it's worth, I only managed to reproduce with a reader. Creating a dataframe from a {{Seq}} works fine:
{code:java}
scala> Seq("2/6/18").toDF.withColumn("parsed", to_date($"value", "MM/dd/yy")).collect
res7: Array[org.apache.spark.sql.Row] = Array([2/6/18,2018-02-06])


scala> Seq("2/6/18").toDF.withColumn("parsed", to_date($"value", "MM/dd/yy")).rdd.collect
res8: Array[org.apache.spark.sql.Row] = Array([2/6/18,2018-02-06]){code};;;, 05/May/21 23:51;rshkv;I think the difference might have to do with the fact that in the RDD case the config isn't in the local properties of the {{TaskContext}}.
 * Stepping through the debugger, I see that both RDD and Dataset decide on using or not using the legacy date formatter in [{{DateFormatter.getFormatter}}|https://github.com/apache/spark/blob/4fe4b65d9e4017654c93c8f7957ae3edbd270d0b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateFormatter.scala#L161].
 * Then in [{{SQLConf.get}}|https://github.com/apache/spark/blob/4fe4b65d9e4017654c93c8f7957ae3edbd270d0b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L172], both cases find a {{TaskContext}} and no {{existingConf}}. So they create a new {{ReadOnlySQLConf}} from the {{TaskContext}} object.
 * RDD and Dataset code path differ in the local properties they find on the {{TaskContext}} [here|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/ReadOnlySQLConf.scala#L32]. The Dataset code path has {{spark.sql.legacy.timeParserPolicy}} in the local properties, but the RDD path doesn't. The {{ReadOnlySQLConf}} is created from the local properties, so in the RDD path the resulting config object doesn't have an override for {{spark.sql.legacy.timeParserPolicy}}.

Just to show you what I see in the debugger. In both cases we stopped [here|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/ReadOnlySQLConf.scala#L32].

!Screen Shot 2021-05-06 at 00.35.10.png|width=300! !Screen Shot 2021-05-06 at 00.33.10.png|width=300! ;;;, 05/May/21 23:55;rshkv;This also reproduces if a launch the shell with {{--conf "spark.sql.legacy.timeParserPolicy=legacy"}}; just to prove that this isn't because I set the config via {{spark.conf.set}}.;;;, 26/May/21 11:37;rshkv;We found the same issue with {{spark.sql.legacy.parquet.datetimeRebaseModeInRead}}. It seems that RDDs are generally not respecting {{spark.sql.*}} configs?;;;, 13/Aug/21 11:42;rshkv;Here's an example of that:

{code:title=Missing config error}
Lost task 20.0 in stage 1.0 (TID 1, redacted, executor 1): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$1(Executor.scala:474)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into Parquet files can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.datetimeRebaseModeInWrite to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability. Or set spark.sql.legacy.parquet.datetimeRebaseModeInWrite to 'CORRECTED' to write the datetime values as it is, if you are 100% sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar.
	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:143)
	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$creteDateRebaseFuncInWrite$1(DataSourceUtils.scala:163)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:169)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:168)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:463)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:451)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)
	... 9 more
{code}

where we're already setting {{...datetimeRebaseModeInWrite: legacy}}.

I appreciate that we can use SQL instead of RDDs for most use cases but in some cases we're still forced to go down to RDDs and since Spark 3 we're effectively blocking those cases from reading / writing old datetimes.;;;, 13/Aug/21 11:50;rshkv;[~mgekk], apologies for the direct ping. Do you know who could look at this? Just hoping to get more jobs upgraded to Spark 3.

To summarize the issue: As you know some datetime reads/writes/parses in Spark 3 rely on additional configs, e.g. reading pre-1900 timestamps. It seems that even if you set those configs they don't get propagated to RDDs and jobs fail as if the config wasn't set.;;;, 06/Apr/23 12:00;rshkv;I understand this better now:
 * When calling {{SQLConf.get}} on executors, the configs are read from the local properties on the {{{}TaskContext{}}}. The local properties are populated driver-side when scheduling the job, using the properties found in {{{}sparkContext.localProperties{}}}.
 * For RDD actions like {{{}rdd.count{}}}, nothing populates moves driver-side SQL configs into the SparkContext's local properites.
 * For datasets, all actions incl. {{{}dataset.count{}}}, are wrapped in an {{withAction}} call [(e.g.)|https://github.com/apache/spark/blob/v3.3.2/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L3160].
* {{withAction}}  wraps the action in {{SQLExecution.withNewExecutionId}}, which in turn wraps it in {{SQLExecution.withSQLConfPropagated}}. This latter method copies SQL configs into the SparkContext's local properties.

So in summary, all actions on datasets get wrapped in {{withSQLConfPropagated}} while actions on RDDs aren't. That's why {{df.count}} works but {{df.rdd.count}} doesn't. With {{count}} the answer is to just use {{Dataset.count}}. But, e.g., {{df.toLocalIterator}} has no alternative.

To fix this, Spark would have to always copy configs into local properties (e.g. in {{submitJob}}). If maintainers likes that, I'll put up a PR. If not, feel free to close.

In the meantime, my work-around is to call {{SQLExecution.withSQLConfPropagated}} myself.

{code}
scala> spark.conf.set("spark.sql.legacy.timeParserPolicy", "legacy")

scala> spark.read.schema("date date").option("dateFormat", "MM/dd/yy").csv(Seq("2/6/18").toDS()).toLocalIterator.next
23/04/06 13:58:26 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER]
...

scala> SQLExecution.withSQLConfPropagated(spark) { 
     |   spark.read.schema("date date").option("dateFormat", "MM/dd/yy").csv(Seq("2/6/18").toDS()).toLocalIterator.next
     | }
res2: org.apache.spark.sql.Row = [2018-02-06]
{code};;;
Affects Version/s.1: 3.1.1
Comment.1: 05/May/21 23:25;rshkv;For what it's worth, I only managed to reproduce with a reader. Creating a dataframe from a {{Seq}} works fine:
{code:java}
scala> Seq("2/6/18").toDF.withColumn("parsed", to_date($"value", "MM/dd/yy")).collect
res7: Array[org.apache.spark.sql.Row] = Array([2/6/18,2018-02-06])


scala> Seq("2/6/18").toDF.withColumn("parsed", to_date($"value", "MM/dd/yy")).rdd.collect
res8: Array[org.apache.spark.sql.Row] = Array([2/6/18,2018-02-06]){code};;;
Comment.2: 05/May/21 23:51;rshkv;I think the difference might have to do with the fact that in the RDD case the config isn't in the local properties of the {{TaskContext}}.
 * Stepping through the debugger, I see that both RDD and Dataset decide on using or not using the legacy date formatter in [{{DateFormatter.getFormatter}}|https://github.com/apache/spark/blob/4fe4b65d9e4017654c93c8f7957ae3edbd270d0b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/DateFormatter.scala#L161].
 * Then in [{{SQLConf.get}}|https://github.com/apache/spark/blob/4fe4b65d9e4017654c93c8f7957ae3edbd270d0b/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L172], both cases find a {{TaskContext}} and no {{existingConf}}. So they create a new {{ReadOnlySQLConf}} from the {{TaskContext}} object.
 * RDD and Dataset code path differ in the local properties they find on the {{TaskContext}} [here|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/ReadOnlySQLConf.scala#L32]. The Dataset code path has {{spark.sql.legacy.timeParserPolicy}} in the local properties, but the RDD path doesn't. The {{ReadOnlySQLConf}} is created from the local properties, so in the RDD path the resulting config object doesn't have an override for {{spark.sql.legacy.timeParserPolicy}}.

Just to show you what I see in the debugger. In both cases we stopped [here|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/ReadOnlySQLConf.scala#L32].

!Screen Shot 2021-05-06 at 00.35.10.png|width=300! !Screen Shot 2021-05-06 at 00.33.10.png|width=300! ;;;
Comment.3: 05/May/21 23:55;rshkv;This also reproduces if a launch the shell with {{--conf "spark.sql.legacy.timeParserPolicy=legacy"}}; just to prove that this isn't because I set the config via {{spark.conf.set}}.;;;
Comment.4: 26/May/21 11:37;rshkv;We found the same issue with {{spark.sql.legacy.parquet.datetimeRebaseModeInRead}}. It seems that RDDs are generally not respecting {{spark.sql.*}} configs?;;;
Comment.5: 13/Aug/21 11:42;rshkv;Here's an example of that:

{code:title=Missing config error}
Lost task 20.0 in stage 1.0 (TID 1, redacted, executor 1): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:296)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$1(Executor.scala:474)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:477)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: writing dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into Parquet files can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.datetimeRebaseModeInWrite to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability. Or set spark.sql.legacy.parquet.datetimeRebaseModeInWrite to 'CORRECTED' to write the datetime values as it is, if you are 100% sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar.
	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInWrite(DataSourceUtils.scala:143)
	at org.apache.spark.sql.execution.datasources.DataSourceUtils$.$anonfun$creteDateRebaseFuncInWrite$1(DataSourceUtils.scala:163)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4(ParquetWriteSupport.scala:169)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$makeWriter$4$adapted(ParquetWriteSupport.scala:168)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$writeFields$1(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeField(ParquetWriteSupport.scala:463)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.writeFields(ParquetWriteSupport.scala:146)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.$anonfun$write$1(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.consumeMessage(ParquetWriteSupport.scala:451)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:136)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport.write(ParquetWriteSupport.scala:54)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:128)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:182)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:44)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.write(ParquetOutputWriter.scala:40)
	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:140)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:278)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1411)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:286)
	... 9 more
{code}

where we're already setting {{...datetimeRebaseModeInWrite: legacy}}.

I appreciate that we can use SQL instead of RDDs for most use cases but in some cases we're still forced to go down to RDDs and since Spark 3 we're effectively blocking those cases from reading / writing old datetimes.;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: timestamp type column analyze result is wrong
Issue key: SPARK-36604
Issue id: 13397941
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yghu
Creator: yghu
Created: 28/Aug/21 07:39
Updated: 02/Mar/23 19:08
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: when we create table with timestamp column type, the min and max data of the analyze result for the timestamp column is wrong

eg:

{code}
> select * from a;
{code}

{code}
2021-08-15 15:30:01
Time taken: 2.789 seconds, Fetched 1 row(s)
spark-sql> desc formatted a a;
col_name a
data_type timestamp
comment NULL
min 2021-08-15 07:30:01.000000
max 2021-08-15 07:30:01.000000
num_nulls 0
distinct_count 1
avg_col_len 8
max_col_len 8
histogram NULL
Time taken: 0.278 seconds, Fetched 10 row(s)
spark-sql> desc a;
a timestamp NULL
Time taken: 1.432 seconds, Fetched 1 row(s)
{code}

 

reproduce step:

{code}
create table a(a timestamp);
insert into a select '2021-08-15 15:30:01';
analyze table a compute statistics for columns a;
desc formatted a a;
select * from a;
{code}

Environment: Spark 3.1.1
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Mar 02 19:08:56 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0udag:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Aug/21 08:53;yghu;I'd like to work on this.;;;, 31/Aug/21 04:49;senthh;[~yghu] I tested this scenario in Spark2.4, but I don't see this issue is occurring.   Are you seeing this issue only in Spark 3.1.1? 

 

 
{panel}


 

_scala> spark.sql("create table c(a timestamp)")_

_res16: org.apache.spark.sql.DataFrame = []_

 __ 

_scala> spark.sql("insert into c select '2021-08-15 15:30:01'")_

_res17: org.apache.spark.sql.DataFrame = []_

 __ 

_scala> spark.sql("analyze table c compute statistics for columns a")_

_res18: org.apache.spark.sql.DataFrame = []_

 __ 

_scala> spark.sql("desc formatted c a").show(true)_

_+--------------+--------------------+_

_|     info_name|          info_value|_

_+--------------+--------------------+_

_|      col_name|                   a|_

_|     data_type|           timestamp|_

_|       comment|                NULL|_

_|           min|2021-08-15 15:30:...|_

_|           max|2021-08-15 15:30:...|_

_|     num_nulls|                   0|_

_|distinct_count|                   1|_

_|   avg_col_len|                   8|_

_|   max_col_len|                   8|_

_|     histogram|                NULL|_

_+--------------+--------------------+_

 
{panel}
 ;;;, 31/Aug/21 06:13;yghu;[~senthh]  i tested with spark2.4.5 also don't have this issue, i checked code  maybe it's caused by this commit: https://github.com/apache/spark/pull/23662/files;;;, 01/Sep/21 07:30;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33886;;;, 14/Apr/22 01:28;yghu;[~senthh] what's the session time zone?

i tested with spark 3.2.1 alse have the issue. The value's '2021-08-15 15:30:01', while the min/max value is 8 hours diff.

scala>  spark.sql("insert into c select '2021-08-15 15:30:01'")
22/04/14 09:23:36 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
res3: org.apache.spark.sql.DataFrame = []

scala> spark.sql("analyze table c compute statistics for columns a")
res4: org.apache.spark.sql.DataFrame = []                                       

scala> spark.sql("desc formatted c a").show(true)
+--------------+--------------------+
|     info_name|          info_value|
+--------------+--------------------+
|      col_name|                   a|
|     data_type|           timestamp|
|       comment|                NULL|
|           min|2021-08-15 07:30:...|
|           max|2021-08-15 07:30:...|
|     num_nulls|                   0|
|distinct_count|                   1|
|   avg_col_len|                   8|
|   max_col_len|                   8|
|     histogram|                NULL|
+--------------+--------------------+


scala> sql("set spark.sql.session.timeZone").show
+--------------------+-------------+
|                 key|        value|
+--------------------+-------------+
|spark.sql.session...|Asia/Shanghai|
+--------------------+-------------+;;;, 02/Mar/23 19:08;ritikam;Seems to be working correctly in  Spark 3.3.0

spark-sql> insert into a values(cast('2021-08-15 15:30:01' as timestamp)
         > );
23/03/02 11:04:11 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Time taken: 3.278 seconds
spark-sql> select * from a;
2021-08-15 15:30:01
Time taken: 0.782 seconds, Fetched 1 row(s)
spark-sql> analyze table a compute statistics for columns a;
Time taken: 1.882 seconds
spark-sql> desc formatted a a;
col_name        a
data_type       timestamp
comment NULL
min     2021-08-15 15:30:01.000000 -0700
max     2021-08-15 15:30:01.000000 -0700
num_nulls       0
distinct_count  1
avg_col_len     8
max_col_len     8
histogram       NULL
Time taken: 0.095 seconds, Fetched 10 row(s)
spark-sql> desc a;
a                       timestamp                                   
Time taken: 0.059 seconds, Fetched 1 row(s)
spark-sql>;;;
Affects Version/s.1: 3.1.2
Comment.1: 31/Aug/21 04:49;senthh;[~yghu] I tested this scenario in Spark2.4, but I don't see this issue is occurring.   Are you seeing this issue only in Spark 3.1.1? 

 

 
{panel}


 

_scala> spark.sql("create table c(a timestamp)")_

_res16: org.apache.spark.sql.DataFrame = []_

 __ 

_scala> spark.sql("insert into c select '2021-08-15 15:30:01'")_

_res17: org.apache.spark.sql.DataFrame = []_

 __ 

_scala> spark.sql("analyze table c compute statistics for columns a")_

_res18: org.apache.spark.sql.DataFrame = []_

 __ 

_scala> spark.sql("desc formatted c a").show(true)_

_+--------------+--------------------+_

_|     info_name|          info_value|_

_+--------------+--------------------+_

_|      col_name|                   a|_

_|     data_type|           timestamp|_

_|       comment|                NULL|_

_|           min|2021-08-15 15:30:...|_

_|           max|2021-08-15 15:30:...|_

_|     num_nulls|                   0|_

_|distinct_count|                   1|_

_|   avg_col_len|                   8|_

_|   max_col_len|                   8|_

_|     histogram|                NULL|_

_+--------------+--------------------+_

 
{panel}
 ;;;
Comment.2: 31/Aug/21 06:13;yghu;[~senthh]  i tested with spark2.4.5 also don't have this issue, i checked code  maybe it's caused by this commit: https://github.com/apache/spark/pull/23662/files;;;
Comment.3: 01/Sep/21 07:30;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33886;;;
Comment.4: 14/Apr/22 01:28;yghu;[~senthh] what's the session time zone?

i tested with spark 3.2.1 alse have the issue. The value's '2021-08-15 15:30:01', while the min/max value is 8 hours diff.

scala>  spark.sql("insert into c select '2021-08-15 15:30:01'")
22/04/14 09:23:36 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
res3: org.apache.spark.sql.DataFrame = []

scala> spark.sql("analyze table c compute statistics for columns a")
res4: org.apache.spark.sql.DataFrame = []                                       

scala> spark.sql("desc formatted c a").show(true)
+--------------+--------------------+
|     info_name|          info_value|
+--------------+--------------------+
|      col_name|                   a|
|     data_type|           timestamp|
|       comment|                NULL|
|           min|2021-08-15 07:30:...|
|           max|2021-08-15 07:30:...|
|     num_nulls|                   0|
|distinct_count|                   1|
|   avg_col_len|                   8|
|   max_col_len|                   8|
|     histogram|                NULL|
+--------------+--------------------+


scala> sql("set spark.sql.session.timeZone").show
+--------------------+-------------+
|                 key|        value|
+--------------------+-------------+
|spark.sql.session...|Asia/Shanghai|
+--------------------+-------------+;;;
Comment.5: 02/Mar/23 19:08;ritikam;Seems to be working correctly in  Spark 3.3.0

spark-sql> insert into a values(cast('2021-08-15 15:30:01' as timestamp)
         > );
23/03/02 11:04:11 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Time taken: 3.278 seconds
spark-sql> select * from a;
2021-08-15 15:30:01
Time taken: 0.782 seconds, Fetched 1 row(s)
spark-sql> analyze table a compute statistics for columns a;
Time taken: 1.882 seconds
spark-sql> desc formatted a a;
col_name        a
data_type       timestamp
comment NULL
min     2021-08-15 15:30:01.000000 -0700
max     2021-08-15 15:30:01.000000 -0700
num_nulls       0
distinct_count  1
avg_col_len     8
max_col_len     8
histogram       NULL
Time taken: 0.095 seconds, Fetched 10 row(s)
spark-sql> desc a;
a                       timestamp                                   
Time taken: 0.059 seconds, Fetched 1 row(s)
spark-sql>;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0

Summary: Create table in overwrite mode fails when interrupted
Issue key: SPARK-39348
Issue id: 13447707
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Uvarov
Creator: Uvarov
Created: 31/May/22 13:21
Updated: 09/Feb/23 12:01
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Input/Output
Due Date: 
Votes: 0
Labels: 
Description: When you attempt to rerun an Apache Spark write operation by cancelling the currently running job, the following error occurs:
{code:java}
Error: org.apache.spark.sql.AnalysisException: Cannot create the managed table('`testdb`.` testtable`').
The associated location ('dbfs:/user/hive/warehouse/testdb.db/metastore_cache_ testtable) already exists.;{code}
This problem can occur if:
 * The cluster is terminated while a write operation is in progress.
 * A temporary network issue occurs.
 * The job is interrupted.

You can reproduce the problem by following these steps:

1. Create a DataFrame:
{code:java}
val df = spark.range(1000){code}
2. Write the DataFrame to a location in overwrite mode:
{code:java}
df.write.mode(SaveMode.Overwrite).saveAsTable("testdb.testtable"){code}
3. Cancel the command while it is executing.

4. Re-run the {{write}} command.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Feb 09 12:01:44 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12tyo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Feb/23 12:01;Wayne Guo;After PR [https://github.com/apache/spark/pull/26559,] it has been removed.
 * Since Spark 2.4, creating a managed table with nonempty location is not allowed. An exception is thrown when attempting to create a managed table with nonempty location. To set {{true}} to {{spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation}} restores the previous behavior. This option will be removed in Spark 3.0.;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: sql hang at planning stage
Issue key: SPARK-37581
Issue id: 13416017
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: oceaneast
Creator: oceaneast
Created: 08/Dec/21 11:11
Updated: 06/Feb/23 01:59
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: when exec a sql, this sql hang at planning stage.

when disable DPP, sql can finish very quickly.

we can reproduce this  problem through example below:

create table test.test_a (
day string,
week int,
weekday int)
partitioned by (
dt varchar(8))
stored as orc;

insert into test.test_a partition (dt=20211126) values('1',1,2);

create table test.test_b (
session_id string,
device_id string,
brand string,
model string,
wx_version string,
os string,
net_work_type string,
app_id string,
app_name string,
col_z string,
page_url string,
page_title string,
olabel string,
otitle string,
source string,
send_dt string,
recv_dt string,
request_time string,
write_time string,
client_ip string,
col_a string,
dt_hour varchar(12),
product string,
channelfrom string,
customer_um string,
kb_code string,
col_b string,
rectype string,
errcode string,
col_c string,
pageid_merge string)
partitioned by (
dt varchar(8))
stored as orc;

insert into test.test_b partition(dt=20211126)
values('2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2','2');

 

 

drop table if exists test.test_c;create table if not exists test.test_c stored as ORCFILE as
select calendar.day,calendar.week,calendar.weekday, a_kbs,
b_kbs, c_kbs,d_kbs,e_kbs,f_kbs,g_kbs,h_kbs,i_kbs,j_kbs,k_kbs
from (select * from test.test_a where dt = '20211126') calendar
left join
(select dt,count(distinct kb_code) as a_kbs
from test.test_b
where dt = '20211126'
group by dt) t1
on calendar.dt = t1.dt

left join
(select dt,count(distinct kb_code) as b_kbs
from test.test_b
where dt = '20211126'
group by dt) t2
on calendar.dt = t2.dt

left join
(select dt,count(distinct kb_code) as c_kbs
from test.test_b
where dt = '20211126'
group by dt) t3
on calendar.dt = t3.dt

left join
(select dt,count(distinct kb_code) as d_kbs
from test.test_b
where dt = '20211126'
group by dt) t4
on calendar.dt = t4.dt

left join
(select dt,count(distinct kb_code) as e_kbs
from test.test_b
where dt = '20211126'
group by dt) t5
on calendar.dt = t5.dt

left join
(select dt,count(distinct kb_code) as f_kbs
from test.test_b
where dt = '20211126'
group by dt) t6
on calendar.dt = t6.dt

left join
(select dt,count(distinct kb_code) as g_kbs
from test.test_b
where dt = '20211126'
group by dt) t7
on calendar.dt = t7.dt

left join
(select dt,count(distinct kb_code) as h_kbs
from test.test_b
where dt = '20211126'
group by dt) t8
on calendar.dt = t8.dt

left join
(select dt,count(distinct kb_code) as i_kbs
from test.test_b
where dt = '20211126'
group by dt) t9
on calendar.dt = t9.dt

left join
(select dt,count(distinct kb_code) as j_kbs
from test.test_b
where dt = '20211126'
group by dt) t10
on calendar.dt = t10.dt

left join
(select dt,count(distinct kb_code) as k_kbs
from test.test_b
where dt = '20211126'
group by dt) t11
on calendar.dt = t11.dt

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Feb 06 01:59:52 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xgm0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Dec/21 01:57;gurwls223;[~oceaneast] do you mind narrowing down and make the reproducer smaller? The reproducer is too complicated to debug and investigate further.;;;, 09/Dec/21 06:00;oceaneast;Hi [~hyukjin.kwon]. This sql have 19 join operators.But these join have the same pattern. I found that ,when have 10 join operators, it costs 17s. when 11 join operators, costs 39s. when 12 join operators, costs 120s. when 13 join operators , it can not finish.  

 

I think, we can debug it at 11 join operators, to find why it is so slow. I had narrowed down 

as below:

=======

drop table if exists test.test_c;create table if not exists test.test_c stored as ORCFILE as
select calendar.day,calendar.week,calendar.weekday, a_kbs,
b_kbs, c_kbs,d_kbs,e_kbs,f_kbs,g_kbs,h_kbs,i_kbs,j_kbs,k_kbs
from (select * from test.test_a where dt = '20211126') calendar
left join
(select dt,count(distinct kb_code) as a_kbs
from test.test_b
where dt = '20211126'
group by dt) t1
on calendar.dt = t1.dt

left join
(select dt,count(distinct kb_code) as b_kbs
from test.test_b
where dt = '20211126'
group by dt) t2
on calendar.dt = t2.dt

left join
(select dt,count(distinct kb_code) as c_kbs
from test.test_b
where dt = '20211126'
group by dt) t3
on calendar.dt = t3.dt

left join
(select dt,count(distinct kb_code) as d_kbs
from test.test_b
where dt = '20211126'
group by dt) t4
on calendar.dt = t4.dt

left join
(select dt,count(distinct kb_code) as e_kbs
from test.test_b
where dt = '20211126'
group by dt) t5
on calendar.dt = t5.dt

left join
(select dt,count(distinct kb_code) as f_kbs
from test.test_b
where dt = '20211126'
group by dt) t6
on calendar.dt = t6.dt

left join
(select dt,count(distinct kb_code) as g_kbs
from test.test_b
where dt = '20211126'
group by dt) t7
on calendar.dt = t7.dt

left join
(select dt,count(distinct kb_code) as h_kbs
from test.test_b
where dt = '20211126'
group by dt) t8
on calendar.dt = t8.dt

left join
(select dt,count(distinct kb_code) as i_kbs
from test.test_b
where dt = '20211126'
group by dt) t9
on calendar.dt = t9.dt

left join
(select dt,count(distinct kb_code) as j_kbs
from test.test_b
where dt = '20211126'
group by dt) t10
on calendar.dt = t10.dt

left join
(select dt,count(distinct kb_code) as k_kbs
from test.test_b
where dt = '20211126'
group by dt) t11
on calendar.dt = t11.dt

 ;;;, 06/Feb/23 00:21;ritikam;Just executed this on 3.3.0  and it takes 4.14 second. I am also including the plan. So does not seem like an issue in 3.3.0

 

> left join

         > (select dt,count(distinct kb_code) as h_kbs

         > from test.test_b

         > where dt = '20211126'

         > group by dt) t8

         > on calendar.dt = t8.dt

         > 

         > left join

         > (select dt,count(distinct kb_code) as i_kbs

         > from test.test_b

         > where dt = '20211126'

         > group by dt) t9

         > on calendar.dt = t9.dt

         > 

         > left join

         > (select dt,count(distinct kb_code) as j_kbs

         > from test.test_b

         > where dt = '20211126'

         > group by dt) t10

         > on calendar.dt = t10.dt

         > 

         > left join

         > (select dt,count(distinct kb_code) as k_kbs

         > from test.test_b

         > where dt = '20211126'

         > group by dt) t11

         > on calendar.dt = t11.dt;

Time taken: 0.609 seconds

23/02/05 16:10:47 WARN HiveMetaStore: Location: file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_c specified for non-external table:test_c

{color:#FF0000}*Time taken: 4.14 seconds*{color}

spark-sql> select * from test.test_c

         > ;

1 1 2 1 1 1 1 1 1 1 1 1 1 1

Time taken: 0.296 seconds, Fetched 1 row(s)

spark-sql> 

 

{color:#FF0000}*The plan for the query is* {color}

 

CommandResult Execute OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

   +- OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

      +- Project [day#11, week#12, weekday#13, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L, k_kbs#10L]

         +- Join LeftOuter, (dt#14 = dt#366)

            :- Join LeftOuter, (dt#14 = dt#334)

            :  :- Join LeftOuter, (dt#14 = dt#302)

            :  :  :- Join LeftOuter, (dt#14 = dt#270)

            :  :  :  :- Join LeftOuter, (dt#14 = dt#238)

            :  :  :  :  :- Join LeftOuter, (dt#14 = dt#206)

            :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#174)

            :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#142)

            :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#110)

            :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#78)

            :  :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#46)

            :  :  :  :  :  :  :  :  :  :  :- SubqueryAlias calendar

            :  :  :  :  :  :  :  :  :  :  :  +- Project [day#11, week#12, weekday#13, dt#14]

            :  :  :  :  :  :  :  :  :  :  :     +- Filter (dt#14 = 20211126)

            :  :  :  :  :  :  :  :  :  :  :        +- SubqueryAlias spark_catalog.test.test_a

            :  :  :  :  :  :  :  :  :  :  :           +- Relation test.test_a[day#11,week#12,weekday#13,dt#14] orc

            :  :  :  :  :  :  :  :  :  :  +- SubqueryAlias t1

            :  :  :  :  :  :  :  :  :  :     +- Aggregate [dt#46], [dt#46, count(distinct kb_code#40) AS a_kbs#0L]

            :  :  :  :  :  :  :  :  :  :        +- Filter (dt#46 = 20211126)

            :  :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#15,device_id#16,brand#17,model#18,wx_version#19,os#20,net_work_type#21,app_id#22,app_name#23,col_z#24,page_url#25,page_title#26,olabel#27,otitle#28,source#29,send_dt#30,recv_dt#31,request_time#32,write_time#33,client_ip#34,col_a#35,dt_hour#36,product#37,channelfrom#38,... 8 more fields] orc

            :  :  :  :  :  :  :  :  :  +- SubqueryAlias t2

            :  :  :  :  :  :  :  :  :     +- Aggregate [dt#78], [dt#78, count(distinct kb_code#72) AS b_kbs#1L]

            :  :  :  :  :  :  :  :  :        +- Filter (dt#78 = 20211126)

            :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#47,device_id#48,brand#49,model#50,wx_version#51,os#52,net_work_type#53,app_id#54,app_name#55,col_z#56,page_url#57,page_title#58,olabel#59,otitle#60,source#61,send_dt#62,recv_dt#63,request_time#64,write_time#65,client_ip#66,col_a#67,dt_hour#68,product#69,channelfrom#70,... 8 more fields] orc

            :  :  :  :  :  :  :  :  +- SubqueryAlias t3

            :  :  :  :  :  :  :  :     +- Aggregate [dt#110], [dt#110, count(distinct kb_code#104) AS c_kbs#2L]

            :  :  :  :  :  :  :  :        +- Filter (dt#110 = 20211126)

            :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#79,device_id#80,brand#81,model#82,wx_version#83,os#84,net_work_type#85,app_id#86,app_name#87,col_z#88,page_url#89,page_title#90,olabel#91,otitle#92,source#93,send_dt#94,recv_dt#95,request_time#96,write_time#97,client_ip#98,col_a#99,dt_hour#100,product#101,channelfrom#102,... 8 more fields] orc

            :  :  :  :  :  :  :  +- SubqueryAlias t4

            :  :  :  :  :  :  :     +- Aggregate [dt#142], [dt#142, count(distinct kb_code#136) AS d_kbs#3L]

            :  :  :  :  :  :  :        +- Filter (dt#142 = 20211126)

            :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :              +- Relation test.test_b[session_id#111,device_id#112,brand#113,model#114,wx_version#115,os#116,net_work_type#117,app_id#118,app_name#119,col_z#120,page_url#121,page_title#122,olabel#123,otitle#124,source#125,send_dt#126,recv_dt#127,request_time#128,write_time#129,client_ip#130,col_a#131,dt_hour#132,product#133,channelfrom#134,... 8 more fields] orc

            :  :  :  :  :  :  +- SubqueryAlias t5

            :  :  :  :  :  :     +- Aggregate [dt#174], [dt#174, count(distinct kb_code#168) AS e_kbs#4L]

            :  :  :  :  :  :        +- Filter (dt#174 = 20211126)

            :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :              +- Relation test.test_b[session_id#143,device_id#144,brand#145,model#146,wx_version#147,os#148,net_work_type#149,app_id#150,app_name#151,col_z#152,page_url#153,page_title#154,olabel#155,otitle#156,source#157,send_dt#158,recv_dt#159,request_time#160,write_time#161,client_ip#162,col_a#163,dt_hour#164,product#165,channelfrom#166,... 8 more fields] orc

            :  :  :  :  :  +- SubqueryAlias t6

            :  :  :  :  :     +- Aggregate [dt#206], [dt#206, count(distinct kb_code#200) AS f_kbs#5L]

            :  :  :  :  :        +- Filter (dt#206 = 20211126)

            :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :              +- Relation test.test_b[session_id#175,device_id#176,brand#177,model#178,wx_version#179,os#180,net_work_type#181,app_id#182,app_name#183,col_z#184,page_url#185,page_title#186,olabel#187,otitle#188,source#189,send_dt#190,recv_dt#191,request_time#192,write_time#193,client_ip#194,col_a#195,dt_hour#196,product#197,channelfrom#198,... 8 more fields] orc

            :  :  :  :  +- SubqueryAlias t7

            :  :  :  :     +- Aggregate [dt#238], [dt#238, count(distinct kb_code#232) AS g_kbs#6L]

            :  :  :  :        +- Filter (dt#238 = 20211126)

            :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :              +- Relation test.test_b[session_id#207,device_id#208,brand#209,model#210,wx_version#211,os#212,net_work_type#213,app_id#214,app_name#215,col_z#216,page_url#217,page_title#218,olabel#219,otitle#220,source#221,send_dt#222,recv_dt#223,request_time#224,write_time#225,client_ip#226,col_a#227,dt_hour#228,product#229,channelfrom#230,... 8 more fields] orc

            :  :  :  +- SubqueryAlias t8

            :  :  :     +- Aggregate [dt#270], [dt#270, count(distinct kb_code#264) AS h_kbs#7L]

            :  :  :        +- Filter (dt#270 = 20211126)

            :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :              +- Relation test.test_b[session_id#239,device_id#240,brand#241,model#242,wx_version#243,os#244,net_work_type#245,app_id#246,app_name#247,col_z#248,page_url#249,page_title#250,olabel#251,otitle#252,source#253,send_dt#254,recv_dt#255,request_time#256,write_time#257,client_ip#258,col_a#259,dt_hour#260,product#261,channelfrom#262,... 8 more fields] orc

            :  :  +- SubqueryAlias t9

            :  :     +- Aggregate [dt#302], [dt#302, count(distinct kb_code#296) AS i_kbs#8L]

            :  :        +- Filter (dt#302 = 20211126)

            :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :              +- Relation test.test_b[session_id#271,device_id#272,brand#273,model#274,wx_version#275,os#276,net_work_type#277,app_id#278,app_name#279,col_z#280,page_url#281,page_title#282,olabel#283,otitle#284,source#285,send_dt#286,recv_dt#287,request_time#288,write_time#289,client_ip#290,col_a#291,dt_hour#292,product#293,channelfrom#294,... 8 more fields] orc

            :  +- SubqueryAlias t10

            :     +- Aggregate [dt#334], [dt#334, count(distinct kb_code#328) AS j_kbs#9L]

            :        +- Filter (dt#334 = 20211126)

            :           +- SubqueryAlias spark_catalog.test.test_b

            :              +- Relation test.test_b[session_id#303,device_id#304,brand#305,model#306,wx_version#307,os#308,net_work_type#309,app_id#310,app_name#311,col_z#312,page_url#313,page_title#314,olabel#315,otitle#316,source#317,send_dt#318,recv_dt#319,request_time#320,write_time#321,client_ip#322,col_a#323,dt_hour#324,product#325,channelfrom#326,... 8 more fields] orc

            +- SubqueryAlias t11

               +- Aggregate [dt#366], [dt#366, count(distinct kb_code#360) AS k_kbs#10L]

                  +- Filter (dt#366 = 20211126)

                     +- SubqueryAlias spark_catalog.test.test_b

                        +- Relation test.test_b[session_id#335,device_id#336,brand#337,model#338,wx_version#339,os#340,net_work_type#341,app_id#342,app_name#343,col_z#344,page_url#345,page_title#346,olabel#347,otitle#348,source#349,send_dt#350,recv_dt#351,request_time#352,write_time#353,client_ip#354,col_a#355,dt_hour#356,product#357,channelfrom#358,... 8 more fields] orc

 

== Analyzed Logical Plan ==

CommandResult Execute OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

   +- OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

      +- Project [day#11, week#12, weekday#13, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L, k_kbs#10L]

         +- Join LeftOuter, (dt#14 = dt#366)

            :- Join LeftOuter, (dt#14 = dt#334)

            :  :- Join LeftOuter, (dt#14 = dt#302)

            :  :  :- Join LeftOuter, (dt#14 = dt#270)

            :  :  :  :- Join LeftOuter, (dt#14 = dt#238)

            :  :  :  :  :- Join LeftOuter, (dt#14 = dt#206)

            :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#174)

            :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#142)

            :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#110)

            :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#78)

            :  :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#46)

            :  :  :  :  :  :  :  :  :  :  :- SubqueryAlias calendar

            :  :  :  :  :  :  :  :  :  :  :  +- Project [day#11, week#12, weekday#13, dt#14]

            :  :  :  :  :  :  :  :  :  :  :     +- Filter (dt#14 = 20211126)

            :  :  :  :  :  :  :  :  :  :  :        +- SubqueryAlias spark_catalog.test.test_a

            :  :  :  :  :  :  :  :  :  :  :           +- Relation test.test_a[day#11,week#12,weekday#13,dt#14] orc

            :  :  :  :  :  :  :  :  :  :  +- SubqueryAlias t1

            :  :  :  :  :  :  :  :  :  :     +- Aggregate [dt#46], [dt#46, count(distinct kb_code#40) AS a_kbs#0L]

            :  :  :  :  :  :  :  :  :  :        +- Filter (dt#46 = 20211126)

            :  :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#15,device_id#16,brand#17,model#18,wx_version#19,os#20,net_work_type#21,app_id#22,app_name#23,col_z#24,page_url#25,page_title#26,olabel#27,otitle#28,source#29,send_dt#30,recv_dt#31,request_time#32,write_time#33,client_ip#34,col_a#35,dt_hour#36,product#37,channelfrom#38,... 8 more fields] orc

            :  :  :  :  :  :  :  :  :  +- SubqueryAlias t2

            :  :  :  :  :  :  :  :  :     +- Aggregate [dt#78], [dt#78, count(distinct kb_code#72) AS b_kbs#1L]

            :  :  :  :  :  :  :  :  :        +- Filter (dt#78 = 20211126)

            :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#47,device_id#48,brand#49,model#50,wx_version#51,os#52,net_work_type#53,app_id#54,app_name#55,col_z#56,page_url#57,page_title#58,olabel#59,otitle#60,source#61,send_dt#62,recv_dt#63,request_time#64,write_time#65,client_ip#66,col_a#67,dt_hour#68,product#69,channelfrom#70,... 8 more fields] orc

            :  :  :  :  :  :  :  :  +- SubqueryAlias t3

            :  :  :  :  :  :  :  :     +- Aggregate [dt#110], [dt#110, count(distinct kb_code#104) AS c_kbs#2L]

            :  :  :  :  :  :  :  :        +- Filter (dt#110 = 20211126)

            :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#79,device_id#80,brand#81,model#82,wx_version#83,os#84,net_work_type#85,app_id#86,app_name#87,col_z#88,page_url#89,page_title#90,olabel#91,otitle#92,source#93,send_dt#94,recv_dt#95,request_time#96,write_time#97,client_ip#98,col_a#99,dt_hour#100,product#101,channelfrom#102,... 8 more fields] orc

            :  :  :  :  :  :  :  +- SubqueryAlias t4

            :  :  :  :  :  :  :     +- Aggregate [dt#142], [dt#142, count(distinct kb_code#136) AS d_kbs#3L]

            :  :  :  :  :  :  :        +- Filter (dt#142 = 20211126)

            :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :              +- Relation test.test_b[session_id#111,device_id#112,brand#113,model#114,wx_version#115,os#116,net_work_type#117,app_id#118,app_name#119,col_z#120,page_url#121,page_title#122,olabel#123,otitle#124,source#125,send_dt#126,recv_dt#127,request_time#128,write_time#129,client_ip#130,col_a#131,dt_hour#132,product#133,channelfrom#134,... 8 more fields] orc

            :  :  :  :  :  :  +- SubqueryAlias t5

            :  :  :  :  :  :     +- Aggregate [dt#174], [dt#174, count(distinct kb_code#168) AS e_kbs#4L]

            :  :  :  :  :  :        +- Filter (dt#174 = 20211126)

            :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :              +- Relation test.test_b[session_id#143,device_id#144,brand#145,model#146,wx_version#147,os#148,net_work_type#149,app_id#150,app_name#151,col_z#152,page_url#153,page_title#154,olabel#155,otitle#156,source#157,send_dt#158,recv_dt#159,request_time#160,write_time#161,client_ip#162,col_a#163,dt_hour#164,product#165,channelfrom#166,... 8 more fields] orc

            :  :  :  :  :  +- SubqueryAlias t6

            :  :  :  :  :     +- Aggregate [dt#206], [dt#206, count(distinct kb_code#200) AS f_kbs#5L]

            :  :  :  :  :        +- Filter (dt#206 = 20211126)

            :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :              +- Relation test.test_b[session_id#175,device_id#176,brand#177,model#178,wx_version#179,os#180,net_work_type#181,app_id#182,app_name#183,col_z#184,page_url#185,page_title#186,olabel#187,otitle#188,source#189,send_dt#190,recv_dt#191,request_time#192,write_time#193,client_ip#194,col_a#195,dt_hour#196,product#197,channelfrom#198,... 8 more fields] orc

            :  :  :  :  +- SubqueryAlias t7

            :  :  :  :     +- Aggregate [dt#238], [dt#238, count(distinct kb_code#232) AS g_kbs#6L]

            :  :  :  :        +- Filter (dt#238 = 20211126)

            :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :              +- Relation test.test_b[session_id#207,device_id#208,brand#209,model#210,wx_version#211,os#212,net_work_type#213,app_id#214,app_name#215,col_z#216,page_url#217,page_title#218,olabel#219,otitle#220,source#221,send_dt#222,recv_dt#223,request_time#224,write_time#225,client_ip#226,col_a#227,dt_hour#228,product#229,channelfrom#230,... 8 more fields] orc

            :  :  :  +- SubqueryAlias t8

            :  :  :     +- Aggregate [dt#270], [dt#270, count(distinct kb_code#264) AS h_kbs#7L]

            :  :  :        +- Filter (dt#270 = 20211126)

            :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :              +- Relation test.test_b[session_id#239,device_id#240,brand#241,model#242,wx_version#243,os#244,net_work_type#245,app_id#246,app_name#247,col_z#248,page_url#249,page_title#250,olabel#251,otitle#252,source#253,send_dt#254,recv_dt#255,request_time#256,write_time#257,client_ip#258,col_a#259,dt_hour#260,product#261,channelfrom#262,... 8 more fields] orc

            :  :  +- SubqueryAlias t9

            :  :     +- Aggregate [dt#302], [dt#302, count(distinct kb_code#296) AS i_kbs#8L]

            :  :        +- Filter (dt#302 = 20211126)

            :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :              +- Relation test.test_b[session_id#271,device_id#272,brand#273,model#274,wx_version#275,os#276,net_work_type#277,app_id#278,app_name#279,col_z#280,page_url#281,page_title#282,olabel#283,otitle#284,source#285,send_dt#286,recv_dt#287,request_time#288,write_time#289,client_ip#290,col_a#291,dt_hour#292,product#293,channelfrom#294,... 8 more fields] orc

            :  +- SubqueryAlias t10

            :     +- Aggregate [dt#334], [dt#334, count(distinct kb_code#328) AS j_kbs#9L]

            :        +- Filter (dt#334 = 20211126)

            :           +- SubqueryAlias spark_catalog.test.test_b

            :              +- Relation test.test_b[session_id#303,device_id#304,brand#305,model#306,wx_version#307,os#308,net_work_type#309,app_id#310,app_name#311,col_z#312,page_url#313,page_title#314,olabel#315,otitle#316,source#317,send_dt#318,recv_dt#319,request_time#320,write_time#321,client_ip#322,col_a#323,dt_hour#324,product#325,channelfrom#326,... 8 more fields] orc

            +- SubqueryAlias t11

               +- Aggregate [dt#366], [dt#366, count(distinct kb_code#360) AS k_kbs#10L]

                  +- Filter (dt#366 = 20211126)

                     +- SubqueryAlias spark_catalog.test.test_b

                        +- Relation test.test_b[session_id#335,device_id#336,brand#337,model#338,wx_version#339,os#340,net_work_type#341,app_id#342,app_name#343,col_z#344,page_url#345,page_title#346,olabel#347,otitle#348,source#349,send_dt#350,recv_dt#351,request_time#352,write_time#353,client_ip#354,col_a#355,dt_hour#356,product#357,channelfrom#358,... 8 more fields] orc

 

== Optimized Logical Plan ==

CommandResult Execute OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

   +- OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

      +- Project [day#11, week#12, weekday#13, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L, k_kbs#10L]

         +- Join LeftOuter, (dt#14 = dt#366)

            :- Join LeftOuter, (dt#14 = dt#334)

            :  :- Join LeftOuter, (dt#14 = dt#302)

            :  :  :- Join LeftOuter, (dt#14 = dt#270)

            :  :  :  :- Join LeftOuter, (dt#14 = dt#238)

            :  :  :  :  :- Join LeftOuter, (dt#14 = dt#206)

            :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#174)

            :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#142)

            :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#110)

            :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#78)

            :  :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#46)

            :  :  :  :  :  :  :  :  :  :  :- SubqueryAlias calendar

            :  :  :  :  :  :  :  :  :  :  :  +- Project [day#11, week#12, weekday#13, dt#14]

            :  :  :  :  :  :  :  :  :  :  :     +- Filter (dt#14 = 20211126)

            :  :  :  :  :  :  :  :  :  :  :        +- SubqueryAlias spark_catalog.test.test_a

            :  :  :  :  :  :  :  :  :  :  :           +- Relation test.test_a[day#11,week#12,weekday#13,dt#14] orc

            :  :  :  :  :  :  :  :  :  :  +- SubqueryAlias t1

            :  :  :  :  :  :  :  :  :  :     +- Aggregate [dt#46], [dt#46, count(distinct kb_code#40) AS a_kbs#0L]

            :  :  :  :  :  :  :  :  :  :        +- Filter (dt#46 = 20211126)

            :  :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#15,device_id#16,brand#17,model#18,wx_version#19,os#20,net_work_type#21,app_id#22,app_name#23,col_z#24,page_url#25,page_title#26,olabel#27,otitle#28,source#29,send_dt#30,recv_dt#31,request_time#32,write_time#33,client_ip#34,col_a#35,dt_hour#36,product#37,channelfrom#38,... 8 more fields] orc

            :  :  :  :  :  :  :  :  :  +- SubqueryAlias t2

            :  :  :  :  :  :  :  :  :     +- Aggregate [dt#78], [dt#78, count(distinct kb_code#72) AS b_kbs#1L]

            :  :  :  :  :  :  :  :  :        +- Filter (dt#78 = 20211126)

            :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#47,device_id#48,brand#49,model#50,wx_version#51,os#52,net_work_type#53,app_id#54,app_name#55,col_z#56,page_url#57,page_title#58,olabel#59,otitle#60,source#61,send_dt#62,recv_dt#63,request_time#64,write_time#65,client_ip#66,col_a#67,dt_hour#68,product#69,channelfrom#70,... 8 more fields] orc

            :  :  :  :  :  :  :  :  +- SubqueryAlias t3

            :  :  :  :  :  :  :  :     +- Aggregate [dt#110], [dt#110, count(distinct kb_code#104) AS c_kbs#2L]

            :  :  :  :  :  :  :  :        +- Filter (dt#110 = 20211126)

            :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#79,device_id#80,brand#81,model#82,wx_version#83,os#84,net_work_type#85,app_id#86,app_name#87,col_z#88,page_url#89,page_title#90,olabel#91,otitle#92,source#93,send_dt#94,recv_dt#95,request_time#96,write_time#97,client_ip#98,col_a#99,dt_hour#100,product#101,channelfrom#102,... 8 more fields] orc

            :  :  :  :  :  :  :  +- SubqueryAlias t4

            :  :  :  :  :  :  :     +- Aggregate [dt#142], [dt#142, count(distinct kb_code#136) AS d_kbs#3L]

            :  :  :  :  :  :  :        +- Filter (dt#142 = 20211126)

            :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :              +- Relation test.test_b[session_id#111,device_id#112,brand#113,model#114,wx_version#115,os#116,net_work_type#117,app_id#118,app_name#119,col_z#120,page_url#121,page_title#122,olabel#123,otitle#124,source#125,send_dt#126,recv_dt#127,request_time#128,write_time#129,client_ip#130,col_a#131,dt_hour#132,product#133,channelfrom#134,... 8 more fields] orc

            :  :  :  :  :  :  +- SubqueryAlias t5

            :  :  :  :  :  :     +- Aggregate [dt#174], [dt#174, count(distinct kb_code#168) AS e_kbs#4L]

            :  :  :  :  :  :        +- Filter (dt#174 = 20211126)

            :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :              +- Relation test.test_b[session_id#143,device_id#144,brand#145,model#146,wx_version#147,os#148,net_work_type#149,app_id#150,app_name#151,col_z#152,page_url#153,page_title#154,olabel#155,otitle#156,source#157,send_dt#158,recv_dt#159,request_time#160,write_time#161,client_ip#162,col_a#163,dt_hour#164,product#165,channelfrom#166,... 8 more fields] orc

            :  :  :  :  :  +- SubqueryAlias t6

            :  :  :  :  :     +- Aggregate [dt#206], [dt#206, count(distinct kb_code#200) AS f_kbs#5L]

            :  :  :  :  :        +- Filter (dt#206 = 20211126)

            :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :              +- Relation test.test_b[session_id#175,device_id#176,brand#177,model#178,wx_version#179,os#180,net_work_type#181,app_id#182,app_name#183,col_z#184,page_url#185,page_title#186,olabel#187,otitle#188,source#189,send_dt#190,recv_dt#191,request_time#192,write_time#193,client_ip#194,col_a#195,dt_hour#196,product#197,channelfrom#198,... 8 more fields] orc

            :  :  :  :  +- SubqueryAlias t7

            :  :  :  :     +- Aggregate [dt#238], [dt#238, count(distinct kb_code#232) AS g_kbs#6L]

            :  :  :  :        +- Filter (dt#238 = 20211126)

            :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :              +- Relation test.test_b[session_id#207,device_id#208,brand#209,model#210,wx_version#211,os#212,net_work_type#213,app_id#214,app_name#215,col_z#216,page_url#217,page_title#218,olabel#219,otitle#220,source#221,send_dt#222,recv_dt#223,request_time#224,write_time#225,client_ip#226,col_a#227,dt_hour#228,product#229,channelfrom#230,... 8 more fields] orc

            :  :  :  +- SubqueryAlias t8

            :  :  :     +- Aggregate [dt#270], [dt#270, count(distinct kb_code#264) AS h_kbs#7L]

            :  :  :        +- Filter (dt#270 = 20211126)

            :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :              +- Relation test.test_b[session_id#239,device_id#240,brand#241,model#242,wx_version#243,os#244,net_work_type#245,app_id#246,app_name#247,col_z#248,page_url#249,page_title#250,olabel#251,otitle#252,source#253,send_dt#254,recv_dt#255,request_time#256,write_time#257,client_ip#258,col_a#259,dt_hour#260,product#261,channelfrom#262,... 8 more fields] orc

            :  :  +- SubqueryAlias t9

            :  :     +- Aggregate [dt#302], [dt#302, count(distinct kb_code#296) AS i_kbs#8L]

            :  :        +- Filter (dt#302 = 20211126)

            :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :              +- Relation test.test_b[session_id#271,device_id#272,brand#273,model#274,wx_version#275,os#276,net_work_type#277,app_id#278,app_name#279,col_z#280,page_url#281,page_title#282,olabel#283,otitle#284,source#285,send_dt#286,recv_dt#287,request_time#288,write_time#289,client_ip#290,col_a#291,dt_hour#292,product#293,channelfrom#294,... 8 more fields] orc

            :  +- SubqueryAlias t10

            :     +- Aggregate [dt#334], [dt#334, count(distinct kb_code#328) AS j_kbs#9L]

            :        +- Filter (dt#334 = 20211126)

            :           +- SubqueryAlias spark_catalog.test.test_b

            :              +- Relation test.test_b[session_id#303,device_id#304,brand#305,model#306,wx_version#307,os#308,net_work_type#309,app_id#310,app_name#311,col_z#312,page_url#313,page_title#314,olabel#315,otitle#316,source#317,send_dt#318,recv_dt#319,request_time#320,write_time#321,client_ip#322,col_a#323,dt_hour#324,product#325,channelfrom#326,... 8 more fields] orc

            +- SubqueryAlias t11

               +- Aggregate [dt#366], [dt#366, count(distinct kb_code#360) AS k_kbs#10L]

                  +- Filter (dt#366 = 20211126)

                     +- SubqueryAlias spark_catalog.test.test_b

                        +- Relation test.test_b[session_id#335,device_id#336,brand#337,model#338,wx_version#339,os#340,net_work_type#341,app_id#342,app_name#343,col_z#344,page_url#345,page_title#346,olabel#347,otitle#348,source#349,send_dt#350,recv_dt#351,request_time#352,write_time#353,client_ip#354,col_a#355,dt_hour#356,product#357,channelfrom#358,... 8 more fields] orc

 

== Physical Plan ==

CommandResult <empty>

   +- Execute OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

      +- AdaptiveSparkPlan isFinalPlan=true

         +- == Final Plan ==

            *(34) Project [day#11, week#12, weekday#13, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L, k_kbs#10L]

            +- *(34) BroadcastHashJoin [dt#14], [dt#366], LeftOuter, BuildRight, false

               :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L]

               :  +- *(34) BroadcastHashJoin [dt#14], [dt#334], LeftOuter, BuildRight, false

               :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L]

               :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#302], LeftOuter, BuildRight, false

               :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L]

               :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#270], LeftOuter, BuildRight, false

               :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L]

               :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#238], LeftOuter, BuildRight, false

               :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L]

               :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#206], LeftOuter, BuildRight, false

               :     :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L]

               :     :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#174], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L]

               :     :     :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#142], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L]

               :     :     :     :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#110], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L]

               :     :     :     :     :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#78], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L]

               :     :     :     :     :     :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#46], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :     :     :- *(34) ColumnarToRow

               :     :     :     :     :     :     :     :     :     :     :  +- FileScan orc test.test_a[day#11,week#12,weekday#13,dt#14] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_a..., PartitionFilters: [isnotnull(dt#14), (dt#14 = 20211126)], PushedFilters: [], ReadSchema: struct<day:string,week:int,weekday:int>

               :     :     :     :     :     :     :     :     :     :     +- BroadcastQueryStage 42

               :     :     :     :     :     :     :     :     :     :        +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     :     :     :     :     :     :           +- *(23) HashAggregate(keys=[dt#46], functions=[count(distinct kb_code#40)], output=[dt#46, a_kbs#0L])

               :     :     :     :     :     :     :     :     :     :              +- AQEShuffleRead coalesced

               :     :     :     :     :     :     :     :     :     :                 +- ShuffleQueryStage 21

               :     :     :     :     :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#46, 200), ENSURE_REQUIREMENTS, [id=#983]

               :     :     :     :     :     :     :     :     :     :                       +- *(12) HashAggregate(keys=[dt#46], functions=[partial_count(distinct kb_code#40)], output=[dt#46, count#427L])

               :     :     :     :     :     :     :     :     :     :                          +- *(12) HashAggregate(keys=[dt#46, kb_code#40], functions=[], output=[dt#46, kb_code#40])

               :     :     :     :     :     :     :     :     :     :                             +- AQEShuffleRead coalesced

               :     :     :     :     :     :     :     :     :     :                                +- ShuffleQueryStage 0

               :     :     :     :     :     :     :     :     :     :                                   +- Exchange hashpartitioning(dt#46, kb_code#40, 200), ENSURE_REQUIREMENTS, [id=#427]

               :     :     :     :     :     :     :     :     :     :                                      +- *(1) HashAggregate(keys=[dt#46, kb_code#40], functions=[], output=[dt#46, kb_code#40])

               :     :     :     :     :     :     :     :     :     :                                         +- *(1) ColumnarToRow

               :     :     :     :     :     :     :     :     :     :                                            +- FileScan orc test.test_b[kb_code#40,dt#46] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#46), (dt#46 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     :     :     :     :     +- BroadcastQueryStage 44

               :     :     :     :     :     :     :     :     :        +- ReusedExchange [dt#78, b_kbs#1L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     :     :     :     :     +- BroadcastQueryStage 46

               :     :     :     :     :     :     :     :        +- ReusedExchange [dt#110, c_kbs#2L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     :     :     :     +- BroadcastQueryStage 48

               :     :     :     :     :     :     :        +- ReusedExchange [dt#142, d_kbs#3L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     :     :     +- BroadcastQueryStage 50

               :     :     :     :     :     :        +- ReusedExchange [dt#174, e_kbs#4L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     :     +- BroadcastQueryStage 52

               :     :     :     :     :        +- ReusedExchange [dt#206, f_kbs#5L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     +- BroadcastQueryStage 54

               :     :     :     :        +- ReusedExchange [dt#238, g_kbs#6L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     +- BroadcastQueryStage 56

               :     :     :        +- ReusedExchange [dt#270, h_kbs#7L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     +- BroadcastQueryStage 58

               :     :        +- ReusedExchange [dt#302, i_kbs#8L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     +- BroadcastQueryStage 60

               :        +- ReusedExchange [dt#334, j_kbs#9L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               +- BroadcastQueryStage 62

                  +- ReusedExchange [dt#366, k_kbs#10L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

         +- == Initial Plan ==

            Project [day#11, week#12, weekday#13, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L, k_kbs#10L]

            +- BroadcastHashJoin [dt#14], [dt#366], LeftOuter, BuildRight, false

               :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L]

               :  +- BroadcastHashJoin [dt#14], [dt#334], LeftOuter, BuildRight, false

               :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L]

               :     :  +- BroadcastHashJoin [dt#14], [dt#302], LeftOuter, BuildRight, false

               :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L]

               :     :     :  +- BroadcastHashJoin [dt#14], [dt#270], LeftOuter, BuildRight, false

               :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L]

               :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#238], LeftOuter, BuildRight, false

               :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L]

               :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#206], LeftOuter, BuildRight, false

               :     :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L]

               :     :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#174], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L]

               :     :     :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#142], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L]

               :     :     :     :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#110], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L]

               :     :     :     :     :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#78], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L]

               :     :     :     :     :     :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#46], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :     :     :- FileScan orc test.test_a[day#11,week#12,weekday#13,dt#14] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_a..., PartitionFilters: [isnotnull(dt#14), (dt#14 = 20211126)], PushedFilters: [], ReadSchema: struct<day:string,week:int,weekday:int>

               :     :     :     :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#302]

               :     :     :     :     :     :     :     :     :     :        +- HashAggregate(keys=[dt#46], functions=[count(distinct kb_code#40)], output=[dt#46, a_kbs#0L])

               :     :     :     :     :     :     :     :     :     :           +- Exchange hashpartitioning(dt#46, 200), ENSURE_REQUIREMENTS, [id=#299]

               :     :     :     :     :     :     :     :     :     :              +- HashAggregate(keys=[dt#46], functions=[partial_count(distinct kb_code#40)], output=[dt#46, count#427L])

               :     :     :     :     :     :     :     :     :     :                 +- HashAggregate(keys=[dt#46, kb_code#40], functions=[], output=[dt#46, kb_code#40])

               :     :     :     :     :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#46, kb_code#40, 200), ENSURE_REQUIREMENTS, [id=#295]

               :     :     :     :     :     :     :     :     :     :                       +- HashAggregate(keys=[dt#46, kb_code#40], functions=[], output=[dt#46, kb_code#40])

               :     :     :     :     :     :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#40,dt#46] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#46), (dt#46 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#312]

               :     :     :     :     :     :     :     :     :        +- HashAggregate(keys=[dt#78], functions=[count(distinct kb_code#72)], output=[dt#78, b_kbs#1L])

               :     :     :     :     :     :     :     :     :           +- Exchange hashpartitioning(dt#78, 200), ENSURE_REQUIREMENTS, [id=#309]

               :     :     :     :     :     :     :     :     :              +- HashAggregate(keys=[dt#78], functions=[partial_count(distinct kb_code#72)], output=[dt#78, count#431L])

               :     :     :     :     :     :     :     :     :                 +- HashAggregate(keys=[dt#78, kb_code#72], functions=[], output=[dt#78, kb_code#72])

               :     :     :     :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#78, kb_code#72, 200), ENSURE_REQUIREMENTS, [id=#305]

               :     :     :     :     :     :     :     :     :                       +- HashAggregate(keys=[dt#78, kb_code#72], functions=[], output=[dt#78, kb_code#72])

               :     :     :     :     :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#72,dt#78] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#78), (dt#78 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#322]

               :     :     :     :     :     :     :     :        +- HashAggregate(keys=[dt#110], functions=[count(distinct kb_code#104)], output=[dt#110, c_kbs#2L])

               :     :     :     :     :     :     :     :           +- Exchange hashpartitioning(dt#110, 200), ENSURE_REQUIREMENTS, [id=#319]

               :     :     :     :     :     :     :     :              +- HashAggregate(keys=[dt#110], functions=[partial_count(distinct kb_code#104)], output=[dt#110, count#435L])

               :     :     :     :     :     :     :     :                 +- HashAggregate(keys=[dt#110, kb_code#104], functions=[], output=[dt#110, kb_code#104])

               :     :     :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#110, kb_code#104, 200), ENSURE_REQUIREMENTS, [id=#315]

               :     :     :     :     :     :     :     :                       +- HashAggregate(keys=[dt#110, kb_code#104], functions=[], output=[dt#110, kb_code#104])

               :     :     :     :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#104,dt#110] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#110), (dt#110 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#332]

               :     :     :     :     :     :     :        +- HashAggregate(keys=[dt#142], functions=[count(distinct kb_code#136)], output=[dt#142, d_kbs#3L])

               :     :     :     :     :     :     :           +- Exchange hashpartitioning(dt#142, 200), ENSURE_REQUIREMENTS, [id=#329]

               :     :     :     :     :     :     :              +- HashAggregate(keys=[dt#142], functions=[partial_count(distinct kb_code#136)], output=[dt#142, count#439L])

               :     :     :     :     :     :     :                 +- HashAggregate(keys=[dt#142, kb_code#136], functions=[], output=[dt#142, kb_code#136])

               :     :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#142, kb_code#136, 200), ENSURE_REQUIREMENTS, [id=#325]

               :     :     :     :     :     :     :                       +- HashAggregate(keys=[dt#142, kb_code#136], functions=[], output=[dt#142, kb_code#136])

               :     :     :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#136,dt#142] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#142), (dt#142 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#342]

               :     :     :     :     :     :        +- HashAggregate(keys=[dt#174], functions=[count(distinct kb_code#168)], output=[dt#174, e_kbs#4L])

               :     :     :     :     :     :           +- Exchange hashpartitioning(dt#174, 200), ENSURE_REQUIREMENTS, [id=#339]

               :     :     :     :     :     :              +- HashAggregate(keys=[dt#174], functions=[partial_count(distinct kb_code#168)], output=[dt#174, count#443L])

               :     :     :     :     :     :                 +- HashAggregate(keys=[dt#174, kb_code#168], functions=[], output=[dt#174, kb_code#168])

               :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#174, kb_code#168, 200), ENSURE_REQUIREMENTS, [id=#335]

               :     :     :     :     :     :                       +- HashAggregate(keys=[dt#174, kb_code#168], functions=[], output=[dt#174, kb_code#168])

               :     :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#168,dt#174] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#174), (dt#174 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#352]

               :     :     :     :     :        +- HashAggregate(keys=[dt#206], functions=[count(distinct kb_code#200)], output=[dt#206, f_kbs#5L])

               :     :     :     :     :           +- Exchange hashpartitioning(dt#206, 200), ENSURE_REQUIREMENTS, [id=#349]

               :     :     :     :     :              +- HashAggregate(keys=[dt#206], functions=[partial_count(distinct kb_code#200)], output=[dt#206, count#447L])

               :     :     :     :     :                 +- HashAggregate(keys=[dt#206, kb_code#200], functions=[], output=[dt#206, kb_code#200])

               :     :     :     :     :                    +- Exchange hashpartitioning(dt#206, kb_code#200, 200), ENSURE_REQUIREMENTS, [id=#345]

               :     :     :     :     :                       +- HashAggregate(keys=[dt#206, kb_code#200], functions=[], output=[dt#206, kb_code#200])

               :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#200,dt#206] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#206), (dt#206 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#362]

               :     :     :     :        +- HashAggregate(keys=[dt#238], functions=[count(distinct kb_code#232)], output=[dt#238, g_kbs#6L])

               :     :     :     :           +- Exchange hashpartitioning(dt#238, 200), ENSURE_REQUIREMENTS, [id=#359]

               :     :     :     :              +- HashAggregate(keys=[dt#238], functions=[partial_count(distinct kb_code#232)], output=[dt#238, count#451L])

               :     :     :     :                 +- HashAggregate(keys=[dt#238, kb_code#232], functions=[], output=[dt#238, kb_code#232])

               :     :     :     :                    +- Exchange hashpartitioning(dt#238, kb_code#232, 200), ENSURE_REQUIREMENTS, [id=#355]

               :     :     :     :                       +- HashAggregate(keys=[dt#238, kb_code#232], functions=[], output=[dt#238, kb_code#232])

               :     :     :     :                          +- FileScan orc test.test_b[kb_code#232,dt#238] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#238), (dt#238 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#372]

               :     :     :        +- HashAggregate(keys=[dt#270], functions=[count(distinct kb_code#264)], output=[dt#270, h_kbs#7L])

               :     :     :           +- Exchange hashpartitioning(dt#270, 200), ENSURE_REQUIREMENTS, [id=#369]

               :     :     :              +- HashAggregate(keys=[dt#270], functions=[partial_count(distinct kb_code#264)], output=[dt#270, count#455L])

               :     :     :                 +- HashAggregate(keys=[dt#270, kb_code#264], functions=[], output=[dt#270, kb_code#264])

               :     :     :                    +- Exchange hashpartitioning(dt#270, kb_code#264, 200), ENSURE_REQUIREMENTS, [id=#365]

               :     :     :                       +- HashAggregate(keys=[dt#270, kb_code#264], functions=[], output=[dt#270, kb_code#264])

               :     :     :                          +- FileScan orc test.test_b[kb_code#264,dt#270] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#270), (dt#270 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#382]

               :     :        +- HashAggregate(keys=[dt#302], functions=[count(distinct kb_code#296)], output=[dt#302, i_kbs#8L])

               :     :           +- Exchange hashpartitioning(dt#302, 200), ENSURE_REQUIREMENTS, [id=#379]

               :     :              +- HashAggregate(keys=[dt#302], functions=[partial_count(distinct kb_code#296)], output=[dt#302, count#459L])

               :     :                 +- HashAggregate(keys=[dt#302, kb_code#296], functions=[], output=[dt#302, kb_code#296])

               :     :                    +- Exchange hashpartitioning(dt#302, kb_code#296, 200), ENSURE_REQUIREMENTS, [id=#375]

               :     :                       +- HashAggregate(keys=[dt#302, kb_code#296], functions=[], output=[dt#302, kb_code#296])

               :     :                          +- FileScan orc test.test_b[kb_code#296,dt#302] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#302), (dt#302 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#392]

               :        +- HashAggregate(keys=[dt#334], functions=[count(distinct kb_code#328)], output=[dt#334, j_kbs#9L])

               :           +- Exchange hashpartitioning(dt#334, 200), ENSURE_REQUIREMENTS, [id=#389]

               :              +- HashAggregate(keys=[dt#334], functions=[partial_count(distinct kb_code#328)], output=[dt#334, count#463L])

               :                 +- HashAggregate(keys=[dt#334, kb_code#328], functions=[], output=[dt#334, kb_code#328])

               :                    +- Exchange hashpartitioning(dt#334, kb_code#328, 200), ENSURE_REQUIREMENTS, [id=#385]

               :                       +- HashAggregate(keys=[dt#334, kb_code#328], functions=[], output=[dt#334, kb_code#328])

               :                          +- FileScan orc test.test_b[kb_code#328,dt#334] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#334), (dt#334 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#402]

                  +- HashAggregate(keys=[dt#366], functions=[count(distinct kb_code#360)], output=[dt#366, k_kbs#10L])

                     +- Exchange hashpartitioning(dt#366, 200), ENSURE_REQUIREMENTS, [id=#399]

                        +- HashAggregate(keys=[dt#366], functions=[partial_count(distinct kb_code#360)], output=[dt#366, count#467L])

                           +- HashAggregate(keys=[dt#366, kb_code#360], functions=[], output=[dt#366, kb_code#360])

                              +- Exchange hashpartitioning(dt#366, kb_code#360, 200), ENSURE_REQUIREMENTS, [id=#395]

                                 +- HashAggregate(keys=[dt#366, kb_code#360], functions=[], output=[dt#366, kb_code#360])

                                    +- FileScan orc test.test_b[kb_code#360,dt#366] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#366), (dt#366 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>;;;, 06/Feb/23 01:59;ulysses;This should be resovled by SPARK-38138. The root reason is dpp will introduce many subqueries.;;;
Affects Version/s.1: 3.2.0
Comment.1: 09/Dec/21 06:00;oceaneast;Hi [~hyukjin.kwon]. This sql have 19 join operators.But these join have the same pattern. I found that ,when have 10 join operators, it costs 17s. when 11 join operators, costs 39s. when 12 join operators, costs 120s. when 13 join operators , it can not finish.  

 

I think, we can debug it at 11 join operators, to find why it is so slow. I had narrowed down 

as below:

=======

drop table if exists test.test_c;create table if not exists test.test_c stored as ORCFILE as
select calendar.day,calendar.week,calendar.weekday, a_kbs,
b_kbs, c_kbs,d_kbs,e_kbs,f_kbs,g_kbs,h_kbs,i_kbs,j_kbs,k_kbs
from (select * from test.test_a where dt = '20211126') calendar
left join
(select dt,count(distinct kb_code) as a_kbs
from test.test_b
where dt = '20211126'
group by dt) t1
on calendar.dt = t1.dt

left join
(select dt,count(distinct kb_code) as b_kbs
from test.test_b
where dt = '20211126'
group by dt) t2
on calendar.dt = t2.dt

left join
(select dt,count(distinct kb_code) as c_kbs
from test.test_b
where dt = '20211126'
group by dt) t3
on calendar.dt = t3.dt

left join
(select dt,count(distinct kb_code) as d_kbs
from test.test_b
where dt = '20211126'
group by dt) t4
on calendar.dt = t4.dt

left join
(select dt,count(distinct kb_code) as e_kbs
from test.test_b
where dt = '20211126'
group by dt) t5
on calendar.dt = t5.dt

left join
(select dt,count(distinct kb_code) as f_kbs
from test.test_b
where dt = '20211126'
group by dt) t6
on calendar.dt = t6.dt

left join
(select dt,count(distinct kb_code) as g_kbs
from test.test_b
where dt = '20211126'
group by dt) t7
on calendar.dt = t7.dt

left join
(select dt,count(distinct kb_code) as h_kbs
from test.test_b
where dt = '20211126'
group by dt) t8
on calendar.dt = t8.dt

left join
(select dt,count(distinct kb_code) as i_kbs
from test.test_b
where dt = '20211126'
group by dt) t9
on calendar.dt = t9.dt

left join
(select dt,count(distinct kb_code) as j_kbs
from test.test_b
where dt = '20211126'
group by dt) t10
on calendar.dt = t10.dt

left join
(select dt,count(distinct kb_code) as k_kbs
from test.test_b
where dt = '20211126'
group by dt) t11
on calendar.dt = t11.dt

 ;;;
Comment.2: 06/Feb/23 00:21;ritikam;Just executed this on 3.3.0  and it takes 4.14 second. I am also including the plan. So does not seem like an issue in 3.3.0

 

> left join

         > (select dt,count(distinct kb_code) as h_kbs

         > from test.test_b

         > where dt = '20211126'

         > group by dt) t8

         > on calendar.dt = t8.dt

         > 

         > left join

         > (select dt,count(distinct kb_code) as i_kbs

         > from test.test_b

         > where dt = '20211126'

         > group by dt) t9

         > on calendar.dt = t9.dt

         > 

         > left join

         > (select dt,count(distinct kb_code) as j_kbs

         > from test.test_b

         > where dt = '20211126'

         > group by dt) t10

         > on calendar.dt = t10.dt

         > 

         > left join

         > (select dt,count(distinct kb_code) as k_kbs

         > from test.test_b

         > where dt = '20211126'

         > group by dt) t11

         > on calendar.dt = t11.dt;

Time taken: 0.609 seconds

23/02/05 16:10:47 WARN HiveMetaStore: Location: file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_c specified for non-external table:test_c

{color:#FF0000}*Time taken: 4.14 seconds*{color}

spark-sql> select * from test.test_c

         > ;

1 1 2 1 1 1 1 1 1 1 1 1 1 1

Time taken: 0.296 seconds, Fetched 1 row(s)

spark-sql> 

 

{color:#FF0000}*The plan for the query is* {color}

 

CommandResult Execute OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

   +- OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

      +- Project [day#11, week#12, weekday#13, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L, k_kbs#10L]

         +- Join LeftOuter, (dt#14 = dt#366)

            :- Join LeftOuter, (dt#14 = dt#334)

            :  :- Join LeftOuter, (dt#14 = dt#302)

            :  :  :- Join LeftOuter, (dt#14 = dt#270)

            :  :  :  :- Join LeftOuter, (dt#14 = dt#238)

            :  :  :  :  :- Join LeftOuter, (dt#14 = dt#206)

            :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#174)

            :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#142)

            :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#110)

            :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#78)

            :  :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#46)

            :  :  :  :  :  :  :  :  :  :  :- SubqueryAlias calendar

            :  :  :  :  :  :  :  :  :  :  :  +- Project [day#11, week#12, weekday#13, dt#14]

            :  :  :  :  :  :  :  :  :  :  :     +- Filter (dt#14 = 20211126)

            :  :  :  :  :  :  :  :  :  :  :        +- SubqueryAlias spark_catalog.test.test_a

            :  :  :  :  :  :  :  :  :  :  :           +- Relation test.test_a[day#11,week#12,weekday#13,dt#14] orc

            :  :  :  :  :  :  :  :  :  :  +- SubqueryAlias t1

            :  :  :  :  :  :  :  :  :  :     +- Aggregate [dt#46], [dt#46, count(distinct kb_code#40) AS a_kbs#0L]

            :  :  :  :  :  :  :  :  :  :        +- Filter (dt#46 = 20211126)

            :  :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#15,device_id#16,brand#17,model#18,wx_version#19,os#20,net_work_type#21,app_id#22,app_name#23,col_z#24,page_url#25,page_title#26,olabel#27,otitle#28,source#29,send_dt#30,recv_dt#31,request_time#32,write_time#33,client_ip#34,col_a#35,dt_hour#36,product#37,channelfrom#38,... 8 more fields] orc

            :  :  :  :  :  :  :  :  :  +- SubqueryAlias t2

            :  :  :  :  :  :  :  :  :     +- Aggregate [dt#78], [dt#78, count(distinct kb_code#72) AS b_kbs#1L]

            :  :  :  :  :  :  :  :  :        +- Filter (dt#78 = 20211126)

            :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#47,device_id#48,brand#49,model#50,wx_version#51,os#52,net_work_type#53,app_id#54,app_name#55,col_z#56,page_url#57,page_title#58,olabel#59,otitle#60,source#61,send_dt#62,recv_dt#63,request_time#64,write_time#65,client_ip#66,col_a#67,dt_hour#68,product#69,channelfrom#70,... 8 more fields] orc

            :  :  :  :  :  :  :  :  +- SubqueryAlias t3

            :  :  :  :  :  :  :  :     +- Aggregate [dt#110], [dt#110, count(distinct kb_code#104) AS c_kbs#2L]

            :  :  :  :  :  :  :  :        +- Filter (dt#110 = 20211126)

            :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#79,device_id#80,brand#81,model#82,wx_version#83,os#84,net_work_type#85,app_id#86,app_name#87,col_z#88,page_url#89,page_title#90,olabel#91,otitle#92,source#93,send_dt#94,recv_dt#95,request_time#96,write_time#97,client_ip#98,col_a#99,dt_hour#100,product#101,channelfrom#102,... 8 more fields] orc

            :  :  :  :  :  :  :  +- SubqueryAlias t4

            :  :  :  :  :  :  :     +- Aggregate [dt#142], [dt#142, count(distinct kb_code#136) AS d_kbs#3L]

            :  :  :  :  :  :  :        +- Filter (dt#142 = 20211126)

            :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :              +- Relation test.test_b[session_id#111,device_id#112,brand#113,model#114,wx_version#115,os#116,net_work_type#117,app_id#118,app_name#119,col_z#120,page_url#121,page_title#122,olabel#123,otitle#124,source#125,send_dt#126,recv_dt#127,request_time#128,write_time#129,client_ip#130,col_a#131,dt_hour#132,product#133,channelfrom#134,... 8 more fields] orc

            :  :  :  :  :  :  +- SubqueryAlias t5

            :  :  :  :  :  :     +- Aggregate [dt#174], [dt#174, count(distinct kb_code#168) AS e_kbs#4L]

            :  :  :  :  :  :        +- Filter (dt#174 = 20211126)

            :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :              +- Relation test.test_b[session_id#143,device_id#144,brand#145,model#146,wx_version#147,os#148,net_work_type#149,app_id#150,app_name#151,col_z#152,page_url#153,page_title#154,olabel#155,otitle#156,source#157,send_dt#158,recv_dt#159,request_time#160,write_time#161,client_ip#162,col_a#163,dt_hour#164,product#165,channelfrom#166,... 8 more fields] orc

            :  :  :  :  :  +- SubqueryAlias t6

            :  :  :  :  :     +- Aggregate [dt#206], [dt#206, count(distinct kb_code#200) AS f_kbs#5L]

            :  :  :  :  :        +- Filter (dt#206 = 20211126)

            :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :              +- Relation test.test_b[session_id#175,device_id#176,brand#177,model#178,wx_version#179,os#180,net_work_type#181,app_id#182,app_name#183,col_z#184,page_url#185,page_title#186,olabel#187,otitle#188,source#189,send_dt#190,recv_dt#191,request_time#192,write_time#193,client_ip#194,col_a#195,dt_hour#196,product#197,channelfrom#198,... 8 more fields] orc

            :  :  :  :  +- SubqueryAlias t7

            :  :  :  :     +- Aggregate [dt#238], [dt#238, count(distinct kb_code#232) AS g_kbs#6L]

            :  :  :  :        +- Filter (dt#238 = 20211126)

            :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :              +- Relation test.test_b[session_id#207,device_id#208,brand#209,model#210,wx_version#211,os#212,net_work_type#213,app_id#214,app_name#215,col_z#216,page_url#217,page_title#218,olabel#219,otitle#220,source#221,send_dt#222,recv_dt#223,request_time#224,write_time#225,client_ip#226,col_a#227,dt_hour#228,product#229,channelfrom#230,... 8 more fields] orc

            :  :  :  +- SubqueryAlias t8

            :  :  :     +- Aggregate [dt#270], [dt#270, count(distinct kb_code#264) AS h_kbs#7L]

            :  :  :        +- Filter (dt#270 = 20211126)

            :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :              +- Relation test.test_b[session_id#239,device_id#240,brand#241,model#242,wx_version#243,os#244,net_work_type#245,app_id#246,app_name#247,col_z#248,page_url#249,page_title#250,olabel#251,otitle#252,source#253,send_dt#254,recv_dt#255,request_time#256,write_time#257,client_ip#258,col_a#259,dt_hour#260,product#261,channelfrom#262,... 8 more fields] orc

            :  :  +- SubqueryAlias t9

            :  :     +- Aggregate [dt#302], [dt#302, count(distinct kb_code#296) AS i_kbs#8L]

            :  :        +- Filter (dt#302 = 20211126)

            :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :              +- Relation test.test_b[session_id#271,device_id#272,brand#273,model#274,wx_version#275,os#276,net_work_type#277,app_id#278,app_name#279,col_z#280,page_url#281,page_title#282,olabel#283,otitle#284,source#285,send_dt#286,recv_dt#287,request_time#288,write_time#289,client_ip#290,col_a#291,dt_hour#292,product#293,channelfrom#294,... 8 more fields] orc

            :  +- SubqueryAlias t10

            :     +- Aggregate [dt#334], [dt#334, count(distinct kb_code#328) AS j_kbs#9L]

            :        +- Filter (dt#334 = 20211126)

            :           +- SubqueryAlias spark_catalog.test.test_b

            :              +- Relation test.test_b[session_id#303,device_id#304,brand#305,model#306,wx_version#307,os#308,net_work_type#309,app_id#310,app_name#311,col_z#312,page_url#313,page_title#314,olabel#315,otitle#316,source#317,send_dt#318,recv_dt#319,request_time#320,write_time#321,client_ip#322,col_a#323,dt_hour#324,product#325,channelfrom#326,... 8 more fields] orc

            +- SubqueryAlias t11

               +- Aggregate [dt#366], [dt#366, count(distinct kb_code#360) AS k_kbs#10L]

                  +- Filter (dt#366 = 20211126)

                     +- SubqueryAlias spark_catalog.test.test_b

                        +- Relation test.test_b[session_id#335,device_id#336,brand#337,model#338,wx_version#339,os#340,net_work_type#341,app_id#342,app_name#343,col_z#344,page_url#345,page_title#346,olabel#347,otitle#348,source#349,send_dt#350,recv_dt#351,request_time#352,write_time#353,client_ip#354,col_a#355,dt_hour#356,product#357,channelfrom#358,... 8 more fields] orc

 

== Analyzed Logical Plan ==

CommandResult Execute OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

   +- OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

      +- Project [day#11, week#12, weekday#13, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L, k_kbs#10L]

         +- Join LeftOuter, (dt#14 = dt#366)

            :- Join LeftOuter, (dt#14 = dt#334)

            :  :- Join LeftOuter, (dt#14 = dt#302)

            :  :  :- Join LeftOuter, (dt#14 = dt#270)

            :  :  :  :- Join LeftOuter, (dt#14 = dt#238)

            :  :  :  :  :- Join LeftOuter, (dt#14 = dt#206)

            :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#174)

            :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#142)

            :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#110)

            :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#78)

            :  :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#46)

            :  :  :  :  :  :  :  :  :  :  :- SubqueryAlias calendar

            :  :  :  :  :  :  :  :  :  :  :  +- Project [day#11, week#12, weekday#13, dt#14]

            :  :  :  :  :  :  :  :  :  :  :     +- Filter (dt#14 = 20211126)

            :  :  :  :  :  :  :  :  :  :  :        +- SubqueryAlias spark_catalog.test.test_a

            :  :  :  :  :  :  :  :  :  :  :           +- Relation test.test_a[day#11,week#12,weekday#13,dt#14] orc

            :  :  :  :  :  :  :  :  :  :  +- SubqueryAlias t1

            :  :  :  :  :  :  :  :  :  :     +- Aggregate [dt#46], [dt#46, count(distinct kb_code#40) AS a_kbs#0L]

            :  :  :  :  :  :  :  :  :  :        +- Filter (dt#46 = 20211126)

            :  :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#15,device_id#16,brand#17,model#18,wx_version#19,os#20,net_work_type#21,app_id#22,app_name#23,col_z#24,page_url#25,page_title#26,olabel#27,otitle#28,source#29,send_dt#30,recv_dt#31,request_time#32,write_time#33,client_ip#34,col_a#35,dt_hour#36,product#37,channelfrom#38,... 8 more fields] orc

            :  :  :  :  :  :  :  :  :  +- SubqueryAlias t2

            :  :  :  :  :  :  :  :  :     +- Aggregate [dt#78], [dt#78, count(distinct kb_code#72) AS b_kbs#1L]

            :  :  :  :  :  :  :  :  :        +- Filter (dt#78 = 20211126)

            :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#47,device_id#48,brand#49,model#50,wx_version#51,os#52,net_work_type#53,app_id#54,app_name#55,col_z#56,page_url#57,page_title#58,olabel#59,otitle#60,source#61,send_dt#62,recv_dt#63,request_time#64,write_time#65,client_ip#66,col_a#67,dt_hour#68,product#69,channelfrom#70,... 8 more fields] orc

            :  :  :  :  :  :  :  :  +- SubqueryAlias t3

            :  :  :  :  :  :  :  :     +- Aggregate [dt#110], [dt#110, count(distinct kb_code#104) AS c_kbs#2L]

            :  :  :  :  :  :  :  :        +- Filter (dt#110 = 20211126)

            :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#79,device_id#80,brand#81,model#82,wx_version#83,os#84,net_work_type#85,app_id#86,app_name#87,col_z#88,page_url#89,page_title#90,olabel#91,otitle#92,source#93,send_dt#94,recv_dt#95,request_time#96,write_time#97,client_ip#98,col_a#99,dt_hour#100,product#101,channelfrom#102,... 8 more fields] orc

            :  :  :  :  :  :  :  +- SubqueryAlias t4

            :  :  :  :  :  :  :     +- Aggregate [dt#142], [dt#142, count(distinct kb_code#136) AS d_kbs#3L]

            :  :  :  :  :  :  :        +- Filter (dt#142 = 20211126)

            :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :              +- Relation test.test_b[session_id#111,device_id#112,brand#113,model#114,wx_version#115,os#116,net_work_type#117,app_id#118,app_name#119,col_z#120,page_url#121,page_title#122,olabel#123,otitle#124,source#125,send_dt#126,recv_dt#127,request_time#128,write_time#129,client_ip#130,col_a#131,dt_hour#132,product#133,channelfrom#134,... 8 more fields] orc

            :  :  :  :  :  :  +- SubqueryAlias t5

            :  :  :  :  :  :     +- Aggregate [dt#174], [dt#174, count(distinct kb_code#168) AS e_kbs#4L]

            :  :  :  :  :  :        +- Filter (dt#174 = 20211126)

            :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :              +- Relation test.test_b[session_id#143,device_id#144,brand#145,model#146,wx_version#147,os#148,net_work_type#149,app_id#150,app_name#151,col_z#152,page_url#153,page_title#154,olabel#155,otitle#156,source#157,send_dt#158,recv_dt#159,request_time#160,write_time#161,client_ip#162,col_a#163,dt_hour#164,product#165,channelfrom#166,... 8 more fields] orc

            :  :  :  :  :  +- SubqueryAlias t6

            :  :  :  :  :     +- Aggregate [dt#206], [dt#206, count(distinct kb_code#200) AS f_kbs#5L]

            :  :  :  :  :        +- Filter (dt#206 = 20211126)

            :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :              +- Relation test.test_b[session_id#175,device_id#176,brand#177,model#178,wx_version#179,os#180,net_work_type#181,app_id#182,app_name#183,col_z#184,page_url#185,page_title#186,olabel#187,otitle#188,source#189,send_dt#190,recv_dt#191,request_time#192,write_time#193,client_ip#194,col_a#195,dt_hour#196,product#197,channelfrom#198,... 8 more fields] orc

            :  :  :  :  +- SubqueryAlias t7

            :  :  :  :     +- Aggregate [dt#238], [dt#238, count(distinct kb_code#232) AS g_kbs#6L]

            :  :  :  :        +- Filter (dt#238 = 20211126)

            :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :              +- Relation test.test_b[session_id#207,device_id#208,brand#209,model#210,wx_version#211,os#212,net_work_type#213,app_id#214,app_name#215,col_z#216,page_url#217,page_title#218,olabel#219,otitle#220,source#221,send_dt#222,recv_dt#223,request_time#224,write_time#225,client_ip#226,col_a#227,dt_hour#228,product#229,channelfrom#230,... 8 more fields] orc

            :  :  :  +- SubqueryAlias t8

            :  :  :     +- Aggregate [dt#270], [dt#270, count(distinct kb_code#264) AS h_kbs#7L]

            :  :  :        +- Filter (dt#270 = 20211126)

            :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :              +- Relation test.test_b[session_id#239,device_id#240,brand#241,model#242,wx_version#243,os#244,net_work_type#245,app_id#246,app_name#247,col_z#248,page_url#249,page_title#250,olabel#251,otitle#252,source#253,send_dt#254,recv_dt#255,request_time#256,write_time#257,client_ip#258,col_a#259,dt_hour#260,product#261,channelfrom#262,... 8 more fields] orc

            :  :  +- SubqueryAlias t9

            :  :     +- Aggregate [dt#302], [dt#302, count(distinct kb_code#296) AS i_kbs#8L]

            :  :        +- Filter (dt#302 = 20211126)

            :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :              +- Relation test.test_b[session_id#271,device_id#272,brand#273,model#274,wx_version#275,os#276,net_work_type#277,app_id#278,app_name#279,col_z#280,page_url#281,page_title#282,olabel#283,otitle#284,source#285,send_dt#286,recv_dt#287,request_time#288,write_time#289,client_ip#290,col_a#291,dt_hour#292,product#293,channelfrom#294,... 8 more fields] orc

            :  +- SubqueryAlias t10

            :     +- Aggregate [dt#334], [dt#334, count(distinct kb_code#328) AS j_kbs#9L]

            :        +- Filter (dt#334 = 20211126)

            :           +- SubqueryAlias spark_catalog.test.test_b

            :              +- Relation test.test_b[session_id#303,device_id#304,brand#305,model#306,wx_version#307,os#308,net_work_type#309,app_id#310,app_name#311,col_z#312,page_url#313,page_title#314,olabel#315,otitle#316,source#317,send_dt#318,recv_dt#319,request_time#320,write_time#321,client_ip#322,col_a#323,dt_hour#324,product#325,channelfrom#326,... 8 more fields] orc

            +- SubqueryAlias t11

               +- Aggregate [dt#366], [dt#366, count(distinct kb_code#360) AS k_kbs#10L]

                  +- Filter (dt#366 = 20211126)

                     +- SubqueryAlias spark_catalog.test.test_b

                        +- Relation test.test_b[session_id#335,device_id#336,brand#337,model#338,wx_version#339,os#340,net_work_type#341,app_id#342,app_name#343,col_z#344,page_url#345,page_title#346,olabel#347,otitle#348,source#349,send_dt#350,recv_dt#351,request_time#352,write_time#353,client_ip#354,col_a#355,dt_hour#356,product#357,channelfrom#358,... 8 more fields] orc

 

== Optimized Logical Plan ==

CommandResult Execute OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

   +- OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

      +- Project [day#11, week#12, weekday#13, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L, k_kbs#10L]

         +- Join LeftOuter, (dt#14 = dt#366)

            :- Join LeftOuter, (dt#14 = dt#334)

            :  :- Join LeftOuter, (dt#14 = dt#302)

            :  :  :- Join LeftOuter, (dt#14 = dt#270)

            :  :  :  :- Join LeftOuter, (dt#14 = dt#238)

            :  :  :  :  :- Join LeftOuter, (dt#14 = dt#206)

            :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#174)

            :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#142)

            :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#110)

            :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#78)

            :  :  :  :  :  :  :  :  :  :- Join LeftOuter, (dt#14 = dt#46)

            :  :  :  :  :  :  :  :  :  :  :- SubqueryAlias calendar

            :  :  :  :  :  :  :  :  :  :  :  +- Project [day#11, week#12, weekday#13, dt#14]

            :  :  :  :  :  :  :  :  :  :  :     +- Filter (dt#14 = 20211126)

            :  :  :  :  :  :  :  :  :  :  :        +- SubqueryAlias spark_catalog.test.test_a

            :  :  :  :  :  :  :  :  :  :  :           +- Relation test.test_a[day#11,week#12,weekday#13,dt#14] orc

            :  :  :  :  :  :  :  :  :  :  +- SubqueryAlias t1

            :  :  :  :  :  :  :  :  :  :     +- Aggregate [dt#46], [dt#46, count(distinct kb_code#40) AS a_kbs#0L]

            :  :  :  :  :  :  :  :  :  :        +- Filter (dt#46 = 20211126)

            :  :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#15,device_id#16,brand#17,model#18,wx_version#19,os#20,net_work_type#21,app_id#22,app_name#23,col_z#24,page_url#25,page_title#26,olabel#27,otitle#28,source#29,send_dt#30,recv_dt#31,request_time#32,write_time#33,client_ip#34,col_a#35,dt_hour#36,product#37,channelfrom#38,... 8 more fields] orc

            :  :  :  :  :  :  :  :  :  +- SubqueryAlias t2

            :  :  :  :  :  :  :  :  :     +- Aggregate [dt#78], [dt#78, count(distinct kb_code#72) AS b_kbs#1L]

            :  :  :  :  :  :  :  :  :        +- Filter (dt#78 = 20211126)

            :  :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#47,device_id#48,brand#49,model#50,wx_version#51,os#52,net_work_type#53,app_id#54,app_name#55,col_z#56,page_url#57,page_title#58,olabel#59,otitle#60,source#61,send_dt#62,recv_dt#63,request_time#64,write_time#65,client_ip#66,col_a#67,dt_hour#68,product#69,channelfrom#70,... 8 more fields] orc

            :  :  :  :  :  :  :  :  +- SubqueryAlias t3

            :  :  :  :  :  :  :  :     +- Aggregate [dt#110], [dt#110, count(distinct kb_code#104) AS c_kbs#2L]

            :  :  :  :  :  :  :  :        +- Filter (dt#110 = 20211126)

            :  :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :  :              +- Relation test.test_b[session_id#79,device_id#80,brand#81,model#82,wx_version#83,os#84,net_work_type#85,app_id#86,app_name#87,col_z#88,page_url#89,page_title#90,olabel#91,otitle#92,source#93,send_dt#94,recv_dt#95,request_time#96,write_time#97,client_ip#98,col_a#99,dt_hour#100,product#101,channelfrom#102,... 8 more fields] orc

            :  :  :  :  :  :  :  +- SubqueryAlias t4

            :  :  :  :  :  :  :     +- Aggregate [dt#142], [dt#142, count(distinct kb_code#136) AS d_kbs#3L]

            :  :  :  :  :  :  :        +- Filter (dt#142 = 20211126)

            :  :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :  :              +- Relation test.test_b[session_id#111,device_id#112,brand#113,model#114,wx_version#115,os#116,net_work_type#117,app_id#118,app_name#119,col_z#120,page_url#121,page_title#122,olabel#123,otitle#124,source#125,send_dt#126,recv_dt#127,request_time#128,write_time#129,client_ip#130,col_a#131,dt_hour#132,product#133,channelfrom#134,... 8 more fields] orc

            :  :  :  :  :  :  +- SubqueryAlias t5

            :  :  :  :  :  :     +- Aggregate [dt#174], [dt#174, count(distinct kb_code#168) AS e_kbs#4L]

            :  :  :  :  :  :        +- Filter (dt#174 = 20211126)

            :  :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :  :              +- Relation test.test_b[session_id#143,device_id#144,brand#145,model#146,wx_version#147,os#148,net_work_type#149,app_id#150,app_name#151,col_z#152,page_url#153,page_title#154,olabel#155,otitle#156,source#157,send_dt#158,recv_dt#159,request_time#160,write_time#161,client_ip#162,col_a#163,dt_hour#164,product#165,channelfrom#166,... 8 more fields] orc

            :  :  :  :  :  +- SubqueryAlias t6

            :  :  :  :  :     +- Aggregate [dt#206], [dt#206, count(distinct kb_code#200) AS f_kbs#5L]

            :  :  :  :  :        +- Filter (dt#206 = 20211126)

            :  :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :  :              +- Relation test.test_b[session_id#175,device_id#176,brand#177,model#178,wx_version#179,os#180,net_work_type#181,app_id#182,app_name#183,col_z#184,page_url#185,page_title#186,olabel#187,otitle#188,source#189,send_dt#190,recv_dt#191,request_time#192,write_time#193,client_ip#194,col_a#195,dt_hour#196,product#197,channelfrom#198,... 8 more fields] orc

            :  :  :  :  +- SubqueryAlias t7

            :  :  :  :     +- Aggregate [dt#238], [dt#238, count(distinct kb_code#232) AS g_kbs#6L]

            :  :  :  :        +- Filter (dt#238 = 20211126)

            :  :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :  :              +- Relation test.test_b[session_id#207,device_id#208,brand#209,model#210,wx_version#211,os#212,net_work_type#213,app_id#214,app_name#215,col_z#216,page_url#217,page_title#218,olabel#219,otitle#220,source#221,send_dt#222,recv_dt#223,request_time#224,write_time#225,client_ip#226,col_a#227,dt_hour#228,product#229,channelfrom#230,... 8 more fields] orc

            :  :  :  +- SubqueryAlias t8

            :  :  :     +- Aggregate [dt#270], [dt#270, count(distinct kb_code#264) AS h_kbs#7L]

            :  :  :        +- Filter (dt#270 = 20211126)

            :  :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :  :              +- Relation test.test_b[session_id#239,device_id#240,brand#241,model#242,wx_version#243,os#244,net_work_type#245,app_id#246,app_name#247,col_z#248,page_url#249,page_title#250,olabel#251,otitle#252,source#253,send_dt#254,recv_dt#255,request_time#256,write_time#257,client_ip#258,col_a#259,dt_hour#260,product#261,channelfrom#262,... 8 more fields] orc

            :  :  +- SubqueryAlias t9

            :  :     +- Aggregate [dt#302], [dt#302, count(distinct kb_code#296) AS i_kbs#8L]

            :  :        +- Filter (dt#302 = 20211126)

            :  :           +- SubqueryAlias spark_catalog.test.test_b

            :  :              +- Relation test.test_b[session_id#271,device_id#272,brand#273,model#274,wx_version#275,os#276,net_work_type#277,app_id#278,app_name#279,col_z#280,page_url#281,page_title#282,olabel#283,otitle#284,source#285,send_dt#286,recv_dt#287,request_time#288,write_time#289,client_ip#290,col_a#291,dt_hour#292,product#293,channelfrom#294,... 8 more fields] orc

            :  +- SubqueryAlias t10

            :     +- Aggregate [dt#334], [dt#334, count(distinct kb_code#328) AS j_kbs#9L]

            :        +- Filter (dt#334 = 20211126)

            :           +- SubqueryAlias spark_catalog.test.test_b

            :              +- Relation test.test_b[session_id#303,device_id#304,brand#305,model#306,wx_version#307,os#308,net_work_type#309,app_id#310,app_name#311,col_z#312,page_url#313,page_title#314,olabel#315,otitle#316,source#317,send_dt#318,recv_dt#319,request_time#320,write_time#321,client_ip#322,col_a#323,dt_hour#324,product#325,channelfrom#326,... 8 more fields] orc

            +- SubqueryAlias t11

               +- Aggregate [dt#366], [dt#366, count(distinct kb_code#360) AS k_kbs#10L]

                  +- Filter (dt#366 = 20211126)

                     +- SubqueryAlias spark_catalog.test.test_b

                        +- Relation test.test_b[session_id#335,device_id#336,brand#337,model#338,wx_version#339,os#340,net_work_type#341,app_id#342,app_name#343,col_z#344,page_url#345,page_title#346,olabel#347,otitle#348,source#349,send_dt#350,recv_dt#351,request_time#352,write_time#353,client_ip#354,col_a#355,dt_hour#356,product#357,channelfrom#358,... 8 more fields] orc

 

== Physical Plan ==

CommandResult <empty>

   +- Execute OptimizedCreateHiveTableAsSelectCommand [Database: test, TableName: test_c, InsertIntoHadoopFsRelationCommand]

      +- AdaptiveSparkPlan isFinalPlan=true

         +- == Final Plan ==

            *(34) Project [day#11, week#12, weekday#13, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L, k_kbs#10L]

            +- *(34) BroadcastHashJoin [dt#14], [dt#366], LeftOuter, BuildRight, false

               :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L]

               :  +- *(34) BroadcastHashJoin [dt#14], [dt#334], LeftOuter, BuildRight, false

               :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L]

               :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#302], LeftOuter, BuildRight, false

               :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L]

               :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#270], LeftOuter, BuildRight, false

               :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L]

               :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#238], LeftOuter, BuildRight, false

               :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L]

               :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#206], LeftOuter, BuildRight, false

               :     :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L]

               :     :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#174], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L]

               :     :     :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#142], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L]

               :     :     :     :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#110], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L]

               :     :     :     :     :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#78], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :     :- *(34) Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L]

               :     :     :     :     :     :     :     :     :     :  +- *(34) BroadcastHashJoin [dt#14], [dt#46], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :     :     :- *(34) ColumnarToRow

               :     :     :     :     :     :     :     :     :     :     :  +- FileScan orc test.test_a[day#11,week#12,weekday#13,dt#14] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_a..., PartitionFilters: [isnotnull(dt#14), (dt#14 = 20211126)], PushedFilters: [], ReadSchema: struct<day:string,week:int,weekday:int>

               :     :     :     :     :     :     :     :     :     :     +- BroadcastQueryStage 42

               :     :     :     :     :     :     :     :     :     :        +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     :     :     :     :     :     :           +- *(23) HashAggregate(keys=[dt#46], functions=[count(distinct kb_code#40)], output=[dt#46, a_kbs#0L])

               :     :     :     :     :     :     :     :     :     :              +- AQEShuffleRead coalesced

               :     :     :     :     :     :     :     :     :     :                 +- ShuffleQueryStage 21

               :     :     :     :     :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#46, 200), ENSURE_REQUIREMENTS, [id=#983]

               :     :     :     :     :     :     :     :     :     :                       +- *(12) HashAggregate(keys=[dt#46], functions=[partial_count(distinct kb_code#40)], output=[dt#46, count#427L])

               :     :     :     :     :     :     :     :     :     :                          +- *(12) HashAggregate(keys=[dt#46, kb_code#40], functions=[], output=[dt#46, kb_code#40])

               :     :     :     :     :     :     :     :     :     :                             +- AQEShuffleRead coalesced

               :     :     :     :     :     :     :     :     :     :                                +- ShuffleQueryStage 0

               :     :     :     :     :     :     :     :     :     :                                   +- Exchange hashpartitioning(dt#46, kb_code#40, 200), ENSURE_REQUIREMENTS, [id=#427]

               :     :     :     :     :     :     :     :     :     :                                      +- *(1) HashAggregate(keys=[dt#46, kb_code#40], functions=[], output=[dt#46, kb_code#40])

               :     :     :     :     :     :     :     :     :     :                                         +- *(1) ColumnarToRow

               :     :     :     :     :     :     :     :     :     :                                            +- FileScan orc test.test_b[kb_code#40,dt#46] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#46), (dt#46 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     :     :     :     :     +- BroadcastQueryStage 44

               :     :     :     :     :     :     :     :     :        +- ReusedExchange [dt#78, b_kbs#1L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     :     :     :     :     +- BroadcastQueryStage 46

               :     :     :     :     :     :     :     :        +- ReusedExchange [dt#110, c_kbs#2L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     :     :     :     +- BroadcastQueryStage 48

               :     :     :     :     :     :     :        +- ReusedExchange [dt#142, d_kbs#3L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     :     :     +- BroadcastQueryStage 50

               :     :     :     :     :     :        +- ReusedExchange [dt#174, e_kbs#4L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     :     +- BroadcastQueryStage 52

               :     :     :     :     :        +- ReusedExchange [dt#206, f_kbs#5L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     :     +- BroadcastQueryStage 54

               :     :     :     :        +- ReusedExchange [dt#238, g_kbs#6L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     :     +- BroadcastQueryStage 56

               :     :     :        +- ReusedExchange [dt#270, h_kbs#7L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     :     +- BroadcastQueryStage 58

               :     :        +- ReusedExchange [dt#302, i_kbs#8L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               :     +- BroadcastQueryStage 60

               :        +- ReusedExchange [dt#334, j_kbs#9L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

               +- BroadcastQueryStage 62

                  +- ReusedExchange [dt#366, k_kbs#10L], BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#1643]

         +- == Initial Plan ==

            Project [day#11, week#12, weekday#13, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L, k_kbs#10L]

            +- BroadcastHashJoin [dt#14], [dt#366], LeftOuter, BuildRight, false

               :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L, j_kbs#9L]

               :  +- BroadcastHashJoin [dt#14], [dt#334], LeftOuter, BuildRight, false

               :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L, i_kbs#8L]

               :     :  +- BroadcastHashJoin [dt#14], [dt#302], LeftOuter, BuildRight, false

               :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L, h_kbs#7L]

               :     :     :  +- BroadcastHashJoin [dt#14], [dt#270], LeftOuter, BuildRight, false

               :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L, g_kbs#6L]

               :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#238], LeftOuter, BuildRight, false

               :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L, f_kbs#5L]

               :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#206], LeftOuter, BuildRight, false

               :     :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L, e_kbs#4L]

               :     :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#174], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L, d_kbs#3L]

               :     :     :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#142], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L, c_kbs#2L]

               :     :     :     :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#110], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L, b_kbs#1L]

               :     :     :     :     :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#78], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :     :- Project [day#11, week#12, weekday#13, dt#14, a_kbs#0L]

               :     :     :     :     :     :     :     :     :     :  +- BroadcastHashJoin [dt#14], [dt#46], LeftOuter, BuildRight, false

               :     :     :     :     :     :     :     :     :     :     :- FileScan orc test.test_a[day#11,week#12,weekday#13,dt#14] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_a..., PartitionFilters: [isnotnull(dt#14), (dt#14 = 20211126)], PushedFilters: [], ReadSchema: struct<day:string,week:int,weekday:int>

               :     :     :     :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#302]

               :     :     :     :     :     :     :     :     :     :        +- HashAggregate(keys=[dt#46], functions=[count(distinct kb_code#40)], output=[dt#46, a_kbs#0L])

               :     :     :     :     :     :     :     :     :     :           +- Exchange hashpartitioning(dt#46, 200), ENSURE_REQUIREMENTS, [id=#299]

               :     :     :     :     :     :     :     :     :     :              +- HashAggregate(keys=[dt#46], functions=[partial_count(distinct kb_code#40)], output=[dt#46, count#427L])

               :     :     :     :     :     :     :     :     :     :                 +- HashAggregate(keys=[dt#46, kb_code#40], functions=[], output=[dt#46, kb_code#40])

               :     :     :     :     :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#46, kb_code#40, 200), ENSURE_REQUIREMENTS, [id=#295]

               :     :     :     :     :     :     :     :     :     :                       +- HashAggregate(keys=[dt#46, kb_code#40], functions=[], output=[dt#46, kb_code#40])

               :     :     :     :     :     :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#40,dt#46] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#46), (dt#46 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#312]

               :     :     :     :     :     :     :     :     :        +- HashAggregate(keys=[dt#78], functions=[count(distinct kb_code#72)], output=[dt#78, b_kbs#1L])

               :     :     :     :     :     :     :     :     :           +- Exchange hashpartitioning(dt#78, 200), ENSURE_REQUIREMENTS, [id=#309]

               :     :     :     :     :     :     :     :     :              +- HashAggregate(keys=[dt#78], functions=[partial_count(distinct kb_code#72)], output=[dt#78, count#431L])

               :     :     :     :     :     :     :     :     :                 +- HashAggregate(keys=[dt#78, kb_code#72], functions=[], output=[dt#78, kb_code#72])

               :     :     :     :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#78, kb_code#72, 200), ENSURE_REQUIREMENTS, [id=#305]

               :     :     :     :     :     :     :     :     :                       +- HashAggregate(keys=[dt#78, kb_code#72], functions=[], output=[dt#78, kb_code#72])

               :     :     :     :     :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#72,dt#78] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#78), (dt#78 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#322]

               :     :     :     :     :     :     :     :        +- HashAggregate(keys=[dt#110], functions=[count(distinct kb_code#104)], output=[dt#110, c_kbs#2L])

               :     :     :     :     :     :     :     :           +- Exchange hashpartitioning(dt#110, 200), ENSURE_REQUIREMENTS, [id=#319]

               :     :     :     :     :     :     :     :              +- HashAggregate(keys=[dt#110], functions=[partial_count(distinct kb_code#104)], output=[dt#110, count#435L])

               :     :     :     :     :     :     :     :                 +- HashAggregate(keys=[dt#110, kb_code#104], functions=[], output=[dt#110, kb_code#104])

               :     :     :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#110, kb_code#104, 200), ENSURE_REQUIREMENTS, [id=#315]

               :     :     :     :     :     :     :     :                       +- HashAggregate(keys=[dt#110, kb_code#104], functions=[], output=[dt#110, kb_code#104])

               :     :     :     :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#104,dt#110] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#110), (dt#110 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#332]

               :     :     :     :     :     :     :        +- HashAggregate(keys=[dt#142], functions=[count(distinct kb_code#136)], output=[dt#142, d_kbs#3L])

               :     :     :     :     :     :     :           +- Exchange hashpartitioning(dt#142, 200), ENSURE_REQUIREMENTS, [id=#329]

               :     :     :     :     :     :     :              +- HashAggregate(keys=[dt#142], functions=[partial_count(distinct kb_code#136)], output=[dt#142, count#439L])

               :     :     :     :     :     :     :                 +- HashAggregate(keys=[dt#142, kb_code#136], functions=[], output=[dt#142, kb_code#136])

               :     :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#142, kb_code#136, 200), ENSURE_REQUIREMENTS, [id=#325]

               :     :     :     :     :     :     :                       +- HashAggregate(keys=[dt#142, kb_code#136], functions=[], output=[dt#142, kb_code#136])

               :     :     :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#136,dt#142] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#142), (dt#142 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#342]

               :     :     :     :     :     :        +- HashAggregate(keys=[dt#174], functions=[count(distinct kb_code#168)], output=[dt#174, e_kbs#4L])

               :     :     :     :     :     :           +- Exchange hashpartitioning(dt#174, 200), ENSURE_REQUIREMENTS, [id=#339]

               :     :     :     :     :     :              +- HashAggregate(keys=[dt#174], functions=[partial_count(distinct kb_code#168)], output=[dt#174, count#443L])

               :     :     :     :     :     :                 +- HashAggregate(keys=[dt#174, kb_code#168], functions=[], output=[dt#174, kb_code#168])

               :     :     :     :     :     :                    +- Exchange hashpartitioning(dt#174, kb_code#168, 200), ENSURE_REQUIREMENTS, [id=#335]

               :     :     :     :     :     :                       +- HashAggregate(keys=[dt#174, kb_code#168], functions=[], output=[dt#174, kb_code#168])

               :     :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#168,dt#174] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#174), (dt#174 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#352]

               :     :     :     :     :        +- HashAggregate(keys=[dt#206], functions=[count(distinct kb_code#200)], output=[dt#206, f_kbs#5L])

               :     :     :     :     :           +- Exchange hashpartitioning(dt#206, 200), ENSURE_REQUIREMENTS, [id=#349]

               :     :     :     :     :              +- HashAggregate(keys=[dt#206], functions=[partial_count(distinct kb_code#200)], output=[dt#206, count#447L])

               :     :     :     :     :                 +- HashAggregate(keys=[dt#206, kb_code#200], functions=[], output=[dt#206, kb_code#200])

               :     :     :     :     :                    +- Exchange hashpartitioning(dt#206, kb_code#200, 200), ENSURE_REQUIREMENTS, [id=#345]

               :     :     :     :     :                       +- HashAggregate(keys=[dt#206, kb_code#200], functions=[], output=[dt#206, kb_code#200])

               :     :     :     :     :                          +- FileScan orc test.test_b[kb_code#200,dt#206] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#206), (dt#206 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#362]

               :     :     :     :        +- HashAggregate(keys=[dt#238], functions=[count(distinct kb_code#232)], output=[dt#238, g_kbs#6L])

               :     :     :     :           +- Exchange hashpartitioning(dt#238, 200), ENSURE_REQUIREMENTS, [id=#359]

               :     :     :     :              +- HashAggregate(keys=[dt#238], functions=[partial_count(distinct kb_code#232)], output=[dt#238, count#451L])

               :     :     :     :                 +- HashAggregate(keys=[dt#238, kb_code#232], functions=[], output=[dt#238, kb_code#232])

               :     :     :     :                    +- Exchange hashpartitioning(dt#238, kb_code#232, 200), ENSURE_REQUIREMENTS, [id=#355]

               :     :     :     :                       +- HashAggregate(keys=[dt#238, kb_code#232], functions=[], output=[dt#238, kb_code#232])

               :     :     :     :                          +- FileScan orc test.test_b[kb_code#232,dt#238] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#238), (dt#238 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#372]

               :     :     :        +- HashAggregate(keys=[dt#270], functions=[count(distinct kb_code#264)], output=[dt#270, h_kbs#7L])

               :     :     :           +- Exchange hashpartitioning(dt#270, 200), ENSURE_REQUIREMENTS, [id=#369]

               :     :     :              +- HashAggregate(keys=[dt#270], functions=[partial_count(distinct kb_code#264)], output=[dt#270, count#455L])

               :     :     :                 +- HashAggregate(keys=[dt#270, kb_code#264], functions=[], output=[dt#270, kb_code#264])

               :     :     :                    +- Exchange hashpartitioning(dt#270, kb_code#264, 200), ENSURE_REQUIREMENTS, [id=#365]

               :     :     :                       +- HashAggregate(keys=[dt#270, kb_code#264], functions=[], output=[dt#270, kb_code#264])

               :     :     :                          +- FileScan orc test.test_b[kb_code#264,dt#270] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#270), (dt#270 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#382]

               :     :        +- HashAggregate(keys=[dt#302], functions=[count(distinct kb_code#296)], output=[dt#302, i_kbs#8L])

               :     :           +- Exchange hashpartitioning(dt#302, 200), ENSURE_REQUIREMENTS, [id=#379]

               :     :              +- HashAggregate(keys=[dt#302], functions=[partial_count(distinct kb_code#296)], output=[dt#302, count#459L])

               :     :                 +- HashAggregate(keys=[dt#302, kb_code#296], functions=[], output=[dt#302, kb_code#296])

               :     :                    +- Exchange hashpartitioning(dt#302, kb_code#296, 200), ENSURE_REQUIREMENTS, [id=#375]

               :     :                       +- HashAggregate(keys=[dt#302, kb_code#296], functions=[], output=[dt#302, kb_code#296])

               :     :                          +- FileScan orc test.test_b[kb_code#296,dt#302] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#302), (dt#302 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#392]

               :        +- HashAggregate(keys=[dt#334], functions=[count(distinct kb_code#328)], output=[dt#334, j_kbs#9L])

               :           +- Exchange hashpartitioning(dt#334, 200), ENSURE_REQUIREMENTS, [id=#389]

               :              +- HashAggregate(keys=[dt#334], functions=[partial_count(distinct kb_code#328)], output=[dt#334, count#463L])

               :                 +- HashAggregate(keys=[dt#334, kb_code#328], functions=[], output=[dt#334, kb_code#328])

               :                    +- Exchange hashpartitioning(dt#334, kb_code#328, 200), ENSURE_REQUIREMENTS, [id=#385]

               :                       +- HashAggregate(keys=[dt#334, kb_code#328], functions=[], output=[dt#334, kb_code#328])

               :                          +- FileScan orc test.test_b[kb_code#328,dt#334] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#334), (dt#334 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>

               +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#402]

                  +- HashAggregate(keys=[dt#366], functions=[count(distinct kb_code#360)], output=[dt#366, k_kbs#10L])

                     +- Exchange hashpartitioning(dt#366, 200), ENSURE_REQUIREMENTS, [id=#399]

                        +- HashAggregate(keys=[dt#366], functions=[partial_count(distinct kb_code#360)], output=[dt#366, count#467L])

                           +- HashAggregate(keys=[dt#366, kb_code#360], functions=[], output=[dt#366, kb_code#360])

                              +- Exchange hashpartitioning(dt#366, kb_code#360, 200), ENSURE_REQUIREMENTS, [id=#395]

                                 +- HashAggregate(keys=[dt#366, kb_code#360], functions=[], output=[dt#366, kb_code#360])

                                    +- FileScan orc test.test_b[kb_code#360,dt#366] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex(1 paths)[file:/Users/ritka/Documents/spark-3.3.0/spark-warehouse/test.db/test_b..., PartitionFilters: [isnotnull(dt#366), (dt#366 = 20211126)], PushedFilters: [], ReadSchema: struct<kb_code:string>;;;
Comment.3: 06/Feb/23 01:59;ulysses;This should be resovled by SPARK-38138. The root reason is dpp will introduce many subqueries.;;;
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0

Summary: On overwrite mode, setting option truncate as true doesn't truncate the table
Issue key: SPARK-36720
Issue id: 13400493
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: balajiit
Creator: balajiit
Created: 10/Sep/21 14:59
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: I'm using PySpark from AWS Glue job to write it to SAP HANA using jdbc. Our requirement is to truncate and load data in HANA.

I've tried both of these options and on both cases, based on the stack trace, it is trying to drop the table which is not allowed by security design.

#df_lake.write.format("jdbc").option("url", edw_jdbc_url).option("driver", "com.sap.db.jdbc.Driver").option("dbtable", edw_jdbc_db_table).option("user", edw_jdbc_userid).option("password", edw_jdbc_password).option("truncate", "true").mode("append").save()
 properties=\{"user": edw_jdbc_userid, "password": edw_jdbc_password, "truncate":"true"}
df_lake.write.jdbc(url=edw_jdbc_url, table=edw_jdbc_db_table, mode='overwrite', properties=properties)

 

I've verified that the schema matches. I did the jdbc read and print out the schema as well as printing the schema from the source table.

Schema from HANA:
root
 |-- RTL_ACCT_ID: long (nullable = true)
 |-- FINE_DINING_PROPOSED: string (nullable = true)
 |-- FINE_WINE_PROPOSED: string (nullable = true)
 |-- FINE_WINE_INF_PROPOSED: string (nullable = true)
 |-- GOLD_SILVER_PROPOSED: string (nullable = true)
 |-- PREMIUM_PROPOSED: string (nullable = true)
 |-- GSP_PROPOSED: string (nullable = true)
 |-- PROPOSED_CRAFT: string (nullable = true)
 |-- FW_REASON: string (nullable = true)
 |-- FWI_REASON: string (nullable = true)
 |-- GS_REASON: string (nullable = true)
 |-- PREM_REASON: string (nullable = true)
 |-- FD_REASON: string (nullable = true)
 |-- CRAFT_REASON: string (nullable = true)
 |-- GSP_FLAG: string (nullable = true)
 |-- GSP_REASON: string (nullable = true)
 |-- ELIGIBILITY: string (nullable = true)
 |-- DW_LD_S: timestamp (nullable = true)

Schema from the source table: 
root
 |-- RTL_ACCT_ID: long (nullable = true)
 |-- FINE_DINING_PROPOSED: string (nullable = true)
 |-- FINE_WINE_PROPOSED: string (nullable = true)
 |-- FINE_WINE_INF_PROPOSED: string (nullable = true)
 |-- GOLD_SILVER_PROPOSED: string (nullable = true)
 |-- PREMIUM_PROPOSED: string (nullable = true)
 |-- GSP_PROPOSED: string (nullable = true)
 |-- PROPOSED_CRAFT: string (nullable = true)
 |-- FW_REASON: string (nullable = true)
 |-- FWI_REASON: string (nullable = true)
 |-- GS_REASON: string (nullable = true)
 |-- PREM_REASON: string (nullable = true)
 |-- FD_REASON: string (nullable = true)
 |-- CRAFT_REASON: string (nullable = true)
 |-- GSP_FLAG: string (nullable = true)
 |-- GSP_REASON: string (nullable = true)
 |-- ELIGIBILITY: string (nullable = true)
 |-- DW_LD_S: timestamp (nullable = true)

This is the stack trace
py4j.protocol.Py4JJavaError: An error occurred while calling o169.jdbc.
: com.sap.db.jdbc.exceptions.JDBCDriverException: SAP DBTech JDBC: [258]: insufficient privilege: Detailed info for this error can be found with guid 'xxxx'
	at com.sap.db.jdbc.exceptions.SQLExceptionSapDB._newInstance(SQLExceptionSapDB.java:191)
	at com.sap.db.jdbc.exceptions.SQLExceptionSapDB.newInstance(SQLExceptionSapDB.java:42)
	at com.sap.db.jdbc.packet.HReplyPacket._buildExceptionChain(HReplyPacket.java:976)
	at com.sap.db.jdbc.packet.HReplyPacket.getSQLExceptionChain(HReplyPacket.java:157)
	at com.sap.db.jdbc.packet.HPartInfo.getSQLExceptionChain(HPartInfo.java:39)
	at com.sap.db.jdbc.ConnectionSapDB._receive(ConnectionSapDB.java:3476)
	at com.sap.db.jdbc.ConnectionSapDB.exchange(ConnectionSapDB.java:1568)
	at com.sap.db.jdbc.StatementSapDB._executeDirect(StatementSapDB.java:1435)
	at com.sap.db.jdbc.StatementSapDB._execute(StatementSapDB.java:1414)
	at com.sap.db.jdbc.StatementSapDB._execute(StatementSapDB.java:1399)
	at com.sap.db.jdbc.StatementSapDB._executeUpdate(StatementSapDB.java:1387)
	at com.sap.db.jdbc.StatementSapDB.executeUpdate(StatementSapDB.java:175)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.executeStatement(JdbcUtils.scala:993)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.dropTable(JdbcUtils.scala:93)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:61)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:817)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python
Custom field (Last public comment date): Sun Sep 12 15:31:00 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ut1c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Sep/21 01:59;gurwls223;The error is from:

{quote}
com.sap.db.jdbc.exceptions.JDBCDriverException: SAP DBTech JDBC: [258]: insufficient privilege: Detailed info for this error can be found with guid 'xxxx'
{quote}

Mind elabourating why is it an issue in PySpark or Apache Spark?;;;, 12/Sep/21 15:31;balajiit;Because even though I’m setting mode is overwrite and truncate option is set to true, I would expect it to truncate the table and not drop the table. 


Sent from Yahoo Mail for iPhone


On Saturday, September 11, 2021, 7:00 PM, Hyukjin Kwon (Jira) <jira@apache.org> wrote:


    [ https://issues.apache.org/jira/browse/SPARK-36720?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=17413639#comment-17413639 ] 

Hyukjin Kwon commented on SPARK-36720:
--------------------------------------

The error is from:

{quote}
com.sap.db.jdbc.exceptions.JDBCDriverException: SAP DBTech JDBC: [258]: insufficient privilege: Detailed info for this error can be found with guid 'xxxx'
{quote}

Mind elabourating why is it an issue in PySpark or Apache Spark?




--
This message was sent by Atlassian Jira
(v8.3.4#803005)
;;;
Affects Version/s.1: 
Comment.1: 12/Sep/21 15:31;balajiit;Because even though I’m setting mode is overwrite and truncate option is set to true, I would expect it to truncate the table and not drop the table. 


Sent from Yahoo Mail for iPhone


On Saturday, September 11, 2021, 7:00 PM, Hyukjin Kwon (Jira) <jira@apache.org> wrote:


    [ https://issues.apache.org/jira/browse/SPARK-36720?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=17413639#comment-17413639 ] 

Hyukjin Kwon commented on SPARK-36720:
--------------------------------------

The error is from:

{quote}
com.sap.db.jdbc.exceptions.JDBCDriverException: SAP DBTech JDBC: [258]: insufficient privilege: Detailed info for this error can be found with guid 'xxxx'
{quote}

Mind elabourating why is it an issue in PySpark or Apache Spark?




--
This message was sent by Atlassian Jira
(v8.3.4#803005)
;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Spark SQL uses the function[ length()] to return the length of the string rather than the length of the character
Issue key: SPARK-34669
Issue id: 13363136
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: lucien
Creator: lucien
Created: 09/Mar/21 01:27
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Such as the title.

For the function length (). MySQL and other relational databases, we get the byte length, but spark SQL gets the string length. For these two cases, please provide a new function to get the byte length, otherwise it is easy to mislead users

----------------------------------------------------------------------------------------
{code:java}
// code placeholder
SparkSession.builder()
  .config(new SparkConf().setMaster("local"))
  .getOrCreate()
  .sql("select length('测a')")
  .show()

{code}
 

[result]

+-----------+
|length(测a)|

+-----------+
|2|

+-----------+

in mysql 

+-----------+
|length(测a)|

+-----------+
|4|

+-----------+

 

 
Environment: spark 3.1.1,scala 2.12
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Mar 11 03:49:51 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0oflc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Mar/21 11:42;gurwls223;The behaviour of function can be DBMS specific. How about other DBMSes other than MySQL?;;;, 11/Mar/21 03:01;lucien;Since spark SQL is a unified SQL programming interface, this basic function should be supported, not selectively.

So I suggest to provide a new function, which users can choose according to different scenarios.;;;, 11/Mar/21 03:49;gurwls223;I am suggesting to check the behaviours of length function in other DBMSes to see if Spark's behaviour makes sense or not, instead of arguing with one specific DBMS.;;;
Affects Version/s.1: 
Comment.1: 11/Mar/21 03:01;lucien;Since spark SQL is a unified SQL programming interface, this basic function should be supported, not selectively.

So I suggest to provide a new function, which users can choose according to different scenarios.;;;
Comment.2: 11/Mar/21 03:49;gurwls223;I am suggesting to check the behaviours of length function in other DBMSes to see if Spark's behaviour makes sense or not, instead of arguing with one specific DBMS.;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: DataFrameWriter.text support zstd compression
Issue key: SPARK-35196
Issue id: 13374496
Parent id: 
Issue Type: Task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: lausen
Creator: lausen
Created: 22/Apr/21 19:57
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: [http://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrameWriter.text.html] specifies that only the following compression codecs are supported: `none, bzip2, gzip, lz4, snappy and deflate`

However, RDD API supports compression with zstd if users specify 'org.apache.hadoop.io.compress.ZStandardCodec' compressor in the saveAsTextFile method.

Please also expose zstd in the DataFrameWriter.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Apr 24 05:58:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qcu8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Apr/21 10:51;gurwls223;It does support, and you can specify {{org.apache.hadoop.io.compress.ZStandardCodec}} for compression option. However, I agree with adding a short name for the easy use. Are you interested in adding an alias?

[~dongjoon]  FYI;;;, 23/Apr/21 14:05;lausen;Great.

Adding the alias should be straightforward but a helpful addition. I found the Python interface at [https://github.com/apache/spark/blob/faa928cefc8c1c6d7771aacd2ae7670162346361/python/pyspark/sql/readwriter.py#L1300-L1301] Could you point out where the _jdf.write / _jwrite / _jwriter are implemented? I suspect the alias needs to be added there.;;;, 23/Apr/21 14:11;gurwls223;Yeah, I think it's all implemented properly. We should probably add the alias at [https://github.com/apache/spark/blob/0494dc90af48ce7da0625485a4dc6917a244d580/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/CompressionCodecs.scala#L30-L36], and fix the documentations at DataFrameWriter.scala, DataStreamWriter.scala, streaming.py readwriter.py;;;, 24/Apr/21 05:42;dongjoon;Hi, [~lausen] and [~hyukjin.kwon]. We still didn't drop Hadoop 2.7.
`org.apache.hadoop.io.compress.ZStandardCodec` is added at Apache Hadoop 2.9.0+.
We may add some notes for the limitation, but I'm -1 for adding an alias.;;;, 24/Apr/21 05:44;dongjoon;In addition, even with Hadoop 3.1, the official Apache Spark distribution raises a failure when you try to use `org.apache.hadoop.io.compress.ZStandardCodec`.;;;, 24/Apr/21 05:46;gurwls223;I see. Thanks Dongjoon for clarification!;;;, 24/Apr/21 05:58;dongjoon;Ya, Sorry for the negative opinion. There was a previous report of that non-working situation. IIRC, there is a document commit to give a warning about that.;;;
Affects Version/s.1: 
Comment.1: 23/Apr/21 14:05;lausen;Great.

Adding the alias should be straightforward but a helpful addition. I found the Python interface at [https://github.com/apache/spark/blob/faa928cefc8c1c6d7771aacd2ae7670162346361/python/pyspark/sql/readwriter.py#L1300-L1301] Could you point out where the _jdf.write / _jwrite / _jwriter are implemented? I suspect the alias needs to be added there.;;;
Comment.2: 23/Apr/21 14:11;gurwls223;Yeah, I think it's all implemented properly. We should probably add the alias at [https://github.com/apache/spark/blob/0494dc90af48ce7da0625485a4dc6917a244d580/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/CompressionCodecs.scala#L30-L36], and fix the documentations at DataFrameWriter.scala, DataStreamWriter.scala, streaming.py readwriter.py;;;
Comment.3: 24/Apr/21 05:42;dongjoon;Hi, [~lausen] and [~hyukjin.kwon]. We still didn't drop Hadoop 2.7.
`org.apache.hadoop.io.compress.ZStandardCodec` is added at Apache Hadoop 2.9.0+.
We may add some notes for the limitation, but I'm -1 for adding an alias.;;;
Comment.4: 24/Apr/21 05:44;dongjoon;In addition, even with Hadoop 3.1, the official Apache Spark distribution raises a failure when you try to use `org.apache.hadoop.io.compress.ZStandardCodec`.;;;
Comment.5: 24/Apr/21 05:46;gurwls223;I see. Thanks Dongjoon for clarification!;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: rolled event log still need be clean after compact
Issue key: SPARK-37640
Issue id: 13417296
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: m-sir
Creator: m-sir
Created: 14/Dec/21 06:56
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: when we set "{{{}spark.eventLog.rolling.enabled{}}} =true", the eventlog will be roll and compact(when set "spark.eventLog.compression.codec"), the directory tree like this

root dir: /spark2xJobHistory2x/eventlog_v2_application_xxxxxxxxxxx_xxx_1

file in dir:

 /spark2xJobHistory2x/eventlog_v2_application_xxxx_xxx_1/events_xxxx_application_xxxxxxxxxxx_xxxx_1.zstd

 /spark2xJobHistory2x/eventlog_v2_application_xxxx_xxx_1/events_xxxx_application_xxxxxxxxxxx_xxxx_1.zstd

 /spark2xJobHistory2x/eventlog_v2_application_xxxx_xxx_1/events_xxxx_application_xxxxxxxxxxx_xxxx_1.zstd

......

......

 

a "long run" spark application, the history server will not clean the 'events_xxxx_application_xxxxxxxxxxx_xxxx_1.zstd' file in /spark2xJobHistory2x/eventlog_v2_application_xxxxxxxxxxx_xxx_1, so the size of directory will be bigger and bigger during the whole lifetime of app. 

so i think we should provide a mechanism for user to clean the “events_xxxx_application_xxxxxxxxxxx_xxxx_1.zstd” file in /spark2xJobHistory2x/eventlog_v2_application_xxxxxxxxxxx_xxx_1 directory

 

our solution：add a clean function in “[https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#checkForLogs]”，this function will list the file in “/spark2xJobHistory2x/eventlog_v2_application_xxxxxxxxxxx_xxx_1” and clean the “events_xxxx_application_xxxxxxxxxxx_xxxx_1.zstd” file according to the config "spark.history.fs.cleaner.maxAge". this solve the unlimited space increase ，but will loss some event，especially the start event，this will lead the history can not show the eventlog correctly。

 

so we will will have a much more proper way to solve this
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jan 10 06:29:39 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xo0o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Dec/21 08:29;gurwls223;[~m-sir] mind filling the JIRA description please?;;;, 10/Jan/22 06:29;m-sir;ok，i will add a more detail description;;;
Affects Version/s.1: 
Comment.1: 10/Jan/22 06:29;m-sir;ok，i will add a more detail description;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Thrown java.lang.NoClassDefFoundError when using spark-submit
Issue key: SPARK-35156
Issue id: 13373960
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: viirya
Creator: viirya
Created: 20/Apr/21 23:43
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Build, Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: Got NoClassDefFoundError when run spark-submit to submit Spark app to K8S cluster.

Master, branch-3.0 are okay. Branch-3.1 is affected.

How to reproduce:

1. Using sbt to build Spark with Kubernetes (-Pkubernetes)
2. Run spark-submit to submit to K8S cluster
3. Get the following exception 

{code:java}
21/04/20 16:33:37 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file                                 
Exception in thread "main" java.lang.NoClassDefFoundError: com/fasterxml/jackson/dataformat/yaml/YAMLFactory                                                      
        at io.fabric8.kubernetes.client.internal.KubeConfigUtils.parseConfigFromString(KubeConfigUtils.java:46)                                                   
        at io.fabric8.kubernetes.client.Config.loadFromKubeconfig(Config.java:564)                                                                                
        at io.fabric8.kubernetes.client.Config.tryKubeConfig(Config.java:530)                                                                                     
        at io.fabric8.kubernetes.client.Config.autoConfigure(Config.java:264)                                                                                     
        at io.fabric8.kubernetes.client.Config.<init>(Config.java:230)                                                                                            
        at io.fabric8.kubernetes.client.Config.<init>(Config.java:224)                                                                                            
        at io.fabric8.kubernetes.client.Config.autoConfigure(Config.java:259)                                                                                             at org.apache.spark.deploy.k8s.SparkKubernetesClientFactory$.createKubernetesClient(SparkKubernetesClientFactory.scala:80)                                
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$2(KubernetesClientApplication.scala:207)                                   
        at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2621)                                                                                         
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:207)                                              
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:179)                                            
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)                                                
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)                                                                                 
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)                                                                                      
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1030)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1039)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.dataformat.yaml.YAMLFactory
        at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
        ... 19 more {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Apr 23 19:27:08 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0q9jc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Apr/21 11:01;gurwls223;[~viirya] no big deal but:

{quote}
Master, branch-3.1 are okay. Branch-3.1 is affected
{quote}

Do you mean Branch-3.0 is affected alone?;;;, 23/Apr/21 18:15;srowen;Can you just use a later version? it may be a problem that was fixed.
Or some issue in how you packager your app, like, having it include incompatible K8S classes.;;;, 23/Apr/21 19:15;viirya;> Do you mean Branch-3.0 is affected alone?

Sorry for the typo. Only branch-3.1 is affected.;;;, 23/Apr/21 19:27;viirya;> Can you just use a later version? it may be a problem that was fixed.
> Or some issue in how you packager your app, like, having it include incompatible K8S classes.

Master is okay. I guess it is not intentionally fixed or is not backport? Because branch-3.1 has the issue. As branch-3.0 is okay too, maybe some change between 3.0 and master causes it.

The exception is seen locally when running spark-submit. You can also see the above stack trace that the exception is thrown early in SparkSubmit. And master, branch-3.0 both are not affected from the issue. It seems not related to how packaging app is done.

It is good if anyone can test it too in case I really did something incorrect during the tests.;;;
Affects Version/s.1: 
Comment.1: 23/Apr/21 18:15;srowen;Can you just use a later version? it may be a problem that was fixed.
Or some issue in how you packager your app, like, having it include incompatible K8S classes.;;;
Comment.2: 23/Apr/21 19:15;viirya;> Do you mean Branch-3.0 is affected alone?

Sorry for the typo. Only branch-3.1 is affected.;;;
Comment.3: 23/Apr/21 19:27;viirya;> Can you just use a later version? it may be a problem that was fixed.
> Or some issue in how you packager your app, like, having it include incompatible K8S classes.

Master is okay. I guess it is not intentionally fixed or is not backport? Because branch-3.1 has the issue. As branch-3.0 is okay too, maybe some change between 3.0 and master causes it.

The exception is seen locally when running spark-submit. You can also see the above stack trace that the exception is thrown early in SparkSubmit. And master, branch-3.0 both are not affected from the issue. It seems not related to how packaging app is done.

It is good if anyone can test it too in case I really did something incorrect during the tests.;;;
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Spark application submitted despite failing to get Hive delegation token
Issue key: SPARK-35160
Issue id: 13373987
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mauzhang
Creator: mauzhang
Created: 21/Apr/21 02:50
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Security
Due Date: 
Votes: 0
Labels: 
Description: Currently, when running on YARN and failing to get Hive delegation token, a Spark SQL application will still be submitted. Eventually, the application will fail on connecting to Hive metastore without a valid delegation token. 

Is there any reason for this design ?

cc [~jerryshao] who originally implemented this in https://issues.apache.org/jira/browse/SPARK-14743

I'd propose to fail immediately like HadoopFSDelegationTokenProvider.

 

Update:

After [https://github.com/apache/spark/pull/23418], HadoopFSDelegationTokenProvider no longer fail on non fatal exception. However, the author changed the behavior just to keep it consistent with other providers. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Apr 24 01:02:56 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0q9pc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Apr/21 11:00;gurwls223;[~mauzhang] if this is a question, it should better be asked to the mailing list. If you file an issue, it would be greatly helpful what suggestion you would propose.;;;, 24/Apr/21 01:02;mauzhang;[~hyukjin.kwon],

Thanks for reminder. I've added my proposal and I did ask about it on mailing list. It will be great if you know the reasoning behind it or you may forward to someone who knows.;;;
Affects Version/s.1: 
Comment.1: 24/Apr/21 01:02;mauzhang;[~hyukjin.kwon],

Thanks for reminder. I've added my proposal and I did ask about it on mailing list. It will be great if you know the reasoning behind it or you may forward to someone who knows.;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Serie to Scalar pandas_udf in GroupedData.agg() breaks the following monotonically_increasing_id()
Issue key: SPARK-35745
Issue id: 13383506
Parent id: 
Issue Type: Bug
Status: Reopened
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Marsu_
Creator: Marsu_
Created: 11/Jun/21 21:21
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Hello,

I encountered an issue when using a Serie to Scalar `{{panda_udf}}` in `{{GroupedData.agg()}}` followed by `{{monotonically_increasing_id()}}`. I obtain duplicated ids. Actually, the partition offset in the id seems to be zero on all partitions. The problem is avoided by using `{{asNondeterministic}}`.

Minimal reproducing example
{code:java}
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql.functions import pandas_udf
import pandas as pd
from pyspark.sql.types import IntegerType

spark = SparkSession.builder\
.config("spark.sql.execution.arrow.pyspark.enabled", "true")\
.config("spark.sql.shuffle.partitions", "8")\
.master("local[4]").getOrCreate()

@pandas_udf(IntegerType())
def sum_pandas(vals: pd.Series) -> int:
    return int(vals.to_numpy().sum())

@pandas_udf(IntegerType())
def sum_pandas2(vals: pd.Series) -> int: 
    return int(vals.to_numpy().sum())

sum_pandas2 = sum_pandas2.asNondeterministic()

l = [(i%100,i) for i in range(2000)]

data = spark.createDataFrame(l, schema=["col1","col2"])
data.groupby("col1").agg(sum_pandas("col2").alias("sum"))\
.withColumn("group_id", F.monotonically_increasing_id()).show()

data = spark.createDataFrame(l, schema=["col1","col2"])
data.groupby("col1").agg(sum_pandas2("col2").alias("sum"))\
.withColumn("group_id", F.monotonically_increasing_id()).show(){code}
Output
{code:java}
+----+-----+--------+
|col1|  sum|group_id|
+----+-----+--------+
|   2|19040|       0|
|  12|19240|       1|
|  26|19520|       2|
|  28|19560|       3|
|  29|19580|       4|
|  30|19600|       5|
|  33|19660|       6|
|  42|19840|       7|
|  48|19960|       8|
|  67|20340|       9|
|  73|20460|      10|
|  88|20760|      11|
|  91|20820|      12|
|  93|20860|      13|
|   9|19180|       0|
|  11|19220|       1|
|  22|19440|       2|
|  32|19640|       3|
|  36|19720|       4|
|  40|19800|       5|
+----+-----+--------+
only showing top 20 rows

+----+-----+----------+
|col1|  sum|  group_id|
+----+-----+----------+
|   2|19040|         0|
|  12|19240|         1|
|  26|19520|         2|
|  28|19560|         3|
|  29|19580|         4|
|  30|19600|         5|
|  33|19660|         6|
|  42|19840|         7|
|  48|19960|         8|
|  67|20340|         9|
|  73|20460|        10|
|  88|20760|        11|
|  91|20820|        12|
|  93|20860|        13|
|   9|19180|8589934592|
|  11|19220|8589934593|
|  22|19440|8589934594|
|  32|19640|8589934595|
|  36|19720|8589934596|
|  40|19800|8589934597|
+----+-----+----------+
only showing top 20 rows
{code}
Environment: I was able to reproduce this with both

pyspark == ' 3.1.1'
 pyarrow == '3.0.0'
Python 3.7.10

and

pyspark == '3.1.2'
 pyarrow == '4.0.1'
Python 3.7.9
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 14 12:46:42 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rwbc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Jun/21 04:49;gurwls223;{quote}
The problem is avoided by using `asNondeterministic`.
{quote}

This is the correct way to avoid this problem.;;;, 14/Jun/21 12:42;Marsu_;> This is the correct way to avoid this problem.

How come this is the correct way to avoid this problem ? There shouldn't be a need for using asNondeterministic as the pandas udf IS deterministic ?!;;;, 14/Jun/21 12:46;Marsu_;The ticket has been marked as resolved because marking the udf function as non-deterministic solves the problem. However, the idf function IS deterministic and so, I don't understand how one could consider this to be a proper solution ?;;;
Affects Version/s.1: 3.1.2
Comment.1: 14/Jun/21 12:42;Marsu_;> This is the correct way to avoid this problem.

How come this is the correct way to avoid this problem ? There shouldn't be a need for using asNondeterministic as the pandas udf IS deterministic ?!;;;
Comment.2: 14/Jun/21 12:46;Marsu_;The ticket has been marked as resolved because marking the udf function as non-deterministic solves the problem. However, the idf function IS deterministic and so, I don't understand how one could consider this to be a proper solution ?;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0

Summary: GeneratePredicate eliminate will fail  in some case
Issue key: SPARK-35688
Issue id: 13382841
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: cjuexuan
Creator: cjuexuan
Created: 09/Jun/21 05:33
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): code: 

```scala

val sparkSession = SparkSession
.builder()
.config(sparkConf)
.getOrCreate()

import sparkSession.implicits._

List(
("[b,c,d]", "{\"uid\": \"2\"}"),
("[a,b,d]", "{}"),
("?", "{}"),
("[d,b,c]", "{\"uid\": \"2\"}")
).toDF("name","json").write.mode("overwrite").parquet("/tmp/tb_eliminate_bad_case_data")
sparkSession.read
.parquet("/tmp/tb_eliminate_bad_case_data")
.createOrReplaceTempView("tb_data")

val name2Array: String => Array[String] = { input =>
if (input.startsWith("[") && input.endsWith("]")) {
input.substring(1, input.length - 1).split(",")
} else {
throw new IllegalArgumentException(s"invalid input $input")
}
}

sparkSession.udf.register("explode_name_array", name2Array)

sparkSession
.sql("""
|select uid ,name_array from (
|select
|explode(explode_name_array(name)) as name_array,
|get_json_object(json,'$.uid') as uid
|from tb_data
|where
| name is not null and name <> '?'
| )
| where uid > 0
|""".stripMargin)
.show()

```



errorMsg: 

```

06-09 13:28:11 175  WARN (org.apache.spark.scheduler.TaskSetManager:69) - Lost task 1.0 in stage 3.0 (TID 6) (192.168.112.64 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(NewDebug$$$Lambda$2695/852243673: (string) => array<string>)06-09 13:28:11 175  WARN (org.apache.spark.scheduler.TaskSetManager:69) - Lost task 1.0 in stage 3.0 (TID 6) (192.168.112.64 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(NewDebug$$$Lambda$2695/852243673: (string) => array<string>) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.subExpr_0$(generated.java:155) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(generated.java:22) at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:249) at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:248) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:509) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:485) at scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:218) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454) at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898) at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:131) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.IllegalArgumentException: invalid input ?

```
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Jun 19 18:12:05 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rs7k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Jun/21 05:44;cjuexuan;in this case ,because get_json_object is  CodegenFallback, so in CollapseCodegenStages, supportCodegen  will return false, and spark will call `org.apache.spark.sql.execution.FilterExec.doExecute`, and use  Predicate.create(condition, child.output)

if we remove condition  uid > 0 ,this plan supportCodegen and we can success execute this query ;;;, 09/Jun/21 06:00;cjuexuan;== Physical Plan ==
Project [get_json_object(json#14, $.uid) AS uid#20, name_array#21]
+- Generate explode(explode_name_array(name#13)), [json#14], false, [name_array#21]
 +- Filter ((((isnotnull(name#13) AND NOT (name#13 = ?)) AND (cast(get_json_object(json#14, $.uid) as int) > 0)) AND (size(explode_name_array(name#13), true) > 0)) AND isnotnull(explode_name_array(name#13)))
 +- *(1) ColumnarToRow
 +- FileScan parquet [name#13,json#14] Batched: true, DataFilters: [isnotnull(name#13), NOT (name#13 = ?), (cast(get_json_object(json#14, $.uid) as int) > 0), (size..., Format: Parquet, Location: InMemoryFileIndex[file:/tmp/tb_eliminate_bad_case_data], PartitionFilters: [], PushedFilters: [IsNotNull(name), Not(EqualTo(name,?))], ReadSchema: struct<name:string,json:string>

 

and from this plan filter invalid data " ?"   will execute before explode ,but because spark.sql.subexpressionElimination.enabled is true ,spark call  explode_name_array before filter data;;;, 09/Jun/21 06:01;cjuexuan;this causeBy [https://github.com/apache/spark/pull/29776|https://github.com/apache/spark/pull/29776/files];;;, 11/Jun/21 07:30;maropu;I run the query above in v3.1.1, but the issue didn't happen. Do I miss something?;;;, 11/Jun/21 07:50;cjuexuan;[~maropu] in this case ,we can't read  memory dataset directly ,we should  read data from file,and set spark.sql.subexpressionElimination.enabled to true  ,because  read memory dataset directly  spark will optimize plan ;;;, 14/Jun/21 05:05;gurwls223;[~cjuexuan] would you mind providing fully self-contained reproducer if the issue depends on external conditions?;;;, 14/Jun/21 16:57;fchen;Hi, [~maropu] , [~hyukjin.kwon] .

I can reproduce the bug by providing snippet code ( in the master branch).


When we enabled subexpression elimination, The interpreted predicate implementation`[GeneratePredicate|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GeneratePredicate.scala#L63-L64]` will eval the subexpressions (in this case `explode_name_array(name)`) before other condition expressions.
But note that the UDF may depend on the other condition. In this case, the UDF `name2Array` depends on the condition `name <> '?'`.;;;, 14/Jun/21 17:03;fchen;I think the subexpressions evaluations in the SpecificPredicate should be call-by-need, this is because the subexpressions may depend on other conditions and this also can improve the performance of subexpressions elimination.;;;, 19/Jun/21 18:12;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/32977;;;
Affects Version/s.1: 
Comment.1: 09/Jun/21 06:00;cjuexuan;== Physical Plan ==
Project [get_json_object(json#14, $.uid) AS uid#20, name_array#21]
+- Generate explode(explode_name_array(name#13)), [json#14], false, [name_array#21]
 +- Filter ((((isnotnull(name#13) AND NOT (name#13 = ?)) AND (cast(get_json_object(json#14, $.uid) as int) > 0)) AND (size(explode_name_array(name#13), true) > 0)) AND isnotnull(explode_name_array(name#13)))
 +- *(1) ColumnarToRow
 +- FileScan parquet [name#13,json#14] Batched: true, DataFilters: [isnotnull(name#13), NOT (name#13 = ?), (cast(get_json_object(json#14, $.uid) as int) > 0), (size..., Format: Parquet, Location: InMemoryFileIndex[file:/tmp/tb_eliminate_bad_case_data], PartitionFilters: [], PushedFilters: [IsNotNull(name), Not(EqualTo(name,?))], ReadSchema: struct<name:string,json:string>

 

and from this plan filter invalid data " ?"   will execute before explode ,but because spark.sql.subexpressionElimination.enabled is true ,spark call  explode_name_array before filter data;;;
Comment.2: 09/Jun/21 06:01;cjuexuan;this causeBy [https://github.com/apache/spark/pull/29776|https://github.com/apache/spark/pull/29776/files];;;
Comment.3: 11/Jun/21 07:30;maropu;I run the query above in v3.1.1, but the issue didn't happen. Do I miss something?;;;
Comment.4: 11/Jun/21 07:50;cjuexuan;[~maropu] in this case ,we can't read  memory dataset directly ,we should  read data from file,and set spark.sql.subexpressionElimination.enabled to true  ,because  read memory dataset directly  spark will optimize plan ;;;
Comment.5: 14/Jun/21 05:05;gurwls223;[~cjuexuan] would you mind providing fully self-contained reproducer if the issue depends on external conditions?;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: More clause needed for combining groupby and cube
Issue key: SPARK-35346
Issue id: 13377348
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wangjinjie722
Creator: wangjinjie722
Created: 08/May/21 03:39
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.2, 3.1.1
Fix Version/s: 
Component/s: PySpark, SQL
Due Date: 
Votes: 0
Labels: 
Description: As we all know, aggregation clause must follow after groupby, rollup or cube clause in pyspark. I think we should have more features in this part. Because in sql, we can write it like this "group by xxx, xxx, cube(xxx,xxx)". While in pyspark, if you just need cube for one field and group for the others, it's not gonna happen. Using cube for all fields brings much more cost for useless data. So I think we need to improve it. Thank you!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu May 13 03:04:43 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0queg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/May/21 07:23;gurwls223;It would be greatly helpful if you add some more references of DBMSes that support "group by xxx, xxx, cube(xxx,xxx)".;;;, 12/May/21 12:20;maropu;Do you mean this feature? https://issues.apache.org/jira/browse/SPARK-33229 ([https://github.com/apache/spark/blame/master/sql/core/src/test/resources/sql-tests/inputs/group-analytics.sql#L74-L81)] 
If yes, we've already support in in the recent master.;;;, 13/May/21 03:04;wangjinjie722;Thanks for the reply [~hyukjin.kwon] , but no. I mean in the operation of dataframe (in pyspark not pysql). We need a mixed case for group and cube or rollup. 

Now:

example_dataframe.cube('xxx','xxx','xxx').agg(XXXXXXXX) or example_dataframe.group('xxx','xxx','xxx').agg(XXXXXXXX)

Improve:

example_dataframe.group('xxx','xxx','xxx',cube('xxx','xxx','xxx')).agg(XXXXXXXX)

 

which is similar to this feature https://issues.apache.org/jira/browse/SPARK-33229 thanks [~maropu]

 ;;;
Affects Version/s.1: 3.0.2
Comment.1: 12/May/21 12:20;maropu;Do you mean this feature? https://issues.apache.org/jira/browse/SPARK-33229 ([https://github.com/apache/spark/blame/master/sql/core/src/test/resources/sql-tests/inputs/group-analytics.sql#L74-L81)] 
If yes, we've already support in in the recent master.;;;
Comment.2: 13/May/21 03:04;wangjinjie722;Thanks for the reply [~hyukjin.kwon] , but no. I mean in the operation of dataframe (in pyspark not pysql). We need a mixed case for group and cube or rollup. 

Now:

example_dataframe.cube('xxx','xxx','xxx').agg(XXXXXXXX) or example_dataframe.group('xxx','xxx','xxx').agg(XXXXXXXX)

Improve:

example_dataframe.group('xxx','xxx','xxx',cube('xxx','xxx','xxx')).agg(XXXXXXXX)

 

which is similar to this feature https://issues.apache.org/jira/browse/SPARK-33229 thanks [~maropu]

 ;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Provide the relationship between batch ID and SQL executions (and/or Jobs) in SS UI page
Issue key: SPARK-34580
Issue id: 13361564
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gsomogyi
Creator: gsomogyi
Created: 01/Mar/21 09:48
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: The current SS UI page focuses to show the trends among the batches, which is great to figure out whether the streaming query is running healthy or not, and the oddness of specific batch.

One thing still bugging you is that what you can get from here is the batch ID (number), which means you have to find out related SQL executions and Jobs manually with the batch ID. It's high likely bound to the recent runs of SQL executions/Jobs so you may not need to find it with searching on lots of pages, but the fact you need to find it by yourself manually is still annoying.

It would be nice if we can provide the relationship between batch ID and SQL executions (probably Jobs as well if the space is enough) and links to these pages, like we see job page links from SQL execution page.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 02 07:56:20 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0o5wo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Mar/21 09:52;gsomogyi;cc [~kabhwan] [~hyukjin.kwon] [~zsxwing] [~viirya]
It requires some time to come up with a makes sense solution but started.
;;;, 02/Mar/21 07:56;gurwls223;Thanks [~gsomogyi]!;;;
Affects Version/s.1: 
Comment.1: 02/Mar/21 07:56;gurwls223;Thanks [~gsomogyi]!;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: spark-hadoop-cloud broken on release and only published via 3rd party repositories
Issue key: SPARK-36936
Issue id: 13405117
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: colin.williams
Creator: colin.williams
Created: 06/Oct/21 05:09
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Input/Output
Due Date: 
Votes: 0
Labels: 
Description: The spark docmentation suggests using `spark-hadoop-cloud` to read / write from S3 in [https://spark.apache.org/docs/latest/cloud-integration.html] . However artifacts are currently published via only 3rd party resolvers in [https://mvnrepository.com/artifact/org.apache.spark/spark-hadoop-cloud] including Cloudera and Palantir.

 

Then apache spark documentation is providing a 3rd party solution for object stores including S3. Furthermore, if you follow the instructions and include one of the 3rd party jars IE the Cloudera jar with the spark 3.1.2 release and try to access object store, the following exception is returned.

 

```

Exception in thread "main" java.lang.NoSuchMethodError: 'void com.google.common.base.Preconditions.checkArgument(boolean, java.lang.String, java.lang.Object, java.lang.Object)'
 at org.apache.hadoop.fs.s3a.S3AUtils.lookupPassword(S3AUtils.java:894)
 at org.apache.hadoop.fs.s3a.S3AUtils.lookupPassword(S3AUtils.java:870)
 at org.apache.hadoop.fs.s3a.S3AUtils.getEncryptionAlgorithm(S3AUtils.java:1605)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:363)
 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
 at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
 at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
 at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
 at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)
 at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)
 at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)
 at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)
 at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:519)
 at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:428)

```

It looks like there are classpath conflicts using the cloudera published `spark-hadoop-cloud` with spark 3.1.2, again contradicting the documentation.

Then the documented `spark-hadoop-cloud` approach to using object stores is poorly supported only by 3rd party repositories and not by the released apache spark whose documentation refers to it.

Perhaps one day apache spark will provide tested software so that developers can quickly and easily access cloud object stores using the documentation.
Environment: name:=spark-demo

version := "0.0.1"

scalaVersion := "2.12.12"

lazy val app = (project in file("app")).settings(
 assemblyPackageScala / assembleArtifact := false,
 assembly / assemblyJarName := "uber.jar",
 assembly / mainClass := Some("com.example.Main"),
 // more settings here ...
 )

resolvers += "Cloudera" at "https://repository.cloudera.com/artifactory/cloudera-repos/"

libraryDependencies += "org.apache.spark" %% "spark-sql" % "3.1.2" % "provided"
libraryDependencies += "org.apache.spark" %% "spark-hadoop-cloud" % "3.1.1.3.1.7270.0-253"
libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.1.1.7.2.7.0-184"
libraryDependencies += "com.amazonaws" % "aws-java-sdk-bundle" % "1.11.901"

libraryDependencies += "org.scalatest" %% "scalatest" % "3.0.1" % "test"

// test suite settings
fork in Test := true
javaOptions ++= Seq("-Xms512M", "-Xmx2048M", "-XX:MaxPermSize=2048M", "-XX:+CMSClassUnloadingEnabled")
// Show runtime of tests
testOptions in Test += Tests.Argument(TestFrameworks.ScalaTest, "-oD")

___________________________________________________________________________________________

 

import org.apache.spark.sql.SparkSession

object SparkApp {
 def main(args: Array[String]){
 val spark = SparkSession.builder().master("local")
 //.config("spark.jars.repositories", "https://repository.cloudera.com/artifactory/cloudera-repos/")
 //.config("spark.jars.packages", "org.apache.spark:spark-hadoop-cloud_2.12:3.1.1.3.1.7270.0-253")
 .appName("spark session").getOrCreate

 val jsonDF = spark.read.json("s3a://path-to-bucket/compact.json")
 val csvDF = spark.read.format("csv").load("s3a://path-to-bucket/some.csv")
 jsonDF.show()
 csvDF.show()
 }
}
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Oct 09 20:31:13 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vljs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Oct/21 06:45;gurwls223;cc [~sunchao] FYI;;;, 06/Oct/21 19:13;csun;[~colin.williams] which version of {{spark-hadoop-cloud}} you were using? I think the above error shouldn't happen if the version is the same as the Spark's version.

We've already started to publish {{spark-hadoop-cloud}} as part of the Spark release procedure, see SPARK-35844.;;;, 07/Oct/21 19:40;colin.williams;[~csun] when I see SPARK-35844 I see 3.2.0 version for the jar. That does not look to be published.



2021.10.07 12:39:03 INFO [warn] Note: Unresolved dependencies path:
2021.10.07 12:39:03 INFO [error] sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-hadoop-cloud_2.12:3.2.0
2021.10.07 12:39:03 INFO [error] Not found
2021.10.07 12:39:03 INFO [error] Not found
2021.10.07 12:39:03 INFO [error] not found: /home/colin/.ivy2/local/org.apache.spark/spark-hadoop-cloud_2.12/3.2.0/ivys/ivy.xml
2021.10.07 12:39:03 INFO [error] not found: https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.12/3.2.0/spark-hadoop-cloud_2.12-3.2.0.pom
2021.10.07 12:39:03 INFO [error] not found: https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/spark/spark-hadoop-cloud_2.12/3.2.0/spark-hadoop-cloud_2.12-3.2.0.pom;;;, 08/Oct/21 16:04;csun;[~colin.williams] Spark 3.2.0 is not released yet - it will be there soon.;;;, 09/Oct/21 20:31;colin.williams;But the Spark 3.1.2 documentation  [https://spark.apache.org/docs/3.1.2/cloud-integration.html] states:

<dependencyManagement>
 ...
 <dependency>
 <groupId>org.apache.spark</groupId>
 <artifactId>hadoop-cloud_2.12</artifactId>
 <version>${spark.version}</version>
 <scope>provided</scope>
 </dependency>
 ...
 </dependencyManagement>

 

For which I show an artifact for 3.1.2 does not exist.

 

 
 2021.10.09 13:34:47 INFO [error] (update) sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-hadoop-cloud_2.12:3.1.2
 2021.10.09 13:34:47 INFO [error] Not found
 2021.10.09 13:34:47 INFO [error] Not found
 2021.10.09 13:34:47 INFO [error] not found: /home/colin/.ivy2/local/org.apache.spark/spark-hadoop-cloud_2.12/3.1.2/ivys/ivy.xml
 2021.10.09 13:34:47 INFO [error] not found: [https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.12/3.1.2/spark-hadoop-cloud_2.12-3.1.2.pom]
 2021.10.09 13:34:47 INFO [error] not found: [https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/spark/spark-hadoop-cloud_2.12/3.1.2/spark-hadoop-cloud_2.12-3.1.2.po];;;
Affects Version/s.1: 3.1.2
Comment.1: 06/Oct/21 19:13;csun;[~colin.williams] which version of {{spark-hadoop-cloud}} you were using? I think the above error shouldn't happen if the version is the same as the Spark's version.

We've already started to publish {{spark-hadoop-cloud}} as part of the Spark release procedure, see SPARK-35844.;;;
Comment.2: 07/Oct/21 19:40;colin.williams;[~csun] when I see SPARK-35844 I see 3.2.0 version for the jar. That does not look to be published.



2021.10.07 12:39:03 INFO [warn] Note: Unresolved dependencies path:
2021.10.07 12:39:03 INFO [error] sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-hadoop-cloud_2.12:3.2.0
2021.10.07 12:39:03 INFO [error] Not found
2021.10.07 12:39:03 INFO [error] Not found
2021.10.07 12:39:03 INFO [error] not found: /home/colin/.ivy2/local/org.apache.spark/spark-hadoop-cloud_2.12/3.2.0/ivys/ivy.xml
2021.10.07 12:39:03 INFO [error] not found: https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.12/3.2.0/spark-hadoop-cloud_2.12-3.2.0.pom
2021.10.07 12:39:03 INFO [error] not found: https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/spark/spark-hadoop-cloud_2.12/3.2.0/spark-hadoop-cloud_2.12-3.2.0.pom;;;
Comment.3: 08/Oct/21 16:04;csun;[~colin.williams] Spark 3.2.0 is not released yet - it will be there soon.;;;
Comment.4: 09/Oct/21 20:31;colin.williams;But the Spark 3.1.2 documentation  [https://spark.apache.org/docs/3.1.2/cloud-integration.html] states:

<dependencyManagement>
 ...
 <dependency>
 <groupId>org.apache.spark</groupId>
 <artifactId>hadoop-cloud_2.12</artifactId>
 <version>${spark.version}</version>
 <scope>provided</scope>
 </dependency>
 ...
 </dependencyManagement>

 

For which I show an artifact for 3.1.2 does not exist.

 

 
 2021.10.09 13:34:47 INFO [error] (update) sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-hadoop-cloud_2.12:3.1.2
 2021.10.09 13:34:47 INFO [error] Not found
 2021.10.09 13:34:47 INFO [error] Not found
 2021.10.09 13:34:47 INFO [error] not found: /home/colin/.ivy2/local/org.apache.spark/spark-hadoop-cloud_2.12/3.1.2/ivys/ivy.xml
 2021.10.09 13:34:47 INFO [error] not found: [https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.12/3.1.2/spark-hadoop-cloud_2.12-3.1.2.pom]
 2021.10.09 13:34:47 INFO [error] not found: [https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/spark/spark-hadoop-cloud_2.12/3.1.2/spark-hadoop-cloud_2.12-3.1.2.po];;;
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0

Summary: non consistent results running count for same dataset after filter and lead window function
Issue key: SPARK-35089
Issue id: 13372623
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Tonzetic
Creator: Tonzetic
Created: 15/Apr/21 12:17
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: ****   edit 2021-05-18

I have make it  simpler to reproduce; I've put already generated data on s3 bucket that is publicly available with 24.000.000 records

Now all you need to do is run this code:
{code:java}
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql._
import org.apache.spark.sql.functions._

val w = Window.partitionBy("user").orderBy("start")
val ts_lead = coalesce(lead("start", 1) .over(w), lit(30000000))

spark.read.orc("s3://dtonzetic-spark-sample-data/sample-data.orc").
 withColumn("end", ts_lead).
 withColumn("duration", col("end")-col("start")).
 where("type='TypeA' and duration>4").count()
{code}
 

this were my results:
 - run 1: 2547559
 - run 2: 2547559
 - run 3: 2547560
 - run 4: 2547558
 - run 5: 2547558
 - run 6: 2547559
 - run 7: 2547558

This results are from new EMR cluster, version 6.3.0, so nothing changed.

****   end edit 2021-05-18

I have found an inconsistency with count function results after lead window function and filter.

 

I have a dataframe (this is simplified version, but it's enough to reproduce) with millions of records, with these columns:
 * df1:
 ** start(timestamp)
 ** user_id(int)
 ** type(string)

I need to define duration between two rows, and filter on that duration and type. I used window lead function to get the next event time (that define end for current event), so every row now gets start and stop times. If NULL (last row for example), add next midnight as stop. Data is stored in ORC file (tried with Parquet format, no difference)

This only happens with multiple cluster nodes, for example AWS EMR cluster or local docker cluster setup. If I run it on single instance (local on laptop), I get consistent results every time. Spark version is 3.0.1, both in AWS and local and docker setup.

Here is some simple code that you can use to reproduce it, I've used jupyterLab notebook on AWS EMR. Spark version is 3.0.1.

 

 
{code:java}
import org.apache.spark.sql.expressions.Window

// this dataframe generation code should be executed only once, and data have to be saved, and then opened from disk, so it's always same.

val getRandomUser = udf(()=>{
    val users = Seq("John","Eve","Anna","Martin","Joe","Steve","Katy")
   users(scala.util.Random.nextInt(7))
})

val getRandomType = udf(()=>{
    val types = Seq("TypeA","TypeB","TypeC","TypeD","TypeE")
    types(scala.util.Random.nextInt(5))
})

val getRandomStart = udf((x:Int)=>{
    x+scala.util.Random.nextInt(47)
})
// for loop is used to avoid out of memory error during creation of dataframe
for( a <- 0 to 23){
        // use iterator a to continue with next million, repeat 1 mil times
        val x=Range(a*1000000,(a*1000000)+1000000).toDF("id").
            withColumn("start",getRandomStart(col("id"))).
            withColumn("user",getRandomUser()).
            withColumn("type",getRandomType()).
            drop("id")

        x.write.mode("append").orc("hdfs:///random.orc")
}

// above code should be run only once, I used a cell in Jupyter

// define window and lead
val w = Window.partitionBy("user").orderBy("start")
// if null, replace with 30.000.000
val ts_lead = coalesce(lead("start", 1) .over(w), lit(30000000))

// read data to dataframe, create stop column and calculate duration
val fox2 = spark.read.orc("hdfs:///random.orc").
    withColumn("end", ts_lead).
    withColumn("duration", col("end")-col("start"))


// repeated executions of this line returns different results for count 
// I have it in separate cell in JupyterLab
fox2.where("type='TypeA' and duration>4").count()
{code}
My results for three consecutive runs of last line were:
 * run 1: 2551259
 * run 2: 2550756
 * run 3: 2551279

It's very important to say that if I use filter:

fox2.where("type='TypeA' ")

or 

fox2.where("duration>4"),

 

each of them can be executed repeatedly and I get consistent result every time.

I can save dataframe after crating stop and duration columns, and after that, I get consistent results every time.

It is not very practical workaround, as I need a lot of space and time to implement it.

This dataset is really big (in my eyes at least, aprox 100.000.000 new records per day).

If I run this same example on my local machine using master = local[*], everything works as expected, it's just on cluster setup. I tried to create cluster using docker on my local machine, created 3.0.1 and 3.1.1 clusters with one master and two workers, and have successfully reproduced issue.

 

 

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jun 18 11:39:57 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0q1a8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Apr/21 03:06;gurwls223;Can you try enable {{asNondeterministic()}}? e.g.) {{getRandomType.asNondeterministic()}};;;, 18/Apr/21 05:50;Tonzetic;I can, but it is not random that is problem.

Once you generate set with random data, you write it an read it to separate dataframe, and than run window function and filter on that dataframe. 

So, in repeated runs you use same data every time.

I generated this random dataset just to have some data to make a proof.

 

I actually discovered this running on real data, that is read from JSONNL files.

There are no UDFs and nothing is random, just data read from file. 

So you need to make two separate steps here:

1 . generate random data and write it to file (here it is ORC)  – THIS STEP SHOULD BE RUN ONLY ONCE 

2. read that data to new dataframe and run window lead and complex filter (with two conditions).  - THIS STEP SHOULD BE REPEATED

 

Again, it has to be in cluster setup, not single node.

On my docker setup (I can provide you with dockerfile) that I need to repartition dataframe on read to get different results.

example :

read.orc("bla_bla").repartition(200)

and 

read.orc("bla_bla").repartition(100)

 On AWS EMR Spark setup I does not matters, returns different numbers on every count.

 

 ;;;, 18/May/21 08:39;Tonzetic;I've made test data available via public s3 bucket, so it's easier to reproduce now.;;;, 29/May/21 16:05;revans2;On window functions if the {{order by}} clause is ambiguous you can get different results from one run to the next.  This is because the order in which shuffle data is read in is not deterministic, even though the sorting is.   In this example you are generating start with

{code}
val getRandomStart = udf((x:Int)=>{
    x+scala.util.Random.nextInt(47)
})
{code}

The input to this {x} appears to be non-abiguous (0-some very large number), but because of the + random(0 to 47) there is the possibility of multiple start values being the same.

So for operations where order matters you can get ambiguous results. For lead and lag a different lead/lag value can show up, because the one right after this one {{lead(1)}} is different.  For operations like rank, dense_rank, and row_number the order of the values output is the same, but the rows are in a different order so the value at each row/rank is different. This can also impact operations like SUM, MIN, and MAX that use a row bounds on a windows instead of value ranges.  I'm not sure if this should be considered a bug or not.  Spark treats all window operations as deterministic, so in theory if there is a crash you can get inconsistent results within the same query, but that only happens if the end user put in a non-deterministic ordering.;;;, 01/Jun/21 07:11;Tonzetic;It's really not important to look at random functions.

They are here just because I do not have permission to share data with you. Therefore I created test set of data using this random functions, merely to have some data to work with.

You can see that I've made sample data publicly available on S3 bucket.

So to cut the long story short, you only need to download data and try to run code with window functions.

Regarding is this is a bug or a design flaw or something else, I must say that I (and probably everyone else) am expecting consistent results from same data.

So yes, this is a problem and explanation why is probably happening will not solve this issue.

This means that using spark in, for example, financial calculations, is practically a big no no, because you cannot get consistent results of calculations.

 ;;;, 01/Jun/21 13:08;revans2;[~Tonzetic] to be clear my point was just to provide more information about the problem. I agree with you that this feels very much like a bug, and I would like to see it fixed. My hope was that with the added information someone in the Spark community could look at ways to fix it and at a minimum you could look at ways to work around it for your particular use case.  One such option is to remove the ambiguity by adding in a total ordering with {{monotonically_increasing_id}} early on in your processing (when you read the data in).  You should not rely on the exact value in this column (as it can change based off of the shape of the cluster you are running on), but you can use it as a part of your ordering to get unambiguous results.

For example.
 
{code:scala}
// define window and lead
val w = Window.partitionBy("user").orderBy("start", "unambiguous_id")
// if null, replace with 30.000.000
val ts_lead = coalesce(lead("start", 1) .over(w), lit(30000000))

// read data to dataframe, create stop column and calculate duration
val fox2 = spark.read.orc("hdfs:///random.orc").
    withColumn("unambiguous_id",  monotonically_increasing_id()).
    withColumn("end", ts_lead).
    withColumn("duration", col("end")-col("start"))


// repeated executions of this line returns different results for count 
// I have it in separate cell in JupyterLab
fox2.where("type='TypeA' and duration>4").count()
{code}

The above code should produce the exact same result, every time, no matter where it is run, or how it is run.  If you have a separate unique ID per row, which often exists as a primary key, you could use that instead of the {{monotonically_increasing_id}} to remove the ambiguity.;;;, 01/Jun/21 13:13;revans2;I should add that the above "solution" is fragile because it relies on Spark to keep the call to {{monotonically_increasing_id}} in the same task the reads in the ORC data. It really would be best if Spark could automatically insert something like this automatically and then drop it later before writing/returning results.;;;, 15/Jun/21 13:54;Tonzetic;I can create similar solution with creating snapshot after calculation of stop column, but this is pretty time expensive, specially on large datasets.

I believe that something should be done with this, and to be fixed without need for a workaround.

I actually have a unique Id (in real data start column is timestamp with milliseconds precision), but I will try this workaround with monotonically_increasing_id. I'm not sure that I understand why it would help, since  there is no primary key in ORC (I may be wrong, but think not).

In the mean time, I have disabled public access to S3 bucket as there is some cost generated and I can no longer have it accessible for public access.;;;, 16/Jun/21 07:24;Tonzetic;[~revans2], I've tried your method with monotonically_increasing_id() and you are right, I get the same result every time.

I've tried it on my production data.

But I've noticed that this result is always 9397299 in my example, but without monotonically_increasing_id()  sometimes I get bigger number (like 9397301 and 9397304).

So I wander is this a correct result, or just a consistent error? How else could I get bigger number of records? I believe that simple sum of results from nodes is probably implemented correct...;;;, 16/Jun/21 12:17;revans2;[~Tonzetic], I don't know what you mean by an error.  You have asked Spark to calculate something that has multiple "correct" answers.  Each time Spark runs one of those "correct" answers is selected, somewhat at random.  The {{monotonically_increasing_id()}} change reduces the number of "correct" answers to 1.

For example 

| Start | User | Type |
|40|Anna| TypeA |
|41|Anna| TypeB |
|40|Anna| TypeB |

You are asking Spark to sort the data by Start, and then do a window operation that depends on the order of the data.  But there are two correct answers to sorting the data.


| Start | User | Type |
|40|Anna| *TypeA* |
|40|Anna| *TypeB* |
|41|Anna| TypeB |


| Start | User | Type |
|40|Anna| *TypeB* |
|40|Anna| *TypeA* |
|41|Anna| TypeB |

So which of these is the "correct" way to sort the data?  Because each of these will produce a different answer from the window operation, and because The order of {{TypeA}} vs {{TypeB}} is different between the two the relative distance between {{start}} and {{end}} will be different (In this case 0 vs 1). So if you can tell me what the correct ordering should be, then I can tell you if adding the new id has made it correct, or if it is just consistent.;;;, 16/Jun/21 13:46;Tonzetic;[~revans2], by error I mean result that is not correct. What is suspicious to me is that I can get more records without monotonically_increasing_id() than with monotonically_increasing_id().  Where did those came from?

I understand ordering data, but I don't see how should it impact results. Data is partitioned with partition method of Window function, and order just says in which order it is accessed by. 

So, since my original data uses timestamp with milliseconds as start column, and creates delta from next data row, if two starts are identical and therefore you need additional column for sorting those identical starts, delta (difference between start and stop) will still be same.

What is more interesting is that this is happening on multiple nodes cluster, so it has to do something with sharding data across nodes and aggregating results of calculation of all nodes back to master.

To put it simple, if you and I are two nodes, and first time I get 52/100 records and you 48/100 records, we both do our calculations and return results to master node, it should not be different from results that we will get if you processes 60/100 and I 40/100.

I know that this is oversimplified, and that there is plenty of complicated code and logic in sharding data between nodes, it just look incorrect to me and unusable in calculations that needs precision. ;;;, 16/Jun/21 14:51;revans2;{quote}I understand ordering data, but I don't see how it impact results.{quote}

OK lets change my example just a bit and completely run through your query. Lets say we have two tasks.

Task 1:
|Start| User | Type|
|44| Anna | TypeA|
|39 | Anna | TypeA|
|10| Joe | TypeB|

Task 2:
|Start| User | Type|
|44| Anna| TypeB|
|21| Joe | TypeB|

The first thing that spark will do is partition the data by {{User}} (which is what the window function asked). So each we end up with

Task 1:
|Start| User | Type|
|44| Anna | TypeA|
|39 | Anna | TypeA|
|44| Anna| TypeB|

Task 2:
|Start| User | Type|
|21| Joe | TypeB|
|10| Joe | TypeB|

Then each task will sort the data ascending by {{User, Start}} (We are going to ignore task 2 for now because there is no ambiguity in the sorting there, but I will show both options of sorting for Task 1).

Task 1 (sorting 1):
|Start| User | Type|
|39 | Anna | TypeA|
|44| Anna| TypeB|
|44| Anna | TypeA|

Task 1 (sorting 2):
|Start| User | Type|
|39 | Anna | TypeA|
|44| Anna| TypeA|
|44| Anna | TypeB|

Then the window function will run to create the end column, and the duration (actually 2 steps, but I'll put it into one here.

Task 1 (sorting 1):
|Start| User | Type|End| Duration|
|39 | Anna | TypeA|44| 5 |
|44| Anna| TypeB| 44| 0 |
|44| Anna | TypeA|30000000| 29999956 |

Task 1 (sorting 2):
|Start| User | Type|End| Duration|
|39 | Anna | TypeA|44| 5 |
|44| Anna| TypeA|44| 0 |
|44| Anna | TypeB|30000000| 29999956 |

Now lets filter {{type = 'TypeA' and duration > 4}}

Task 1 (sorting 1):
|Start| User | Type|End| Duration|
|39 | Anna | TypeA|44| 5 |
|44| Anna | TypeA|30000000| 29999956 |

Task 1 (sorting 2):
|Start| User | Type|End| Duration|
|39 | Anna | TypeA|44| 5 |

In sorting 2 we dropped 2 entries, but in sorting 1 we dropped only 1. This is because the order of the results in can matter if there is ambiguity in the ordering and the types are different within that ambiguity too. 

All {{monotonically_increasing_id}} did was make sure that only one order would be produced.  So is sorting 1 correct or is sorting 2 correct? From a SQL perspective either of them is a correct answer and spark happened to pick one of them. ;;;, 18/Jun/21 11:39;Tonzetic;[~revans2], tnx for detailed explanation.

I still have a problem understanding why you dropped one record. Where did other record go?  Why it disappeared? Filtering is the method of removing data, not sorting. Sort should not drop any data being ambiguous or not.

So if we look at Task 1 (sorting 1, redundant if you ask me, there is no point of sorting when column data is equal) and task 2 before applying filter for duration, they both have all 3 rows.

And after filtering, task 1 have both record, but sorting 2 lost the one with big duration? How can that happen? I cannot understand relation between sorting and missing data.

I believe you (and tried and it worked) that adding monotonically_increasing_id helps, but cannot understand why? 

If one worker has calculated duration with window function, next step should just remove rows where filter condition is not satisfied, regarding of sorting data.

It look to me that sorting have some implications with data exchange between worker nodes, but I cannot understand how. 

 

So, because data is missing because of strange reasons, I still believe that this is something that should be taken care of in code, instead of users who should remember that this is danger situation.

It looks that we should add id for every dataset to be sure that this will not happen. This will sure slow process down, and is prone to errors (forgot to add or something like that).

Furthermore, it is interesting that there is no problems on single instance jobs.

 ;;;
Affects Version/s.1: 3.1.1
Comment.1: 18/Apr/21 05:50;Tonzetic;I can, but it is not random that is problem.

Once you generate set with random data, you write it an read it to separate dataframe, and than run window function and filter on that dataframe. 

So, in repeated runs you use same data every time.

I generated this random dataset just to have some data to make a proof.

 

I actually discovered this running on real data, that is read from JSONNL files.

There are no UDFs and nothing is random, just data read from file. 

So you need to make two separate steps here:

1 . generate random data and write it to file (here it is ORC)  – THIS STEP SHOULD BE RUN ONLY ONCE 

2. read that data to new dataframe and run window lead and complex filter (with two conditions).  - THIS STEP SHOULD BE REPEATED

 

Again, it has to be in cluster setup, not single node.

On my docker setup (I can provide you with dockerfile) that I need to repartition dataframe on read to get different results.

example :

read.orc("bla_bla").repartition(200)

and 

read.orc("bla_bla").repartition(100)

 On AWS EMR Spark setup I does not matters, returns different numbers on every count.

 

 ;;;, 16/Jun/21 13:46;Tonzetic;[~revans2], by error I mean result that is not correct. What is suspicious to me is that I can get more records without monotonically_increasing_id() than with monotonically_increasing_id().  Where did those came from?

I understand ordering data, but I don't see how should it impact results. Data is partitioned with partition method of Window function, and order just says in which order it is accessed by. 

So, since my original data uses timestamp with milliseconds as start column, and creates delta from next data row, if two starts are identical and therefore you need additional column for sorting those identical starts, delta (difference between start and stop) will still be same.

What is more interesting is that this is happening on multiple nodes cluster, so it has to do something with sharding data across nodes and aggregating results of calculation of all nodes back to master.

To put it simple, if you and I are two nodes, and first time I get 52/100 records and you 48/100 records, we both do our calculations and return results to master node, it should not be different from results that we will get if you processes 60/100 and I 40/100.

I know that this is oversimplified, and that there is plenty of complicated code and logic in sharding data between nodes, it just look incorrect to me and unusable in calculations that needs precision. ;;;, 16/Jun/21 14:51;revans2;{quote}I understand ordering data, but I don't see how it impact results.{quote}

OK lets change my example just a bit and completely run through your query. Lets say we have two tasks.

Task 1:
|Start| User | Type|
|44| Anna | TypeA|
|39 | Anna | TypeA|
|10| Joe | TypeB|

Task 2:
|Start| User | Type|
|44| Anna| TypeB|
|21| Joe | TypeB|

The first thing that spark will do is partition the data by {{User}} (which is what the window function asked). So each we end up with

Task 1:
|Start| User | Type|
|44| Anna | TypeA|
|39 | Anna | TypeA|
|44| Anna| TypeB|

Task 2:
|Start| User | Type|
|21| Joe | TypeB|
|10| Joe | TypeB|

Then each task will sort the data ascending by {{User, Start}} (We are going to ignore task 2 for now because there is no ambiguity in the sorting there, but I will show both options of sorting for Task 1).

Task 1 (sorting 1):
|Start| User | Type|
|39 | Anna | TypeA|
|44| Anna| TypeB|
|44| Anna | TypeA|

Task 1 (sorting 2):
|Start| User | Type|
|39 | Anna | TypeA|
|44| Anna| TypeA|
|44| Anna | TypeB|

Then the window function will run to create the end column, and the duration (actually 2 steps, but I'll put it into one here.

Task 1 (sorting 1):
|Start| User | Type|End| Duration|
|39 | Anna | TypeA|44| 5 |
|44| Anna| TypeB| 44| 0 |
|44| Anna | TypeA|30000000| 29999956 |

Task 1 (sorting 2):
|Start| User | Type|End| Duration|
|39 | Anna | TypeA|44| 5 |
|44| Anna| TypeA|44| 0 |
|44| Anna | TypeB|30000000| 29999956 |

Now lets filter {{type = 'TypeA' and duration > 4}}

Task 1 (sorting 1):
|Start| User | Type|End| Duration|
|39 | Anna | TypeA|44| 5 |
|44| Anna | TypeA|30000000| 29999956 |

Task 1 (sorting 2):
|Start| User | Type|End| Duration|
|39 | Anna | TypeA|44| 5 |

In sorting 2 we dropped 2 entries, but in sorting 1 we dropped only 1. This is because the order of the results in can matter if there is ambiguity in the ordering and the types are different within that ambiguity too. 

All {{monotonically_increasing_id}} did was make sure that only one order would be produced.  So is sorting 1 correct or is sorting 2 correct? From a SQL perspective either of them is a correct answer and spark happened to pick one of them. ;;;, 18/Jun/21 11:39;Tonzetic;[~revans2], tnx for detailed explanation.

I still have a problem understanding why you dropped one record. Where did other record go?  Why it disappeared? Filtering is the method of removing data, not sorting. Sort should not drop any data being ambiguous or not.

So if we look at Task 1 (sorting 1, redundant if you ask me, there is no point of sorting when column data is equal) and task 2 before applying filter for duration, they both have all 3 rows.

And after filtering, task 1 have both record, but sorting 2 lost the one with big duration? How can that happen? I cannot understand relation between sorting and missing data.

I believe you (and tried and it worked) that adding monotonically_increasing_id helps, but cannot understand why? 

If one worker has calculated duration with window function, next step should just remove rows where filter condition is not satisfied, regarding of sorting data.

It look to me that sorting have some implications with data exchange between worker nodes, but I cannot understand how. 

 

So, because data is missing because of strange reasons, I still believe that this is something that should be taken care of in code, instead of users who should remember that this is danger situation.

It looks that we should add id for every dataset to be sure that this will not happen. This will sure slow process down, and is prone to errors (forgot to add or something like that).

Furthermore, it is interesting that there is no problems on single instance jobs.

 ;;;
Comment.2: 18/May/21 08:39;Tonzetic;I've made test data available via public s3 bucket, so it's easier to reproduce now.;;;
Comment.3: 29/May/21 16:05;revans2;On window functions if the {{order by}} clause is ambiguous you can get different results from one run to the next.  This is because the order in which shuffle data is read in is not deterministic, even though the sorting is.   In this example you are generating start with

{code}
val getRandomStart = udf((x:Int)=>{
    x+scala.util.Random.nextInt(47)
})
{code}

The input to this {x} appears to be non-abiguous (0-some very large number), but because of the + random(0 to 47) there is the possibility of multiple start values being the same.

So for operations where order matters you can get ambiguous results. For lead and lag a different lead/lag value can show up, because the one right after this one {{lead(1)}} is different.  For operations like rank, dense_rank, and row_number the order of the values output is the same, but the rows are in a different order so the value at each row/rank is different. This can also impact operations like SUM, MIN, and MAX that use a row bounds on a windows instead of value ranges.  I'm not sure if this should be considered a bug or not.  Spark treats all window operations as deterministic, so in theory if there is a crash you can get inconsistent results within the same query, but that only happens if the end user put in a non-deterministic ordering.;;;
Comment.4: 01/Jun/21 07:11;Tonzetic;It's really not important to look at random functions.

They are here just because I do not have permission to share data with you. Therefore I created test set of data using this random functions, merely to have some data to work with.

You can see that I've made sample data publicly available on S3 bucket.

So to cut the long story short, you only need to download data and try to run code with window functions.

Regarding is this is a bug or a design flaw or something else, I must say that I (and probably everyone else) am expecting consistent results from same data.

So yes, this is a problem and explanation why is probably happening will not solve this issue.

This means that using spark in, for example, financial calculations, is practically a big no no, because you cannot get consistent results of calculations.

 ;;;
Comment.5: 01/Jun/21 13:08;revans2;[~Tonzetic] to be clear my point was just to provide more information about the problem. I agree with you that this feels very much like a bug, and I would like to see it fixed. My hope was that with the added information someone in the Spark community could look at ways to fix it and at a minimum you could look at ways to work around it for your particular use case.  One such option is to remove the ambiguity by adding in a total ordering with {{monotonically_increasing_id}} early on in your processing (when you read the data in).  You should not rely on the exact value in this column (as it can change based off of the shape of the cluster you are running on), but you can use it as a part of your ordering to get unambiguous results.

For example.
 
{code:scala}
// define window and lead
val w = Window.partitionBy("user").orderBy("start", "unambiguous_id")
// if null, replace with 30.000.000
val ts_lead = coalesce(lead("start", 1) .over(w), lit(30000000))

// read data to dataframe, create stop column and calculate duration
val fox2 = spark.read.orc("hdfs:///random.orc").
    withColumn("unambiguous_id",  monotonically_increasing_id()).
    withColumn("end", ts_lead).
    withColumn("duration", col("end")-col("start"))


// repeated executions of this line returns different results for count 
// I have it in separate cell in JupyterLab
fox2.where("type='TypeA' and duration>4").count()
{code}

The above code should produce the exact same result, every time, no matter where it is run, or how it is run.  If you have a separate unique ID per row, which often exists as a primary key, you could use that instead of the {{monotonically_increasing_id}} to remove the ambiguity.;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Kafka doesn't recover from data loss
Issue key: SPARK-35915
Issue id: 13386232
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yuvalys
Creator: yuvalys
Created: 28/Jun/21 09:30
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: I configured a strcutured streaming source for kafka with failOnDataLoss=false, 

Getting this error when checkopint offsets are not found :

 
{code:java}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 5.0 failed 1 times, most recent failure: Lost task 7.0 in stage 5.0 (TID 113) ( executor driver): java.lang.IllegalStateException: This consumer has already been closed.
  at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:2439)
  at org.apache.kafka.clients.consumer.KafkaConsumer.seekToBeginning(KafkaConsumer.java:1656)
  at org.apache.spark.sql.kafka010.consumer.InternalKafkaConsumer.getAvailableOffsetRange(KafkaDataConsumer.scala:108)
  at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.getEarliestAvailableOffsetBetween(KafkaDataConsumer.scala:385)
  at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.$anonfun$get$1(KafkaDataConsumer.scala:332)
  at org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)
  at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.runUninterruptiblyIfPossible(KafkaDataConsumer.scala:604)
  at org.apache.spark.sql.kafka010.consumer.KafkaDataConsumer.get(KafkaDataConsumer.scala:287)
  at org.apache.spark.sql.kafka010.KafkaBatchPartitionReader.next(KafkaBatchPartitionReader.scala:63)
  at org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:79)
  at org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:112)
  at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)

{code}
 

The issue seems to me to be related to the OffsetOutOfRange exception in (line (323 in KafkaDataConsumer): 

 
{code:java}
 case e: OffsetOutOfRangeException =>
    // When there is some error thrown, it's better to use a new consumer to drop all cached
    // states in the old consumer. We don't need to worry about the performance because this
    // is not a common path.
    releaseConsumer()
    fetchedData.reset()

    reportDataLoss(topicPartition, groupId, failOnDataLoss,
      s"Cannot fetch offset $toFetchOffset", e)
    toFetchOffset = getEarliestAvailableOffsetBetween(consumer, toFetchOffset, untilOffset)
}
{code}
seems like releaseConsumer will destoy the consumer , which later is used ...

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jun 30 05:26:49 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sd4g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Jun/21 06:40;gurwls223;cc [~kabhwan] FYI;;;, 29/Jun/21 08:25;kabhwan;releaseConsumer just returns the Kafka consumer to the pool. We don't destroy the consumer instance there.

The possible case I can imagine is that InternalKafkaConsumer.close() is called (either automatically as it implements Closeable, or manually) from somewhere outside of pool. It's only expected to be called in destroying object in the pool.;;;, 29/Jun/21 14:07;code_kr_dev_s;[~yuvalys]  Hi I want to work on this issue.
If it is unresolved can I work upon it?;;;, 29/Jun/21 15:14;yuvalys;[~code_kr_dev_s] sure

[~kabhwan] -> In that case would it be advisable to change the comment ( since the state of the consumer isn't exepcted to break) ? Also, I guessed something in the pool itself clears consumers , or the consumer may be taken by some other thread and then closed there (since it is released ) ? ;;;, 30/Jun/21 05:26;kabhwan;OK the code comment was referring the old behavior, and it may be still better to follow the comment. That looks to be something I missed when introducing consumer pool.

So we don't expect anything other than pool to call close(), but I admit it's easy to make mistake. (I meant in Spark codebase.) We should probably need to change something to advise callers not to do it.;;;
Affects Version/s.1: 
Comment.1: 29/Jun/21 08:25;kabhwan;releaseConsumer just returns the Kafka consumer to the pool. We don't destroy the consumer instance there.

The possible case I can imagine is that InternalKafkaConsumer.close() is called (either automatically as it implements Closeable, or manually) from somewhere outside of pool. It's only expected to be called in destroying object in the pool.;;;
Comment.2: 29/Jun/21 14:07;code_kr_dev_s;[~yuvalys]  Hi I want to work on this issue.
If it is unresolved can I work upon it?;;;
Comment.3: 29/Jun/21 15:14;yuvalys;[~code_kr_dev_s] sure

[~kabhwan] -> In that case would it be advisable to change the comment ( since the state of the consumer isn't exepcted to break) ? Also, I guessed something in the pool itself clears consumers , or the consumer may be taken by some other thread and then closed there (since it is released ) ? ;;;
Comment.4: 30/Jun/21 05:26;kabhwan;OK the code comment was referring the old behavior, and it may be still better to follow the comment. That looks to be something I missed when introducing consumer pool.

So we don't expect anything other than pool to call close(), but I admit it's easy to make mistake. (I meant in Spark codebase.) We should probably need to change something to advise callers not to do it.;;;
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: DataFrame.take() only uses one worker
Issue key: SPARK-37185
Issue id: 13409531
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mathieulongtin
Creator: mathieulongtin
Created: 01/Nov/21 20:01
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Say you have query:
{code:java}
>>> df = spark.sql("select * from mytable where x = 99"){code}
Now, out of billions of row, there's only ten rows where x is 99.

If I do:
{code:java}
>>> df.limit(10).collect()
[Stage 1:>      (0 + 1) / 1]{code}
It only uses one worker. This takes a really long time since one CPU is reading the billions of row.

However, if I do this:
{code:java}
>>> df.limit(10).rdd.collect()
[Stage 1:>      (0 + 10) / 22]{code}
All the workers are running.

I think there's some optimization issue DataFrame.take(...).

This did not use to be an issue, but I'm not sure if it was working with 3.0 or 2.4.
Environment: CentOS 7
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python
Custom field (Last public comment date): Tue Nov 02 21:04:37 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wcrs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Nov/21 20:08;mathieulongtin;Additional note: if there's a "group by" in the query, this is not an issue.;;;, 02/Nov/21 07:54;gurwls223;isn't it more optimized to use only one partition on one worker if less data is required? ;;;, 02/Nov/21 07:55;gurwls223;can you show the perf diff between both codes?;;;, 02/Nov/21 21:04;mathieulongtin;It seems to try to optimize for a simple query, but not more complex queries. It kind of make sense for "select * from t", but any where clause can make it quite restrictive.

It looks like it scans the first part, doesn't find enough data, then scans four parts, then decides to scan everything. This is nice, but meanwhile, I have 20 workers already reserved, it wouldn't cost anything more to just go ahead right away.

Timing, table is not cached, contains 69 csv.gz files with anywhere from 1MB to 2.2GB of data:
{code:java}
In [1]: %time spark.sql("select * from t where x = 99").take(10)
CPU times: user 83.9 ms, sys: 112 ms, total: 196 ms
Wall time: 6min 44s
...
In [2]: %time spark.sql("select * from t where x = 99").limit(10).rdd.collect()
CPU times: user 45.7 ms, sys: 73.9 ms, total: 120 ms
Wall time: 3min 59s
...


{code}
I ran the two tests a few times to make sure there was no OS level caching effect, the timing didn't change much.

If I cache the table first, then "take(10)" is faster than "limit(10).rdd.collect()".;;;
Affects Version/s.1: 3.2.0
Comment.1: 02/Nov/21 07:54;gurwls223;isn't it more optimized to use only one partition on one worker if less data is required? ;;;
Comment.2: 02/Nov/21 07:55;gurwls223;can you show the perf diff between both codes?;;;
Comment.3: 02/Nov/21 21:04;mathieulongtin;It seems to try to optimize for a simple query, but not more complex queries. It kind of make sense for "select * from t", but any where clause can make it quite restrictive.

It looks like it scans the first part, doesn't find enough data, then scans four parts, then decides to scan everything. This is nice, but meanwhile, I have 20 workers already reserved, it wouldn't cost anything more to just go ahead right away.

Timing, table is not cached, contains 69 csv.gz files with anywhere from 1MB to 2.2GB of data:
{code:java}
In [1]: %time spark.sql("select * from t where x = 99").take(10)
CPU times: user 83.9 ms, sys: 112 ms, total: 196 ms
Wall time: 6min 44s
...
In [2]: %time spark.sql("select * from t where x = 99").limit(10).rdd.collect()
CPU times: user 45.7 ms, sys: 73.9 ms, total: 120 ms
Wall time: 3min 59s
...


{code}
I ran the two tests a few times to make sure there was no OS level caching effect, the timing didn't change much.

If I cache the table first, then "take(10)" is faster than "limit(10).rdd.collect()".;;;
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0

Summary: PartitionReaderFactory's Implemention Class of DataSourceV2: sqlConf parameter is null
Issue key: SPARK-35252
Issue id: 13375475
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: lynnyuan
Creator: lynnyuan
Created: 28/Apr/21 02:42
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.2, 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The codes of "MyPartitionReaderFactory" :
{code:scala}
// Implemention Class
package com.lynn.spark.sql.v2

import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.InternalRow
import com.lynn.spark.sql.v2.MyPartitionReaderFactory.{MY_VECTORIZED_READER_BATCH_SIZE, MY_VECTORIZED_READER_ENABLED}
import org.apache.spark.sql.connector.read.{InputPartition, PartitionReader, PartitionReaderFactory}
import org.apache.spark.sql.internal.SQLConf
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.vectorized.ColumnarBatch
import org.apache.spark.sql.internal.SQLConf.buildConf

case class MyPartitionReaderFactory(sqlConf: SQLConf,
                                    dataSchema: StructType,
                                    readSchema: StructType)
  extends PartitionReaderFactory with Logging {

  val enableVectorized = sqlConf.getConf(MY_VECTORIZED_READER_ENABLED, false)
  val batchSize = sqlConf.getConf(MY_VECTORIZED_READER_BATCH_SIZE, 4096)

  override def createReader(partition: InputPartition): PartitionReader[InternalRow] = {
    MyRowReader(batchSize, dataSchema, readSchema)
  }

  override def createColumnarReader(partition: InputPartition): PartitionReader[ColumnarBatch] = {
    if(!supportColumnarReads(partition))
      throw new UnsupportedOperationException("Cannot create columnar reader.")

       MyColumnReader(batchSize, dataSchema, readSchema)

  }

  override def supportColumnarReads(partition: InputPartition) = enableVectorized

}

object MyPartitionReaderFactory {

  val MY_VECTORIZED_READER_ENABLED =
    buildConf("spark.sql.my.enableVectorizedReader")
      .doc("Enables vectorized my source scan.")
      .version("1.0.0")
      .booleanConf
      .createWithDefault(false)

  val MY_VECTORIZED_READER_BATCH_SIZE =
    buildConf("spark.sql.my.columnarReaderBatchSize")
      .doc("The number of rows to include in a my source vectorized reader batch. The number should " +
        "be carefully chosen to minimize overhead and avoid OOMs in reading data.")
      .version("1.0.0")
      .intConf
      .createWithDefault(4096)
}
{code}

The driver construct a RDD instance(DataSourceRDD), the sqlConf parameter pass to the MyPartitionReaderFactory  is not null.
But when the executor deserialize the RDD, the sqlConf parameter is null.

The codes as follows:

{code:scala}
// RunTask.scala
override def runTask(context: TaskContext): U = {
    // Deserialize the RDD and the func using the broadcast variables.
    val threadMXBean = ManagementFactory.getThreadMXBean
    val deserializeStartTimeNs = System.nanoTime()
    val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {
      threadMXBean.getCurrentThreadCpuTime
    } else 0L
    val ser = SparkEnv.get.closureSerializer.newInstance()
   //  the rdd 
    val (rdd, func) = ser.deserialize[(RDD[T], (TaskContext, Iterator[T]) => U)](
      ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
    _executorDeserializeTimeNs = System.nanoTime() - deserializeStartTimeNs
    _executorDeserializeCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {
      threadMXBean.getCurrentThreadCpuTime - deserializeStartCpuTime
    } else 0L

    func(context, rdd.iterator(partition, context))
  }
{code}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-38328
Attachment: 28/Apr/21 05:40;lynnyuan;spark-sqlconf-isnull.png;https://issues.apache.org/jira/secure/attachment/13024698/spark-sqlconf-isnull.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon May 03 10:42:04 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qivc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/May/21 10:42;gurwls223;You can use {{SQLConf.get}} instead.;;;
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Cached Table (parquet) with old Configs Used
Issue key: SPARK-34780
Issue id: 13365937
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mikechen
Creator: mikechen
Created: 17/Mar/21 20:00
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.4.4, 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When a dataframe is cached, the logical plan can contain copies of the spark session meaning the SQLConfs are stored. Then if a different dataframe can replace parts of it's logical plan with a cached logical plan, the cached SQLConfs will be used for the evaluation of the cached logical plan. This is because HadoopFsRelation ignores sparkSession for equality checks (introduced in https://issues.apache.org/jira/browse/SPARK-17358).
{code:java}
test("cache uses old SQLConf") {
  import testImplicits._
  withTempDir { dir =>
    val tableDir = dir.getAbsoluteFile + "/table"
    val df = Seq("a").toDF("key")
    df.write.parquet(tableDir)
    SQLConf.get.setConfString(SQLConf.FILE_COMPRESSION_FACTOR.key, "1")
    val compression1Stats = spark.read.parquet(tableDir).select("key").
      queryExecution.optimizedPlan.collect {
      case l: LogicalRelation => l
      case m: InMemoryRelation => m
    }.map(_.computeStats())

    SQLConf.get.setConfString(SQLConf.FILE_COMPRESSION_FACTOR.key, "10")
    val df2 = spark.read.parquet(tableDir).select("key")
    df2.cache()
    val compression10Stats = df2.queryExecution.optimizedPlan.collect {
      case l: LogicalRelation => l
      case m: InMemoryRelation => m
    }.map(_.computeStats())

    SQLConf.get.setConfString(SQLConf.FILE_COMPRESSION_FACTOR.key, "1")
    val compression1StatsWithCache = spark.read.parquet(tableDir).select("key").
      queryExecution.optimizedPlan.collect {
      case l: LogicalRelation => l
      case m: InMemoryRelation => m
    }.map(_.computeStats())

    // I expect these stats to be the same because file compression factor is the same
    assert(compression1Stats == compression1StatsWithCache)
    // Instead, we can see the file compression factor is being cached and used along with
    // the logical plan
    assert(compression10Stats == compression1StatsWithCache)
  }
}{code}
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 14 19:12:54 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0owrc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Mar/21 01:17;gurwls223;cc [~sunchao] [~maxgekk] FYI;;;, 19/Mar/21 18:32;csun;Thanks for the reporting [~mikechen], the test case you provided is very useful. 

I'm not sure, though, how severe is the issue since it only affects {{computeStats}}, and when the cache is actually materialized (e.g., via {{df2.count()}} after {{df2.cache()}}), the value from {{computeStats}} will be different anyways. Could you give more details?;;;, 19/Mar/21 19:52;mikechen;I used computeStats because it was a simple way to display the problem. But any config that is read through the relation's spark session would have the same problem where the cached config is read instead of config set through SQLConf (or other mechanisms). For example, when building the file readers in DataSourceScanExec, relation.sparkSession is passed along so configs in these readers could be wrong.;;;, 25/Mar/21 00:55;csun;Sorry for the late reply [~mikechen]! There's something I still not quite clear: when the cache is retrieved, a {{InMemoryRelation}} will be used to replace the plan fragment that is matched. Therefore, how can the old stale conf still be used in places like {{DataSourceScanExec}}?;;;, 25/Mar/21 16:44;mikechen;Np [~csun]. If the cache isn't materialized until after the configs change, then I believe the input RDDs for InMemoryTableScanExec are still built with the old stale confs so the stale confs would also be used in the DataSourceScanExec? Even if the cache was materialized before the configs changed, reading an RDD that was created with a stale conf would be a concern if any of the confs can change results right? (not sure if this is possible);;;, 25/Mar/21 17:40;csun;[~mikechen], yes you're right. I'm not sure if this is a big concern though, since it just means the plan fragment for the cache is executed with the stale conf. I guess as long as there is no correctness issue (which I'd be surprised to see if there's any), it should be fine?

It seems a bit tricky to fix the issue, since the {{SparkSession}} is leaked to many places. I guess one way is to follow the idea of SPARK-33389 and change {{SessionState}} to always use the active conf. ;;;, 26/Mar/21 15:51;mikechen;Yes, I think as long as there aren't any correctness issues it's fine. ;;;, 29/Mar/21 22:05;mikechen;Hey [~csun]. I did find a conf that can cause a diff in correctness, but I'm not sure how serious it is (I didn't find other confs affecting correctness, but also didn't look too hard/do people flip this conf?) I added a test case that shows the problem though.
{code:java}
test("cache uses old SQLConf") {
  SQLConf.get.setConfString(SQLConf.CSV_PARSER_COLUMN_PRUNING.key, "true")
  val carsFile = "test-data/cars.csv"
  val cars = spark.read
    .format("csv")
    .option("multiLine", false)
    .options(Map("header" -> "true", "mode" -> "dropmalformed"))
    .load(testFile(carsFile))

  val numRows = cars.select("year").collect().length

  SQLConf.get.setConfString(SQLConf.CSV_PARSER_COLUMN_PRUNING.key, "false")
  spark.read
    .format("csv")
    .option("multiLine", false)
    .options(Map("header" -> "true", "mode" -> "dropmalformed"))
    .load(testFile(carsFile)).cache().count()

  SQLConf.get.setConfString(SQLConf.CSV_PARSER_COLUMN_PRUNING.key, "true")
  val numRowsReadCache = spark.read
    .format("csv")
    .option("multiLine", false)
    .options(Map("header" -> "true", "mode" -> "dropmalformed"))
    .load(testFile(carsFile)).select("year").collect().length

  assert(numRows == numRowsReadCache)
}
{code};;;, 07/Apr/21 21:49;csun;Hi [~mikechen] (and sorry for the late reply again), thanks for providing another very useful code snippet! I'm not sure if this qualifies as correctness issue though since it is (to me) more like different interpretations of malformed columns in CSV? 

My previous statement about {{SessionState}} is incorrect. It seems the conf in {{SessionState}} is always the most up-to-date one. The only solution I can think of to solve this issue is to take conf into account when checking equality of {{HadoopFsRelation}} (and potentially others), which means we'd need to define equality for {{SQLConf}}..;;;, 14/Apr/21 19:12;mikechen;Hi [~csun]. Also sorry for late reply. My understanding is the config lets you ignore malformed columns if they aren't part of the schema. I thought this affected correctness because you would get different results with the config on/off based on the presence of malformed columns that aren't relevant to the query.
I also think the way to solve this issue would be defining equality for SQLConf.;;;
Affects Version/s.1: 3.1.1
Comment.1: 19/Mar/21 18:32;csun;Thanks for the reporting [~mikechen], the test case you provided is very useful. 

I'm not sure, though, how severe is the issue since it only affects {{computeStats}}, and when the cache is actually materialized (e.g., via {{df2.count()}} after {{df2.cache()}}), the value from {{computeStats}} will be different anyways. Could you give more details?;;;
Comment.2: 19/Mar/21 19:52;mikechen;I used computeStats because it was a simple way to display the problem. But any config that is read through the relation's spark session would have the same problem where the cached config is read instead of config set through SQLConf (or other mechanisms). For example, when building the file readers in DataSourceScanExec, relation.sparkSession is passed along so configs in these readers could be wrong.;;;
Comment.3: 25/Mar/21 00:55;csun;Sorry for the late reply [~mikechen]! There's something I still not quite clear: when the cache is retrieved, a {{InMemoryRelation}} will be used to replace the plan fragment that is matched. Therefore, how can the old stale conf still be used in places like {{DataSourceScanExec}}?;;;
Comment.4: 25/Mar/21 16:44;mikechen;Np [~csun]. If the cache isn't materialized until after the configs change, then I believe the input RDDs for InMemoryTableScanExec are still built with the old stale confs so the stale confs would also be used in the DataSourceScanExec? Even if the cache was materialized before the configs changed, reading an RDD that was created with a stale conf would be a concern if any of the confs can change results right? (not sure if this is possible);;;
Comment.5: 25/Mar/21 17:40;csun;[~mikechen], yes you're right. I'm not sure if this is a big concern though, since it just means the plan fragment for the cache is executed with the stale conf. I guess as long as there is no correctness issue (which I'd be surprised to see if there's any), it should be fine?

It seems a bit tricky to fix the issue, since the {{SparkSession}} is leaked to many places. I guess one way is to follow the idea of SPARK-33389 and change {{SessionState}} to always use the active conf. ;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Support user defined types in Pandas UDF
Issue key: SPARK-34600
Issue id: 13361974
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: eddyxu
Creator: eddyxu
Created: 02/Mar/21 23:47
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.2, 3.1.1
Fix Version/s: 
Component/s: PySpark, SQL
Due Date: 
Votes: 1
Labels: pandas, sql, udf
Description: This is an umbrella ticket.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Mar 19 17:10:49 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0o8fk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Mar/21 07:17;apachespark;User 'eddyxu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31735;;;, 04/Mar/21 07:24;eddyxu;Hi, Here

I've worked on a PR to support returning UDT from Pandas UDF.

https://github.com/apache/spark/pull/31735

Looking forward to take feedbacks!;;;, 19/Mar/21 01:25;gurwls223;[~eddyxu] if this ticket is an umbrella ticket, let's create another JIRA and move your PR (https://github.com/apache/spark/pull/31735) to that JIRA. umbrella JIRA doesn't usually have a PR against it.;;;, 19/Mar/21 02:56;sadhen;[~hyukjin.kwon][~eddyxu]

Let me move the PR to the newly-created JIRA: https://issues.apache.org/jira/browse/SPARK-34799;;;, 19/Mar/21 17:10;apachespark;User 'eddyxu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31735;;;
Affects Version/s.1: 3.1.1
Comment.1: 04/Mar/21 07:24;eddyxu;Hi, Here

I've worked on a PR to support returning UDT from Pandas UDF.

https://github.com/apache/spark/pull/31735

Looking forward to take feedbacks!;;;
Comment.2: 19/Mar/21 01:25;gurwls223;[~eddyxu] if this ticket is an umbrella ticket, let's create another JIRA and move your PR (https://github.com/apache/spark/pull/31735) to that JIRA. umbrella JIRA doesn't usually have a PR against it.;;;
Comment.3: 19/Mar/21 02:56;sadhen;[~hyukjin.kwon][~eddyxu]

Let me move the PR to the newly-created JIRA: https://issues.apache.org/jira/browse/SPARK-34799;;;
Comment.4: 19/Mar/21 17:10;apachespark;User 'eddyxu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31735;;;
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Pyspark UDF wrongly changes timestamps to UTC
Issue key: SPARK-33863
Issue id: 13347055
Parent id: 
Issue Type: Bug
Status: Reopened
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: nasirali
Creator: nasirali
Created: 20/Dec/20 23:32
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.0.2, 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: *Problem*:

I have a dataframe with a ts (timestamp) column in UTC. If I create a new column using udf, pyspark udf wrongly changes timestamps into UTC time. ts (timestamp) column is already in UTC time. Therefore, pyspark udf should not convert ts (timestamp) column into UTC timestamp. 

I have used following configs to let spark know the timestamps are in UTC:

 
{code:java}
--conf spark.driver.extraJavaOptions=-Duser.timezone=UTC 
--conf spark.executor.extraJavaOptions=-Duser.timezone=UTC
--conf spark.sql.session.timeZone=UTC
{code}
Below is a code snippet to reproduce the error:

 
{code:java}
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StringType, TimestampType
import datetime

spark = SparkSession.builder.config("spark.sql.session.timeZone", "UTC").getOrCreate()

df = spark.createDataFrame([("usr1",17.00, "2018-02-10T15:27:18+00:00"),
                            ("usr1",13.00, "2018-02-11T12:27:18+00:00"),
                            ("usr1",25.00, "2018-02-12T11:27:18+00:00"),
                            ("usr1",20.00, "2018-02-13T15:27:18+00:00"),
                            ("usr1",17.00, "2018-02-14T12:27:18+00:00"),
                            ("usr2",99.00, "2018-02-15T11:27:18+00:00"),
                            ("usr2",156.00, "2018-02-22T11:27:18+00:00")
                            ],
                           ["user","id", "ts"])

df = df.withColumn('ts', df.ts.cast('timestamp'))
df.show(truncate=False)

def some_time_udf(i):
    if  datetime.time(5, 0)<=i.time() < datetime.time(12, 0):
        tmp= "Morning: " + str(i)
    elif  datetime.time(12, 0)<=i.time() < datetime.time(17, 0):
        tmp= "Afternoon: " + str(i)
    elif  datetime.time(17, 0)<=i.time() < datetime.time(21, 0):
        tmp= "Evening"
    elif  datetime.time(21, 0)<=i.time() < datetime.time(0, 0):
        tmp= "Night"
    elif  datetime.time(0, 0)<=i.time() < datetime.time(5, 0):
        tmp= "Night"
    return tmp

udf = F.udf(some_time_udf,StringType())
df.withColumn("day_part", udf(df.ts)).show(truncate=False)


{code}
 

Below is the output of the above code:
{code:java}
+----+-----+-------------------+----------------------------+
|user|id   |ts                 |day_part                    |
+----+-----+-------------------+----------------------------+
|usr1|17.0 |2018-02-10 15:27:18|Morning: 2018-02-10 09:27:18|
|usr1|13.0 |2018-02-11 12:27:18|Morning: 2018-02-11 06:27:18|
|usr1|25.0 |2018-02-12 11:27:18|Morning: 2018-02-12 05:27:18|
|usr1|20.0 |2018-02-13 15:27:18|Morning: 2018-02-13 09:27:18|
|usr1|17.0 |2018-02-14 12:27:18|Morning: 2018-02-14 06:27:18|
|usr2|99.0 |2018-02-15 11:27:18|Morning: 2018-02-15 05:27:18|
|usr2|156.0|2018-02-22 11:27:18|Morning: 2018-02-22 05:27:18|
+----+-----+-------------------+----------------------------+
{code}
Above output is incorrect. You can see ts and day_part columns don't have same timestamps. Below is the output I would expect:

 
{code:java}
+----+-----+-------------------+----------------------------+
|user|id   |ts                 |day_part                    |
+----+-----+-------------------+----------------------------+
|usr1|17.0 |2018-02-10 15:27:18|Afternoon: 2018-02-10 15:27:18|
|usr1|13.0 |2018-02-11 12:27:18|Afternoon: 2018-02-11 12:27:18|
|usr1|25.0 |2018-02-12 11:27:18|Morning: 2018-02-12 11:27:18|
|usr1|20.0 |2018-02-13 15:27:18|Afternoon: 2018-02-13 15:27:18|
|usr1|17.0 |2018-02-14 12:27:18|Afternoon: 2018-02-14 12:27:18|
|usr2|99.0 |2018-02-15 11:27:18|Morning: 2018-02-15 11:27:18|
|usr2|156.0|2018-02-22 11:27:18|Morning: 2018-02-22 11:27:18|
+----+-----+-------------------+----------------------------+{code}
 
Environment: MAC/Linux

Standalone cluster / local machine
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 24 01:51:46 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0low0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Dec/20 07:37;gurwls223;[~nasirali] Can you show expected results and actual results of {{df.show()}}?;;;, 23/Dec/20 19:39;viirya;It is unclear what the issue is. "Pyspark UDF changes timestamps to UTC" does it mean the UDF wrongly changes timestamps to UTC?;;;, 08/Jan/21 01:30;nasirali;[~hyukjin.kwon] and [~viirya] I have simplified example code, added/revised details, and also added output and expected output. Please let me know if you need more information.;;;, 23/Feb/21 02:55;nasirali;[~hyukjin.kwon] and [~viirya] any update on this issue?;;;, 08/May/21 00:00;nasirali;[~hyukjin.kwon] and [~viirya] This bug exist in all the 3.x.x versions of Pyspark. Any update or suggestion?;;;, 04/Jun/21 02:52;dc-heros;Have this issues resolved, I couldn't reproduce it

 ;;;, 24/Jun/21 01:51;nasirali;Issue is not resolved. ;;;, 24/Jun/21 01:51;nasirali;[~dc-heros] Could you please share the output you got when you ran the above code?;;;
Affects Version/s.1: 3.0.2
Comment.1: 23/Dec/20 19:39;viirya;It is unclear what the issue is. "Pyspark UDF changes timestamps to UTC" does it mean the UDF wrongly changes timestamps to UTC?;;;
Comment.2: 08/Jan/21 01:30;nasirali;[~hyukjin.kwon] and [~viirya] I have simplified example code, added/revised details, and also added output and expected output. Please let me know if you need more information.;;;
Comment.3: 23/Feb/21 02:55;nasirali;[~hyukjin.kwon] and [~viirya] any update on this issue?;;;
Comment.4: 08/May/21 00:00;nasirali;[~hyukjin.kwon] and [~viirya] This bug exist in all the 3.x.x versions of Pyspark. Any update or suggestion?;;;
Comment.5: 04/Jun/21 02:52;dc-heros;Have this issues resolved, I couldn't reproduce it

 ;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Index Appending when Duplicate Column names
Issue key: SPARK-34992
Issue id: 13370421
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: hallahbutt
Creator: hallahbutt
Created: 08/Apr/21 15:21
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In reference to below mentioned link
[https://github.com/apache/spark/pull/14745]
I am reporting this issue that can it should be configurable to append values when we have duplicate columns? For instance, instead of always appending column index, there should be configurable leverage to append any sort of string. Lets say, instead of Cola2 and Cola34, we can set as Cola[xyz] and Cola1. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Apr 11 03:54:58 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0pods:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Apr/21 03:54;gurwls223;We could make it configurable. That referred to R's read_csv and pandas' read_csv. if they have the options, I think we can have that too.;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: spark should support distribute directory to cluster
Issue key: SPARK-36518
Issue id: 13395403
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yghu
Creator: yghu
Created: 16/Aug/21 06:59
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Deploy
Due Date: 
Votes: 0
Labels: 
Description: Spark now only supports distribute files to cluster, but in some scenario, we need upload a directory to cluster.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Aug 17 08:16:48 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0txmw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Aug/21 05:01;gurwls223;I think you can use --archive for this purpose.;;;, 17/Aug/21 06:24;yghu;[~hyukjin.kwon] ya, i know this parameter, but this will have one extra layer of directory structure, i just only want uploading one directory structure as it is in local or dfs. ;;;, 17/Aug/21 06:27;gurwls223;Can you try {{SparkContext.addFile("...", recursive = true)}}?;;;, 17/Aug/21 06:49;yghu;[~hyukjin.kwon] we should support it like using --files command;;;, 17/Aug/21 08:16;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33760;;;
Affects Version/s.1: 3.1.1
Comment.1: 17/Aug/21 06:24;yghu;[~hyukjin.kwon] ya, i know this parameter, but this will have one extra layer of directory structure, i just only want uploading one directory structure as it is in local or dfs. ;;;
Comment.2: 17/Aug/21 06:27;gurwls223;Can you try {{SparkContext.addFile("...", recursive = true)}}?;;;
Comment.3: 17/Aug/21 06:49;yghu;[~hyukjin.kwon] we should support it like using --files command;;;
Comment.4: 17/Aug/21 08:16;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33760;;;
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Spark submit k8s with proxy user
Issue key: SPARK-38390
Issue id: 13431470
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: krokpop
Creator: krokpop
Created: 02/Mar/22 10:41
Updated: 19/Oct/22 05:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: In the process of trying to run a spark test using spark submit k8s, I ran into a problem running it with the proxy user option. Judging by the stack trace and the authentication, it is clear that on the side of the spark submit there is a problem with authorization through the user's proxy using delegation token. Command line bellow
{code:java}
exec /usr/bin/tini -s -- /bin/sh -c /usr/bin/kinit -c FILE:/tmp/krb5cc -kt  /etc/test.keytab principal@REALM \
--proxy-user ambari-qa \
--master k8s://https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT} \
--deploy-mode cluster \
--conf spark.app.name=spark-dfsreadwrite \
--conf spark.kubernetes.namespace=namespace \
--conf spark.kubernetes.container.image=gct.io/spark-operator/spark:v3.1.1 \
--conf spark.kubernetes.submission.waitAppCompletion=true \
--conf spark.driver.cores=1 \
--conf spark.driver.memory=512m \
--conf spark.kubernetes.driver.limit.cores=1 \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-application-sa \
--conf spark.kubernetes.driver.label.app=spark-dfsreadwrite \
--conf spark.executor.instance=1 \
--conf spark.executor.cores=1 \
--conf spark.executor.limit.cores=1 \
--conf spark.kubernetes.executor.label.app=spark-dfsreadwrite \
--conf spark.kubernetes.hadoop.configMapName=hadoop-configmap \
--conf spark.kubernetes.kerberos.krb5.configMapName=kerberos-configmap \
--conf spark.kerberos.renewal.credentials=ccache \
--conf spark.hadoop.kerberos.keytab.login.autorenewal.enabled=true \
local:///opt/spark/examples/jars/spark-examples_2.13.-3.1.1.jar \
/etc/profile /tmp/
{code}
Output from command with stack trace.
{code:java}
++ id -u
+ myuid=185
++ id -g
+ mygid=0
+ set +e
++ getent passwd 185
+ uidentry=
+ set -e
+ '[' -z '' ']'
+ '[' -w /etc/passwd ']'
+ echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false'
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' -z ']'
+ '[' -z ']'
+ '[' -n '' ']'
+ '[' -z x ']'
+ SPARK_CLASSPATH='/opt/hadoop/conf::/opt/spark/jars/*'
+ '[' -z x ']'
+ SPARK_CLASSPATH='/opt/spark/conf:/opt/hadoop/conf::/opt/spark/jars/*'
+ case "$1" in
+ shift 1
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=<ip> --deploy-mode client --proxy-user ambari-qa --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.examples.DFSReadWriteTest local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar /etc/profile /tmp/
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
22/02/25 10:33:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting spark.hadoop.yarn.resourcemanager.principal to ambari-qa
Performing local word count
Creating SparkSession
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
22/02/25 10:33:31 INFO SparkContext: Running Spark version 3.1.1
22/02/25 10:33:31 INFO ResourceUtils: ==============================================================
22/02/25 10:33:31 INFO ResourceUtils: No custom resources configured for spark.driver.
22/02/25 10:33:31 INFO ResourceUtils: ==============================================================
22/02/25 10:33:31 INFO SparkContext: Submitted application: DFS Read Write Test
22/02/25 10:33:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 512, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
22/02/25 10:33:31 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
22/02/25 10:33:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
22/02/25 10:33:31 INFO SecurityManager: Changing view acls to: 185,ambari-qa
22/02/25 10:33:31 INFO SecurityManager: Changing modify acls to: 185,ambari-qa
22/02/25 10:33:31 INFO SecurityManager: Changing view acls groups to: 
22/02/25 10:33:31 INFO SecurityManager: Changing modify acls groups to: 
22/02/25 10:33:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(185, ambari-qa); groups with view permissions: Set(); users  with modify permissions: Set(185, ambari-qa); groups with modify permissions: Set()
22/02/25 10:33:32 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
22/02/25 10:33:32 INFO SparkEnv: Registering MapOutputTracker
22/02/25 10:33:32 INFO SparkEnv: Registering BlockManagerMaster
22/02/25 10:33:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22/02/25 10:33:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
22/02/25 10:33:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
22/02/25 10:33:32 INFO DiskBlockManager: Created local directory at /var/data/spark-3b0fe4a4-edb4-4144-9f9c-74e3ea583def/blockmgr-33259dcc-20aa-47cd-b09c-8c128de5f5eb
22/02/25 10:33:32 INFO MemoryStore: MemoryStore started with capacity 117.0 MiB
22/02/25 10:33:32 INFO SparkEnv: Registering OutputCommitCoordinator
22/02/25 10:33:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
22/02/25 10:33:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://spark-dfsreadwrite-09f12c7f30714a72-driver-svc.compute.svc:4040
22/02/25 10:33:33 INFO SparkContext: Added JAR local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar at file:/opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar with timestamp 1645785211700
22/02/25 10:33:33 WARN SparkContext: The jar local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar has been added already. Overwriting of added jars is not supported in the current version.
22/02/25 10:33:33 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
22/02/25 10:33:35 INFO ExecutorPodsAllocator: Going to request 1 executors from Kubernetes for ResourceProfile Id: 0, target: 1 running: 0.
22/02/25 10:33:36 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
22/02/25 10:33:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
22/02/25 10:33:36 INFO NettyBlockTransferService: Server created on spark-dfsreadwrite-09f12c7f30714a72-driver-svc.compute.svc:7079
22/02/25 10:33:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22/02/25 10:33:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, spark-dfsreadwrite-09f12c7f30714a72-driver-svc.compute.svc, 7079, None)
22/02/25 10:33:36 INFO BlockManagerMasterEndpoint: Registering block manager spark-dfsreadwrite-09f12c7f30714a72-driver-svc.compute.svc:7079 with 117.0 MiB RAM, BlockManagerId(driver, spark-dfsreadwrite-09f12c7f30714a72-driver-svc.compute.svc, 7079, None)
22/02/25 10:33:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, spark-dfsreadwrite-09f12c7f30714a72-driver-svc.compute.svc, 7079, None)
22/02/25 10:33:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, spark-dfsreadwrite-09f12c7f30714a72-driver-svc.compute.svc, 7079, None)
22/02/25 10:33:39 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (<ip>:<port>) with ID 1,  ResourceProfileId 0
22/02/25 10:33:39 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
22/02/25 10:33:39 INFO BlockManagerMasterEndpoint: Registering block manager 10.42.0.221:33711 with 117.0 MiB RAM, BlockManagerId(1, <ip>, <port>, None)
Writing local file to DFS
22/02/25 10:33:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/opt/spark/work-dir/spark-warehouse').
22/02/25 10:33:39 INFO SharedState: Warehouse path is 'file:/opt/spark/work-dir/spark-warehouse'.
22/02/25 10:33:41 WARN Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
22/02/25 10:33:41 WARN Client: Exception encountered while connecting to the server : javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
22/02/25 10:33:41 INFO RetryInvocationHandler: Exception while invoking getFileInfo of class ClientNamenodeProtocolTranslatorPB over <address>/<ip>:8020 after 1 fail over attempts. Trying to fail over immediately.
java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]; Host Details : local host is: "spark-dfsreadwrite-09f12c7f30714a72-driver/<ip>"; destination host is: "<address>":8020; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:776)
	at org.apache.hadoop.ipc.Client.call(Client.java:1480)
	at org.apache.hadoop.ipc.Client.call(Client.java:1413)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)
	at com.sun.proxy.$Proxy34.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:776)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:191)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy35.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2108)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)
	at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)
	at org.apache.spark.examples.DFSReadWriteTest$.main(DFSReadWriteTest.scala:115)
	at org.apache.spark.examples.DFSReadWriteTest.main(DFSReadWriteTest.scala)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
	at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:165)
	at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:163)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1030)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1039)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
	at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:688)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:651)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:738)
	at org.apache.hadoop.ipc.Client$Connection.access$2900(Client.java:376)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1529)
	at org.apache.hadoop.ipc.Client.call(Client.java:1452)
	... 36 more
Caused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]
	at jdk.security.jgss/com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(Unknown Source)
	at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:414)
	at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:561)
	at org.apache.hadoop.ipc.Client$Connection.access$1900(Client.java:376)
	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:730)
	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:726)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Unknown Source)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:726)
	... 39 more
Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)
	at java.security.jgss/sun.security.jgss.krb5.Krb5InitCredential.getInstance(Unknown Source)
	at java.security.jgss/sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Unknown Source)
	at java.security.jgss/sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Unknown Source)
	at java.security.jgss/sun.security.jgss.GSSManagerImpl.getMechanismContext(Unknown Source)
	at java.security.jgss/sun.security.jgss.GSSContextImpl.initSecContext(Unknown Source)
	at java.security.jgss/sun.security.jgss.GSSContextImpl.initSecContext(Unknown Source)
	... 49 more
{code}
Main question is why the same case but with yarn works well. Is it some restriction for spark submit k8s or configuration issue?
{code:java}
kinit -kt test.keytab principal@REALM 
spark-submit \
--class org.apache.spark.examples.DFSReadWriteTest \
--deploy-mode client  \
--proxy-user ambari-qa \ 
--conf spark.app.name=spark-dfsreadwrite \ 
--conf spark.driver.cores=1 \ 
--conf spark.driver.memory=512m \
--conf spark.executor.instances=1 \
--conf spark.executor.cores=1 \
--conf spark.executor.memory=512m \
/opt/spark/examples/jars/spark-examples_2.12-3.0.1.jar \ 
/etc/profile /tmp
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 19 05:10:52 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1034o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Mar/22 11:39;MikhailPochatkin;Correct command line
{code:java}
/usr/bin/kinit -c FILE:/tmp/krb5cc -kt /etc/test.keytab principal@REALM
export KRB5CCNAME=FILE:/tmp/krb5cc
/opt/spark/bin/spark-submit \
--class org.apache.spark.examples.DFSReadWriteTest \
--master k8s://https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT} \
--deploy-mode cluster \
--proxy-user ambari-qa \
--conf spark.app.name=spark-dfsreadwrite \
--conf spark.kubernetes.namespace=sdp-compute \
--conf spark.kubernetes.container.image=gcr.io/spark-operator/spark:v3.1.1 \
--conf spark.kubernetes.submission.waitAppCompletion=true \
--conf spark.driver.cores=1 \
--conf spark.driver.memory=512m \
--conf spark.kubernetes.driver.limit.cores=1 \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-application-sa \
--conf spark.kubernetes.driver.label.app=spark-dfsreadwrite \
--conf spark.executor.instances=1 \
--conf spark.executor.cores=1 \
--conf spark.executor.limit.cores=1 \
--conf spark.executor.memory=512m \
--conf spark.kubernetes.executor.label.app=spark-dfsreadwrite \
--conf spark.kubernetes.hadoop.configMapName=hadoop-configmap \
--conf spark.kubernetes.kerberos.krb5.configMapName=kerberos-configmap \
--conf spark.kerberos.renewal.credentials=ccache \
local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar \
/etc/profile /tmp/ {code};;;, 19/Oct/22 05:10;av010;This code should work on driver 
{code:java}
val ugi = UserGroupInformation.loginUsingKeyTabAndProvideUserGroupInformation(localKeytabFile)
val ugiProxy = UserGroupInformation.createProxyUser("proxy-user-name", ugi)
ugi.setLoginUser(ugiProxy){code}
This works on a standalone spark when running above code however I have had little or no success when using this on kubernetes -

 

an appropriate additional setting required to make this work is:
{code:java}
--conf spark.driver.extraJavaOptions=-Djavax.security.auth.useSubjectCreds=false{code}
this would allow super user login to function appropriately.

 

All these are hacks, ideally this should be supported natively on spark with k8s.;;;
Affects Version/s.1: 
Comment.1: 19/Oct/22 05:10;av010;This code should work on driver 
{code:java}
val ugi = UserGroupInformation.loginUsingKeyTabAndProvideUserGroupInformation(localKeytabFile)
val ugiProxy = UserGroupInformation.createProxyUser("proxy-user-name", ugi)
ugi.setLoginUser(ugiProxy){code}
This works on a standalone spark when running above code however I have had little or no success when using this on kubernetes -

 

an appropriate additional setting required to make this work is:
{code:java}
--conf spark.driver.extraJavaOptions=-Djavax.security.auth.useSubjectCreds=false{code}
this would allow super user login to function appropriately.

 

All these are hacks, ideally this should be supported natively on spark with k8s.;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: "Cannot use an UnspecifiedFrame" error for User Defined Aggregation Function over Window
Issue key: SPARK-40668
Issue id: 13484662
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: harold
Creator: harold
Created: 05/Oct/22 21:35
Updated: 14/Oct/22 05:00
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I have simplified reproducing the problem to the following. The original details for this bug description are below.
{code:java}
import org.apache.spark.sql.{Encoder, Encoders}
import org.apache.spark.sql.expressions.{Aggregator, Window}
import org.apache.spark.sql.functionsimport spark.implicits._

// A record that a user did something.
case class UserAction(who: Long, what: Long)

implicit val userActionEncoder: Encoder[UserAction] = Encoders.product[UserAction]

val top1 = new Aggregator[UserAction, UserAction, UserAction] {
  override def zero: UserAction = UserAction(-1L, -1L)
  override def reduce(b: UserAction, a: UserAction): UserAction = if (a.what > b.what) a else b
  override def merge(b1: UserAction, b2: UserAction): UserAction = reduce(b1, b2)
  override def finish(reduction: UserAction): UserAction = reduction
  override def bufferEncoder: Encoder[UserAction] = userActionEncoder
  override def outputEncoder: Encoder[UserAction] = userActionEncoder
}

val values = Seq(
  UserAction(1, 10),
  UserAction(1, 11),
  UserAction(1, 12),
  UserAction(1, 13),
  UserAction(2, 20)
)

val ds = spark.createDataset[UserAction](values)

// Using my user defined aggregation function works:
ds.select(top1.toColumn.name("top1")).show

// +---+----+
// |who|what|
// +---+----+
// |  2|  20|
// +---+----+

// Using a built-in aggregation function over a window works:
ds.select(max("what").over(Window.partitionBy("who"))).show

// +------------------------------------------------------------------------------------------+
// |max(what) OVER (PARTITION BY who ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)|
// +------------------------------------------------------------------------------------------+
// |                                                                                        13|
// |                                                                                        13|
// |                                                                                        13|
// |                                                                                        13|
// |                                                                                        20|
// +------------------------------------------------------------------------------------------+

// Using my user defined aggregation function over a window fails:
ds.select(top1.toColumn.over(Window.partitionBy("who"))).show

// org.apache.spark.sql.AnalysisException: [DATATYPE_MISMATCH.UNSPECIFIED_FRAME] Cannot resolve "(PARTITION BY who unspecifiedframe$())" due to data type mismatch: Cannot use an UnspecifiedFrame. This should have been converted during analysis.;
// 'Aggregate [unresolvedalias($anon$1($anon$1@1b4f42df, None, None, None, knownnotnull(assertnotnull(input[0, UserAction, true])).who, knownnotnull(assertnotnull(input[0, UserAction, true])).what, newInstance(class UserAction), if (isnull(input[0, UserAction, true])) null else named_struct(who, knownnotnull(input[0, UserAction, true]).who, what, knownnotnull(input[0, UserAction, true]).what), StructField(who,LongType,false), StructField(what,LongType,false), true, 0, 0) windowspecdefinition(who#614L, unspecifiedframe$()), Some(org.apache.spark.sql.Column$$Lambda$5083/1086002511@ea147ea))]
// +- LocalRelation [who#614L, what#615L] {code}
 

*Original Description*

The exception from Spark says "Please file a bug report." so here I am.

Seen with Spark 3.1.1.327 on Scala 2.12. Haven't tried other versions.

Error stack trace:
{code:java}
 testTop3ByUser FAILED
    org.apache.spark.sql.AnalysisException: cannot resolve '(PARTITION BY `who` unspecifiedframe$())' due to data type mismatch: Cannot use an UnspecifiedFrame. This should have been converted during analysis. Please file a bug report.;
    'Aggregate [$anon$1(TestSparkTop3$$anon$1@56cd6f12, None, None, None, encodeusingserializer(input[0, java.lang.Object, true], false), decodeusingserializer(input[0, binary, true], scala.collection.mutable.PriorityQueue, false), encodeusingserializer(input[0, java.lang.Object, true], false), BinaryType, true, 0, 0) windowspecdefinition(who#2L, unspecifiedframe$()) AS $anon$1(encodeusingserializer(input[0, java.lang.Object, true], false) AS `value`, decodeusingserializer(input[0, binary, true], scala.collection.mutable.PriorityQueue, false), encodeusingserializer(input[0, java.lang.Object, true], false)) AS `top3` OVER (PARTITION BY who unspecifiedframe$())#15]
    +- LocalRelation [who#2L, what#3L]
        at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:161)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:152)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:347)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:347)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:344)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:413)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:249)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:411)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:364)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:344)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:344)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:413)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:249)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:411)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:364)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:344)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:76)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at scala.collection.TraversableLike.map(TraversableLike.scala:238)
        at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
        at scala.collection.AbstractTraversable.map(Traversable.scala:108)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:249)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:152)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:93)
        at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:189)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:93)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:90)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:176)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
        at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
        at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
        at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
        at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
        at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
        at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
        at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
        at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
        at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3715)
        at org.apache.spark.sql.Dataset.select(Dataset.scala:1462)
        at TestSparkTop3.testTop3ByUser(TestSparkTop3.scala:54){code}
Sample unit test to reproduce:
{code:java}
import org.apache.spark.sql.{Encoder, Encoders}
import org.apache.spark.sql.expressions.{Aggregator, Window}
import org.scalatest.Matchers
import org.scalatest.testng.TestNGSuite
import org.testng.annotations.Test

// A record that a user did something.
case class UserAction(who: Long, what: Long)

class TestSparkTop3 extends TestNGSuite with Matchers with TestSparkSession {
  @Test
  def testTop3ByUser(): Unit = {
    import spark.implicits._

    implicit val userActionEncoder: Encoder[UserAction] = Encoders.product[UserAction]

    val top3 = new Aggregator[UserAction, mutable.PriorityQueue[UserAction], List[UserAction]] {
      def limitSize(b: mutable.PriorityQueue[UserAction]): mutable.PriorityQueue[UserAction] = {
        while (b.size > 3) {
          b.dequeue()
        }
        b
      }

      override def zero: mutable.PriorityQueue[UserAction] = mutable.PriorityQueue.empty[UserAction](Ordering.by(_.what))

      override def reduce(b: mutable.PriorityQueue[UserAction], a: UserAction): mutable.PriorityQueue[UserAction] =
        limitSize(b += a)

      override def merge(b1: mutable.PriorityQueue[UserAction],
        b2: mutable.PriorityQueue[UserAction]): mutable.PriorityQueue[UserAction] = limitSize(b1 ++ b2)

      override def finish(reduction: mutable.PriorityQueue[UserAction]): List[UserAction] = reduction.toList

      override def bufferEncoder: Encoder[mutable.PriorityQueue[UserAction]] = Encoders.javaSerialization

      override def outputEncoder: Encoder[List[UserAction]] = Encoders.javaSerialization
    }

    val values = Seq(
      UserAction(1, 10),
      UserAction(1, 11),
      UserAction(1, 12),
      UserAction(1, 13),
      UserAction(2, 20)
    )

    val ds = spark.createDataset[UserAction](values)

    val result = ds.select(top3.toColumn.name("top3").over(Window.partitionBy("who")))
      .collect()
    println(result.map(_.toString).mkString(", "))
  }
} {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-10-05 21:35:25.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z19394:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Support recursiveFileLookup for partitioned datasource
Issue key: SPARK-40600
Issue id: 13483675
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wforget
Creator: wforget
Created: 28/Sep/22 13:12
Updated: 30/Sep/22 01:28
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I use hive tez engine to execute union statement and insert into partitioned table may generate HIVE_UNION_SUBDIR subdirectory, and when I use spark sql to read this partitioned table, the data below HIVE_UNION_SUBDIR is not read.

For non-partitioned table, I can read the subdirectories of the table when setting recursiveFileLookup to true, but for partitioned table, it seems impossible to set recursiveFileLookup to true.

So I want to support recursiveFileLookup for partitioned table.
Environment: Spark: 3.1.1

Hive: 3.1.2
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-28098
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Sep 30 01:28:33 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18x6w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Sep/22 01:13;wforget;There is a related implementation in [https://github.com/lyft/spark/pull/40.]

However, I tested and found the following issues:

1. Querying a non-partitioned table with .staging subdirectory has no data.

2. Makes some sql parsing very slow:
{code:java}
org.apache.hadoop.fs.Path.equals(Path.java:400)
scala.runtime.BoxesRunTime.equals2(BoxesRunTime.java:137)
scala.runtime.BoxesRunTime.equals(BoxesRunTime.java:123)
scala.collection.LinearSeqOptimized.contains(LinearSeqOptimized.scala:105)
scala.collection.LinearSeqOptimized.contains$(LinearSeqOptimized.scala:102)
scala.collection.immutable.Stream.contains(Stream.scala:204)
org.apache.spark.sql.execution.datasources.InMemoryFileIndex.getRootPathsLeafDir(InMemoryFileIndex.scala:112)
org.apache.spark.sql.execution.datasources.InMemoryFileIndex.$anonfun$refresh0$2(InMemoryFileIndex.scala:102)
org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$Lambda$3720/369851600.apply(Unknown Source)
scala.collection.TraversableLike$grouper$1$.apply(TraversableLike.scala:465)
scala.collection.TraversableLike$grouper$1$.apply(TraversableLike.scala:455)
scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
scala.collection.TraversableLike.groupBy(TraversableLike.scala:524)
scala.collection.TraversableLike.groupBy$(TraversableLike.scala:454)
scala.collection.mutable.ArrayOps$ofRef.groupBy(ArrayOps.scala:198)
org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:102)
org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:69)
org.apache.spark.sql.execution.datasources.CatalogFileIndex.filterPartitions(CatalogFileIndex.scala:90)
org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:98)
org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:78)
org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:317)
org.apache.spark.sql.catalyst.trees.TreeNode$$Lambda$1229/1024786495.apply(Unknown Source)
org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:317) {code}
 

 

 ;;;, 30/Sep/22 01:28;apachespark;User 'wForget' has created a pull request for this issue:
https://github.com/apache/spark/pull/38053;;;
Affects Version/s.1: 
Comment.1: 30/Sep/22 01:28;apachespark;User 'wForget' has created a pull request for this issue:
https://github.com/apache/spark/pull/38053;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java'
Issue key: SPARK-36862
Issue id: 13403501
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Mpilaw
Creator: Mpilaw
Created: 27/Sep/21 11:08
Updated: 01/Sep/22 14:55
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Submit, SQL
Due Date: 
Votes: 1
Labels: 
Description: Hi,

I am getting the following error running spark-submit command:

ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 321, Column 103: ')' expected instead of '['

 

It fails running the spark sql command on delta lake: spark.sql(sqlTransformation)

The template of sqlTransformation is as follows:

MERGE INTO target_table AS d
 USING source_table AS s 
 on s.id = d.id
 WHEN MATCHED AND d.hash_value <> s.hash_value
 THEN UPDATE SET d.name =s.name, d.address = s.address

 

It is permanent error both for *spark 3.1.1* version.

 

The same works fine with spark 3.0.0.

 

Here is the full log:

2021-09-22 16:43:22,110 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 55, Column 103: ')' expected instead of '['2021-09-22 16:43:22,110 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 55, Column 103: ')' expected instead of '['org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 55, Column 103: ')' expected instead of '[' at org.codehaus.janino.TokenStreamImpl.compileException(TokenStreamImpl.java:362) at org.codehaus.janino.TokenStreamImpl.read(TokenStreamImpl.java:150) at org.codehaus.janino.Parser.read(Parser.java:3703) at org.codehaus.janino.Parser.parseFormalParameters(Parser.java:1622) at org.codehaus.janino.Parser.parseMethodDeclarationRest(Parser.java:1518) at org.codehaus.janino.Parser.parseClassBodyDeclaration(Parser.java:1028) at org.codehaus.janino.Parser.parseClassBody(Parser.java:841) at org.codehaus.janino.Parser.parseClassDeclarationRest(Parser.java:736) at org.codehaus.janino.Parser.parseClassBodyDeclaration(Parser.java:941) at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:234) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:205) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:80) at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:1427) at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1524) at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:1521) at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599) at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379) at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342) at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257) at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000) at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004) at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874) at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1375) at org.apache.spark.sql.execution.WholeStageCodegenExec.liftedTree1$1(WholeStageCodegenExec.scala:721) at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720) at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181) at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:160) at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:160) at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:164) at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:163) at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$2(ShuffleExchangeExec.scala:100) at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52) at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$1(ShuffleExchangeExec.scala:100) at org.apache.spark.sql.util.LazyValue.getOrInit(LazyValue.scala:41) at org.apache.spark.sql.execution.exchange.Exchange.getOrInitMaterializeFuture(Exchange.scala:68) at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materializeFuture(ShuffleExchangeExec.scala:96) at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize(ShuffleExchangeExec.scala:84) at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize$(ShuffleExchangeExec.scala:83) at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.materialize(ShuffleExchangeExec.scala:128) at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:161) at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:74) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220) at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:74) at org.apache.spark.sql.execution.adaptive.MaterializeExecutable.tryStart(AdaptiveExecutable.scala:396) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.startChild(AdaptiveExecutor.scala:225) at org.apache.spark.sql.execution.adaptive.ExecutionHelper.start(ExecutionHelper.scala:47) at org.apache.spark.sql.execution.adaptive.QueryStageExecutable$$anon$2.$anonfun$new$1(AdaptiveExecutable.scala:251) at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2(ExecutionHelper.scala:55) at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2$adapted(ExecutionHelper.scala:54) at scala.Option.foreach(Option.scala:407) at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1(ExecutionHelper.scala:54) at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1$adapted(ExecutionHelper.scala:53) at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.onChildSuccess(ExecutionHelper.scala:53) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2(AdaptiveExecutor.scala:314) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2$adapted(AdaptiveExecutor.scala:314) at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onActiveChildSuccess(AdaptiveExecutor.scala:314) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onChildSuccess(AdaptiveExecutor.scala:284) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1(AdaptiveExecutor.scala:92) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1$adapted(AdaptiveExecutor.scala:91) at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149) at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237) at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230) at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44) at scala.collection.mutable.HashMap.foreach(HashMap.scala:149) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:91) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:434) at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181) at org.apache.spark.sql.delta.constraints.DeltaInvariantCheckerExec.doExecute(DeltaInvariantCheckerExec.scala:78) at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181) at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177) at org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:192) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68) at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:163) at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:142) at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:84) at org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$writeAllChanges$1(MergeIntoCommand.scala:552) at org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:654) at org.apache.spark.sql.delta.commands.MergeIntoCommand.writeAllChanges(MergeIntoCommand.scala:460) at org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$4(MergeIntoCommand.scala:274) at org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53) at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32) at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27) at org.apache.spark.sql.delta.commands.MergeIntoCommand.withStatusCode(MergeIntoCommand.scala:201) at org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2(MergeIntoCommand.scala:274) at org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2$adapted(MergeIntoCommand.scala:255) at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:187) at org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:255) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77) at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67) at org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:201) at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:106) at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:91) at org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:201) at org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:253) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79) at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229) at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229) at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97) at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610) at com.aup.daab.ds.cf.external.DeltaIntegration.executeDeltaQuery(DeltaIntegration.scala:214) at com.aup.daab.ds.cf.TraServs.Step$.ExecuteTransformation(Step.scala:77) at com.aup.daab.ds.cf.TraServs.TraServ.$anonfun$transformData$5(TraServ.scala:268) at com.aup.daab.ds.cf.TraServs.TraServ.$anonfun$transformData$5$adapted(TraServ.scala:240) at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198) at com.aup.daab.ds.cf.TraServs.TraServ.transformData(TraServ.scala:240) at com.aup.daab.ds.cf.driver.Driver$.main(Driver.scala:147) at com.aup.daab.ds.cf.driver.Driver.main(Driver.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:959) at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180) at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203) at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90) at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1038) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1047) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)2021-09-22 16:43:22,112 WARN WholeStageCodegenExec: Whole-stage codegen disabled for plan (id=9):
Environment: Spark 3.1.1 and Spark 3.1.2

hadoop 3.2.1
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-19984
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 01 14:54:47 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vbl4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Sep/21 11:14;kabhwan;Could you please provide more information? I guess the log would contain the generated code which we can investigate on. Otherwise I could give some instruction to enable DEBUG log to retain the generated code.

Also could you please provide which operation(s) your query executes, if they're OK to be exposed to the public?;;;, 27/Sep/21 13:10;Mpilaw;Hi [~kabhwan],

I updated the description with triggered operation and log details, thanks.;;;, 27/Sep/21 13:37;kabhwan;I guess you'd have the generated code in the log. Please attach the snippet. If there's no log for generated code, please raise a log level for CodeGenerator in log4j config and try running the query again.

org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator=DEBUG;;;, 28/Sep/21 13:11;Mpilaw;I get the physical execution plan as a part of output log but I cannot share that in public if you mean so.

Any thoughts why the same works on spark 3.0.0? ;;;, 28/Sep/21 23:08;kabhwan;No, I meant you're encouraged to paste the "generated" code in the log. It's less thing to worry about, as it's quite hard to infer the actual business logic from generated code.;;;, 01/Sep/22 14:54;luky;I managed to reproduce the issue in my environment. Problem is on line 192 - variable name in function header having array index

Here is the generated code
{code:java}
/* 001 */ public Object generate(Object[] references) {
/* 002 */ return new GeneratedIteratorForCodegenStage636(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=636
/* 006 */ final class GeneratedIteratorForCodegenStage636 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */ private Object[] references;
/* 008 */ private scala.collection.Iterator[] inputs;
/* 009 */ private scala.collection.Iterator smj_leftInput_0;
/* 010 */ private scala.collection.Iterator smj_rightInput_0;
/* 011 */ private InternalRow smj_leftRow_0;
/* 012 */ private InternalRow smj_rightRow_0;
/* 013 */ private boolean smj_globalIsNull_0;
/* 014 */ private boolean smj_globalIsNull_1;
/* 015 */ private double smj_value_27;
/* 016 */ private org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray smj_matches_0;
/* 017 */ private double smj_value_28;
/* 018 */ private boolean smj_isNull_25;
/* 019 */ private boolean smj_isNull_26;
/* 020 */ private boolean smj_isNull_27;
/* 021 */ private boolean smj_isNull_28;
/* 022 */ private boolean smj_isNull_29;
/* 023 */ private boolean smj_isNull_30;
/* 024 */ private boolean project_subExprIsNull_0;
/* 025 */ private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] smj_mutableStateArray_2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 026 */ private java.util.regex.Pattern[] project_mutableStateArray_0 = new java.util.regex.Pattern[1];
/* 027 */ private Decimal[] smj_mutableStateArray_1 = new Decimal[1];
/* 028 */ private String[] project_mutableStateArray_1 = new String[1];
/* 029 */ private UTF8String[] smj_mutableStateArray_0 = new UTF8String[7];
/* 030 */
/* 031 */ public GeneratedIteratorForCodegenStage636(Object[] references) {
/* 032 */ this.references = references;
/* 033 */ }
/* 034 */
/* 035 */ public void init(int index, scala.collection.Iterator[] inputs) {
/* 036 */ partitionIndex = index;
/* 037 */ this.inputs = inputs;
/* 038 */ smj_leftInput_0 = inputs[0];
/* 039 */ smj_rightInput_0 = inputs[1];
/* 040 */
/* 041 */ smj_matches_0 = new org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray(2147483632, 2147483647);
/* 042 */ smj_mutableStateArray_2[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 043 */ smj_mutableStateArray_2[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 044 */
/* 045 */ }
/* 046 */
/* 047 */ private boolean smj_findNextOuterJoinRows_0(
/* 048 */ scala.collection.Iterator leftIter,
/* 049 */ scala.collection.Iterator rightIter) {
/* 050 */ smj_leftRow_0 = null;
/* 051 */ int comp = 0;
/* 052 */ while (smj_leftRow_0 == null) {
/* 053 */ if (!leftIter.hasNext()) return false;
/* 054 */ smj_leftRow_0 = (InternalRow) leftIter.next();
/* 055 */ UTF8String smj_value_22 = smj_If_0(smj_leftRow_0);
/* 056 */ boolean smj_isNull_2 = smj_globalIsNull_1;
/* 057 */ double smj_value_2 = -1.0;
/* 058 */ if (!smj_globalIsNull_1) {
/* 059 */ final String smj_doubleStr_0 = smj_value_22.toString();
/* 060 */ try {
/* 061 */ smj_value_2 = Double.valueOf(smj_doubleStr_0);
/* 062 */ } catch (java.lang.NumberFormatException e) {
/* 063 */ final Double d = (Double) Cast.processFloatingPointSpecialLiterals(smj_doubleStr_0, false);
/* 064 */ if (d == null) {
/* 065 */ smj_isNull_2 = true;
/* 066 */ } else {
/* 067 */ smj_value_2 = d.doubleValue();
/* 068 */ }
/* 069 */ }
/* 070 */ }
/* 071 */ boolean smj_isNull_1 = smj_isNull_2;
/* 072 */ double smj_value_1 = -1.0;
/* 073 */
/* 074 */ if (!smj_isNull_2) {
/* 075 */ if (Double.isNaN(smj_value_2)) {
/* 076 */ smj_value_1 = Double.NaN;
/* 077 */ } else if (smj_value_2 == -0.0d) {
/* 078 */ smj_value_1 = 0.0d;
/* 079 */ } else {
/* 080 */ smj_value_1 = smj_value_2;
/* 081 */ }
/* 082 */
/* 083 */ }
/* 084 */ if (smj_isNull_1) {
/* 085 */ if (!smj_matches_0.isEmpty()) {
/* 086 */ smj_matches_0.clear();
/* 087 */ }
/* 088 */ return true;
/* 089 */ }
/* 090 */ if (!smj_matches_0.isEmpty()) {
/* 091 */ comp = 0;
/* 092 */ if (comp == 0) {
/* 093 */ comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(smj_value_1, smj_value_28);
/* 094 */ }
/* 095 */
/* 096 */ if (comp == 0) {
/* 097 */ return true;
/* 098 */ }
/* 099 */ smj_matches_0.clear();
/* 100 */ }
/* 101 */
/* 102 */ do {
/* 103 */ if (smj_rightRow_0 == null) {
/* 104 */ if (!rightIter.hasNext()) {
/* 105 */ if (!smj_matches_0.isEmpty()) {
/* 106 */ smj_value_28 = smj_value_1;
/* 107 */ }
/* 108 */ return true;
/* 109 */ }
/* 110 */ smj_rightRow_0 = (InternalRow) rightIter.next();
/* 111 */ Decimal smj_value_26 = smj_rightRow_0.getDecimal(1, 38, 0);
/* 112 */ boolean smj_isNull_23 = false;
/* 113 */ double smj_value_25 = -1.0;
/* 114 */ if (!false) {
/* 115 */ smj_value_25 = smj_value_26.toDouble();
/* 116 */ }
/* 117 */ double smj_value_24 = -1.0;
/* 118 */
/* 119 */ if (Double.isNaN(smj_value_25)) {
/* 120 */ smj_value_24 = Double.NaN;
/* 121 */ } else if (smj_value_25 == -0.0d) {
/* 122 */ smj_value_24 = 0.0d;
/* 123 */ } else {
/* 124 */ smj_value_24 = smj_value_25;
/* 125 */ }
/* 126 */ if (false) {
/* 127 */ smj_rightRow_0 = null;
/* 128 */ continue;
/* 129 */ }
/* 130 */ smj_value_27 = smj_value_24;
/* 131 */ }
/* 132 */
/* 133 */ comp = 0;
/* 134 */ if (comp == 0) {
/* 135 */ comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(smj_value_1, smj_value_27);
/* 136 */ }
/* 137 */
/* 138 */ if (comp > 0) {
/* 139 */ smj_rightRow_0 = null;
/* 140 */ } else if (comp < 0) {
/* 141 */ if (!smj_matches_0.isEmpty()) {
/* 142 */ smj_value_28 = smj_value_1;
/* 143 */ }
/* 144 */ return true;
/* 145 */ } else {
/* 146 */ smj_matches_0.add((UnsafeRow) smj_rightRow_0);
/* 147 */ smj_rightRow_0 = null;
/* 148 */ }
/* 149 */ } while (true);
/* 150 */ }
/* 151 */ return false; // unreachable
/* 152 */ }
/* 153 */
/* 154 */ private UTF8String smj_If_0(InternalRow smj_leftRow_0) {
/* 155 */ boolean smj_isNull_5 = false;
/* 156 */ boolean smj_isNull_6 = true;
/* 157 */ ArrayData smj_value_6 = null;
/* 158 */ boolean smj_isNull_7 = smj_leftRow_0.isNullAt(2);
/* 159 */ UTF8String smj_value_7 = smj_isNull_7 ?
/* 160 */ null : (smj_leftRow_0.getUTF8String(2));
/* 161 */ if (!smj_isNull_7) {
/* 162 */ smj_isNull_6 = false; // resultCode could change nullability.
/* 163 */ smj_value_6 = new org.apache.spark.sql.catalyst.util.GenericArrayData(smj_value_7.split(((UTF8String) references[0] /* literal */),-1));
/* 164 */
/* 165 */ }
/* 166 */ int smj_value_5 = smj_isNull_6 ? -1 :
/* 167 */ (smj_value_6).numElements();
/* 168 */
/* 169 */ boolean smj_value_4 = false;
/* 170 */ smj_value_4 = smj_value_5 > 2;
/* 171 */ boolean smj_isNull_3 = false;
/* 172 */ UTF8String smj_value_3 = null;
/* 173 */ if (!false && smj_value_4) {
/* 174 */ boolean smj_isNull_11 = true;
/* 175 */ UTF8String smj_value_11 = null;
/* 176 */ UTF8String smj_value_18 = smj_GetArrayItem_0(smj_leftRow_0);
/* 177 */ if (!smj_globalIsNull_0) {
/* 178 */ smj_isNull_11 = false; // resultCode could change nullability.
/* 179 */ smj_value_11 = smj_value_18.substringSQL(1, 10);
/* 180 */
/* 181 */ }
/* 182 */ smj_isNull_3 = smj_isNull_11;
/* 183 */ smj_value_3 = smj_value_11;
/* 184 */ } else {
/* 185 */ smj_isNull_3 = true;
/* 186 */ smj_value_3 = ((UTF8String)null);
/* 187 */ }
/* 188 */ smj_globalIsNull_1 = smj_isNull_3;
/* 189 */ return smj_value_3;
/* 190 */ }
/* 191 */
/* 192 */ private UTF8String project_subExpr_0(org.apache.spark.unsafe.types.UTF8String smj_mutableStateArray_0[3], boolean smj_isNull_28) {
/* 193 */ boolean project_isNull_0 = true;
/* 194 */ UTF8String project_value_0 = null;
/* 195 */
/* 196 */ if (!smj_isNull_28) {
/* 197 */ project_isNull_0 = false; // resultCode could change nullability.
/* 198 */
/* 199 */ if (!((UTF8String) references[3] /* literal */).equals(smj_mutableStateArray_0[5])) {
/* 200 */ // regex value changed
/* 201 */ smj_mutableStateArray_0[5] = ((UTF8String) references[3] /* literal */).clone();
/* 202 */ project_mutableStateArray_0[0] = java.util.regex.Pattern.compile(smj_mutableStateArr",
 "ay_0[5].toString());
/* 203 */ }
/* 204 */ if (!((UTF8String) references[4] /* literal */).equals(smj_mutableStateArray_0[6])) {
/* 205 */ // replacement string changed
/* 206 */ smj_mutableStateArray_0[6] = ((UTF8String) references[4] /* literal */).clone();
/* 207 */ project_mutableStateArray_1[0] = smj_mutableStateArray_0[6].toString();
/* 208 */ }
/* 209 */ String project_source_0 = smj_mutableStateArray_0[3].toString();
/* 210 */ int project_position_0 = 1 - 1;
/* 211 */ if (project_position_0 < project_source_0.length()) {
/* 212 */ java.lang.StringBuffer project_termResult_0 = new java.lang.StringBuffer();
/* 213 */ java.util.regex.Matcher project_matcher_0 = project_mutableStateArray_0[0].matcher(project_source_0);
/* 214 */ project_matcher_0.region(project_position_0, project_source_0.length());
/* 215 */
/* 216 */ while (project_matcher_0.find()) {
/* 217 */ project_matcher_0.appendReplacement(project_termResult_0, project_mutableStateArray_1[0]);
/* 218 */ }
/* 219 */ project_matcher_0.appendTail(project_termResult_0);
/* 220 */ project_value_0 = UTF8String.fromString(project_termResult_0.toString());
/* 221 */ project_termResult_0 = null;
/* 222 */ } else {
/* 223 */ project_value_0 = smj_mutableStateArray_0[3];
/* 224 */ }
/* 225 */ project_isNull_0 = false;
/* 226 */
/* 227 */ }
/* 228 */ project_subExprIsNull_0 = project_isNull_0;
/* 229 */ return project_value_0;
/* 230 */ }
/* 231 */
/* 232 */ private UTF8String smj_GetArrayItem_0(InternalRow smj_leftRow_0) {
/* 233 */ boolean smj_isNull_12 = true;
/* 234 */ UTF8String smj_value_12 = null;
/* 235 */ boolean smj_isNull_13 = true;
/* 236 */ ArrayData smj_value_13 = null;
/* 237 */ boolean smj_isNull_14 = smj_leftRow_0.isNullAt(2);
/* 238 */ UTF8String smj_value_14 = smj_isNull_14 ?
/* 239 */ null : (smj_leftRow_0.getUTF8String(2));
/* 240 */ if (!smj_isNull_14) {
/* 241 */ smj_isNull_13 = false; // resultCode could change nullability.
/* 242 */ smj_value_13 = new org.apache.spark.sql.catalyst.util.GenericArrayData(smj_value_14.split(((UTF8String) references[1] /* literal */),-1));
/* 243 */
/* 244 */ }
/* 245 */ if (!smj_isNull_13) {
/* 246 */ smj_isNull_12 = false; // resultCode could change nullability.
/* 247 */
/* 248 */ final int smj_index_0 = (int) 2;
/* 249 */ if (smj_index_0 >= smj_value_13.numElements() || smj_index_0 < 0) {
/* 250 */ smj_isNull_12 = true;
/* 251 */ } else if (smj_value_13.isNullAt(smj_index_0)) {
/* 252 */ smj_isNull_12 = true;
/* 253 */ }
/* 254 */ else {
/* 255 */ smj_value_12 = smj_value_13.getUTF8String(smj_index_0);
/* 256 */ }
/* 257 */
/* 258 */ }
/* 259 */ smj_globalIsNull_0 = smj_isNull_12;
/* 260 */ return smj_value_12;
/* 261 */ }
/* 262 */
/* 263 */ protected void processNext() throws java.io.IOException {
/* 264 */ while (smj_findNextOuterJoinRows_0(smj_leftInput_0, smj_rightInput_0)) {
/* 265 */ boolean smj_loaded_0 = false;
/* 266 */ smj_isNull_25 = smj_leftRow_0.isNullAt(0);
/* 267 */ smj_mutableStateArray_0[0] = smj_isNull_25 ? null : (smj_leftRow_0.getUTF8String(0));
/* 268 */ smj_isNull_26 = smj_leftRow_0.isNullAt(1);
/* 269 */ smj_mutableStateArray_0[1] = smj_isNull_26 ? null : (smj_leftRow_0.getUTF8String(1));
/* 270 */ smj_isNull_27 = smj_leftRow_0.isNullAt(2);
/* 271 */ smj_mutableStateArray_0[2] = smj_isNull_27 ? null : (smj_leftRow_0.getUTF8String(2));
/* 272 */ smj_isNull_28 = smj_leftRow_0.isNullAt(3);
/* 273 */ smj_mutableStateArray_0[3] = smj_isNull_28 ? null : (smj_leftRow_0.getUTF8String(3));
/* 274 */ scala.collection.Iterator<UnsafeRow> smj_iterator_0 = smj_matches_0.generateIterator();
/* 275 */ while (smj_iterator_0.hasNext()) {
/* 276 */ InternalRow smj_rightRow_1 = (InternalRow) smj_iterator_0.next();
/* 277 */ smj_isNull_29 = smj_rightRow_1.isNullAt(0);
/* 278 */ smj_mutableStateArray_0[4] = smj_isNull_29 ? null : (smj_rightRow_1.getUTF8String(0));
/* 279 */ smj_isNull_30 = false;
/* 280 */ smj_mutableStateArray_1[0] = smj_rightRow_1.getDecimal(1, 38, 0);
/* 281 */
/* 282 */ smj_loaded_0 = true;
/* 283 */ smj_writeJoinRows_0();
/* 284 */ }
/* 285 */ if (!smj_loaded_0) {
/* 286 */ smj_isNull_29 = true;
/* 287 */ smj_isNull_30 = true;
/* 288 */ smj_writeJoinRows_0();
/* 289 */ }
/* 290 */ if (shouldStop()) return;
/* 291 */ }
/* 292 */ ((org.apache.spark.sql.execution.joins.SortMergeJoinExec) references[5] /* plan */).cleanupResources();
/* 293 */ }
/* 294 */
/* 295 */ private void smj_writeJoinRows_0() throws java.io.IOException {
/* 296 */ ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);
/* 297 */
/* 298 */ // common sub-expressions
/* 299 */
/* 300 */ UTF8String project_subExprValue_0 = project_subExpr_0(smj_mutableStateArray_0[3], smj_isNull_28);
/* 301 */
/* 302 */ smj_mutableStateArray_2[1].reset();
/* 303 */
/* 304 */ smj_mutableStateArray_2[1].zeroOutNullBytes();
/* 305 */
/* 306 */ if (smj_isNull_25) {
/* 307 */ smj_mutableStateArray_2[1].setNullAt(0);
/* 308 */ } else {
/* 309 */ smj_mutableStateArray_2[1].write(0, smj_mutableStateArray_0[0]);
/* 310 */ }
/* 311 */
/* 312 */ if (smj_isNull_26) {
/* 313 */ smj_mutableStateArray_2[1].setNullAt(1);
/* 314 */ } else {
/* 315 */ smj_mutableStateArray_2[1].write(1, smj_mutableStateArray_0[1]);
/* 316 */ }
/* 317 */
/* 318 */ if (project_subExprIsNull_0) {
/* 319 */ smj_mutableStateArray_2[1].setNullAt(2);
/* 320 */ } else {
/* 321 */ smj_mutableStateArray_2[1].write(2, project_subExprValue_0);
/* 322 */ }
/* 323 */
/* 324 */ if (project_subExprIsNull_0) {
/* 325 */ smj_mutableStateArray_2[1].setNullAt(3);
/* 326 */ } else {
/* 327 */ smj_mutableStateArray_2[1].write(3, project_subExprValue_0);
/* 328 */ }
/* 329 */
/* 330 */ if (smj_isNull_29) {
/* 331 */ smj_mutableStateArray_2[1].setNullAt(4);
/* 332 */ } else {
/* 333 */ smj_mutableStateArray_2[1].write(4, smj_mutableStateArray_0[4]);
/* 334 */ }
/* 335 */
/* 336 */ if (smj_isNull_25) {
/* 337 */ smj_mutableStateArray_2[1].setNullAt(5);
/* 338 */ } else {
/* 339 */ smj_mutableStateArray_2[1].write(5, smj_mutableStateArray_0[0]);
/* 340 */ }
/* 341 */ append((smj_mutableStateArray_2[1].getRow()).copy());
/* 342 */
/* 343 */ }
/* 344 */
/* 345 */ } {code}
 ;;;
Affects Version/s.1: 
Comment.1: 27/Sep/21 13:10;Mpilaw;Hi [~kabhwan],

I updated the description with triggered operation and log details, thanks.;;;
Comment.2: 27/Sep/21 13:37;kabhwan;I guess you'd have the generated code in the log. Please attach the snippet. If there's no log for generated code, please raise a log level for CodeGenerator in log4j config and try running the query again.

org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator=DEBUG;;;
Comment.3: 28/Sep/21 13:11;Mpilaw;I get the physical execution plan as a part of output log but I cannot share that in public if you mean so.

Any thoughts why the same works on spark 3.0.0? ;;;
Comment.4: 28/Sep/21 23:08;kabhwan;No, I meant you're encouraged to paste the "generated" code in the log. It's less thing to worry about, as it's quite hard to infer the actual business logic from generated code.;;;
Comment.5: 01/Sep/22 14:54;luky;I managed to reproduce the issue in my environment. Problem is on line 192 - variable name in function header having array index

Here is the generated code
{code:java}
/* 001 */ public Object generate(Object[] references) {
/* 002 */ return new GeneratedIteratorForCodegenStage636(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=636
/* 006 */ final class GeneratedIteratorForCodegenStage636 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */ private Object[] references;
/* 008 */ private scala.collection.Iterator[] inputs;
/* 009 */ private scala.collection.Iterator smj_leftInput_0;
/* 010 */ private scala.collection.Iterator smj_rightInput_0;
/* 011 */ private InternalRow smj_leftRow_0;
/* 012 */ private InternalRow smj_rightRow_0;
/* 013 */ private boolean smj_globalIsNull_0;
/* 014 */ private boolean smj_globalIsNull_1;
/* 015 */ private double smj_value_27;
/* 016 */ private org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray smj_matches_0;
/* 017 */ private double smj_value_28;
/* 018 */ private boolean smj_isNull_25;
/* 019 */ private boolean smj_isNull_26;
/* 020 */ private boolean smj_isNull_27;
/* 021 */ private boolean smj_isNull_28;
/* 022 */ private boolean smj_isNull_29;
/* 023 */ private boolean smj_isNull_30;
/* 024 */ private boolean project_subExprIsNull_0;
/* 025 */ private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] smj_mutableStateArray_2 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 026 */ private java.util.regex.Pattern[] project_mutableStateArray_0 = new java.util.regex.Pattern[1];
/* 027 */ private Decimal[] smj_mutableStateArray_1 = new Decimal[1];
/* 028 */ private String[] project_mutableStateArray_1 = new String[1];
/* 029 */ private UTF8String[] smj_mutableStateArray_0 = new UTF8String[7];
/* 030 */
/* 031 */ public GeneratedIteratorForCodegenStage636(Object[] references) {
/* 032 */ this.references = references;
/* 033 */ }
/* 034 */
/* 035 */ public void init(int index, scala.collection.Iterator[] inputs) {
/* 036 */ partitionIndex = index;
/* 037 */ this.inputs = inputs;
/* 038 */ smj_leftInput_0 = inputs[0];
/* 039 */ smj_rightInput_0 = inputs[1];
/* 040 */
/* 041 */ smj_matches_0 = new org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray(2147483632, 2147483647);
/* 042 */ smj_mutableStateArray_2[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 043 */ smj_mutableStateArray_2[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(6, 192);
/* 044 */
/* 045 */ }
/* 046 */
/* 047 */ private boolean smj_findNextOuterJoinRows_0(
/* 048 */ scala.collection.Iterator leftIter,
/* 049 */ scala.collection.Iterator rightIter) {
/* 050 */ smj_leftRow_0 = null;
/* 051 */ int comp = 0;
/* 052 */ while (smj_leftRow_0 == null) {
/* 053 */ if (!leftIter.hasNext()) return false;
/* 054 */ smj_leftRow_0 = (InternalRow) leftIter.next();
/* 055 */ UTF8String smj_value_22 = smj_If_0(smj_leftRow_0);
/* 056 */ boolean smj_isNull_2 = smj_globalIsNull_1;
/* 057 */ double smj_value_2 = -1.0;
/* 058 */ if (!smj_globalIsNull_1) {
/* 059 */ final String smj_doubleStr_0 = smj_value_22.toString();
/* 060 */ try {
/* 061 */ smj_value_2 = Double.valueOf(smj_doubleStr_0);
/* 062 */ } catch (java.lang.NumberFormatException e) {
/* 063 */ final Double d = (Double) Cast.processFloatingPointSpecialLiterals(smj_doubleStr_0, false);
/* 064 */ if (d == null) {
/* 065 */ smj_isNull_2 = true;
/* 066 */ } else {
/* 067 */ smj_value_2 = d.doubleValue();
/* 068 */ }
/* 069 */ }
/* 070 */ }
/* 071 */ boolean smj_isNull_1 = smj_isNull_2;
/* 072 */ double smj_value_1 = -1.0;
/* 073 */
/* 074 */ if (!smj_isNull_2) {
/* 075 */ if (Double.isNaN(smj_value_2)) {
/* 076 */ smj_value_1 = Double.NaN;
/* 077 */ } else if (smj_value_2 == -0.0d) {
/* 078 */ smj_value_1 = 0.0d;
/* 079 */ } else {
/* 080 */ smj_value_1 = smj_value_2;
/* 081 */ }
/* 082 */
/* 083 */ }
/* 084 */ if (smj_isNull_1) {
/* 085 */ if (!smj_matches_0.isEmpty()) {
/* 086 */ smj_matches_0.clear();
/* 087 */ }
/* 088 */ return true;
/* 089 */ }
/* 090 */ if (!smj_matches_0.isEmpty()) {
/* 091 */ comp = 0;
/* 092 */ if (comp == 0) {
/* 093 */ comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(smj_value_1, smj_value_28);
/* 094 */ }
/* 095 */
/* 096 */ if (comp == 0) {
/* 097 */ return true;
/* 098 */ }
/* 099 */ smj_matches_0.clear();
/* 100 */ }
/* 101 */
/* 102 */ do {
/* 103 */ if (smj_rightRow_0 == null) {
/* 104 */ if (!rightIter.hasNext()) {
/* 105 */ if (!smj_matches_0.isEmpty()) {
/* 106 */ smj_value_28 = smj_value_1;
/* 107 */ }
/* 108 */ return true;
/* 109 */ }
/* 110 */ smj_rightRow_0 = (InternalRow) rightIter.next();
/* 111 */ Decimal smj_value_26 = smj_rightRow_0.getDecimal(1, 38, 0);
/* 112 */ boolean smj_isNull_23 = false;
/* 113 */ double smj_value_25 = -1.0;
/* 114 */ if (!false) {
/* 115 */ smj_value_25 = smj_value_26.toDouble();
/* 116 */ }
/* 117 */ double smj_value_24 = -1.0;
/* 118 */
/* 119 */ if (Double.isNaN(smj_value_25)) {
/* 120 */ smj_value_24 = Double.NaN;
/* 121 */ } else if (smj_value_25 == -0.0d) {
/* 122 */ smj_value_24 = 0.0d;
/* 123 */ } else {
/* 124 */ smj_value_24 = smj_value_25;
/* 125 */ }
/* 126 */ if (false) {
/* 127 */ smj_rightRow_0 = null;
/* 128 */ continue;
/* 129 */ }
/* 130 */ smj_value_27 = smj_value_24;
/* 131 */ }
/* 132 */
/* 133 */ comp = 0;
/* 134 */ if (comp == 0) {
/* 135 */ comp = org.apache.spark.sql.catalyst.util.SQLOrderingUtil.compareDoubles(smj_value_1, smj_value_27);
/* 136 */ }
/* 137 */
/* 138 */ if (comp > 0) {
/* 139 */ smj_rightRow_0 = null;
/* 140 */ } else if (comp < 0) {
/* 141 */ if (!smj_matches_0.isEmpty()) {
/* 142 */ smj_value_28 = smj_value_1;
/* 143 */ }
/* 144 */ return true;
/* 145 */ } else {
/* 146 */ smj_matches_0.add((UnsafeRow) smj_rightRow_0);
/* 147 */ smj_rightRow_0 = null;
/* 148 */ }
/* 149 */ } while (true);
/* 150 */ }
/* 151 */ return false; // unreachable
/* 152 */ }
/* 153 */
/* 154 */ private UTF8String smj_If_0(InternalRow smj_leftRow_0) {
/* 155 */ boolean smj_isNull_5 = false;
/* 156 */ boolean smj_isNull_6 = true;
/* 157 */ ArrayData smj_value_6 = null;
/* 158 */ boolean smj_isNull_7 = smj_leftRow_0.isNullAt(2);
/* 159 */ UTF8String smj_value_7 = smj_isNull_7 ?
/* 160 */ null : (smj_leftRow_0.getUTF8String(2));
/* 161 */ if (!smj_isNull_7) {
/* 162 */ smj_isNull_6 = false; // resultCode could change nullability.
/* 163 */ smj_value_6 = new org.apache.spark.sql.catalyst.util.GenericArrayData(smj_value_7.split(((UTF8String) references[0] /* literal */),-1));
/* 164 */
/* 165 */ }
/* 166 */ int smj_value_5 = smj_isNull_6 ? -1 :
/* 167 */ (smj_value_6).numElements();
/* 168 */
/* 169 */ boolean smj_value_4 = false;
/* 170 */ smj_value_4 = smj_value_5 > 2;
/* 171 */ boolean smj_isNull_3 = false;
/* 172 */ UTF8String smj_value_3 = null;
/* 173 */ if (!false && smj_value_4) {
/* 174 */ boolean smj_isNull_11 = true;
/* 175 */ UTF8String smj_value_11 = null;
/* 176 */ UTF8String smj_value_18 = smj_GetArrayItem_0(smj_leftRow_0);
/* 177 */ if (!smj_globalIsNull_0) {
/* 178 */ smj_isNull_11 = false; // resultCode could change nullability.
/* 179 */ smj_value_11 = smj_value_18.substringSQL(1, 10);
/* 180 */
/* 181 */ }
/* 182 */ smj_isNull_3 = smj_isNull_11;
/* 183 */ smj_value_3 = smj_value_11;
/* 184 */ } else {
/* 185 */ smj_isNull_3 = true;
/* 186 */ smj_value_3 = ((UTF8String)null);
/* 187 */ }
/* 188 */ smj_globalIsNull_1 = smj_isNull_3;
/* 189 */ return smj_value_3;
/* 190 */ }
/* 191 */
/* 192 */ private UTF8String project_subExpr_0(org.apache.spark.unsafe.types.UTF8String smj_mutableStateArray_0[3], boolean smj_isNull_28) {
/* 193 */ boolean project_isNull_0 = true;
/* 194 */ UTF8String project_value_0 = null;
/* 195 */
/* 196 */ if (!smj_isNull_28) {
/* 197 */ project_isNull_0 = false; // resultCode could change nullability.
/* 198 */
/* 199 */ if (!((UTF8String) references[3] /* literal */).equals(smj_mutableStateArray_0[5])) {
/* 200 */ // regex value changed
/* 201 */ smj_mutableStateArray_0[5] = ((UTF8String) references[3] /* literal */).clone();
/* 202 */ project_mutableStateArray_0[0] = java.util.regex.Pattern.compile(smj_mutableStateArr",
 "ay_0[5].toString());
/* 203 */ }
/* 204 */ if (!((UTF8String) references[4] /* literal */).equals(smj_mutableStateArray_0[6])) {
/* 205 */ // replacement string changed
/* 206 */ smj_mutableStateArray_0[6] = ((UTF8String) references[4] /* literal */).clone();
/* 207 */ project_mutableStateArray_1[0] = smj_mutableStateArray_0[6].toString();
/* 208 */ }
/* 209 */ String project_source_0 = smj_mutableStateArray_0[3].toString();
/* 210 */ int project_position_0 = 1 - 1;
/* 211 */ if (project_position_0 < project_source_0.length()) {
/* 212 */ java.lang.StringBuffer project_termResult_0 = new java.lang.StringBuffer();
/* 213 */ java.util.regex.Matcher project_matcher_0 = project_mutableStateArray_0[0].matcher(project_source_0);
/* 214 */ project_matcher_0.region(project_position_0, project_source_0.length());
/* 215 */
/* 216 */ while (project_matcher_0.find()) {
/* 217 */ project_matcher_0.appendReplacement(project_termResult_0, project_mutableStateArray_1[0]);
/* 218 */ }
/* 219 */ project_matcher_0.appendTail(project_termResult_0);
/* 220 */ project_value_0 = UTF8String.fromString(project_termResult_0.toString());
/* 221 */ project_termResult_0 = null;
/* 222 */ } else {
/* 223 */ project_value_0 = smj_mutableStateArray_0[3];
/* 224 */ }
/* 225 */ project_isNull_0 = false;
/* 226 */
/* 227 */ }
/* 228 */ project_subExprIsNull_0 = project_isNull_0;
/* 229 */ return project_value_0;
/* 230 */ }
/* 231 */
/* 232 */ private UTF8String smj_GetArrayItem_0(InternalRow smj_leftRow_0) {
/* 233 */ boolean smj_isNull_12 = true;
/* 234 */ UTF8String smj_value_12 = null;
/* 235 */ boolean smj_isNull_13 = true;
/* 236 */ ArrayData smj_value_13 = null;
/* 237 */ boolean smj_isNull_14 = smj_leftRow_0.isNullAt(2);
/* 238 */ UTF8String smj_value_14 = smj_isNull_14 ?
/* 239 */ null : (smj_leftRow_0.getUTF8String(2));
/* 240 */ if (!smj_isNull_14) {
/* 241 */ smj_isNull_13 = false; // resultCode could change nullability.
/* 242 */ smj_value_13 = new org.apache.spark.sql.catalyst.util.GenericArrayData(smj_value_14.split(((UTF8String) references[1] /* literal */),-1));
/* 243 */
/* 244 */ }
/* 245 */ if (!smj_isNull_13) {
/* 246 */ smj_isNull_12 = false; // resultCode could change nullability.
/* 247 */
/* 248 */ final int smj_index_0 = (int) 2;
/* 249 */ if (smj_index_0 >= smj_value_13.numElements() || smj_index_0 < 0) {
/* 250 */ smj_isNull_12 = true;
/* 251 */ } else if (smj_value_13.isNullAt(smj_index_0)) {
/* 252 */ smj_isNull_12 = true;
/* 253 */ }
/* 254 */ else {
/* 255 */ smj_value_12 = smj_value_13.getUTF8String(smj_index_0);
/* 256 */ }
/* 257 */
/* 258 */ }
/* 259 */ smj_globalIsNull_0 = smj_isNull_12;
/* 260 */ return smj_value_12;
/* 261 */ }
/* 262 */
/* 263 */ protected void processNext() throws java.io.IOException {
/* 264 */ while (smj_findNextOuterJoinRows_0(smj_leftInput_0, smj_rightInput_0)) {
/* 265 */ boolean smj_loaded_0 = false;
/* 266 */ smj_isNull_25 = smj_leftRow_0.isNullAt(0);
/* 267 */ smj_mutableStateArray_0[0] = smj_isNull_25 ? null : (smj_leftRow_0.getUTF8String(0));
/* 268 */ smj_isNull_26 = smj_leftRow_0.isNullAt(1);
/* 269 */ smj_mutableStateArray_0[1] = smj_isNull_26 ? null : (smj_leftRow_0.getUTF8String(1));
/* 270 */ smj_isNull_27 = smj_leftRow_0.isNullAt(2);
/* 271 */ smj_mutableStateArray_0[2] = smj_isNull_27 ? null : (smj_leftRow_0.getUTF8String(2));
/* 272 */ smj_isNull_28 = smj_leftRow_0.isNullAt(3);
/* 273 */ smj_mutableStateArray_0[3] = smj_isNull_28 ? null : (smj_leftRow_0.getUTF8String(3));
/* 274 */ scala.collection.Iterator<UnsafeRow> smj_iterator_0 = smj_matches_0.generateIterator();
/* 275 */ while (smj_iterator_0.hasNext()) {
/* 276 */ InternalRow smj_rightRow_1 = (InternalRow) smj_iterator_0.next();
/* 277 */ smj_isNull_29 = smj_rightRow_1.isNullAt(0);
/* 278 */ smj_mutableStateArray_0[4] = smj_isNull_29 ? null : (smj_rightRow_1.getUTF8String(0));
/* 279 */ smj_isNull_30 = false;
/* 280 */ smj_mutableStateArray_1[0] = smj_rightRow_1.getDecimal(1, 38, 0);
/* 281 */
/* 282 */ smj_loaded_0 = true;
/* 283 */ smj_writeJoinRows_0();
/* 284 */ }
/* 285 */ if (!smj_loaded_0) {
/* 286 */ smj_isNull_29 = true;
/* 287 */ smj_isNull_30 = true;
/* 288 */ smj_writeJoinRows_0();
/* 289 */ }
/* 290 */ if (shouldStop()) return;
/* 291 */ }
/* 292 */ ((org.apache.spark.sql.execution.joins.SortMergeJoinExec) references[5] /* plan */).cleanupResources();
/* 293 */ }
/* 294 */
/* 295 */ private void smj_writeJoinRows_0() throws java.io.IOException {
/* 296 */ ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* numOutputRows */).add(1);
/* 297 */
/* 298 */ // common sub-expressions
/* 299 */
/* 300 */ UTF8String project_subExprValue_0 = project_subExpr_0(smj_mutableStateArray_0[3], smj_isNull_28);
/* 301 */
/* 302 */ smj_mutableStateArray_2[1].reset();
/* 303 */
/* 304 */ smj_mutableStateArray_2[1].zeroOutNullBytes();
/* 305 */
/* 306 */ if (smj_isNull_25) {
/* 307 */ smj_mutableStateArray_2[1].setNullAt(0);
/* 308 */ } else {
/* 309 */ smj_mutableStateArray_2[1].write(0, smj_mutableStateArray_0[0]);
/* 310 */ }
/* 311 */
/* 312 */ if (smj_isNull_26) {
/* 313 */ smj_mutableStateArray_2[1].setNullAt(1);
/* 314 */ } else {
/* 315 */ smj_mutableStateArray_2[1].write(1, smj_mutableStateArray_0[1]);
/* 316 */ }
/* 317 */
/* 318 */ if (project_subExprIsNull_0) {
/* 319 */ smj_mutableStateArray_2[1].setNullAt(2);
/* 320 */ } else {
/* 321 */ smj_mutableStateArray_2[1].write(2, project_subExprValue_0);
/* 322 */ }
/* 323 */
/* 324 */ if (project_subExprIsNull_0) {
/* 325 */ smj_mutableStateArray_2[1].setNullAt(3);
/* 326 */ } else {
/* 327 */ smj_mutableStateArray_2[1].write(3, project_subExprValue_0);
/* 328 */ }
/* 329 */
/* 330 */ if (smj_isNull_29) {
/* 331 */ smj_mutableStateArray_2[1].setNullAt(4);
/* 332 */ } else {
/* 333 */ smj_mutableStateArray_2[1].write(4, smj_mutableStateArray_0[4]);
/* 334 */ }
/* 335 */
/* 336 */ if (smj_isNull_25) {
/* 337 */ smj_mutableStateArray_2[1].setNullAt(5);
/* 338 */ } else {
/* 339 */ smj_mutableStateArray_2[1].write(5, smj_mutableStateArray_0[0]);
/* 340 */ }
/* 341 */ append((smj_mutableStateArray_2[1].getRow()).copy());
/* 342 */
/* 343 */ }
/* 344 */
/* 345 */ } {code}
 ;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: [k8s] Though finishing a job, the driver pod is running infinitely
Issue key: SPARK-35304
Issue id: 13376487
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ocworld
Creator: ocworld
Created: 04/May/21 06:44
Updated: 08/Aug/22 15:51
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.0.2, 3.1.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: Though finishing a job, the driver pod is running infinitely.

Executors are terminated. However, the driver status is not changed to succeeded.

It is not experienced in spark 2 on k8s.

It is only appeared on spark 3.

 

my jvm dump is that
{code:java}
2021-05-04 15:11:37
Full thread dump OpenJDK 64-Bit Server VM (25.252-b09 mixed mode):

"Attach Listener" #182 daemon prio=9 os_prio=0 tid=0x00007f02bc001000 nid=0x106 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"DestroyJavaVM" #179 prio=5 os_prio=0 tid=0x00007f0fe0017000 nid=0x35 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"s3a-transfer-unbounded-pool2-t1" #172 daemon prio=5 os_prio=0 tid=0x00007f025d98d000 nid=0xe5 waiting on condition [0x00007f01f86f3000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f0353681b38> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

"java-sdk-progress-listener-callback-thread" #169 daemon prio=5 os_prio=0 tid=0x00007f002000f000 nid=0xe2 waiting on condition [0x00007f004f7f6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f0bdb1ba7c0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

"pool-26-thread-1" #72 prio=5 os_prio=0 tid=0x00007f025c829000 nid=0x80 waiting on condition [0x00007f01ba931000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f0bfdeaa8f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

"java-sdk-http-connection-reaper" #56 daemon prio=5 os_prio=0 tid=0x00007f025d818000 nid=0x6e waiting on condition [0x00007f01fb9fe000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at com.amazonaws.http.IdleConnectionReaper.run(IdleConnectionReaper.java:188)

   Locked ownable synchronizers:
	- None

"Timer for 's3a-file-system' metrics system" #55 daemon prio=5 os_prio=0 tid=0x00007f0fe19e6800 nid=0x6d in Object.wait() [0x00007f029c1d8000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked <0x00007f0353383bd0> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

   Locked ownable synchronizers:
	- None

"MutableQuantiles-0" #54 daemon prio=5 os_prio=0 tid=0x00007f025d78b800 nid=0x6c runnable [0x00007f029c2d9000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00007f0351a09dd8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093)
	at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1074)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1134)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

"org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner" #13 daemon prio=5 os_prio=0 tid=0x00007f0fe1182000 nid=0x45 in Object.wait() [0x00007f02c50d7000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007f0350ea4390> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
	- locked <0x00007f0350ea4390> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
	at org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:3839)
	at java.lang.Thread.run(Thread.java:748)

   Locked ownable synchronizers:
	- None

"Service Thread" #7 daemon prio=9 os_prio=0 tid=0x00007f0fe00e2000 nid=0x3f runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C1 CompilerThread1" #6 daemon prio=9 os_prio=0 tid=0x00007f0fe00c7800 nid=0x3e waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"C2 CompilerThread0" #5 daemon prio=9 os_prio=0 tid=0x00007f0fe00c4800 nid=0x3d waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Signal Dispatcher" #4 daemon prio=9 os_prio=0 tid=0x00007f0fe00c2800 nid=0x3c runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

   Locked ownable synchronizers:
	- None

"Finalizer" #3 daemon prio=8 os_prio=0 tid=0x00007f0fe0090000 nid=0x3b in Object.wait() [0x00007f033fffe000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:144)
	- locked <0x00007f03504176d8> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:165)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:216)

   Locked ownable synchronizers:
	- None

"Reference Handler" #2 daemon prio=10 os_prio=0 tid=0x00007f0fe008b800 nid=0x3a in Object.wait() [0x00007f034416a000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference.tryHandlePending(Reference.java:191)
	- locked <0x00007f0350424c20> (a java.lang.ref.Reference$Lock)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153)

   Locked ownable synchronizers:
	- None

"VM Thread" os_prio=0 tid=0x00007f0fe0082000 nid=0x39 runnable 

"GC task thread#0 (ParallelGC)" os_prio=0 tid=0x00007f0fe002c000 nid=0x36 runnable 

"GC task thread#1 (ParallelGC)" os_prio=0 tid=0x00007f0fe002d800 nid=0x37 runnable 

"GC task thread#2 (ParallelGC)" os_prio=0 tid=0x00007f0fe002f800 nid=0x38 runnable 

"VM Periodic Task Thread" os_prio=0 tid=0x00007f0fe00ec800 nid=0x40 waiting on condition 

JNI global references: 6244
 {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-34645
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 08 15:51:52 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qp3k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Aug/22 15:51;Emilie;Hi [~ocworld] do you have any updates for this issue?;;;
Affects Version/s.1: 3.0.2
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Spark-SQL test fails on IBM Z for certain config combinations.
Issue key: SPARK-35520
Issue id: 13380385
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: simrit
Creator: simrit
Created: 25/May/21 16:24
Updated: 04/Aug/22 13:02
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core, SQL
Due Date: 
Votes: 0
Labels: 
Description: Some queries of SQL related test cases: in-joins.sql, in-order-by.sql, not-in-group-by.sql and SubquerySuite.scala are failing with specific configuration combinations on IBM Z(s390x).

For example: 

sql("select * from l where a = 6 and a not in (select c from r where c is not null)") query from SubquerySuite.scala fails for following config combinations:
|enableNAAJ|enableAQE|enableCodegen|
|TRUE|FALSE|FALSE|
|TRUE|TRUE|FALSE|

The above combination is also causing 2 other queries in in-joins.sql and in-order-by.sql failing.

Another query: 

SELECT Count(*)
 FROM (SELECT *
 FROM t2
 WHERE t2a NOT IN (SELECT t3a
 FROM t3
 WHERE t3h != t2h)) t2
 WHERE t2b NOT IN (SELECT Min(t2b)
 FROM t2
 WHERE t2b = t2b
 GROUP BY t2c);

from not-in-group-by.sql is failing for following combinations:
|enableAQE|enableCodegen|
|FALSE|TRUE|
|FALSE|FALSE|

 

These Test cases are not failing for 3.0.1 release and I believe might have been introduced with [SPARK-32290|https://issues.apache.org/jira/browse/SPARK-32290] . 

There is another strange behaviour observed, if expected output is 1,3 , I am getting 1, 3, 9. If I update the Golden file to expect 1, 3, 9, the output will be 1, 3.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 04 13:02:41 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rd4g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Oct/21 21:06;kunlu;I've also observed this issue on Spark v3.1.2 on IBM Z. Any comments from the community would be greatly appreciated.;;;, 04/Aug/22 13:02;vivkong;The test is still failing on Spark v3.3.0 on IBM Z.  We appreciate any pointers community can share so we can look into it.  Thanks.;;;
Affects Version/s.1: 
Comment.1: 04/Aug/22 13:02;vivkong;The test is still failing on Spark v3.3.0 on IBM Z.  We appreciate any pointers community can share so we can look into it.  Thanks.;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: An error occurred while concurrently writing to different static partitions
Issue key: SPARK-37210
Issue id: 13410167
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wforget
Creator: wforget
Created: 05/Nov/21 03:35
Updated: 30/Jul/22 06:28
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: An error occurred while concurrently writing to different static partitions.

For writing to a static partition, committerOutputPath is the location path of the table. When multiple tasks write to the same table concurrently, the _temporary path will be deleted after one task ends, causing another task to fail.

 

test code:

 
{code:java}
// code placeholder
object HiveTests {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .master("local[*]")
      .appName("HiveTests")
      .enableHiveSupport()
      .getOrCreate()

    //rows
    val users1 = new util.ArrayList[Row]()
    users1.add(Row(1, "user1", "2021-11-03", 10))
    users1.add(Row(2, "user2", "2021-11-03", 10))
    users1.add(Row(3, "user3", "2021-11-03", 10))

    //schema
    val structType = StructType(Array(
      StructField("id", IntegerType, true),
      StructField("name", StringType, true),
      StructField("dt", StringType, true),
      StructField("hour", IntegerType, true)
    ))

    spark.sql("set hive.exec.dynamic.partition=true")
    spark.sql("set hive.exec.dynamic.partition.mode=nonstrict")

    spark.sql("drop table if exists default.test")

    spark.sql(
      """
        |create table if not exists default.test (
        |  id int,
        |  name string)
        |partitioned by (dt string, hour int)
        |stored as parquet
        |""".stripMargin)

    spark.sql("desc formatted default.test").show()

    spark.sqlContext
      .createDataFrame(users1, structType)
      .select("id", "name")
      .createOrReplaceTempView("user1")

    val thread1 = new Thread(() => {
      // spark.sql("INSERT INTO TABLE test PARTITION(dt = '2021-11-03', hour=10) select * from user1")
      spark.sql("INSERT OVERWRITE TABLE test PARTITION(dt = '2021-11-03', hour=10) select * from user1")
    })
    thread1.start()

    val thread2 = new Thread(() => {
      // spark.sql("INSERT INTO TABLE test PARTITION(dt = '2021-11-04', hour=10) select * from user1")
      spark.sql("INSERT OVERWRITE TABLE test PARTITION(dt = '2021-11-04', hour=10) select * from user1")
    })
    thread2.start()

    thread1.join()
    thread2.join()

    spark.sql("select * from test").show()

    spark.stop()

  }

} {code}
 

error message:

 
{code:java}
// code placeholder

21/11/04 19:01:21 ERROR Utils: Aborting task
ExitCodeException exitCode=1: chmod: cannot access '/data/spark-examples/spark-warehouse/test/_temporary/0/_temporary/attempt_202111041901182933014038999149736_0001_m_000001_
4/dt=2021-11-03/hour=10/.part-00001-95895b03-45d2-4ac6-806b-b76fd1dfa3dc.c000.snappy.parquet.crc': No such file or directory        at org.apache.hadoop.util.Shell.runCommand(Shell.java:1008)
        at org.apache.hadoop.util.Shell.run(Shell.java:901)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1213)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:1307)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:1289)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)
        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)
        at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)
        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)
        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521)
        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)
        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)
        at org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)
        at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)
        at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)
        at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)
        at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)
        at org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter.renewCurrentWriter(FileFormatDataWriter.scala:290)
        at org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter.write(FileFormatDataWriter.scala:357)
        at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)
        at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:304)
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:311)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:131)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
21/11/04 19:01:21 WARN FileOutputCommitter: Could not delete file:/data/spark-examples/spark-warehouse/test/_temporary/0/_temporary/attempt_202111041901182933014038999149736_
0001_m_000001_4
{code}
 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 05/Nov/21 07:24;wforget;[SPARK-37210]_Write_to_static_partition_in_dynamic_write_mode.patch;https://issues.apache.org/jira/secure/attachment/13035718/%5BSPARK-37210%5D_Write_to_static_partition_in_dynamic_write_mode.patch, 05/Nov/21 07:28;wforget;image-2021-11-05-15-28-41-393.png;https://issues.apache.org/jira/secure/attachment/13035719/image-2021-11-05-15-28-41-393.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Jul 30 06:28:59 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wgoo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Nov/21 04:40;wforget;The test code can be executed normally in spark 2.4.3. I noticed that spark 2.4.3 uses InsertIntoHiveTable, and spark 3.1.1 uses InsertIntoHadoopFsRelationCommand, is this a problem?;;;, 05/Nov/21 07:27;wforget;In DataSourceAnalysis, static partition writing is converted to dynamic partition writing. When using static partition writing, dynamicPartitionOverwrite should also be set to true.

!image-2021-11-05-15-28-41-393.png!;;;, 05/Nov/21 08:34;apachespark;User 'wForget' has created a pull request for this issue:
https://github.com/apache/spark/pull/34489;;;, 05/Nov/21 21:32;dongjoon;Is this a correct use case?
bq. When multiple tasks write to the same table concurrently, the _temporary path will be deleted after one task ends, causing another task to fail.;;;, 07/Nov/21 09:16;wforget;Yes [~dongjoon] , for parquet table, this error will occur when two tasks concurrently insert different static partitions of the same table. Do I need to add test cases in HiveParquetSuite?;;;, 01/Dec/21 07:45;wforget;There seem to be two bugs:
 # For the insert overwrite fully static partition, when the insert data size is 0, the existing partition data will be deleted.

 # For concurrent writes to fully static partitions, exceptions may occur in {{insert into}} or {{{}insert overwrite{}}}. Due to temporary path '${tableLocation}/_temporary' will be deleted in {{{}FileOutputCommitter.cleanupJob{}}}.;;;, 01/Dec/21 07:51;wforget;I made some adjustments to the test cases to make it more likely to conflict and lead to exception. For details:  [https://github.com/apache/spark/pull/34489].
 # Increase the size of the inserted data.
 # Different tasks write different data sizes, so that the task execution duration is different.
 # Modify the {{spark.sql.test.master}} configuration to execute tasks in parallel.;;;, 28/Jul/22 12:02;wforget;The concurrent writes in INSERT INTO mode seems to have the same problem, which is caused by not using the staging dir, can we add a configuration that allows the staging dir to always be used?;;;, 28/Jul/22 14:25;dongjoon;I'm not sure if the concurrent static INSERT INTO execution is valid use case in Spark layer.

Could you try to use Apache Iceberg instead?;;;, 28/Jul/22 14:27;dongjoon;BTW, could you please reopen the PR freshly? I'll try to ping the other committers too.;;;, 29/Jul/22 05:51;wforget;{quote}I'm not sure if the concurrent static INSERT INTO execution is valid use case in Spark layer.
{quote}
We found some data loss due to this issue after migrating hive sql to spark, I still hope Spark can support it if possible.

 
{quote}Could you try to use Apache Iceberg instead?
{quote}
Thanks, this seems like a good option, I will try it too.

 
{quote}BTW, could you please reopen the PR freshly? I'll try to ping the other committers too.
{quote}
I'm trying to implement to allow always use of staging dir, I'll send a PR after adding test cases.;;;, 30/Jul/22 06:28;apachespark;User 'wForget' has created a pull request for this issue:
https://github.com/apache/spark/pull/37346;;;
Affects Version/s.1: 3.2.0
Comment.1: 05/Nov/21 07:27;wforget;In DataSourceAnalysis, static partition writing is converted to dynamic partition writing. When using static partition writing, dynamicPartitionOverwrite should also be set to true.

!image-2021-11-05-15-28-41-393.png!;;;, 29/Jul/22 05:51;wforget;{quote}I'm not sure if the concurrent static INSERT INTO execution is valid use case in Spark layer.
{quote}
We found some data loss due to this issue after migrating hive sql to spark, I still hope Spark can support it if possible.

 
{quote}Could you try to use Apache Iceberg instead?
{quote}
Thanks, this seems like a good option, I will try it too.

 
{quote}BTW, could you please reopen the PR freshly? I'll try to ping the other committers too.
{quote}
I'm trying to implement to allow always use of staging dir, I'll send a PR after adding test cases.;;;, 30/Jul/22 06:28;apachespark;User 'wForget' has created a pull request for this issue:
https://github.com/apache/spark/pull/37346;;;
Comment.2: 05/Nov/21 08:34;apachespark;User 'wForget' has created a pull request for this issue:
https://github.com/apache/spark/pull/34489;;;
Comment.3: 05/Nov/21 21:32;dongjoon;Is this a correct use case?
bq. When multiple tasks write to the same table concurrently, the _temporary path will be deleted after one task ends, causing another task to fail.;;;
Comment.4: 07/Nov/21 09:16;wforget;Yes [~dongjoon] , for parquet table, this error will occur when two tasks concurrently insert different static partitions of the same table. Do I need to add test cases in HiveParquetSuite?;;;
Comment.5: 01/Dec/21 07:45;wforget;There seem to be two bugs:
 # For the insert overwrite fully static partition, when the insert data size is 0, the existing partition data will be deleted.

 # For concurrent writes to fully static partitions, exceptions may occur in {{insert into}} or {{{}insert overwrite{}}}. Due to temporary path '${tableLocation}/_temporary' will be deleted in {{{}FileOutputCommitter.cleanupJob{}}}.;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0

Summary: setPredictionCol for OneVsRest does not persist when saving model to disk
Issue key: SPARK-39544
Issue id: 13454564
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kobakhit
Creator: kobakhit
Created: 21/Jun/22 12:32
Updated: 21/Jun/22 12:38
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: ML
Due Date: 
Votes: 0
Labels: 
Description: The naming of rawPredcitionCol in OneVsRest does not persist after saving and loading a trained model. This becomes an issue when I try to stack multiple One Vs Rest models in a pipeline. Code example below. 
{code:java}
from pyspark.ml.classification import LinearSVC, OneVsRest, OneVsRestModel

data_path = "/sample_multiclass_classification_data.txt"
df = spark.read.format("libsvm").load(data_path)
lr = LinearSVC(regParam=0.01)

# set the name of rawPrediction column
ovr = OneVsRest(classifier=lr, rawPredictionCol = 'raw_prediction')
print(ovr.getRawPredictionCol())

model = ovr.fit(df)model_path = 'temp' + "/ovr_model"

# save and read back in
model.write().overwrite().save(model_path)
model2 = OneVsRestModel.load(model_path)
model2.getRawPredictionCol()

Output:
raw_prediction
'rawPrediction' {code}
 

 
Environment: Python 3.6

Spark 3.2
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-06-21 12:32:11.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z13zfs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.0.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Spark Streaming 3.1.1 hangs on shutdown
Issue key: SPARK-33121
Issue id: 13335027
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tverdokhlebd
Creator: tverdokhlebd
Created: 12/Oct/20 14:33
Updated: 02/Jun/22 11:52
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: DStreams
Due Date: 
Votes: 0
Labels: hang, shutdown, Streaming
Description: Hi. I am trying to migrate from spark 2.4.5 to 3.1.1 and there is a problem in graceful shutdown.

Config parameter "spark.streaming.stopGracefullyOnShutdown" is set as "true".

Here is the code:
{code:java}
inputStream.foreachRDD {
  rdd =>
    rdd.foreachPartition {
        Thread.sleep(5000)
    }
}
{code}
I send a SIGTERM signal to stop the spark streaming and after sleeping an exception arises:
{noformat}
streaming-agg-tds-data_1  | java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@7ca7f0b8 rejected from java.util.concurrent.ThreadPoolExecutor@2474219c[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1]
streaming-agg-tds-data_1  |     at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
streaming-agg-tds-data_1  |     at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
streaming-agg-tds-data_1  |     at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
streaming-agg-tds-data_1  |     at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
streaming-agg-tds-data_1  |     at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)
streaming-agg-tds-data_1  |     at org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)
streaming-agg-tds-data_1  |     at scala.collection.Iterator.foreach(Iterator.scala:941)
streaming-agg-tds-data_1  |     at scala.collection.Iterator.foreach$(Iterator.scala:941)
streaming-agg-tds-data_1  |     at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
streaming-agg-tds-data_1  |     at scala.collection.IterableLike.foreach(IterableLike.scala:74)
streaming-agg-tds-data_1  |     at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
streaming-agg-tds-data_1  |     at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
streaming-agg-tds-data_1  |     at org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)
streaming-agg-tds-data_1  |     at org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:68)
streaming-agg-tds-data_1  |     at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
streaming-agg-tds-data_1  |     at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
streaming-agg-tds-data_1  |     at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
streaming-agg-tds-data_1  |     at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
streaming-agg-tds-data_1  |     at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
streaming-agg-tds-data_1  |     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
streaming-agg-tds-data_1  |     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
streaming-agg-tds-data_1  |     at java.lang.Thread.run(Thread.java:748)
streaming-agg-tds-data_1  | 2021-04-22 13:33:41 WARN  JobGenerator - Timed out while stopping the job generator (timeout = 10000)
streaming-agg-tds-data_1  | 2021-04-22 13:33:41 INFO  JobGenerator - Waited for jobs to be processed and checkpoints to be written
streaming-agg-tds-data_1  | 2021-04-22 13:33:41 INFO  JobGenerator - Stopped JobGenerator{noformat}
After this exception and "JobGenerator - Stopped JobGenerator" log, streaming freezes, and halts by timeout (Config parameter "hadoop.service.shutdown.timeout").

Besides, there is no problem with the graceful shutdown in spark 2.4.5.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 02 11:52:47 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0jmrk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Apr/21 13:46;tverdokhlebd;Tested on Spark Streaming 3.1.1 with a simple example.;;;, 23/May/21 21:13;dongjoon;cc [~viirya];;;, 23/May/21 21:18;dongjoon;Hi, [~tverdokhlebd]. Did you have a chance to try to use Apache Spark 3.0.2?;;;, 24/May/21 21:24;viirya;Hmm, I cannot reproduce in branch-3.1/2.4 or master branch.

If you don't have Thread.sleep(5000), does it stop gracefully?;;;, 25/May/21 11:56;tverdokhlebd;L. C. Hsieh, have you tested this case with sending SIGTERM signal when "for each" operation entered in sleeping mode?;;;, 02/Jun/22 11:52;stephenk;I seem to have encountered this problem without streaming in version 3.2.1. My large session has many complex stages and writes unique data. I've enabled re-runs, and so do an anti-join on already written data in the (jdbc) database.

At this stage, it seems to be applying the anti-join right at the end after all the complex computations and reduces millions of rows to 0. Because I have many executors, I coalesce down to 30 partitions for writing to jdbc.

I'm getting hundreds of {{RejectedExecutionExceptions}} - it seems to me that the coalescing starts, and simultaneously it determines 0 rows and finishes the write and exits, resulting in non-graceful shutdown.

Calling {{sc.stop()}} does nothing, but {{df.cache()}} before coalescing and writing does.

Should this be reported as a separate ticket? I asked on [gitter|https://gitter.im/spark-scala/Lobby?at=6298a15306a77e1e18684826] too, and thought this actually did seem similar enough to comment.;;;
Affects Version/s.1: 
Comment.1: 23/May/21 21:13;dongjoon;cc [~viirya];;;
Comment.2: 23/May/21 21:18;dongjoon;Hi, [~tverdokhlebd]. Did you have a chance to try to use Apache Spark 3.0.2?;;;
Comment.3: 24/May/21 21:24;viirya;Hmm, I cannot reproduce in branch-3.1/2.4 or master branch.

If you don't have Thread.sleep(5000), does it stop gracefully?;;;
Comment.4: 25/May/21 11:56;tverdokhlebd;L. C. Hsieh, have you tested this case with sending SIGTERM signal when "for each" operation entered in sleeping mode?;;;
Comment.5: 02/Jun/22 11:52;stephenk;I seem to have encountered this problem without streaming in version 3.2.1. My large session has many complex stages and writes unique data. I've enabled re-runs, and so do an anti-join on already written data in the (jdbc) database.

At this stage, it seems to be applying the anti-join right at the end after all the complex computations and reduces millions of rows to 0. Because I have many executors, I coalesce down to 30 partitions for writing to jdbc.

I'm getting hundreds of {{RejectedExecutionExceptions}} - it seems to me that the coalescing starts, and simultaneously it determines 0 rows and finishes the write and exits, resulting in non-graceful shutdown.

Calling {{sc.stop()}} does nothing, but {{df.cache()}} before coalescing and writing does.

Should this be reported as a separate ticket? I asked on [gitter|https://gitter.im/spark-scala/Lobby?at=6298a15306a77e1e18684826] too, and thought this actually did seem similar enough to comment.;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: TaskSchedulerImpl should quickly ignore task finished event if its task was  finished state
Issue key: SPARK-39287
Issue id: 13446761
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 25/May/22 10:16
Updated: 25/May/22 11:46
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed May 25 11:46:21 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12o6w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/May/22 11:45;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36665;;;, 25/May/22 11:46;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36665;;;
Affects Version/s.1: 3.1.1
Comment.1: 25/May/22 11:46;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36665;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, Unknown

Summary: Jdbc driver class not found  when enable hive and call  jdbc action function multiple times
Issue key: SPARK-38815
Issue id: 13438337
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zery
Creator: zery
Created: 07/Apr/22 09:29
Updated: 07/Apr/22 09:36
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Kubernetes, Spark Core, SQL
Due Date: 
Votes: 0
Labels: 
Description: Hello, the spark code is:
{code:java}
// Some comments here
def main(args : Array[String]): Unit = {
    val spark = SparkSession.builder
      .appName("TestCKJdbc")
      .enableHiveSupport()
      .getOrCreate()

    spark.read
      .format("jdbc")
      .option("driver","ru.yandex.clickhouse.ClickHouseDriver")
      .option("url", "jdbc:clickhouse://clickhouse-server-svc.admin.svc.cluster.local:8123/aaa")
      .option("dbtable", "A")
      .option("user", "default")
      .option("password", "abc")
      .load()
      .show()

    spark.read
      .format("jdbc")
      .option("driver","ru.yandex.clickhouse.ClickHouseDriver")
      .option("url", "jdbc:clickhouse://clickhouse-server-svc.admin.svc.cluster.local:8123/aaa")
      .option("dbtable", "A")
      .option("user", "default")
      .option("password", "abc")
      .load()
      .show()

    spark.stop()
  }
{code}
when I submit it to k8s, it will get error:
{code:java}
22/04/07 09:12:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.233.67.129, executor 1, partition 0, PROCESS_LOCAL, 4318 bytes) taskResourceAssignments Map()
22/04/07 09:12:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.233.67.129:41816 (size: 5.1 KiB, free: 413.9 MiB)
22/04/07 09:12:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6596 ms on 10.233.67.129 (executor 1) (1/1)
22/04/07 09:12:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
22/04/07 09:12:15 INFO DAGScheduler: ResultStage 0 (show at App.scala:24) finished in 8.524 s
22/04/07 09:12:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22/04/07 09:12:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
22/04/07 09:12:15 INFO DAGScheduler: Job 0 finished: show at App.scala:24, took 8.611591 s
22/04/07 09:12:15 INFO CodeGenerator: Code generated in 47.462472 ms
+---+---------+---------+
| id|discrete1|discrete2|
+---+---------+---------+
|  1|        A|        a|
|  1|        B|        b|
|  1|        A|        c|
|  2|        C|        a|
|  1|        A|        a|
+---+---------+---------+22/04/07 09:12:15 INFO SparkUI: Stopped Spark web UI at http://spark-7f8c3c80034ac34e-driver-svc.spark-operator.svc:4040
22/04/07 09:12:15 INFO KubernetesClusterSchedulerBackend: Shutting down all executors
22/04/07 09:12:15 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down
22/04/07 09:12:15 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)
22/04/07 09:12:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
22/04/07 09:12:15 INFO MemoryStore: MemoryStore cleared
22/04/07 09:12:15 INFO BlockManager: BlockManager stopped
22/04/07 09:12:15 INFO BlockManagerMaster: BlockManagerMaster stopped
22/04/07 09:12:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
22/04/07 09:12:15 INFO SparkContext: Successfully stopped SparkContext
Exception in thread "main" java.lang.ClassNotFoundException: ru.yandex.clickhouse.ClickHouseDriver
    at java.base/java.net.URLClassLoader.findClass(Unknown Source)
    at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
    at java.base/java.lang.ClassLoader.loadClass(Unknown Source)
    at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:102)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:102)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:102)
    at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:38)
    at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)
    at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:354)
    at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:326)
    at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:308)
    at scala.Option.getOrElse(Option.scala:189)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:308)
    at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:226)
    at org.example.App$.main(App.scala:34)
    at org.example.App.main(App.scala)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.base/java.lang.reflect.Method.invoke(Unknown Source)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/04/07 09:12:15 INFO ShutdownHookManager: Shutdown hook called
22/04/07 09:12:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-77261146-995b-42e6-b5e8-eb7cbf8eda49
22/04/07 09:12:15 INFO ShutdownHookManager: Deleting directory /var/data/spark-5afa86cb-67c6-493a-90f5-ea1c23089c9f/spark-d1c96f2e-c40c-4942-bd99-0451d23159b3
22/04/07 09:12:15 INFO AuditProviderFactory: ==> JVMShutdownHook.run()
22/04/07 09:12:15 INFO AuditProviderFactory: JVMShutdownHook: Signalling async audit cleanup to start.
22/04/07 09:12:15 INFO AuditProviderFactory: JVMShutdownHook: Waiting up to 30 seconds for audit cleanup to finish.
22/04/07 09:12:15 INFO AuditProviderFactory: RangerAsyncAuditCleanup: Starting cleanup
22/04/07 09:12:15 INFO AuditAsyncQueue: Stop called. name=hiveCLI.async
22/04/07 09:12:15 INFO AuditAsyncQueue: Interrupting consumerThread. name=hiveCLI.async, consumer=hiveCLI.async.batch
22/04/07 09:12:15 INFO AuditProviderFactory: RangerAsyncAuditCleanup: Done cleanup {code}
**but, if del ".enableHiveSupport()" code, it will run success.*
like:
{code:java}
def main(args : Array[String]): Unit = {
    val spark = SparkSession.builder
      .appName("TestCKJdbc")
      //.enableHiveSupport()
      .getOrCreate()

    spark.read
      .format("jdbc")
      .option("driver","ru.yandex.clickhouse.ClickHouseDriver")
      .option("url", "jdbc:clickhouse://clickhouse-server-svc.admin.svc.cluster.local:8123/abc")
      .option("dbtable", "A")
      .option("user", "default")
      .option("password", "aaa")
      .load()
      .show()

    spark.read
      .format("jdbc")
      .option("driver","ru.yandex.clickhouse.ClickHouseDriver")
      .option("url", "jdbc:clickhouse://clickhouse-server-svc.admin.svc.cluster.local:8123/abc")
      .option("dbtable", "A")
      .option("user", "default")
      .option("password", "aaa")
      .load()
      .show()

    spark.stop()
  }
{code}

It will happen only when enableHive and call multiple jdbc action function.
Environment: k8s: v1.20.4
spark: v3.1.1
hive: 4.0.0-SNAPSHOT
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-07 09:29:19.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z118uo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: When ConfigMap creation fails, Spark driver starts but fails to start executors
Issue key: SPARK-38794
Issue id: 13438014
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: seth.horrigan
Creator: seth.horrigan
Created: 05/Apr/22 22:34
Updated: 05/Apr/22 22:41
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: When running Spark in Kubernetes client mode, all executors assume that a ConfigMap exactly matching `KubernetesClientUtils.configMapNameExecutor` will exist (see [https://github.com/apache/spark/blob/02a055a42de5597cd42c1c0d4470f0e769571dc3/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala#L98])

If the ConfigMap creation fails, [https://github.com/apache/spark/blob/02a055a42de5597cd42c1c0d4470f0e769571dc3/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala#L80], (due to the Kubernetes control plane being temporarily unavailable or the permissions of the serviceaccount being insufficient to create a ConfigMap), the driver will start fully, then will wait for executors that will forever fail to start due to "MountVolume.SetUp failed for volume \"spark-conf-volume-exec\" : configmap \"spark-exec-...-conf-map\" not found" 

 

Either the driver start-up should fail with an error, or the driver should retry the attempt to create the ConfigMap

--

To reproduce the problem when the Kubernetes control plane is not experiencing issues, start Spark in client mode, but do not give the Kubernetes ServiceAccount permission to create ConfigMap. The driver pod will start successfully, but the executor pods will terminate upon creation, and the driver will not create new executors.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-38079
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-05 22:34:39.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z116vk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0

Summary: spark thrift server issue: Length field is empty for varchar fields
Issue key: SPARK-38764
Issue id: 13437305
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ayanray089
Creator: ayanray089
Created: 01/Apr/22 17:39
Updated: 03/Apr/22 01:05
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I am trying to read Data from Spark Thrift Server using SAS. In the table definition through DBeaver, I am seeing that *Length* field is empty only for fields with *VARCHAR* data type. I can see the length in the Data Type field as {*}varchar(32){*}. But that doesn't suffice my purpose as the SAS application taps into the Length field. Since, this field is not populated now, SAS is defaulting to the max size and as a result its becoming extremely slow. I get the length field populated in Hive.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-01 17:39:05.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z112kw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: support load json file in case-insensitive way
Issue key: SPARK-38599
Issue id: 13434542
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: makeboluo
Creator: makeboluo
Created: 18/Mar/22 09:47
Updated: 18/Mar/22 11:00
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Input/Output, SQL
Due Date: 
Votes: 0
Labels: 
Description: The task is to load json files into dataFrame.

 

Currently we use this method:

// textfile is rdd[string], read from json files

val table = spark.table(hiveTableName)
val hiveSchema = table.schema
var df = spark.read.option("mode", "DROPMALFORMED").schema(hiveSchema).json(textfile)

 

The problem is that the field in hiveSchema is all in lower-case,  however the field of json string have upper case. 

For example:

hive schema:

(id  bigint,  name string)

 

json string

{"Id":123, "Name":"Tom"}

 

in this case,  the json string will not be loaded into dataFrame

I have to use the schema of hive table, due to business requirement, that's the pre-condition.

currently I have to transform the key in json string to lower case, like \{"id":123, "name":"Tom"}

 

but I was wondering if there's any better solution for this issue?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Mar 18 11:00:08 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10m00:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Mar/22 11:00;makeboluo;I'd like to contribute to this issue.;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Spark 3.1.1 is slower than 3.0.2 by 4-5 times
Issue key: SPARK-35066
Issue id: 13372217
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: maziyar
Creator: maziyar
Created: 14/Apr/21 07:12
Updated: 17/Mar/22 16:29
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: ML, SQL
Due Date: 
Votes: 0
Labels: 
Description: Hi,

The following snippet code runs 4-5 times slower when it's used in Apache Spark or PySpark 3.1.1 compare to Apache Spark or PySpark 3.0.2:

 
{code:java}
spark = SparkSession.builder \
        .master("local[*]") \
        .config("spark.driver.memory", "16G") \
        .config("spark.driver.maxResultSize", "0") \
        .config("spark.serializer",
"org.apache.spark.serializer.KryoSerializer") \
        .config("spark.kryoserializer.buffer.max", "2000m") \
        .getOrCreate()

Toys = spark.read \
  .parquet('./toys-cleaned').repartition(12)

# tokenize the text
regexTokenizer = RegexTokenizer(inputCol="reviewText",
outputCol="all_words", pattern="\\W")
toys_with_words = regexTokenizer.transform(Toys)

# remove stop words
remover = StopWordsRemover(inputCol="all_words", outputCol="words")
toys_with_tokens = remover.transform(toys_with_words).drop("all_words")

all_words = toys_with_tokens.select(explode("words").alias("word"))
# group by, sort and limit to 50k
top50k =
all_words.groupBy("word").agg(count("*").alias("total")).sort(col("total").desc()).limit(50000)

top50k.show()
{code}
 

Some debugging on my side revealed that in Spark/PySpark 3.0.2 the 12 partitions are respected in a way that all 12 tasks are being processed altogether. However, in Spark/PySpark 3.1.1 even though we have 12 tasks, 10 of them finish immediately and only 2 are being processed. (I've tried to disable a couple of configs related to something similar, but none of them worked)

Screenshot of spark 3.1.1 task:

!image-2022-03-17-17-18-36-793.png|width=1073,height=652!

!image-2022-03-17-17-19-11-655.png!

 

Screenshot of spark 3.0.2 task:

 

!image-2022-03-17-17-19-34-906.png!

For a longer discussion: [Spark User List |https://lists.apache.org/thread/1hlg9fpxnw8dzx8bd2fvffmk7yozoszf]

 

You can reproduce this big difference of performance between Spark 3.1.1 and Spark 3.0.2 by using the shared code with any dataset that is large enough to take longer than a minute. Not sure if this is related to SQL, any Spark config being enabled in 3.x but not really into action before 3.1.1, or it's about .transform in Spark ML.
Environment: Spark/PySpark: 3.1.1

Language: Python 3.7.x / Scala 12

OS: macOS, Linux, and Windows

Cloud: Databricks 7.3 for 3.0.1 and 8 for 3.1.1
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 14/Apr/21 07:16;maziyar;Screenshot 2021-04-07 at 11.15.48.png;https://issues.apache.org/jira/secure/attachment/13023828/Screenshot+2021-04-07+at+11.15.48.png, 14/Apr/21 07:16;maziyar;Screenshot 2021-04-08 at 15.08.09.png;https://issues.apache.org/jira/secure/attachment/13023829/Screenshot+2021-04-08+at+15.08.09.png, 17/Mar/22 16:18;maziyar;Screenshot 2021-04-08 at 15.13.19-1.png;https://issues.apache.org/jira/secure/attachment/13041286/Screenshot+2021-04-08+at+15.13.19-1.png, 14/Apr/21 07:16;maziyar;Screenshot 2021-04-08 at 15.13.19.png;https://issues.apache.org/jira/secure/attachment/13023830/Screenshot+2021-04-08+at+15.13.19.png, 17/Mar/22 16:18;maziyar;image-2022-03-17-17-18-36-793.png;https://issues.apache.org/jira/secure/attachment/13041285/image-2022-03-17-17-18-36-793.png, 17/Mar/22 16:19;maziyar;image-2022-03-17-17-19-11-655.png;https://issues.apache.org/jira/secure/attachment/13041287/image-2022-03-17-17-19-11-655.png, 17/Mar/22 16:19;maziyar;image-2022-03-17-17-19-34-906.png;https://issues.apache.org/jira/secure/attachment/13041288/image-2022-03-17-17-19-34-906.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 7.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-04-14 07:12:11.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0pys0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Supports customization of the entire  optimizer and planner.
Issue key: SPARK-38405
Issue id: 13431685
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dahaishuantuoba
Creator: dahaishuantuoba
Created: 03/Mar/22 11:07
Updated: 07/Mar/22 02:42
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: In some scenarios, users need to add some optimization rules after all existing optimization rules are executed. However, the current sparksession extension cannot meet the requirements.

PR:https://github.com/apache/spark/pull/35721
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Mar 07 02:42:17 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z104g0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Mar/22 02:42;apachespark;User 'chenjunbiao001' has created a pull request for this issue:
https://github.com/apache/spark/pull/35721;;;
Affects Version/s.1: 3.2.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.7.0

Summary: The Rank windows to be ordered is not necessary in a query
Issue key: SPARK-38280
Issue id: 13429829
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 22/Feb/22 05:59
Updated: 22/Feb/22 07:37
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Feb 22 07:37:00 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zt20:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Feb/22 07:37;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/35605;;;
Affects Version/s.1: 3.0.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, Unknown

Summary: Repartition+Shuffle+ non deterministic function leads to bad results
Issue key: SPARK-38137
Issue id: 13427139
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jles
Creator: jles
Created: 08/Feb/22 09:17
Updated: 08/Feb/22 09:32
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.1
Fix Version/s: 
Component/s: Shuffle, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Hi,

The results when using a non deterministic function in repartition (like rand) leads into incorrect results.

Reproduce: (correct)

 
{code:java}
// code placeholder
import scala.sys.process._
import org.apache.spark.TaskContext
import org.apache.spark.sql.functions.rand

val res = spark.range(0, 100 * 100, 1).repartition(200).map { x =>
  x
}.repartition(200).map { x =>
  if (TaskContext.get.attemptNumber == 0 && TaskContext.get.partitionId < 2) {
    throw new Exception("pkill -f java".!!)
  }
  x
}
res.distinct().count() {code}
The correct result 10000 

Reproduce: (bad)

 
{code:java}
// code placeholder
import scala.sys.process._
import org.apache.spark.TaskContext
import org.apache.spark.sql.functions.rand

val res = spark.range(0, 100 * 100, 1).repartition(200).map { x =>
  x
}.repartition(10, Array(rand):_*).map { x =>
  if (TaskContext.get.attemptNumber == 0 && TaskContext.get.partitionId < 2) {
    throw new Exception("pkill -f java".!!)
  }
  x
}
res.distinct().count() {code}
The bad result 9396 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-02-08 09:17:30.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zck8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.7.0

Summary: AvroSerializer can cause java.lang.ClassCastException at run time
Issue key: SPARK-38091
Issue id: 13426260
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Zhen-hao
Creator: Zhen-hao
Created: 02/Feb/22 19:02
Updated: 03/Feb/22 09:38
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: Avro, serializers
Description: {{{}AvroSerializer{}}}'s implementation, at least in {{{}newConverter{}}}, was not 100% based on the {{nternalRow}} and {{SpecializedGetters}} interface. It assumes many implementation details of the interface. 

For example, in 

{code}
      case (TimestampType, LONG) => avroType.getLogicalType match {
          // For backward compatibility, if the Avro type is Long and it is not logical type
          // (the `null` case), output the timestamp value as with millisecond precision.
          case null | _: TimestampMillis => (getter, ordinal) =>
            DateTimeUtils.microsToMillis(timestampRebaseFunc(getter.getLong(ordinal)))
          case _: TimestampMicros => (getter, ordinal) =>
            timestampRebaseFunc(getter.getLong(ordinal))
          case other => throw new IncompatibleSchemaException(errorPrefix +
            s"SQL type ${TimestampType.sql} cannot be converted to Avro logical type $other")
        }
{code}

it assumes the {{InternalRow}} instance encodes {{TimestampType}} as {{{}java.lang.Long{}}}. That's true for {{Unsaferow}} but not for {{{}GenericInternalRow{}}}. 

Hence the above code will end up with runtime exceptions when used on an instance of {{{}GenericInternalRow{}}}, which is the case for Python UDF. 

I didn't get time to dig deeper than that. I got the impression that Spark's optimizer(s) will turn a row into a {{UnsafeRow}} and Python UDF doesn't involve the optimizer(s) and hence each row is a {{{}GenericInternalRow{}}}. 

It would be great if someone can correct me or offer a better explanation. 

 

To reproduce the issue, 

{{git checkout master}} and {{git cherry-pick --no-commit}} [this-commit|https://github.com/Zhen-hao/spark/commit/1ffe8e8f35273b2f3529f6c2d004822f480e4c88]

and run the test {{{}org.apache.spark.sql.avro.AvroSerdeSuite{}}}.

 

You will see runtime exceptions like the following one

\\{code}

Serialize DecimalType to Avro BYTES with logical type decimal *** FAILED ***
  java.lang.ClassCastException: class java.math.BigDecimal cannot be cast to class org.apache.spark.sql.types.Decimal (java.math.BigDecimal is in module java.base of loader 'bootstrap'; org.apache.spark.sql.types.Decimal is in unnamed module of loader 'app')
  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getDecimal(rows.scala:45)
  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getDecimal$(rows.scala:45)
  at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getDecimal(rows.scala:195)
  at org.apache.spark.sql.avro.AvroSerializer.$anonfun$newConverter$10(AvroSerializer.scala:136)
  at org.apache.spark.sql.avro.AvroSerializer.$anonfun$newConverter$10$adapted(AvroSerializer.scala:135)
  at org.apache.spark.sql.avro.AvroSerializer.$anonfun$newStructConverter$2(AvroSerializer.scala:283)
  at org.apache.spark.sql.avro.AvroSerializer.serialize(AvroSerializer.scala:60)
  at org.apache.spark.sql.avro.AvroSerdeSuite.$anonfun$new$5(AvroSerdeSuite.scala:82)
  at org.apache.spark.sql.avro.AvroSerdeSuite.$anonfun$new$5$adapted(AvroSerdeSuite.scala:67)
  at org.apache.spark.sql.avro.AvroSerdeSuite.$anonfun$withFieldMatchType$2(AvroSerdeSuite.scala:217)
\\{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Feb 02 20:45:57 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z75s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Feb/22 19:04;Zhen-hao;Can someone tell me how to let Jira render markdown? ;;;, 02/Feb/22 19:08;apachespark;User 'Zhen-hao' has created a pull request for this issue:
https://github.com/apache/spark/pull/35379;;;, 02/Feb/22 20:45;xkrogen;[~Zhen-hao] for formatting you need to use the Atlassian markup: [https://jira.atlassian.com/secure/WikiRendererHelpAction.jspa?section=all]
Basically replace ` ... ` with \{{ ... }} and replace ``` ... ``` with \{code} ... \{code};;;
Affects Version/s.1: 3.0.1
Comment.1: 02/Feb/22 19:08;apachespark;User 'Zhen-hao' has created a pull request for this issue:
https://github.com/apache/spark/pull/35379;;;
Comment.2: 02/Feb/22 20:45;xkrogen;[~Zhen-hao] for formatting you need to use the Atlassian markup: [https://jira.atlassian.com/secure/WikiRendererHelpAction.jspa?section=all]
Basically replace ` ... ` with \{{ ... }} and replace ``` ... ``` with \{code} ... \{code};;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, Unknown

Summary: From Scala/Java API, higher-level functions (like exists) produce wrong expressions when nested
Issue key: SPARK-38039
Issue id: 13424960
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: LDVSoft
Creator: LDVSoft
Created: 26/Jan/22 15:43
Updated: 26/Jan/22 15:46
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Java API, SQL
Due Date: 
Votes: 0
Labels: 
Description: Consider this code in Scala:
{code:scala}
case class DemoSubRow(a: Int, b: Array[Int])
case class DemoRow(elems: Array[DemoSubRow])
dataFrame.withColumn(
    "goodElems",
    filter(elems, x -> exists(x.getField("b"), y -> x.getField("a") == y)
{code}
One could expect that is would be equivalent to
{code:sql}
SELECT *, filter(elems, x -> exists(x.b, y -> x.a == y)) AS goodElems
    FROM dataFrame
{code}
However, it's not. If you look into {{org.apache.spark.sql.functions}} object, you'll see that private method:
{code:scala}
private def createLambda(f: Column => Column) = {
  val x = UnresolvedNamedLambdaVariable(Seq("x"))
  val function = f(Column(x)).expr
  LambdaFunction(function, Seq(x))
}
{code}
If you look closely you'll see that column that is passed into the lambda is always *unresolved* variable {{{}x{}}}. Because of that, column from Scala sample above is seen as:
{code:sql}
… filter(elems, x -> exists(x.b, x -> x.a == x)) AS goodElems …
                                 ^^^^^^^^^^^^^
{code}
and is obviously wrong. In given example, it will produce {_}AnalysisException{_}, however it can also silently execute wrong code (i.e., imagine there actually is dataframe column _x_ or something).

My current workaround is a reflection hack to call {{{}functions.withExpr{}}}, but it's a really bad one.

What should probably be done is instead of hard-coded name {{x}} there should be a generated unique variable name, or even some proper locally bound resolved variables (because at the moment of lambda creation this variable can be considered already resolved), however there are concerns about how that name would be displayed to an end user if there is an analysis error. Sorry, but at the moment of reporting this issue I have no ideas how to solve that.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-01-26 15:43:30.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yz7s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Remove the unnecessary ChunkFetchFailureException class
Issue key: SPARK-37978
Issue id: 13423924
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 21/Jan/22 07:43
Updated: 21/Jan/22 08:07
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.3, 3.1.1, 3.2.0
Fix Version/s: 
Component/s: Shuffle, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: The ChunkFetchFailureException is unnecessary and can be replaced by RuntimeException.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jan 21 08:05:07 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ysug:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Jan/22 08:05;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/35267;;;
Affects Version/s.1: 3.0.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, Unknown

Summary: Reduce the output partition of output stage to avoid producing small files.
Issue key: SPARK-37674
Issue id: 13418188
Parent id: 13384290.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 17/Dec/21 08:13
Updated: 17/Dec/21 08:57
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.2, 3.0.3, 3.1.1, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The partition size of the finalStage with `DataWritingCommand` or
 `V2TableWriteExec` may use the ADVISORY_PARTITION_SIZE_IN_BYTES which is smaller one and  produce some small files, it may bad for production, we should use a new partition size for the finalStage with `DataWritingCommand` or `V2TableWriteExec` to avoid small files.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Dec 17 08:57:36 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xtio:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Dec/21 08:56;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34933;;;, 17/Dec/21 08:57;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34933;;;
Affects Version/s.1: 3.0.2
Comment.1: 17/Dec/21 08:57;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34933;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, Unknown

Summary: spark history server clean event log directory with out check status file
Issue key: SPARK-37639
Issue id: 13417294
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: m-sir
Creator: m-sir
Created: 14/Dec/21 06:54
Updated: 15/Dec/21 06:31
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: i foud a problem, the thrift server create event log file(.inprogress file create at init), and history server clean the application event log file according size and modtime. so there is a potential problem under this situation

*if the thrift server accept no quest long time(longer than time config by spark.history.fs.cleaner.maxAge), the history server will clean  the applicaiton log [directory] with the inprogress file; after clean  the thrift server accept a lot of request ,and will generate new event log directory without inprogress status file, and the director will never be clean by history server because it not contain status file. this will leads spack leak*

i think whenever create new log file , need to check wether the status file is exist, if not create it

last i think extra function need add, like log4j the compact file stii need to be clean after a period(config by user)，so ，long run spark service like thrift server‘s event log file space can be limit in a config size
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Dec 15 06:20:17 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xo08:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Dec/21 06:20;m-sir;to solve the problem, i modify the logic in EventLogFileWriters : when invoke method rollEventLogFile, check whether the status file exist first, if not recreate it;  it works.

 
{code:java}
/** exposed for testing only */
  private[history] def rollEventLogFile(): Unit = {
      // check wether the status file exist, if not recreate it 
       val isExist = checkAppStatusFileExist(true)
       if(!isExist){
           createAppStatusFile(inProgress = true)
       }
     closeWriter()    
      index += 1
    currentEventLogFilePath = getEventLogFilePath(logDirForAppPath, appId, appAttemptId, index,
      compressionCodecName)    initLogFile(currentEventLogFilePath) { os =>
      countingOutputStream = Some(new CountingOutputStream(os))
      new PrintWriter(
        new OutputStreamWriter(countingOutputStream.get, StandardCharsets.UTF_8))
    }
  } 

// new check method
private def checkAppStatusFileExist(inProgress:Boolean):Boolean = {
    val appStatusPath = getAppStatusFilePath(logDirForAppPath, appId, appAttempId, inProgress)
    val isExist = fileSystem.exists(appStatusPath)
    isExist
}{code}
 

why i choose to modify like this, rather than change the logic in history server, because the long run spark thrift server might abnormal exit without change the status file,  but this directory still need be delete.

during the testing, i found another problem, if the thrift server normal exit with out any eventlog,  the history can not delete this kind of directory, because the history server will thrown illegalArgumentException“directory must have at least one eventlog file”。;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Support pushing down a dynamic partition pruning from one join to other joins
Issue key: SPARK-37616
Issue id: 13416873
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 12/Dec/21 09:03
Updated: 12/Dec/21 11:55
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Support pushing down a dynamic partition pruning from one join to other joins
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Dec 12 11:55:57 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xleo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Dec/21 11:55;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34871;;;, 12/Dec/21 11:55;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34871;;;
Affects Version/s.1: 3.1.2
Comment.1: 12/Dec/21 11:55;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34871;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Forever-running streams get completed with finished status when k8s worker is lost
Issue key: SPARK-37587
Issue id: 13416129
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: chajath
Creator: chajath
Created: 08/Dec/21 18:20
Updated: 08/Dec/21 18:20
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Kubernetes, Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: We have forever-running streaming jobs, defined as:

{{spark.readStream}}
{{.format("avro")}}
{{.option("maxFilesPerTrigger", 10)}}
{{.schema(schema)}}
{{.load(loadPath)}}
{{.as[T]}}
{{.writeStream}}
{{.option("checkpointLocation", checkpointPath)}}

We have several of these running, and at the end of main, I call

{{spark.streams.awaitAnyTermination()}}

To fail the job on any failing jobs.

Now, we are running on a k8s runner with arguments (showing only relevant ones):

{{- /opt/spark/bin/spark-submit}}
{{# From `kubectl cluster-info`}}
{{- --master}}
{{- k8s://https://SOMEADDRESS.gr7.us-east-1.eks.amazonaws.com:443}}
{{- --deploy-mode}}
{{- client}}
{{- --name}}
{{- ourstreamingjob}}
{{# Driver connection via headless service}}
{{- --conf}}
{{- spark.driver.host=spark-driver-service}}
{{- --conf}}
{{- spark.driver.port=31337}}
{{# Move ivy temp dirs under /tmp. See https://stackoverflow.com/a/55921242/760482}}
{{- --conf}}
{{- spark.jars.ivy=/tmp/.ivy}}
{{# JVM settings for ivy and log4j.}}
{{- --conf}}
{{- spark.driver.extraJavaOptions=-Divy.cache.dir=/tmp -Divy.home=/tmp -Dlog4j.configuration=file:///opt/spark/log4j.properties}}
{{- --conf}}
{{- spark.executor.extraJavaOptions=-Dlog4j.configuration=file:///opt/spark/log4j.properties}}
{{# Spark on k8s settings.}}
{{- --conf}}
{{- spark.executor.instances=10}}
{{- --conf}}
{{- spark.executor.memory=8g}}
{{- --conf}}
{{- spark.executor.memoryOverhead=2g}}
{{- --conf}}
{{{}- spark.kubernetes.container.image=xxx.dkr.ecr.us-east-1.amazonaws.com/{}}}{{{}ourstreamingjob{}}}{{{}:latesthash{}}}
{{- --conf}}
{{- spark.kubernetes.container.image.pullPolicy=Always}}



The container image is built with provided spark:3.1.1-hadoop3.2 image and we add our jar to it.

We have found issues with our monitoring that a couple of these streaming were not running anymore, and found that these streams were deemed "complete" with FINISHED status. I couldn't find any error logs from the streams themselves. We did find that the time when these jobs were completed happen to coincide with the time when one of the k8s workers were lost and a new worker was added to the cluster.

From the console, it says:
{quote}Executor 7
Removed at 2021/12/07 22:26:12
Reason: The executor with ID 7 (registered at 1638858647668 ms) was not found in the cluster at the polling time (1638915971948 ms) which is after the accepted detect delta time (30000 ms) configured by `spark.kubernetes.executor.missingPodDetectDelta`. The executor may have been deleted but the driver missed the deletion event. Marking this executor as failed.
{quote}
I think the correct behavior would be to continue on the streaming from the last known checkpoint. If there is really an error, it should throw and the error should propagate so that eventually the pipeline will terminate.

I've checked out changes since 3.1.1 but couldn't find any fix to this specific issue. Also, is there any workaround you could recommend?

Thanks!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-08 18:20:33.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xhaw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: MapType supports orderable semantics
Issue key: SPARK-34819
Issue id: 13366643
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: EdisonWang
Creator: EdisonWang
Created: 22/Mar/21 06:06
Updated: 07/Dec/21 06:24
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Comparable/orderable semantics for map types is useful in some scenarios, and it's implemented in hive/presto. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): SPARK-37560
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-18134
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri May 14 15:39:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0p148:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Mar/21 02:35;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/31967;;;, 14/May/21 15:39;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32552;;;
Affects Version/s.1: 3.1.2
Comment.1: 14/May/21 15:39;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32552;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Optimize the dynamic partitioning prune rules to avoid inserting unnecessary predicates to improve performance
Issue key: SPARK-37542
Issue id: 13415368
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 04/Dec/21 05:46
Updated: 04/Dec/21 06:51
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, the dynamic partition pruning rule will insert a predicate on the filterable table using the filter from the other side of the join and a custom wrapper called DynamicPruning，and the predicate will be re-optimized by the AQE or non-AQE.

But, sometimes the predicate may be unnecessary if the join can NOT reuse broadcastExchange or it is not benefit，and it will be dropped by the rules of  the AQE or non-AQE.

We should optimize the dynamic partitioning pruning rule to avoid inserting unnecessary predicates to improve performance.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Dec 04 06:50:56 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xcm8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Dec/21 06:50;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34805;;;
Affects Version/s.1: 3.0.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Spark Thrift Server (STS) driver fullFC becourse of timeoutExecutor not shutdown correctly
Issue key: SPARK-37347
Issue id: 13412020
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: KK_3740
Creator: KK_3740
Created: 16/Nov/21 11:32
Updated: 17/Nov/21 03:23
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When spark.sql.thriftServer.queryTimeout or java.sql.Statement.setQueryTimeout is setted >0 , SparkExecuteStatementOperation add timeoutExecutor to kill time-consumed query in [SPARK-26533|https://issues.apache.org/jira/browse/SPARK-26533]. But timeoutExecutor is not shutdown correctly when statement is finished, it can only be shutdown when timeout. When we set timeout to a long time for example 1 hour, the long-running STS driver will FullGC and the application is not available for  a long time.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Nov 17 03:23:35 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wrzc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Nov/21 12:27;apachespark;User 'lk246' has created a pull request for this issue:
https://github.com/apache/spark/pull/34617;;;, 17/Nov/21 03:23;apachespark;User 'lk246' has created a pull request for this issue:
https://github.com/apache/spark/pull/34623;;;
Affects Version/s.1: 3.1.1
Comment.1: 17/Nov/21 03:23;apachespark;User 'lk246' has created a pull request for this issue:
https://github.com/apache/spark/pull/34623;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, Unknown

Summary: split function behave differently between spark 2.3 and spark 3.2
Issue key: SPARK-37344
Issue id: 13411934
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: oceaneast
Creator: oceaneast
Created: 16/Nov/21 02:00
Updated: 16/Nov/21 10:40
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: incorrect
Description: while use split function in sql, it behave differently between 2.3 and 3.2, which cause incorrect problem.

we can use this sql to reproduce this problem:

 

create table split_test ( id int,name string)

insert into split_test values(1,"abc;def")

explain extended select split(name,'\\\\;') from split_test

 

spark3:

spark-sql> Explain extended select split(name,'\\\\;') from split_test;

== Parsed Logical Plan ==

'Project [unresolvedalias('split('name, \\;), None)]

+- 'UnresolvedRelation [split_test], [], false

 

spark2:

 

spark-sql> Explain extended select split(name,'\\\\;') from split_test;

== Parsed Logical Plan ==

'Project [unresolvedalias('split('name, \;), None)]

+- 'UnresolvedRelation split_test

 

It looks like the deal of escape is different
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Nov 16 10:40:32 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wrg8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Nov/21 02:17;angerszhuuu;for same SQL  
{code}
explain extended select split('dawdawdawd','\\\\;');

{code}
In hive 1.2
{code}
OK
ABSTRACT SYNTAX TREE:

TOK_QUERY
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            TOK_FUNCTION
               split
               'dawdawdawd'
               '\\\;'
{code}

In hive 3
{code}
OK
ABSTRACT SYNTAX TREE:

TOK_QUERY
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            TOK_FUNCTION
               split
               'dawdawdawd'
               '\\\\;'
{code}

So it should be caused by hive's code.;;;, 16/Nov/21 10:40;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34616;;;, 16/Nov/21 10:40;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34616;;;
Affects Version/s.1: 3.1.2
Comment.1: 16/Nov/21 10:40;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34616;;;
Comment.2: 16/Nov/21 10:40;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34616;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Spark Dynamic Resource Allocation in Standalone Mode
Issue key: SPARK-37305
Issue id: 13411471
Parent id: 
Issue Type: Question
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: fdorado
Creator: fdorado
Created: 12/Nov/21 11:37
Updated: 12/Nov/21 11:37
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Hello, 

I have some questions about whether it is possible to use "Dynamic Resource Allocation" in standalone mode (2 workers and 1 master). I have been trying the suggestions proposed in your code, either by modifying by adding configuration variables to the SparkSession, or by including them directly as environment variables in Docker. None of these modifications have worked, and until one task is finished, I don't have the resources to do the next one (FIFO mode tasks). I wanted to ask if there is a way to run the tasks in parallel between several workers in standalone mode, since according to your website, it is possible, but it is disabled by default. Following the official documentation, I understand that it is possible, but realising the changes, I don't see any change in the concurrency: 

_Standalone mode: By default, applications submitted to the standalone mode cluster will run in FIFO (first-in-first-out) order, and each application will try to use all available nodes. You can limit the number of nodes an application uses by setting the spark.cores.max configuration property in it, or change the default for applications that don’t set this setting through spark.deploy.defaultCores. Finally, in addition to controlling cores, each application’s spark.executor.memory setting controls its memory use._


Environment: 
Original Estimate: 1800.0
Remaining Estimate: 1800.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 1800.0
Σ Remaining Estimate: 1800.0
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-11-12 11:37:52.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wolk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: java.lang.IllegalArgumentException Related to Prometheus
Issue key: SPARK-37122
Issue id: 13408546
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: brsingh1977
Creator: brsingh1977
Created: 26/Oct/21 21:19
Updated: 28/Oct/21 02:01
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: This issue is similar to https://issues.apache.org/jira/browse/SPARK-35237?focusedCommentId=17340723&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17340723. We receive the Following warning continuously:

 

21:00:26.277 [rpc-server-4-2] WARN  o.a.s.n.s.TransportChannelHandler - Exception in connection from /10.198.3.179:51184java.lang.IllegalArgumentException: Too large frame: 5135603447297303916 at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119) at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148) at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) at java.base/java.lang.Thread.run(Unknown Source)

 

Below are other details related to prometheus and my findings. Please SCROLL DOWN to see the details:

 
{noformat}
Prometheus Scrape Configuration
===============================
- job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
          action: replace
          target_label: __scheme__
          regex: (https?)
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_prometheus_io_port]
          action: replace
          target_label: __address__
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2

tcptrack command output in spark3 pod
======================================
10.198.22.240:51258  10.198.40.143:7079  CLOSED 10s 0 B/s
10.198.22.240:51258  10.198.40.143:7079  CLOSED 10s 0 B/s
10.198.22.240:50354  10.198.40.143:7079  CLOSED 40s 0 B/s
10.198.22.240:33152  10.198.40.143:4040  ESTABLISHED 2s 0 B/s
10.198.22.240:47726  10.198.40.143:8090  ESTABLISHED 9s 0 B/s

10.198.22.240 = prometheus pod 

ip10.198.40.143 = testpod ip 

Issue
======
Though the scrape config is expected to scrape on port 8090. I see prometheus tries to initiate scrape on ports like 7079, 7078, 4040, etc on
the spark3 pod and hence the exception in spark3 pod. But is this really a prometheus issue or something at spark side? We don't see any such exception in any of the other pods. All our pods including spark3 are annotated with:

annotations:
   prometheus.io/port: "8090"
   prometheus.io/scrape: "true"

We get the metrics and everything fine just extra warning for this exception.{noformat}
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-10-26 21:19:33.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0w6ow:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Setting CSV reader option "multiLine" to "true" causes URISyntaxException when colon is in file path
Issue key: SPARK-34883
Issue id: 13368231
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bctello8
Creator: bctello8
Created: 28/Mar/21 19:40
Updated: 25/Oct/21 14:24
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: Setting the CSV reader's "multiLine" option to "True" throws the following exception when a ':' character is in the file path.

 
{code:java}
java.net.URISyntaxException: Relative path in absolute URI: test:dir
{code}
I've tested this in both Spark 3.0.0 and Spark 3.1.1 and I get the same error whether I use Scala, Python, or SQL.

The following code works fine:

 
{code:java}
csvFile = "/FileStore/myDir/test:dir/pageviews_by_second.tsv" 
tempDF = (spark.read.option("sep", "\t").csv(csvFile)
{code}
While the following code fails:

 
{code:java}
csvFile = "/FileStore/myDir/test:dir/pageviews_by_second.tsv"
tempDF = (spark.read.option("sep", "\t").option("multiLine", "True").csv(csvFile)
{code}
Full Stack Trace from Python:

 
{code:java}
--------------------------------------------------------------------------- 
IllegalArgumentException Traceback (most recent call last) <command-8965899> in <module> 
3 csvFile = "/FileStore/myDir/test:dir/pageviews_by_second.tsv" 
4 
----> 5  tempDF = (spark.read.option("sep", "\t").option("multiLine", "True") 

/databricks/spark/python/pyspark/sql/readwriter.py in csv(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling) 

735 path = [path] 
736 if type(path) == list: 
--> 737 return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path))) 
738 elif isinstance(path, RDD): 
739 def func(iterator): 

/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args) 
1302 
1303 answer = self.gateway_client.send_command(command) 
-> 1304 return_value = get_return_value( 
1305 answer, self.gateway_client, self.target_id, self.name) 
1306 

/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw) 
114 # Hide where the exception came from that shows a non-Pythonic 
115 # JVM exception message. 
--> 116 raise converted from None 
117 else: 
118 raise IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: test:dir
{code}
 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Oct 25 14:24:07 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0pavs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/Mar/21 15:30;srowen;I'm guessing it's somehow that a different code path takes over for multiline parsing with univocity, but not sure. Do you have the JVM side stack trace?;;;, 31/Mar/21 16:02;bctello8;Sure thing.  This Scala code results in the JVM stack trace below.   Looks like an issue with hadoop's Globber class
{code:java}
var csvFile = "/FileStore/myDir/test:dir/pageviews_by_second.tsv"
var tempDF = (spark.read .option("sep", "\t") .option("multiLine", "True") .csv(csvFile) )
{code}
{code:java}
IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: test:dir
Caused by: URISyntaxException: Relative path in absolute URI: test:dir
	at org.apache.hadoop.fs.Path.initialize(Path.java:205)
	at org.apache.hadoop.fs.Path.<init>(Path.java:171)
	at org.apache.hadoop.fs.Path.<init>(Path.java:93)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:211)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:294)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)
	at org.apache.spark.input.StreamFileInputFormat.setMinPartitions(PortableDataStream.scala:51)
	at org.apache.spark.rdd.BinaryFileRDD.getPartitions(BinaryFileRDD.scala:51)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1435)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:419)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1429)
	at org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$.infer(CSVDataSource.scala:225)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:70)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:230)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:223)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:460)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:408)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:390)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:390)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:912)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:705)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-8965899:6)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-8965899:50)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw.<init>(command-8965899:52)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw.<init>(command-8965899:54)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw.<init>(command-8965899:56)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw.<init>(command-8965899:58)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read.<init>(command-8965899:60)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$.<init>(command-8965899:64)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$.<clinit>(command-8965899)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval$.$print$lzycompute(<notebook>:7)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval$.$print(<notebook>:6)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:235)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:836)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:789)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:235)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:494)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:471)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: test:dir
	at java.net.URI.checkPath(URI.java:1849)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:202)
	at org.apache.hadoop.fs.Path.<init>(Path.java:171)
	at org.apache.hadoop.fs.Path.<init>(Path.java:93)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:211)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:294)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)
	at org.apache.spark.input.StreamFileInputFormat.setMinPartitions(PortableDataStream.scala:51)
	at org.apache.spark.rdd.BinaryFileRDD.getPartitions(BinaryFileRDD.scala:51)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1435)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:419)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1429)
	at org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$.infer(CSVDataSource.scala:225)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:70)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:230)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:223)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:460)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:408)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:390)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:390)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:912)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:705)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-8965899:6)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-8965899:50)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw.<init>(command-8965899:52)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw.<init>(command-8965899:54)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw.<init>(command-8965899:56)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw.<init>(command-8965899:58)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read.<init>(command-8965899:60)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$.<init>(command-8965899:64)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$.<clinit>(command-8965899)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval$.$print$lzycompute(<notebook>:7)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval$.$print(<notebook>:6)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:235)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:836)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:789)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:235)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:494)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:471)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)
	at java.lang.Thread.run(Thread.java:748)
{code};;;, 11/Apr/21 23:03;Vikas_Yadav;I was able to use multiLine option without any issue .
it seems  cluster/infra environment  issue nothing related to spark or spark csv package. 

 

____ __
 / __/__ ___ _____/ /__
 _\ \/ _ \/ _ `/ __/ '_/
 /__ / .__/\_,_/_/ /_/\_\ version 3.0.0
 /_/

>>> inputFile="/Users/home_dir/Downloads/pageviews-by-second-tsv"
>>> tempDF = (spark.read.option("sep", "\t").option("multiLine", "True").option("quote", "\"").option("escape", "\"").csv(inputFile))
>>> tempDF.show()
+-------------------+------+----+
| _c0| _c1| _c2|
+-------------------+------+----+
|2015-03-16T00:09:55|mobile|1595|
+-------------------+------+----+;;;, 22/Apr/21 13:51;bctello8;[~Vikas_Yadav]

You don't have a colon in your `inputFile` path.  See the following code that contains a colon in the path to the dataset.  It fails with a URISyntaxException as expected:
{code:java}
>>> inputFile = "/Users/home_dir/Workspaces/datasets/with:colon/iris.csv" 
>>> tempDF = spark.read.csv(inputFile, multiLine=True) 
Traceback (most recent call last): File "<stdin>", line 1, in <module> File "/Users/home_dir/Workspaces/spark/python/pyspark/sql/readwriter.py", line 737, in csv return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path))) File "/Users/home_dir/Workspaces/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__ File "/Users/home_dir/Workspaces/spark/python/pyspark/sql/utils.py", line 117, in deco raise converted from None pyspark.sql.utils.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: with:colon
{code};;;, 10/May/21 17:19;bctello8;Update to this thread:

I found that if I provide a schema to the csv reader, it works fine even if I use the multiLine option.  I've only verified this on my laptop so far but I plan to see if it works on something like Databricks soon.  I suspect that the bug is probably isolated to `inferSchema`:
{code:java}
org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
{code};;;, 02/Jul/21 13:51;mrpowerus;I've got the same error here when I try to run:
{code:java}
spark.read.csv(URL_ABFS_RAW + "/salesforce/Case/timestamp=2021-07-02 00:14:15.129481", header=True, multiLine=True)
{code}
I'm running Spark 3.0.1

 ;;;, 25/Oct/21 14:24;bctello8;This issue actually affects more APIs than the JSON multiline reader.  You cannot read a text file using the RDD API in Spark 3.2.0 if there is a colon in your path.  The following stack trace was generated on my laptop using Spark 3.2.0-SNAPSHOT.

 
{code:java}
scala> val df = sc.textFile("/Users/myUserName/test:me/test.txt").take(1) java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: test:me at org.apache.hadoop.fs.Path.initialize(Path.java:259) at org.apache.hadoop.fs.Path.<init>(Path.java:217) at org.apache.hadoop.fs.Path.<init>(Path.java:125) at org.apache.hadoop.fs.Globber.doGlob(Globber.java:229) at org.apache.hadoop.fs.Globber.glob(Globber.java:149) at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2034) at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:269) at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:239) at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:325) at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205) at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300) at scala.Option.getOrElse(Option.scala:189) at org.apache.spark.rdd.RDD.partitions(RDD.scala:296) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300) at scala.Option.getOrElse(Option.scala:189) at org.apache.spark.rdd.RDD.partitions(RDD.scala:296) at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1428) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:414) at org.apache.spark.rdd.RDD.take(RDD.scala:1422) ... 47 elided Caused by: java.net.URISyntaxException: Relative path in absolute URI: test:me at java.base/java.net.URI.checkPath(URI.java:1990) at java.base/java.net.URI.<init>(URI.java:780) at org.apache.hadoop.fs.Path.initialize(Path.java:256) ... 68 more
{code};;;
Affects Version/s.1: 3.1.1
Comment.1: 31/Mar/21 16:02;bctello8;Sure thing.  This Scala code results in the JVM stack trace below.   Looks like an issue with hadoop's Globber class
{code:java}
var csvFile = "/FileStore/myDir/test:dir/pageviews_by_second.tsv"
var tempDF = (spark.read .option("sep", "\t") .option("multiLine", "True") .csv(csvFile) )
{code}
{code:java}
IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: test:dir
Caused by: URISyntaxException: Relative path in absolute URI: test:dir
	at org.apache.hadoop.fs.Path.initialize(Path.java:205)
	at org.apache.hadoop.fs.Path.<init>(Path.java:171)
	at org.apache.hadoop.fs.Path.<init>(Path.java:93)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:211)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:294)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)
	at org.apache.spark.input.StreamFileInputFormat.setMinPartitions(PortableDataStream.scala:51)
	at org.apache.spark.rdd.BinaryFileRDD.getPartitions(BinaryFileRDD.scala:51)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1435)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:419)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1429)
	at org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$.infer(CSVDataSource.scala:225)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:70)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:230)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:223)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:460)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:408)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:390)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:390)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:912)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:705)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-8965899:6)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-8965899:50)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw.<init>(command-8965899:52)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw.<init>(command-8965899:54)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw.<init>(command-8965899:56)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw.<init>(command-8965899:58)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read.<init>(command-8965899:60)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$.<init>(command-8965899:64)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$.<clinit>(command-8965899)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval$.$print$lzycompute(<notebook>:7)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval$.$print(<notebook>:6)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:235)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:836)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:789)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:235)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:494)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:471)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.URISyntaxException: Relative path in absolute URI: test:dir
	at java.net.URI.checkPath(URI.java:1849)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:202)
	at org.apache.hadoop.fs.Path.<init>(Path.java:171)
	at org.apache.hadoop.fs.Path.<init>(Path.java:93)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:211)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:294)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)
	at org.apache.spark.input.StreamFileInputFormat.setMinPartitions(PortableDataStream.scala:51)
	at org.apache.spark.rdd.BinaryFileRDD.getPartitions(BinaryFileRDD.scala:51)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:307)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:303)
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1435)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:419)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1429)
	at org.apache.spark.sql.execution.datasources.csv.MultiLineCSVDataSource$.infer(CSVDataSource.scala:225)
	at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:70)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:230)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:223)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:460)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:408)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:390)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:390)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:912)
	at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:705)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-8965899:6)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-8965899:50)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw$$iw.<init>(command-8965899:52)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw$$iw.<init>(command-8965899:54)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw$$iw.<init>(command-8965899:56)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$$iw.<init>(command-8965899:58)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read.<init>(command-8965899:60)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$.<init>(command-8965899:64)
	at $lined238ff10de5c41758f3b1477d427b85e25.$read$.<clinit>(command-8965899)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval$.$print$lzycompute(<notebook>:7)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval$.$print(<notebook>:6)
	at $lined238ff10de5c41758f3b1477d427b85e25.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:235)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:836)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:789)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:235)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:494)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:471)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)
	at java.lang.Thread.run(Thread.java:748)
{code};;;
Comment.2: 11/Apr/21 23:03;Vikas_Yadav;I was able to use multiLine option without any issue .
it seems  cluster/infra environment  issue nothing related to spark or spark csv package. 

 

____ __
 / __/__ ___ _____/ /__
 _\ \/ _ \/ _ `/ __/ '_/
 /__ / .__/\_,_/_/ /_/\_\ version 3.0.0
 /_/

>>> inputFile="/Users/home_dir/Downloads/pageviews-by-second-tsv"
>>> tempDF = (spark.read.option("sep", "\t").option("multiLine", "True").option("quote", "\"").option("escape", "\"").csv(inputFile))
>>> tempDF.show()
+-------------------+------+----+
| _c0| _c1| _c2|
+-------------------+------+----+
|2015-03-16T00:09:55|mobile|1595|
+-------------------+------+----+;;;
Comment.3: 22/Apr/21 13:51;bctello8;[~Vikas_Yadav]

You don't have a colon in your `inputFile` path.  See the following code that contains a colon in the path to the dataset.  It fails with a URISyntaxException as expected:
{code:java}
>>> inputFile = "/Users/home_dir/Workspaces/datasets/with:colon/iris.csv" 
>>> tempDF = spark.read.csv(inputFile, multiLine=True) 
Traceback (most recent call last): File "<stdin>", line 1, in <module> File "/Users/home_dir/Workspaces/spark/python/pyspark/sql/readwriter.py", line 737, in csv return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path))) File "/Users/home_dir/Workspaces/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__ File "/Users/home_dir/Workspaces/spark/python/pyspark/sql/utils.py", line 117, in deco raise converted from None pyspark.sql.utils.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: with:colon
{code};;;
Comment.4: 10/May/21 17:19;bctello8;Update to this thread:

I found that if I provide a schema to the csv reader, it works fine even if I use the multiLine option.  I've only verified this on my laptop so far but I plan to see if it works on something like Databricks soon.  I suspect that the bug is probably isolated to `inferSchema`:
{code:java}
org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
{code};;;
Comment.5: 02/Jul/21 13:51;mrpowerus;I've got the same error here when I try to run:
{code:java}
spark.read.csv(URL_ABFS_RAW + "/salesforce/Case/timestamp=2021-07-02 00:14:15.129481", header=True, multiLine=True)
{code}
I'm running Spark 3.0.1

 ;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Publicise UpperCaseCharStream
Issue key: SPARK-37016
Issue id: 13406703
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dohongdayi
Creator: dohongdayi
Created: 15/Oct/21 04:55
Updated: 22/Oct/21 10:14
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.2.3, 2.3.4, 2.4.8, 3.0.3, 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Many Spark extension projects are copying `UpperCaseCharStream` because it is private beneath `parser` package, such as:

[Delta Lake|https://github.com/delta-io/delta/blob/625de3b305f109441ad04b20dba91dd6c4e1d78e/core/src/main/scala/io/delta/sql/parser/DeltaSqlParser.scala#L290]

[Hudi|https://github.com/apache/hudi/blob/3f8ca1a3552bb866163d3b1648f68d9c4824e21d/hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/spark/sql/parser/HoodieCommonSqlParser.scala#L112]

[Iceberg|https://github.com/apache/iceberg/blob/c3ac4c6ca74a0013b4705d5bd5d17fade8e6f499/spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala#L175]

[Submarine|https://github.com/apache/submarine/blob/2faebb8efd69833853f62d89b4f1fea1b1718148/submarine-security/spark-security/src/main/scala/org/apache/submarine/spark/security/parser/UpperCaseCharStream.scala#L31]

[Kyuubi|https://github.com/apache/incubator-kyuubi/blob/8a5134e3223844714fc58833a6859d4df5b68d57/dev/kyuubi-extension-spark-common/src/main/scala/org/apache/kyuubi/sql/zorder/ZorderSparkSqlExtensionsParserBase.scala#L108]

[Spark-ACID|https://github.com/qubole/spark-acid/blob/19bd6db757677c40f448e85c74d9995ba97d5942/src/main/scala/com/qubole/spark/datasources/hiveacid/sql/catalyst/parser/ParseDriver.scala#L13]

We can publicise `UpperCaseCharStream` to eliminate code duplication.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Oct 22 10:14:42 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vvbc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Oct/21 05:20;apachespark;User 'dohongdayi' has created a pull request for this issue:
https://github.com/apache/spark/pull/34290;;;, 22/Oct/21 10:14;dohongdayi;Anyone care about this issue?;;;
Affects Version/s.1: 2.3.4
Comment.1: 22/Oct/21 10:14;dohongdayi;Anyone care about this issue?;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: fixing "SQL column nullable setting not retained as part of spark read" issue
Issue key: SPARK-36996
Issue id: 13406312
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: senthh
Creator: senthh
Created: 13/Oct/21 09:18
Updated: 13/Oct/21 10:53
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Sql 'nullable' columns are not retaining 'nullable' type as it is while reading from Spark read using jdbc format.

 

SQL :

------------

 

mysql> CREATE TABLE Persons(Id int NOT NULL, FirstName varchar(255), LastName varchar(255), Age int);

 

mysql> desc Persons;
+-----------+--------------+------+-----+---------+-------+
| Field | Type | Null | Key | Default | Extra |
+-----------+--------------+------+-----+---------+-------+
| Id | int | NO | | NULL | |
| FirstName | varchar(255) | YES | | NULL | |
| LastName | varchar(255) | YES | | NULL | |
| Age | int | YES | | NULL | |
+-----------+--------------+------+-----+---------+-------+

 

But in Spark  we get all the columns as "Nullable":

=============

scala> val df = spark.read.format("jdbc").option("database","Test_DB").option("user", "root").option("password", "").option("driver", "com.mysql.cj.jdbc.Driver").option("url", "jdbc:mysql://localhost:3306/Test_DB").option("dbtable", "Persons").load()
df: org.apache.spark.sql.DataFrame = [Id: int, FirstName: string ... 2 more fields]

scala> df.printSchema()
root
 |-- Id: integer (nullable = true)
 |-- FirstName: string (nullable = true)
 |-- LastName: string (nullable = true)
 |-- Age: integer (nullable = true)

=============

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 13 10:53:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vsxk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Oct/21 09:21;senthh;Based on further analysis, Spark is hard-coding "nullable" as "true" always. This change has been inccluded due to "https://issues.apache.org/jira/browse/SPARK-19726".

 

 ;;;, 13/Oct/21 09:21;senthh;I m working on this;;;, 13/Oct/21 10:40;apachespark;User 'senthh' has created a pull request for this issue:
https://github.com/apache/spark/pull/34272;;;, 13/Oct/21 10:50;senthh;We need to consider 2 scenarios

 
 # maintain NULLABLE value as per SQL metadata for non timestamp columns
 # set NULLABLE as true(always) for timestamp columns

 

 ;;;, 13/Oct/21 10:53;senthh;Sample Output after this changes:

SQL :

mysql> CREATE TABLE Persons(Id int NOT NULL, FirstName varchar(255), LastName varchar(255), Age int);

 

mysql> desc Persons;
+-----------+--------------+------+-----+---------+-------+
| Field | Type | Null | Key | Default | Extra |
+-----------+--------------+------+-----+---------+-------+
| Id | int | NO | | NULL | |
| FirstName | varchar(255) | YES | | NULL | |
| LastName | varchar(255) | YES | | NULL | |
| Age | int | YES | | NULL | |
+-----------+--------------+------+-----+---------+-------+

----------++-----------++----------------+

Spark:

scala> val df = spark.read.format("jdbc").option("database","Test_DB").option("user", "root").option("password", "").option("driver", "com.mysql.cj.jdbc.Driver").option("url", "jdbc:mysql://localhost:3306/Test_DB").option("dbtable", "Persons").load()
 df: org.apache.spark.sql.DataFrame = [Id: int, FirstName: string ... 2 more fields]

scala> df.printSchema()
 root
 |-- Id: integer (nullable = false)
 |-- FirstName: string (nullable = true)
 |-- LastName: string (nullable = true)
 |-- Age: integer (nullable = true)

 

 

And for TIMESTAMP columns

 

SQL:
create table timestamp_test(id int(11), time_stamp timestamp not null default current_timestamp);

SPARK:

scala> val df = spark.read.format("jdbc").option("database","Test_DB").option("user", "root").option("password", "").option("driver", "com.mysql.cj.jdbc.Driver").option("url", "jdbc:mysql://localhost:3306/Test_DB").option("dbtable", "timestamp_test").load()
df: org.apache.spark.sql.DataFrame = [id: int, time_stamp: timestamp]

scala> df.printSchema()
root
|-- id: integer (nullable = true)
|-- time_stamp: timestamp (nullable = true);;;
Affects Version/s.1: 3.1.0
Comment.1: 13/Oct/21 09:21;senthh;I m working on this;;;
Comment.2: 13/Oct/21 10:40;apachespark;User 'senthh' has created a pull request for this issue:
https://github.com/apache/spark/pull/34272;;;
Comment.3: 13/Oct/21 10:50;senthh;We need to consider 2 scenarios

 
 # maintain NULLABLE value as per SQL metadata for non timestamp columns
 # set NULLABLE as true(always) for timestamp columns

 

 ;;;
Comment.4: 13/Oct/21 10:53;senthh;Sample Output after this changes:

SQL :

mysql> CREATE TABLE Persons(Id int NOT NULL, FirstName varchar(255), LastName varchar(255), Age int);

 

mysql> desc Persons;
+-----------+--------------+------+-----+---------+-------+
| Field | Type | Null | Key | Default | Extra |
+-----------+--------------+------+-----+---------+-------+
| Id | int | NO | | NULL | |
| FirstName | varchar(255) | YES | | NULL | |
| LastName | varchar(255) | YES | | NULL | |
| Age | int | YES | | NULL | |
+-----------+--------------+------+-----+---------+-------+

----------++-----------++----------------+

Spark:

scala> val df = spark.read.format("jdbc").option("database","Test_DB").option("user", "root").option("password", "").option("driver", "com.mysql.cj.jdbc.Driver").option("url", "jdbc:mysql://localhost:3306/Test_DB").option("dbtable", "Persons").load()
 df: org.apache.spark.sql.DataFrame = [Id: int, FirstName: string ... 2 more fields]

scala> df.printSchema()
 root
 |-- Id: integer (nullable = false)
 |-- FirstName: string (nullable = true)
 |-- LastName: string (nullable = true)
 |-- Age: integer (nullable = true)

 

 

And for TIMESTAMP columns

 

SQL:
create table timestamp_test(id int(11), time_stamp timestamp not null default current_timestamp);

SPARK:

scala> val df = spark.read.format("jdbc").option("database","Test_DB").option("user", "root").option("password", "").option("driver", "com.mysql.cj.jdbc.Driver").option("url", "jdbc:mysql://localhost:3306/Test_DB").option("dbtable", "timestamp_test").load()
df: org.apache.spark.sql.DataFrame = [id: int, time_stamp: timestamp]

scala> df.printSchema()
root
|-- id: integer (nullable = true)
|-- time_stamp: timestamp (nullable = true);;;
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: SQL column nullable setting not retained as part of spark read
Issue key: SPARK-36758
Issue id: 13401143
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: darrenpricedcww
Creator: darrenpricedcww
Created: 14/Sep/21 20:52
Updated: 15/Sep/21 01:15
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: When reading in a column set as not null this is not retained as part of the spark.read.
All columns are showing as nullable = true

Is there a way to change this behaviour to retain the null setting from the source?

See here for more info

https://github.com/microsoft/sql-spark-connector/issues/121

 

Example code from databricks:

tableName = "dbo.MyTable"

df = spark.read
.format("com.microsoft.sqlserver.jdbc.spark")
.option("url", myJdbcUrl)
.option("accessToken", accessToken)
.option("dbTable", tableName)
.load()

df.printSchema()

 
Environment: Databricks:

Runtime 8.2

Spark 3.1.1
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): English
Custom field (Last public comment date): 2021-09-14 20:52:48.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ux1s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Spark and Hive are inconsistent in inferring the expression type in the view, resulting in AnalysisException
Issue key: SPARK-36413
Issue id: 13393441
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: bingfeng
Creator: bingfeng
Created: 04/Aug/21 10:41
Updated: 10/Sep/21 00:55
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.4.1, 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The scene of the restoration is as follows:

 

Suppose I have a hive table：
{quote}> desc sales;
 id              bigint

name       varchar(4096)

price        decimal(19,4)
{quote}
and I create a View：
{quote}CREATE VIEW `sales_view_bf` AS select `sales`.`id`,
 case when `sales`.`name` = 'abc' then `sales`.`price`*1.1 else 0 end as `price_new`
 from `SALES`;

> desc sales_view_bf;
 id              bigint

price        double
{quote}
then I query follow sql on hive with spark:
{quote}select PRICE_NEW from SALES_VIEW_BF;
{quote}
will throw:
{quote}Caused by: org.apache.spark.sql.AnalysisException: Cannot up cast `price_new` from decimal(22,5) to double as it may truncateCaused by: org.apache.spark.sql.AnalysisException: Cannot up cast `price_new` from decimal(22,5) to double as it may truncate; at org.apache.spark.sql.catalyst.analysis.AliasViewChild$$anonfun$apply$1$$anonfun$2.apply(view.scala:78) at org.apache.spark.sql.catalyst.analysis.AliasViewChild$$anonfun$apply$1$$anonfun$2.apply(view.scala:72) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.immutable.List.foreach(List.scala:392) at scala.collection.TraversableLike$class.map(TraversableLike.scala:234) at scala.collection.immutable.List.map(List.scala:296) at org.apache.spark.sql.catalyst.analysis.AliasViewChild$$anonfun$apply$1.applyOrElse(view.scala:72) at org.apache.spark.sql.catalyst.analysis.AliasViewChild$$anonfun$apply$1.applyOrElse(view.scala:51) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326) at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326) at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326) at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326) at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29) at org.apache.spark.sql.catalyst.analysis.AliasViewChild.apply(view.scala:51) at org.apache.spark.sql.catalyst.analysis.AliasViewChild.apply(view.scala:50) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84) at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57) at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66) at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76) at scala.collection.immutable.List.foreach(List.scala:392) at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76) at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127) at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121) at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106) at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201) at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105) at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57) at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55) at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)
{quote}
 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 04 10:53:23 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tliw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Aug/21 10:53;bingfeng;This scenario is very common in my business and blocked my entire query. can i fix it by myself;;;
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Spark FetchFailedException Stream is corrupted Error
Issue key: SPARK-36196
Issue id: 13390543
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: arghya18
Creator: arghya18
Created: 18/Jul/21 12:56
Updated: 30/Aug/21 08:26
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Kubernetes, PySpark, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: I am running Spark on K8S. There are around thousands of jobs runs everyday but few are getting failed everyday(not same job) and with below exception. It succeed on retry. I have read about the error in multiple Jira and saw its resolved with Spark 3.0.0 but I am still getting the error with higher version.
{code:java}
org.apache.spark.shuffle.FetchFailedException: Stream is corrupted
org.apache.spark.shuffle.FetchFailedException: Stream is corrupted at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:770) at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:845) at java.base/java.io.BufferedInputStream.fill(Unknown Source) at java.base/java.io.BufferedInputStream.read1(Unknown Source) at java.base/java.io.BufferedInputStream.read(Unknown Source) at java.base/java.io.DataInputStream.read(Unknown Source) at org.sparkproject.guava.io.ByteStreams.read(ByteStreams.java:899) at org.sparkproject.guava.io.ByteStreams.readFully(ByteStreams.java:733) at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:127) at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$2$$anon$3.next(UnsafeRowSerializer.scala:110) at scala.collection.Iterator$$anon$11.next(Iterator.scala:494) at scala.collection.Iterator$$anon$10.next(Iterator.scala:459) at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29) at org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40) at scala.collection.Iterator$$anon$10.next(Iterator.scala:459) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.sort_addToSorter_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755) at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:83) at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoinExec.scala:817) at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.<init>(SortMergeJoinExec.scala:687) at org.apache.spark.sql.execution.joins.SortMergeJoinExec.$anonfun$doExecute$1(SortMergeJoinExec.scala:197) at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:131) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.base/java.lang.Thread.run(Unknown Source)Caused by: java.io.IOException: Stream is corrupted at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:250) at net.jpountz.lz4.LZ4BlockInputStream.read(LZ4BlockInputStream.java:157) at org.apache.spark.storage.BufferReleasingInputStream.read(ShuffleBlockFetcherIterator.scala:841) ... 38 moreCaused by: net.jpountz.lz4.LZ4Exception: Error decoding offset 8785 of input buffer at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:39) at net.jpountz.lz4.LZ4BlockInputStream.refill(LZ4BlockInputStream.java:245) ... 40 more
{code}
 
Environment: Spark on K8s
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 30 08:26:11 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0t3nc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Aug/21 08:26;Ngone51;Hi [~arghya18] Did you try to apply the fix of https://issues.apache.org/jira/browse/SPARK-32658?;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Spark on K8s: Driver pod keeps running when executor allocator terminates with Fatal exception
Issue key: SPARK-36577
Issue id: 13397012
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gargv
Creator: gargv
Created: 24/Aug/21 20:09
Updated: 29/Aug/21 01:45
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.4.8, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In Spark on Kubernetes, the class ExecutorPodsSnapshotsStoreImpl creates a thread which is responsible for creating new executor pods. The thread catches all 'NonFatal' exceptions, logs them and ignores these NonFatal exceptions. However, from Fatal exceptions, it only handles {color:#20999d}IllegalArgumentException {color}exception and terminates the driver pod in that case. Other Fatal exceptions are not handled at all, which means that if such a Fatal exception occurs, the executor creation thread abruptly terminates, while the main thread keeps running. Thus, the Spark application/job would keep running indefinitely without making any progress.

To fix this, 2 of the possible options are:

*Option#1*: Fail the driver pod whenever any Fatal exception happens. However, this approach has following disadvantages:
 # A few number of executors may have already been created when this Fatal exception happens. These executors can still take the job to completion, although slower than expected as all executors were not launched. Thus, we would fail the job instead of letting the job succeed slowly.
 # JVM can sometimes recover from Fatal exceptions on its own. Thus, we are not giving a chance to driver pod to recover from failure, rather we are killing it on first occurrence of {{Fatal}} exception.

*Option#2*: Fail the driver pod only when there are 0 executors running currently

In this approach, we fail the driver pod only when number of currently running executors is 0. This is so that we don’t kill a job which can potentially complete. Thus, 1 single running executor can keep the driver pod from dying. This may mean that job may make very slow progress when actual number of requested executors is very large.

*Option#3*: Expose configurations which allows customers to control this behavior.

spark.kubernetes.driver.executor-creation.terminate-on-fatal-exception=<true|false>
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-24 20:09:32.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u7kg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.0.0
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Make CatalogString contains nullable information
Issue key: SPARK-35289
Issue id: 13376261
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: apachespark
Reporter: hezuojiao
Creator: hezuojiao
Created: 02/May/21 09:39
Updated: 24/Aug/21 12:27
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When trying to cast ArrayType/MapType/StructType from containing NULL to not contain NULL, we will get a confusing failure message.
 Here's a example:
{code:java}
// new UT in CastSuite
test("array casting") {
 val arrayContainsNull = Literal.create(Seq("contains", "null", null),
 ArrayType(StringType, containsNull = true))
 val ret = cast(arrayContainsNull, ArrayType(StringType, containsNull = false))
 assert(ret.resolved === false)
 // scalastyle:off println
 println(ret.typeCheckFailureMessage)
 // scalastyle:on println
}
{code}
Failure massage:
{code:java}
cannot cast array<string> to array<string>{code}
From the above error message, it is not clear why the error is reported, because the data type before and after is the same.
 Therefore, I think we need to specify nullable info for ArrayType/MapType?/StructType in the Catalog String to identify the reason for Cast Failures.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 24/Aug/21 12:23;jp.xiong;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13032364/screenshot-1.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Aug 24 12:27:08 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qnpk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/May/21 13:11;apachespark;User 'hezuojiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/32417;;;, 24/Aug/21 12:27;jp.xiong;i had meet this issue in my project, and my error message is below 
!screenshot-1.png! 
And the data in the test is :
hesnx;qgdph;qydpt;26;rtaco;2:51.1
updle;tlpuu;agyue;10;mbdnn;3:22.2
sjjzs;tpnkb;pytdu;14;vfogo;4:70.3
ozacs;afcpc;zkfhz;14;nqpwa;5:94.4
uibxi;hbggo;zstuo;10;xiibw;6:05.5
bphga;xajrb;juwwh;18;lkndi;7:71.6
phswl;zivnz;ujsui;24;pyhhk;8:79.7

The command of the test is : 
spark-sql：
create table testtmp01 (
`date` string,
`user` string,
`name` string,
`age` string,
`str` string,
`extends_map` map<int,double>
) ROW FORMAT DELIMITED FIELDS TERMINATED BY ';' 
COLLECTION ITEMS TERMINATED BY '|' 
MAP KEYS TERMINATED BY ':' 
STORED AS TEXTFILE;

spark-shell：
case class DailySessionIdLabelInfo(extends_map: Map[Int, Double])
val dddd = sql("select extends_map from testtmp01 limit 10").as[DailySessionIdLabelInfo]

And can we add a full error log for this issue to let the user know the real issue for this problem？;;;
Affects Version/s.1: 
Comment.1: 24/Aug/21 12:27;jp.xiong;i had meet this issue in my project, and my error message is below 
!screenshot-1.png! 
And the data in the test is :
hesnx;qgdph;qydpt;26;rtaco;2:51.1
updle;tlpuu;agyue;10;mbdnn;3:22.2
sjjzs;tpnkb;pytdu;14;vfogo;4:70.3
ozacs;afcpc;zkfhz;14;nqpwa;5:94.4
uibxi;hbggo;zstuo;10;xiibw;6:05.5
bphga;xajrb;juwwh;18;lkndi;7:71.6
phswl;zivnz;ujsui;24;pyhhk;8:79.7

The command of the test is : 
spark-sql：
create table testtmp01 (
`date` string,
`user` string,
`name` string,
`age` string,
`str` string,
`extends_map` map<int,double>
) ROW FORMAT DELIMITED FIELDS TERMINATED BY ';' 
COLLECTION ITEMS TERMINATED BY '|' 
MAP KEYS TERMINATED BY ':' 
STORED AS TEXTFILE;

spark-shell：
case class DailySessionIdLabelInfo(extends_map: Map[Int, Double])
val dddd = sql("select extends_map from testtmp01 limit 10").as[DailySessionIdLabelInfo]

And can we add a full error log for this issue to let the user know the real issue for this problem？;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Improve push based shuffle to work with AQE by fetching partial map indexes for a reduce partition
Issue key: SPARK-35036
Issue id: 13371868
Parent id: 13337114.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: vsowrirajan
Creator: vsowrirajan
Created: 12/Apr/21 19:54
Updated: 16/Aug/21 17:07
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Currently when both Push based shuffle and AQE is enabled and when partial set of map indexes are requested to MapOutputTracker this is delegated the regular shuffle instead of push based shuffle reading map blocks. This is because blocks from mapper in push based shuffle are merged out of order due to which its hard to only get the matching blocks of the reduce partition for the requested start and end map indexes.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 16 17:07:09 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0pwmo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Aug/21 17:07;mshen;Not sure if this is fixable, given the reasons you already described.

The partial set of map indexes are used in AQE only to handle skewed partitions.

Since it's a skewed partition to begin with, in practice it would only affect very few shuffle partitions.

We could alternatively handle skewed partitions with push-based shuffle differently from how AQE handles it, i.e. instead of subdividing a shuffle partition using continuous map index sub-ranges we could subdivide a skewed merged shuffle partition based on boundaries of the MB-sized chunks.

This should be relatively easier to achieve and can also handle skewed partitions.

Furthermore, just to clarify that push-based shuffle can already work with AQE for shuffle partition coalescing.;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: SortMergeJoin do unnecessary shuffle for tables whose provider is hive
Issue key: SPARK-36494
Issue id: 13394876
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zengrui
Creator: zengrui
Created: 12/Aug/21 10:49
Updated: 12/Aug/21 13:03
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Create two tables in spark like this:
{code:java}
CREATE TABLE catalog_sales
(
    cs_sold_date_sk           BIGINT,
    cs_sold_time_sk           BIGINT,
    cs_ship_date_sk           BIGINT,
    cs_bill_customer_sk       BIGINT,
    cs_bill_cdemo_sk          BIGINT,
    cs_bill_hdemo_sk          BIGINT,
    cs_bill_addr_sk           BIGINT,
    cs_ship_customer_sk       BIGINT,
    cs_ship_cdemo_sk          BIGINT,
    cs_ship_hdemo_sk          BIGINT,
    cs_ship_addr_sk           BIGINT,
    cs_call_center_sk         BIGINT,
    cs_catalog_page_sk        BIGINT,
    cs_ship_mode_sk           BIGINT,
    cs_warehouse_sk           BIGINT,
    cs_item_sk                BIGINT,
    cs_promo_sk               BIGINT,
    cs_order_number           BIGINT,
    cs_quantity               BIGINT,
    cs_wholesale_cost         FLOAT,
    cs_list_price             FLOAT,
    cs_sales_price            FLOAT,
    cs_ext_discount_amt       FLOAT,
    cs_ext_sales_price        FLOAT,
    cs_ext_wholesale_cost     FLOAT,
    cs_ext_list_price         FLOAT,
    cs_ext_tax                FLOAT,
    cs_coupon_amt             FLOAT,
    cs_ext_ship_cost          FLOAT,
    cs_net_paid               FLOAT,
    cs_net_paid_inc_tax       FLOAT,
    cs_net_paid_inc_ship      FLOAT,
    cs_net_paid_inc_ship_tax  FLOAT,
    cs_net_profit             FLOAT
)
clustered by (cs_item_sk)  into  10 buckets
stored as ORC;

CREATE TABLE store_sales
(
    ss_sold_date_sk           BIGINT,
    ss_sold_time_sk           BIGINT,
    ss_item_sk                BIGINT,
    ss_customer_sk            BIGINT,
    ss_cdemo_sk               BIGINT,
    ss_hdemo_sk               BIGINT,
    ss_addr_sk                BIGINT,
    ss_store_sk               BIGINT,
    ss_promo_sk               BIGINT,
    ss_ticket_number          BIGINT,
    ss_quantity               BIGINT,
    ss_wholesale_cost         FLOAT,
    ss_list_price             FLOAT,
    ss_sales_price            FLOAT,
    ss_ext_discount_amt       FLOAT,
    ss_ext_sales_price        FLOAT,
    ss_ext_wholesale_cost     FLOAT,
    ss_ext_list_price         FLOAT,
    ss_ext_tax                FLOAT,
    ss_coupon_amt             FLOAT,
    ss_net_paid               FLOAT,
    ss_net_paid_inc_tax       FLOAT,
    ss_net_profit             FLOAT
)
CLUSTERED BY (ss_item_sk) INTO 10 buckets
stored as ORC;{code}
Then the table's provider is hive, when execute sql:
{code:java}
SELECT * FROM tpcds_orc_mintest.store_sales a join tpcds_orc_mintest.catalog_sales b on a.ss_item_sk = b.cs_item_sk limit 1000;
{code}
Spark do unnecessary shuffle before SortMergeJoin. I found that when create LogicalRelation for the hive table in HiveMetastoreCatalog.covertToLogicalRelation, the bucketSpec is missing.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 12 13:03:42 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tuds:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Aug/21 13:03;apachespark;User 'zengruios' has created a pull request for this issue:
https://github.com/apache/spark/pull/33725;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Bug when creating dataframe without schema and with Arrow disabled
Issue key: SPARK-35211
Issue id: 13374786
Parent id: 13361974.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: sadhen
Creator: sadhen
Created: 24/Apr/21 07:53
Updated: 27/Jul/21 08:40
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: correctness
Description: A reproducible small repo can be found here: https://github.com/darcy-shen/spark-36283

h2. Case 1: Create PySpark Dataframe using Pandas DataFrame with Arrow  disabled and without schema

{code:python}
spark = SparkSession.builder.getOrCreate()
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "false")
pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
df = spark.createDataFrame(pdf)
df.show()
{code}

h3. Incorrect result
{code}
+----------+
|     point|
+----------+
|(0.0, 0.0)|
|(0.0, 0.0)|
+----------+
{code}

h2. Case 2: Create PySpark Dataframe using Pandas DataFrame with Arrow disabled and with unmatched schema

{code}
spark = SparkSession.builder.getOrCreate()
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "false")
pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
schema = StructType([StructField('point', ExamplePointUDT(), False)])
df = spark.createDataFrame(pdf, schema)
df.show()
{code}

h3. Error throwed as expected
{code}
Traceback (most recent call last):
  File "bug2.py", line 54, in <module>
    df = spark.createDataFrame(pdf, schema)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/session.py", line 673, in createDataFrame
    return super(SparkSession, self).createDataFrame(
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py", line 300, in createDataFrame
    return self._create_dataframe(data, schema, samplingRatio, verifySchema)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/session.py", line 700, in _create_dataframe
    rdd, schema = self._createFromLocal(map(prepare, data), schema)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/session.py", line 509, in _createFromLocal
    data = list(data)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/session.py", line 682, in prepare
    verify_func(obj)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/types.py", line 1390, in verify_struct
    verifier(v)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/types.py", line 1304, in verify_udf
    verifier(dataType.toInternal(obj))
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/types.py", line 1354, in verify_array
    element_verifier(i)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/types.py", line 1403, in verify_default
    verify_acceptable_types(obj)
  File "/Users/da/.pyenv/versions/spark-36283/lib/python3.8/site-packages/pyspark/sql/types.py", line 1291, in verify_acceptable_types
    raise TypeError(new_msg("%s can not accept object %r in type %s"
TypeError: element in array field point: DoubleType can not accept object 1 in type <class 'int'>
{code}


h2. Case 3: Create PySpark Dataframe using Pandas DataFrame with Arrow disabled and with matched schema

{code}
spark = SparkSession.builder.getOrCreate()
spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", "false")
pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1.0, 1.0), ExamplePoint(2.0, 2.0)])})
schema = StructType([StructField('point', ExamplePointUDT(), False)])
df = spark.createDataFrame(pdf, schema)
df.show()
{code}

h3. Correct result
{code}
+----------+
|     point|
+----------+
|(1.0, 1.0)|
|(2.0, 2.0)|
+----------+
{code}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 27 08:40:39 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qemg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Apr/21 08:51;sadhen;With schema provided, it works fine.
{code}
(spark) ➜  spark git:(sadhen/SPARK-35211) ✗ bin/pyspark
Python 3.8.8 (default, Feb 24 2021, 13:46:16)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/04/24 16:49:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0-SNAPSHOT
      /_/

Using Python version 3.8.8 (default, Feb 24 2021 13:46:16)
Spark context Web UI available at http://172.30.0.12:4040
Spark context available as 'sc' (master = local[*], app id = local-1619254179325).
SparkSession available as 'spark'.
>>> spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", False)
>>> from pyspark.testing.sqlutils  import ExamplePoint, ExamplePointUDT
>>> from pyspark.sql.types import StructType, StructField
>>> import pandas as pd
>>> schema = StructType([StructField('point', ExamplePointUDT(), False)])
>>> pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1.0, 1.0), ExamplePoint(2.0, 2.0)])})
>>> df = spark.createDataFrame(pdf, schema)
>>>
>>> df.show()
+----------+
|     point|
+----------+
|(1.0, 1.0)|
|(2.0, 2.0)|
+----------+
{code};;;, 24/Apr/21 09:12;sadhen;Provided with schema, strict type check will be performed:
{code}
(spark) ➜  spark git:(sadhen/SPARK-35211) ✗ bin/pyspark
Python 3.8.8 (default, Feb 24 2021, 13:46:16)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/04/24 17:10:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0-SNAPSHOT
      /_/

Using Python version 3.8.8 (default, Feb 24 2021 13:46:16)
Spark context Web UI available at http://172.30.0.12:4040
Spark context available as 'sc' (master = local[*], app id = local-1619255429482).
SparkSession available as 'spark'.
>>> spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", False)
>>> from pyspark.testing.sqlutils  import ExamplePoint, ExamplePointUDT
>>> from pyspark.sql.types import StructType, StructField
>>> import pandas as pd
>>> schema = StructType([StructField('point', ExamplePointUDT(), False)])
>>> # pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1.0, 1.0), ExamplePoint(2.0, 2.0)])})
>>> pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
>>> df = spark.createDataFrame(pdf, schema)
result: _convert_from_pandas
[(ExamplePoint(1,1),), (ExamplePoint(2,2),)]
StructType(List(StructField(point,ExamplePointUDT,false)))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/da/github/apache/spark/python/pyspark/sql/session.py", line 678, in createDataFrame
    return super(SparkSession, self).createDataFrame(
  File "/Users/da/github/apache/spark/python/pyspark/sql/pandas/conversion.py", line 343, in createDataFrame
    return self._create_dataframe(data, schema, samplingRatio, verifySchema)
  File "/Users/da/github/apache/spark/python/pyspark/sql/session.py", line 705, in _create_dataframe
    rdd, schema = self._createFromLocal(map(prepare, data), schema)
  File "/Users/da/github/apache/spark/python/pyspark/sql/session.py", line 509, in _createFromLocal
    data = list(data)
  File "/Users/da/github/apache/spark/python/pyspark/sql/session.py", line 687, in prepare
    verify_func(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1390, in verify_struct
    verifier(v)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1304, in verify_udf
    verifier(dataType.toInternal(obj))
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1354, in verify_array
    element_verifier(i)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1403, in verify_default
    verify_acceptable_types(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1291, in verify_acceptable_types
    raise TypeError(new_msg("%s can not accept object %r in type %s"
TypeError: element in array field point: DoubleType can not accept object 1 in type <class 'int'>
{code};;;, 24/Apr/21 09:52;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32319;;;, 24/Apr/21 10:22;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32320;;;, 25/Apr/21 03:49;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32327;;;, 25/Apr/21 08:49;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32332;;;, 26/Jul/21 21:41;viirya;The Jira title looks more like a new feature/improvement. But from the description and the PR, looks like it is a bug? Could you also update with a proper the Jira title? Thanks.;;;, 27/Jul/21 08:40;sadhen;Updated;;;
Affects Version/s.1: 
Comment.1: 24/Apr/21 09:12;sadhen;Provided with schema, strict type check will be performed:
{code}
(spark) ➜  spark git:(sadhen/SPARK-35211) ✗ bin/pyspark
Python 3.8.8 (default, Feb 24 2021, 13:46:16)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/04/24 17:10:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0-SNAPSHOT
      /_/

Using Python version 3.8.8 (default, Feb 24 2021 13:46:16)
Spark context Web UI available at http://172.30.0.12:4040
Spark context available as 'sc' (master = local[*], app id = local-1619255429482).
SparkSession available as 'spark'.
>>> spark.conf.set("spark.sql.execution.arrow.pyspark.enabled", False)
>>> from pyspark.testing.sqlutils  import ExamplePoint, ExamplePointUDT
>>> from pyspark.sql.types import StructType, StructField
>>> import pandas as pd
>>> schema = StructType([StructField('point', ExamplePointUDT(), False)])
>>> # pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1.0, 1.0), ExamplePoint(2.0, 2.0)])})
>>> pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
>>> df = spark.createDataFrame(pdf, schema)
result: _convert_from_pandas
[(ExamplePoint(1,1),), (ExamplePoint(2,2),)]
StructType(List(StructField(point,ExamplePointUDT,false)))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/Users/da/github/apache/spark/python/pyspark/sql/session.py", line 678, in createDataFrame
    return super(SparkSession, self).createDataFrame(
  File "/Users/da/github/apache/spark/python/pyspark/sql/pandas/conversion.py", line 343, in createDataFrame
    return self._create_dataframe(data, schema, samplingRatio, verifySchema)
  File "/Users/da/github/apache/spark/python/pyspark/sql/session.py", line 705, in _create_dataframe
    rdd, schema = self._createFromLocal(map(prepare, data), schema)
  File "/Users/da/github/apache/spark/python/pyspark/sql/session.py", line 509, in _createFromLocal
    data = list(data)
  File "/Users/da/github/apache/spark/python/pyspark/sql/session.py", line 687, in prepare
    verify_func(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1390, in verify_struct
    verifier(v)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1304, in verify_udf
    verifier(dataType.toInternal(obj))
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1354, in verify_array
    element_verifier(i)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1409, in verify
    verify_value(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1403, in verify_default
    verify_acceptable_types(obj)
  File "/Users/da/github/apache/spark/python/pyspark/sql/types.py", line 1291, in verify_acceptable_types
    raise TypeError(new_msg("%s can not accept object %r in type %s"
TypeError: element in array field point: DoubleType can not accept object 1 in type <class 'int'>
{code};;;
Comment.2: 24/Apr/21 09:52;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32319;;;
Comment.3: 24/Apr/21 10:22;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32320;;;
Comment.4: 25/Apr/21 03:49;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32327;;;
Comment.5: 25/Apr/21 08:49;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32332;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: ExternalBlockHandler metrics have misleading unit in the name
Issue key: SPARK-35259
Issue id: 13375666
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: xkrogen
Creator: xkrogen
Created: 28/Apr/21 18:25
Updated: 26/Jul/21 16:15
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: Today {{ExternalBlockHandler}} exposes a few {{Timer}} metrics:
{code}
    // Time latency for open block request in ms
    private final Timer openBlockRequestLatencyMillis = new Timer();
    // Time latency for executor registration latency in ms
    private final Timer registerExecutorRequestLatencyMillis = new Timer();
    // Time latency for processing fetch merged blocks meta request latency in ms
    private final Timer fetchMergedBlocksMetaLatencyMillis = new Timer();
    // Time latency for processing finalize shuffle merge request latency in ms
    private final Timer finalizeShuffleMergeLatencyMillis = new Timer();
{code}
However these Dropwizard Timers by default use nanoseconds ([documentation|https://metrics.dropwizard.io/3.2.3/getting-started.html#timers]). It's certainly possible to extract milliseconds from them, but it seems misleading to have millis in the name here.

This causes {{YarnShuffleServiceMetrics}} to expose confusingly-named metrics like {{openBlockRequestLatencyMillis_count}} and {{openBlockRequestLatencyMillis_nanos}}. It should be up to the metrics exporter, like {{YarnShuffleServiceMetrics}}, to decide the unit and adjust the name accordingly, so the unit shouldn't be included in the name of the metric itself.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-35258
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jul 26 16:15:40 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qk1s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Apr/21 18:37;xkrogen;I have a PR for this but it is based on the PR for SPARK-35258 so I will hold off posting it for now.

While that goes through -- [~rxin] or [~jlaskowski] -- I see you participated in the discussions on SPARK-16405 when these were added, do you have any comment here? Maybe I am missing something?;;;, 28/Jun/21 18:41;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33116;;;, 28/Jun/21 18:42;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33116;;;, 26/Jul/21 16:15;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33523;;;, 26/Jul/21 16:15;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33524;;;
Affects Version/s.1: 
Comment.1: 28/Jun/21 18:41;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33116;;;
Comment.2: 28/Jun/21 18:42;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33116;;;
Comment.3: 26/Jul/21 16:15;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33523;;;
Comment.4: 26/Jul/21 16:15;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33524;;;
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Add expression ToHiveString for keep consistent between hive/spark format in df.show and transform
Issue key: SPARK-35228
Issue id: 13374995
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 26/Apr/21 07:20
Updated: 20/Jul/21 05:31
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: According to [https://github.com/apache/spark/pull/32335#discussion_r620027850] 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Apr 27 10:10:56 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qfwo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Apr/21 10:09;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32365;;;, 27/Apr/21 10:10;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32365;;;
Affects Version/s.1: 
Comment.1: 27/Apr/21 10:10;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32365;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: [Dynamic allocation] Executor grace period (ExecutorIdleTimeout) ignored due to nulll startTime for pods in pending state
Issue key: SPARK-36042
Issue id: 13388313
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: apclement
Creator: apclement
Created: 08/Jul/21 07:42
Updated: 08/Jul/21 07:47
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: Pending executor are always timeouted due to null startTime and funtion returning true in case of exception in parsing startTime.

 

In class ExecutorPodsAllocator:

{{private def isExecutorIdleTimedOut(state: ExecutorPodState, currentTime: Long): Boolean = {}}
 {{try {}}
 {{ val startTime = Instant.parse(state.pod.getStatus.getStartTime).toEpochMilli()}}
 {{ currentTime - startTime > executorIdleTimeout}}
 {{catch {}}
 {{  case _: Exception =>}}
 {{  logDebug(s"Cannot get startTime of pod ${state.pod}")}}}}
 {{  true}}
 }}
Environment: AWS EKS with dynamic allocation 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-08 07:42:46.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0spx4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: spark.driver.log.dfsDir with hdfs scheme failed
Issue key: SPARK-35902
Issue id: 13385952
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yghu
Creator: yghu
Created: 26/Jun/21 01:49
Updated: 27/Jun/21 00:01
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: when i set spark.driver.log.dfsDir value with hdfs scheme path, it throw an exception:

spark.driver.log.persistToDfs.enabled = true

spark.driver.log.dfsDir = hdfs://hacluster/spark2xdriverlogs1

 

2021-06-25 14:56:45,786 | ERROR | main | Could not persist driver logs to dfs | org.apache.spark.util.logging.DriverLogger.logError(Logging.scala:94)
 java.lang.IllegalArgumentException: Pathname /opt/client811/Spark2x/spark/hdfs:/hacluster/spark2xdriverlogs1 from /opt/client811/Spark2x/spark/hdfs:/hacluster/spark2xdriverlogs1 is not a valid DFS filename.
 at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:252)
 at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1375)
 at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1372)
 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
 at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1389)
 at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1364)
 at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2410)
 at org.apache.spark.deploy.SparkHadoopUtil$.createFile(SparkHadoopUtil.scala:528)
 at org.apache.spark.util.logging.DriverLogger$DfsAsyncWriter.init(DriverLogger.scala:118)
 at org.apache.spark.util.logging.DriverLogger$DfsAsyncWriter.<init>(DriverLogger.scala:104)
 at org.apache.spark.util.logging.DriverLogger.startSync(DriverLogger.scala:72)
 at org.apache.spark.SparkContext.$anonfun$postApplicationStart$1(SparkContext.scala:2688)
 at org.apache.spark.SparkContext.$anonfun$postApplicationStart$1$adapted(SparkContext.scala:2688)
 at scala.Option.foreach(Option.scala:407)
 at org.apache.spark.SparkContext.postApplicationStart(SparkContext.scala:2688)
 at org.apache.spark.SparkContext.<init>(SparkContext.scala:640)
 at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2814)
 at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:947)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:941)
 at org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)
 at $line3.$read$$iw$$iw.<init>(<console>:15)
 at $line3.$read$$iw.<init>(<console>:42)
 at $line3.$read.<init>(<console>:44)
 at $line3.$read$.<init>(<console>:48)
 at $line3.$read$.<clinit>(<console>)
 at $line3.$eval$.$print$lzycompute(<console>:7)
 at $line3.$eval$.$print(<console>:6)
 at $line3.$eval.$print(<console>)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
 at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
 at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
 at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
 at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
 at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
 at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
 at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
 at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
 at scala.tools.nsc.interpreter.IMain.$anonfun$quietRun$1(IMain.scala:224)
 at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:214)
 at scala.tools.nsc.interpreter.IMain.quietRun(IMain.scala:224)
 at org.apache.spark.repl.SparkILoop.$anonfun$initializeSpark$2(SparkILoop.scala:83)
 at scala.collection.immutable.List.foreach(List.scala:392)
 at org.apache.spark.repl.SparkILoop.$anonfun$initializeSpark$1(SparkILoop.scala:83)
 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
 at scala.tools.nsc.interpreter.ILoop.savingReplayStack(ILoop.scala:99)
 at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:83)
 at org.apache.spark.repl.SparkILoop.$anonfun$process$4(SparkILoop.scala:165)
 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
 at scala.tools.nsc.interpreter.ILoop.$anonfun$mumly$1(ILoop.scala:168)
 at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:214)
 at scala.tools.nsc.interpreter.ILoop.mumly(ILoop.scala:165)
 at org.apache.spark.repl.SparkILoop.loopPostInit$1(SparkILoop.scala:153)
 at org.apache.spark.repl.SparkILoop.$anonfun$process$10(SparkILoop.scala:221)
 at org.apache.spark.repl.SparkILoop.withSuppressedSettings$1(SparkILoop.scala:189)
 at org.apache.spark.repl.SparkILoop.startup$1(SparkILoop.scala:201)
 at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:236)
 at org.apache.spark.repl.Main$.doMain(Main.scala:78)
 at org.apache.spark.repl.Main$.main(Main.scala:58)
 at org.apache.spark.repl.Main.main(Main.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:993)
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:183)
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:206)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:93)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1072)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1081)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

 
Environment: Spark3.1.1 Hadoop 3.1.1
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jun 27 00:01:47 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sbe8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Jun/21 01:50;yghu;I'd like to work on this.;;;, 26/Jun/21 02:16;dongjoon;Thank you for reporting, [~yghu]. Go for it!;;;, 27/Jun/21 00:01;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33104;;;
Affects Version/s.1: 3.1.1
Comment.1: 26/Jun/21 02:16;dongjoon;Thank you for reporting, [~yghu]. Go for it!;;;
Comment.2: 27/Jun/21 00:01;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33104;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: numPartitions does not work when saves the RDD to database
Issue key: SPARK-35892
Issue id: 13385787
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zengrui
Creator: zengrui
Created: 25/Jun/21 07:53
Updated: 25/Jun/21 14:12
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When use SQL to insert data in spark to database, suppose the original RDD's partition num is 10, and when i set the numPartitions to 20 in SQL(because i need more parallelism to insert data to database), but it does not work.

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jun 25 14:12:55 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sadk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Jun/21 14:12;apachespark;User 'zengruios' has created a pull request for this issue:
https://github.com/apache/spark/pull/33089;;;, 25/Jun/21 14:12;apachespark;User 'zengruios' has created a pull request for this issue:
https://github.com/apache/spark/pull/33089;;;
Affects Version/s.1: 
Comment.1: 25/Jun/21 14:12;apachespark;User 'zengruios' has created a pull request for this issue:
https://github.com/apache/spark/pull/33089;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Query performance degradation additional predicate and UDF call for explode
Issue key: SPARK-35882
Issue id: 13385692
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: johnhb
Creator: johnhb
Created: 24/Jun/21 17:49
Updated: 25/Jun/21 07:41
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: This issue cannot be seen in 3.0.1, but has been introduced since and observed in 3.1.1 and 3.1.2. I have a reproduce for this issue here: [https://github.com/johnbateman/spark-udf-slowdown] just change the sbt file between 3.1.2 and 3.0.1 to observe the difference in performance. It is a rather silly example but it demonstrates the issue.

Physical plan for 3.0.1, it executes on my machine in about 40 seconds.

 
{code:java}
== Physical Plan == Generate explode(fib#3), [id#1L, fib#3], false, [fib2#7] 
+- *(1) Project [id#1L, UDF(cast(id#1L as int)) AS fib#3] 
  +- *(1) Range (1, 500000, step=1, splits=8)
{code}
 

Physical plan for 3.1.2, it executes on my machine in about 4.7 min.

 
{code:java}
== Physical Plan ==
Generate (4)
+- * Project (3)
   +- * Filter (2)
      +- * Range (1)

(1) Range [codegen id : 1]
Output [1]: [id#2L]
Arguments: Range (1, 500000, step=1, splits=Some(8))

(2) Filter [codegen id : 1]
Input [1]: [id#2L]
Condition : ((size(UDF(cast(id#2L as int)), true) > 0) AND isnotnull(UDF(cast(id#2L as int))))

(3) Project [codegen id : 1]
Output [2]: [id#2L, UDF(cast(id#2L as int)) AS fib#4]
Input [1]: [id#2L]

(4) Generate
Input [2]: [id#2L, fib#4]
Arguments: explode(fib#4), [id#2L, fib#4], false, [fib2#11]{code}
 

You can see that there is an additional predicate generated in step 2, I can also confirm that the UDF is now called multiple times instead of once. I am aware that this is to be expected sometimes, but it is a change that has resulted in performance degradation particularly for expensive UDFs. Obviously, there is something specific to this query (ie the explode) that seems to be responsible for this predicate and UDF issue occurring, but I am not sure what that is.

For reference, this is the same issue (https://issues.apache.org/jira/browse/SPARK-35787)
Environment: Present on local ubuntu machine. Also CentOS VMs.
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): SPARK-35787
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jun 25 07:41:49 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s9sg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Jun/21 07:41;vdrasutis;Maybe it could be related to https://issues.apache.org/jira/browse/SPARK-35767 ?;;;
Affects Version/s.1: 3.1.2
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0

Summary: SparkThriftServer can not start when HMS use DBTokenStore
Issue key: SPARK-35875
Issue id: 13385607
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: YulongZ
Creator: YulongZ
Created: 24/Jun/21 11:36
Updated: 24/Jun/21 11:36
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: CMD:
./sbin/start-thriftserver.sh --master=local --conf spark.sql.hive.metastore.version=3.1.2 --conf spark.sql.hive.metastore.jars=path --conf spark.sql.hive.metastore.jars.path="file:///tmp/metastorejars/*.jar"


ERROR LOG:
21/06/24 16:43:09 ERROR HiveThriftServer2: Error starting HiveThriftServer2
org.apache.hive.service.ServiceException: Failed to Start HiveServer2
        at org.apache.hive.service.CompositeService.start(CompositeService.java:80)
        at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:105)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.start(HiveThriftServer2.scala:154)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.startWithContext(HiveThriftServer2.scala:64)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:104)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1030)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1039)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hive.service.ServiceException: Error initializing ThriftBinaryCLIService
        at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.initializeServer(ThriftBinaryCLIService.java:113)
        at org.apache.hive.service.cli.thrift.ThriftCLIService.start(ThriftCLIService.java:177)
        at org.apache.hive.service.CompositeService.start(CompositeService.java:70)
        ... 17 more
Caused by: org.apache.thrift.transport.TTransportException: Failed to start token manager
        at org.apache.hive.service.auth.HiveAuthFactory.<init>(HiveAuthFactory.java:167)
        at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.initializeServer(ThriftBinaryCLIService.java:66)
        ... 19 more
Caused by: java.io.IOException: Failed to initialize master key
        at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.startThreads(TokenStoreDelegationTokenSecretManager.java:213)
        at org.apache.hadoop.hive.thrift.HiveDelegationTokenManager.startDelegationTokenSecretManager(HiveDelegationTokenManager.java:97)
        at org.apache.hive.service.auth.HiveAuthFactory.<init>(HiveAuthFactory.java:162)
        ... 20 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.startThreads(TokenStoreDelegationTokenSecretManager.java:211)
        ... 22 more
Caused by: java.lang.ClassCastException: com.sun.proxy.$Proxy45 cannot be cast to java.lang.Class
        at org.apache.hadoop.hive.thrift.DBTokenStore.invokeOnTokenStore(DBTokenStore.java:154)
        at org.apache.hadoop.hive.thrift.DBTokenStore.addMasterKey(DBTokenStore.java:43)
        at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.logUpdateMasterKey(TokenStoreDelegationTokenSecretManager.java:193)
        at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.updateCurrentKey(AbstractDelegationTokenSecretManager.java:364)
        ... 27 more

Environment: Spark 3.1.1
Hive metastore 3.1.2
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-06-24 11:36:19.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s99k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Do not drop cached RDD blocks to accommodate blocks from decommissioned block manager if enough memory is not available
Issue key: SPARK-35533
Issue id: 13380516
Parent id: 13069723.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: abhishek_tiwari
Creator: abhishek_tiwari
Created: 26/May/21 10:27
Updated: 03/Jun/21 18:58
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In current block manager decommissioning flow, existing cached blocks in memory are dropped if enough memory is not available to accommodate blocks from decommissioned block manager.

 

Why should blocks from a decommissioned block manager have more priority than an already cached block? 

We should place blocks from decommission block manager on a peer block manager only when enough memory is available
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed May 26 10:39:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rdxk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/May/21 10:38;apachespark;User 'q2w' has created a pull request for this issue:
https://github.com/apache/spark/pull/32677;;;, 26/May/21 10:39;apachespark;User 'q2w' has created a pull request for this issue:
https://github.com/apache/spark/pull/32677;;;
Affects Version/s.1: 
Comment.1: 26/May/21 10:39;apachespark;User 'q2w' has created a pull request for this issue:
https://github.com/apache/spark/pull/32677;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Optional source codes uploading in k8s
Issue key: SPARK-35534
Issue id: 13380523
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dizie
Creator: dizie
Created: 26/May/21 11:03
Updated: 26/May/21 11:51
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: It seems there is currently a hard requirement on uploading files in k8s mode via {{spark.kubernetes.file.upload.path config option that actually doesn't make sense if your docker image already contains all of the necessary files.}}

I would propose a local:// extension to spark.kubernetes.file.upload.path, that will point to the local folder containing files without their actual upload.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-05-26 11:03:18.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rdz4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: structured streaming multiple queries can not be execute concurrently 
Issue key: SPARK-35484
Issue id: 13379834
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jianxu@xtronica.no
Creator: jianxu@xtronica.no
Created: 21/May/21 18:26
Updated: 26/May/21 06:46
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Hi There,

 

I have similar problem as described in an earlier issue reported issue of SPARK-23766. The issue was closed. though would like to know if multiple queries are supported in structured streaming and how to manage them.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-05-21 18:26:53.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0r9qg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Define UDTs in schemas using string format
Issue key: SPARK-35525
Issue id: 13380444
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jshalaby
Creator: jshalaby
Created: 26/May/21 02:16
Updated: 26/May/21 06:31
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: schema, SparkSQL, UDF, UDT
Description: In PySpark where UDTs are public in 3.1.1 for example, you can define a schema using UDTs in the format:

schema = StructType([StructField("Stuff", MyUDT())])

but the format

schema = "Stuff MyUDT"

does not work.

UDTs are officially being made public again in 3.2.0 for Scala, so this issue is pretty important now.
Environment: 
Original Estimate: 10800.0
Remaining Estimate: 10800.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 10800.0
Σ Remaining Estimate: 10800.0
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-05-26 02:16:25.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rdhk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: no rowcount estimation returned if groupby clause involves substring
Issue key: SPARK-22639
Issue id: 13121497
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: eychih
Creator: eychih
Created: 29/Nov/17 00:33
Updated: 25/May/21 07:26
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.2.0, 3.1.1
Fix Version/s: 
Component/s: Optimizer, SQL
Due Date: 
Votes: 0
Labels: 
Description: CBO can not estimate rowcount if the groupby clause of a query involves the expression substring.  For example, we can not estimate the row count of the following query, extracted from TPC-DS queries and based on the TPC-DS schema:

SELECT item.`i_brand`, count(1), date_dim.`d_year`, item.`i_brand_id`, sum(store_sales.`ss_ext_sales_price`) AS `ext_price`, item.`i_item_sk`   
FROM  store_sales  INNER JOIN date_dim ON (date_dim.`d_date_sk` = store_sales.`ss_sold_date_sk`)  INNER JOIN item ON (store_sales.`ss_item_sk` = item.`i_item_sk`)  
GROUP BY item.`i_brand`, date_dim.`d_date`, substring(item.`i_item_desc`, 1, 30), date_dim.`d_year`, item.`i_brand_id`, item.`i_item_sk`
  
Environment: 
Original Estimate: 1814400.0
Remaining Estimate: 1814400.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 1814400.0
Σ Remaining Estimate: 1814400.0
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important, Patch
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue May 25 07:20:47 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|i3nbe7:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/May/21 07:19;apachespark;User 'shahidki31' has created a pull request for this issue:
https://github.com/apache/spark/pull/32659;;;, 25/May/21 07:20;shahid;This is happening with latest master as well;;;
Affects Version/s.1: 3.1.1
Comment.1: 25/May/21 07:20;shahid;This is happening with latest master as well;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Spark Streaming LocationStrategy should provide a random option that mapping kafka partitions randomly to spark executors
Issue key: SPARK-35212
Issue id: 13374829
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tiehexue
Creator: tiehexue
Created: 24/Apr/21 16:34
Updated: 23/May/21 20:47
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: DStreams, Spark Core
Due Date: 
Votes: 0
Labels: pull-request-available
Description: There are three LocationStrategy: PreferBrokers, PreferConsistent, PreferFixed. I got a scenario that I need a random one. There are plenty of topic partitions that are varies from each other with different records inside. And I have a lot of executors. PreferBrokers does not help here. PreferConsistent will make things worse that some executor will always get heavy tasks. PreferFixed does not help too, because it is fixed, neither to say I have to create a mapping manually.

A random LocationStrategy should dispatch a topic partition to different executors in different window. This would balance the load among spark executors.
Environment: 
Original Estimate: 7200.0
Remaining Estimate: 7200.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 7200.0
Σ Remaining Estimate: 7200.0
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Apr 26 02:52:44 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qew0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Apr/21 02:03;apachespark;User 'tiehexue' has created a pull request for this issue:
https://github.com/apache/spark/pull/32326;;;, 25/Apr/21 02:48;tiehexue;By the way, after looking into getPreferredLocations method of KafkaRDD, the method implementation takes "PreferConsitent" logic as default. We also could change KafkaRDD's constructor, replace "preferredHosts" with LocationStrategy, and implement getPreferredLocations with "random" as default.;;;, 26/Apr/21 02:51;apachespark;User 'tiehexue' has created a pull request for this issue:
https://github.com/apache/spark/pull/32341;;;, 26/Apr/21 02:52;apachespark;User 'tiehexue' has created a pull request for this issue:
https://github.com/apache/spark/pull/32341;;;
Affects Version/s.1: 
Comment.1: 25/Apr/21 02:48;tiehexue;By the way, after looking into getPreferredLocations method of KafkaRDD, the method implementation takes "PreferConsitent" logic as default. We also could change KafkaRDD's constructor, replace "preferredHosts" with LocationStrategy, and implement getPreferredLocations with "random" as default.;;;
Comment.2: 26/Apr/21 02:51;apachespark;User 'tiehexue' has created a pull request for this issue:
https://github.com/apache/spark/pull/32341;;;
Comment.3: 26/Apr/21 02:52;apachespark;User 'tiehexue' has created a pull request for this issue:
https://github.com/apache/spark/pull/32341;;;
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Pyspark - Using importlib + filter + named function + take causes pyspark to restart continuously until machine runs out of memory
Issue key: SPARK-35336
Issue id: 13377244
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: rajraj
Creator: rajraj
Created: 07/May/21 13:52
Updated: 07/May/21 13:53
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.1.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Repo to reproduce issue

[https://github.com/CanaryWharf/pyspark-mem-importlib-bug-reproduction]

 

Expected behavour:

Program runs and exits cleanly

 

Actual behaviour:

Program runs forever, eating up all the memory on the machine

 

Steps to reproduce:

```

pip install -r requirements.txt

python run.py

``` 

The problem only occurs if you run the code via `importlib`. The problem does not occur running `sparky.py` directly.

Furthermore, the problem occurs if you replace filter with map or flatMap (anything that takes in a lambda function).

The problem only occurs if you call a named function (i.e., when you use `def func`).

So these break:

```

def func(stuff):

    return True

 

dataset.filter(func)

 ```

 

```

def func(stuff):

    return True

 

dataset.filter(lambda s: func(s))

 ```

 

The problem does *NOT* occur if you do this:

```

dataset.filter(lambda x: True)

```

```

dataset.filter(lambda x: x == 'stuff')

 ```
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-05-07 13:52:29.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qtrc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Use 'SPARK_LOG_URL_' as env prefix for getting driver log urls by default
Issue key: SPARK-35328
Issue id: 13377044
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: sharkd
Creator: sharkd
Created: 06/May/21 14:03
Updated: 06/May/21 14:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Currently, spark on kubernetes can't show logs url on ui. To check history logs, we usually collect pod logs to third-party logging services, which can be accessed by urls. To show log urls,  we can set env prefixed with 'SPARK_LOG_URL_' for executors. But for driver, there is no way to show log urls by set env. 

 

I will create a new pr that use 'SPARK_LOG_URL_' as env prefix for getting driver log urls by default.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu May 06 14:11:57 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qsj4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/May/21 14:11;apachespark;User 'sharkdtu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32456;;;, 06/May/21 14:11;apachespark;User 'sharkdtu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32456;;;
Affects Version/s.1: 
Comment.1: 06/May/21 14:11;apachespark;User 'sharkdtu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32456;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: After CrossValidator/TrainValidationSplit fit raised error, some backgroud threads may still continue run or launch new spark jobs
Issue key: SPARK-35271
Issue id: 13375802
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: weichenxu123
Reporter: weichenxu123
Creator: weichenxu123
Created: 29/Apr/21 10:06
Updated: 30/Apr/21 06:06
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.1.1
Fix Version/s: 
Component/s: ML, PySpark
Due Date: 
Votes: 0
Labels: 
Description: CrossValidator/TrainValidationSplit fit will spawn many threads to run trial tasks.
if one of the trial tasks failed, the CrossValidator/TrainValidationSplit fit will raise error and break the main thread, but other backgroud threads running other trial tasks will continue to run, and trial tasks which are pending to run in thread queue will also continue to launch.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Apr 30 06:05:55 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qkw0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Apr/21 06:05;apachespark;User 'WeichenXu123' has created a pull request for this issue:
https://github.com/apache/spark/pull/32399;;;
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Remove the use of guava in order to upgrade guava version to 27
Issue key: SPARK-35270
Issue id: 13375763
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wforget
Creator: wforget
Created: 29/Apr/21 07:10
Updated: 30/Apr/21 05:53
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core, SQL
Due Date: 
Votes: 0
Labels: 
Description: Hadoop 3.2.2 Guava version is 27.0-jre. A compilation error occurred after modifying the guava version to 27.0-jre.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Apr 29 08:49:29 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qknc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Apr/21 08:21;wforget;Modify with reference to https://issues.apache.org/jira/browse/SPARK-30272;;;, 29/Apr/21 08:49;apachespark;User 'wForget' has created a pull request for this issue:
https://github.com/apache/spark/pull/32395;;;
Affects Version/s.1: 
Comment.1: 29/Apr/21 08:49;apachespark;User 'wForget' has created a pull request for this issue:
https://github.com/apache/spark/pull/32395;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Spark shall load system class first in IsolatedClientLoader
Issue key: SPARK-35248
Issue id: 13375446
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: daijy
Creator: daijy
Created: 27/Apr/21 21:14
Updated: 28/Apr/21 01:49
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.4.7, 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: This happens when Spark try to load HMS client jars using IsolatedClientLoader, in particular, when "spark.sql.hive.metastore.jars"="builtin" (default). Spark try to load a conflicting thrift lib in user jar, which results in exception:
{code:java}
 21/04/22 04:10:53 ERROR [main] yarn.Client: Application diagnostics message: User class threw exception: org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:106)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:220)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)
	at org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:141)
	at org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:136)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.isTemporaryTable(SessionCatalog.scala:738)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.isRunningDirectlyOnFiles(Analyzer.scala:748)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:682)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:714)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:707)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:707)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:653)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:76)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:644)
	at com.pinterest.pintel.homefeed.pinnability.CreateFollowingScorpionDataInstances$.run(CreateFollowingScorpionDataInstances.scala:95)
	at com.pinterest.pintel.homefeed.pinnability.CreateFollowingScorpionDataInstances$.main(CreateFollowingScorpionDataInstances.scala:59)
	at com.pinterest.pintel.homefeed.pinnability.CreateFollowingScorpionDataInstances.main(CreateFollowingScorpionDataInstances.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:684)
Caused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:528)
	at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:187)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:121)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:271)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:384)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:221)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:221)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:221)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	... 71 more
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1532)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3066)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3085)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:509)
	... 86 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1530)
	... 92 more
Caused by: java.lang.NoSuchMethodError: com.facebook.fb303.FacebookService$Client.<init>(Lorg/apache/thrift/protocol/TProtocol;Lorg/apache/thrift/protocol/TProtocol;)V
	at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.<init>(ThriftHiveMetastore.java:552)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:424)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:238)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 97 more
{code}
By convention, Spark shall use system jars first unless spark.driver.userClassPathFirst is defined. The current behavior of IsolatedClientLoader breaks the contract and we'd like to fix.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Apr 27 21:17:44 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qiow:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Apr/21 21:17;apachespark;User 'daijyc' has created a pull request for this issue:
https://github.com/apache/spark/pull/32373;;;, 27/Apr/21 21:17;apachespark;User 'daijyc' has created a pull request for this issue:
https://github.com/apache/spark/pull/32373;;;
Affects Version/s.1: 3.1.1
Comment.1: 27/Apr/21 21:17;apachespark;User 'daijyc' has created a pull request for this issue:
https://github.com/apache/spark/pull/32373;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: [SPARK-35222] In YARN mode, for better user experience, when Spark is started, not only the AppID is printed, but the Tracking URL is also printed to allow users to better track Spark Job
Issue key: SPARK-35222
Issue id: 13374898
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: runlin
Creator: runlin
Created: 25/Apr/21 10:39
Updated: 25/Apr/21 11:03
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.4.3, 3.1.1
Fix Version/s: 3.1.1
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In YARN mode, for better user experience, when Spark is started, not only the AppID is printed, but the Tracking URL is also printed to allow users to better track Spark Job
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Apr 25 11:03:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qfbc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.1.1
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Apr/21 11:03;apachespark;User 'runlin512' has created a pull request for this issue:
https://github.com/apache/spark/pull/32336;;;
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: YearMonthIntervalType/DayTimeIntervalType 's string value not same in spark default serde and hive serde
Issue key: SPARK-35219
Issue id: 13374892
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 25/Apr/21 10:00
Updated: 25/Apr/21 10:00
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: YearMonthIntervalType/DayTimeIntervalType 's string value not same in spark default serde and hive serde
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-04-25 10:00:54.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qfa0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Support UDT for Pandas/Spark conversion with Arrow support Enabled
Issue key: SPARK-34771
Issue id: 13365721
Parent id: 13361974.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: sadhen
Creator: sadhen
Created: 17/Mar/21 06:42
Updated: 25/Apr/21 03:38
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.2, 3.1.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: {code:python}
spark.conf.set("spark.sql.execution.arrow.enabled", "true")
from pyspark.testing.sqlutils  import ExamplePoint
import pandas as pd
pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
df = spark.createDataFrame(pdf)
df.toPandas()
{code}

with `spark.sql.execution.arrow.enabled` = false, the above snippet works fine without WARNINGS.

with `spark.sql.execution.arrow.enabled` = true, the above snippet works fine with WARNINGS. Because of Unsupported type in conversion, the Arrow optimization is actually turned off. 


Detailed steps to reproduce:

{code:python}
$ bin/pyspark
Python 3.8.8 (default, Feb 24 2021, 13:46:16)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/03/17 23:13:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0-SNAPSHOT
      /_/

Using Python version 3.8.8 (default, Feb 24 2021 13:46:16)
Spark context Web UI available at http://172.30.0.226:4040
Spark context available as 'sc' (master = local[*], app id = local-1615994008526).
SparkSession available as 'spark'.
>>> spark.conf.set("spark.sql.execution.arrow.enabled", "true")
21/03/17 23:13:31 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.
>>> from pyspark.testing.sqlutils  import ExamplePoint
>>> import pandas as pd
>>> pdf = pd.DataFrame({'point': pd.Series([ExamplePoint(1, 1), ExamplePoint(2, 2)])})
>>> df = spark.createDataFrame(pdf)
/Users/da/github/apache/spark/python/pyspark/sql/pandas/conversion.py:332: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Could not convert (1,1) with type ExamplePoint: did not recognize Python value type when inferring an Arrow data type
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warnings.warn(msg)
>>>
>>> df.show()
+----------+
|     point|
+----------+
|(0.0, 0.0)|
|(0.0, 0.0)|
+----------+

>>> df.schema
StructType(List(StructField(point,ExamplePointUDT,true)))
>>> df.toPandas()
/Users/da/github/apache/spark/python/pyspark/sql/pandas/conversion.py:87: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  Unsupported type in conversion to Arrow: ExamplePointUDT
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warnings.warn(msg)
       point
0  (0.0,0.0)
1  (0.0,0.0)

{code}





Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Apr 25 03:34:36 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ovfk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Mar/21 01:09;apachespark;User 'eddyxu' has created a pull request for this issue:
https://github.com/apache/spark/pull/31735;;;, 01/Apr/21 12:54;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32026;;;, 24/Apr/21 10:29;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32321;;;, 25/Apr/21 03:34;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32327;;;, 25/Apr/21 03:34;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32327;;;
Affects Version/s.1: 3.1.1
Comment.1: 01/Apr/21 12:54;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32026;;;
Comment.2: 24/Apr/21 10:29;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32321;;;
Comment.3: 25/Apr/21 03:34;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32327;;;
Comment.4: 25/Apr/21 03:34;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/32327;;;
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Make graceful shutdown controllable by property
Issue key: SPARK-34997
Issue id: 13370499
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: olkuznsmith
Creator: olkuznsmith
Created: 08/Apr/21 22:42
Updated: 22/Apr/21 16:14
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: I'm using Spark in embedded mode, i.e. Spark is one of many components in my app.

Currently, when JVM received sigterm, Spark immediately start shutting down.

 

Expected: provide client code with an option to control how and when to shutdown Spark app, i.e. create a config to disable unconditional shutdown by Spark itself.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Apr 22 16:14:37 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0pov4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Apr/21 16:14;holden;We could add a configuration for which signals spark should enable graceful shutdown.;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Execute jdbc cancellation method when jdbc load job is interrupted
Issue key: SPARK-35126
Issue id: 13373325
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zhangrenhua
Creator: zhangrenhua
Created: 18/Apr/21 07:51
Updated: 21/Apr/21 10:38
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I have a long-running spark service that continuously receives and runs spark programs submitted by the client. There is a program to load jdbc table. Query sql is very complicated. Each execution takes a lot of time and resources. When the client submits such a similar request, the client may interrupt the job at any time. At that time, I found that the database select after the job was interrupted. The process is still executing and has not been killed.

 

*Scene demonstration:*

1. Prepare two tables: SPARK_TEST1/SPARK_TEST2, each of which has 1000 records)

2. Test code
{code:java}
import org.apache.spark.SparkConf;
import org.apache.spark.SparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import java.util.concurrent.TimeUnit;

/**
 * jdbc load cancel test
 *
 * @author gavin
 * @create 2021/4/18 10:58
 */
public class JdbcLoadCancelTest {

    public static void main(String[] args) throws Exception {
        final SparkConf sparkConf = new SparkConf();
        sparkConf.setAppName("jdbc load test");
        sparkConf.setMaster("local[*]");
        final SparkContext sparkContext = new SparkContext(sparkConf);
        final SparkSession sparkSession = new SparkSession(sparkContext);

        // This is a sql that takes about a minute to execute
        String querySql = "select t1.*\n" +
                "from SPARK_TEST1 t1\n" +
                "left join SPARK_TEST1 t2 on 1=1\n" +
                "left join (select aa from SPARK_TEST1 limit 3) t3  on 1=1";

        // Specify job information
        final String jobGroup = "test";
        sparkContext.clearJobGroup();
        sparkContext.setJobGroup(jobGroup, "test", true);

        // Start the independent thread to start the jdbc load test logic
        new Thread(() -> {
            final Dataset<Row> table = sparkSession.read()
                    .format("org.apache.spark.sql.execution.datasources.jdbc3")
                    .option("url", "jdbc:mysql://192.168.10.226:32320/test?useUnicode=true&characterEncoding=utf-8&useSSL=false")
                    .option("user", "root")
                    .option("password", "123456")
                    .option("query", querySql)
                    .load();

            // Print the first data
            System.out.println(table.limit(1).first());
        }).start();

        // Wait for the jdbc load job to start
        TimeUnit.SECONDS.sleep(10);

        // Cancel the job just now
        sparkContext.cancelJobGroup(jobGroup);

        // Simulate a long-running service without stopping the driver process, which is used to wait for new jobs to be received
        TimeUnit.SECONDS.sleep(Integer.MAX_VALUE);
    }
}
{code}
 

3. View the mysql process
{code:java}
select * from information_schema.`PROCESSLIST` where info is not null;{code}
When the program started 10 seconds later, and interrupted the job, it was found that the database query process has not been killed.

 

 

 
Environment: Environment version:
 * spark3.1.1
 * jdk1.8.201
 * scala2.12
 * mysql5.7.31
 * mysql-connector-java-5.1.32.jar /mysql-connector-java-8.0.32.jar
Original Estimate: 7200.0
Remaining Estimate: 7200.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 7200.0
Σ Remaining Estimate: 7200.0
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 21 09:21:50 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0q5m8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): zhangrenhua
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Apr/21 08:28;zhangrenhua;The issues has been processed [pull request|https://github.com/apache/spark/pull/32222];;;, 18/Apr/21 08:30;apachespark;User 'zhangrenhua' has created a pull request for this issue:
https://github.com/apache/spark/pull/32222;;;, 19/Apr/21 16:33;zhangrenhua;I found that by setting FetchSize, can respond quickly and return the ResultSet object. If it is mysql database, you need to add the useCursorFetch=true parameter in the jdbc url. Thank you for your answer, I will close this pull request.;;;, 19/Apr/21 16:34;zhangrenhua;I found that by setting FetchSize, can respond quickly and return the ResultSet object. If it is mysql database, you need to add the useCursorFetch=true parameter in the jdbc url. Thank you for your answer, I will close this pull request.;;;, 21/Apr/21 09:21;zhangrenhua;I'm sorry I opened this question again. Because there were a lot of things in the previous two days, I closed this question in a hurry.
If a more complex query sql specifies the fetchsize parameter, it is also invalid, because sql logic processing takes a long time, so the ResultSet cursor will not be returned immediately.;;;
Affects Version/s.1: 
Comment.1: 18/Apr/21 08:30;apachespark;User 'zhangrenhua' has created a pull request for this issue:
https://github.com/apache/spark/pull/32222;;;
Comment.2: 19/Apr/21 16:33;zhangrenhua;I found that by setting FetchSize, can respond quickly and return the ResultSet object. If it is mysql database, you need to add the useCursorFetch=true parameter in the jdbc url. Thank you for your answer, I will close this pull request.;;;
Comment.3: 19/Apr/21 16:34;zhangrenhua;I found that by setting FetchSize, can respond quickly and return the ResultSet object. If it is mysql database, you need to add the useCursorFetch=true parameter in the jdbc url. Thank you for your answer, I will close this pull request.;;;
Comment.4: 21/Apr/21 09:21;zhangrenhua;I'm sorry I opened this question again. Because there were a lot of things in the previous two days, I closed this question in a hurry.
If a more complex query sql specifies the fetchsize parameter, it is also invalid, because sql logic processing takes a long time, so the ResultSet cursor will not be returned immediately.;;;
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Local mode fails to start cluster due to configuration value escape issue
Issue key: SPARK-35124
Issue id: 13373271
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: malthe
Creator: malthe
Created: 18/Apr/21 06:07
Updated: 19/Apr/21 18:06
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Java API
Due Date: 
Votes: 0
Labels: 
Description: At least on Windows and perhaps also other systems, running Spark in local mode fails to start a cluster when a configuration key contains a value with "&" (ampersand) in it.

This happens during the "getOrCreate()" call from a Spark session builder.

The reason seems to be incorrect or insufficient escaping since on Windows, the attempt to start a Spark process actually ends up running multiple commands for each occurrence of the "&" character.

On Windows specifically, the correct way to escape "&" would be "^&", but I have not been able yet to figure out exactly where the process is started and how the configuration is passed.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Apr 19 18:06:48 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0q5a8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Apr/21 18:06;malthe;The issue seems to stem from the way Python's "Popen" command handles arguments on Windows – basically it doesn't have a clue that "&" has a special meaning on Windows (and a number of other characters – in particular {{&}}, {{|}}, {{(}}, {{)}}, {{<}}, {{>}}, {{^}}).;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Spark App in container will not exit when exception happen
Issue key: SPARK-35000
Issue id: 13370510
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: KarlManong
Creator: KarlManong
Created: 09/Apr/21 01:00
Updated: 12/Apr/21 05:36
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Kubernetes, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: when submitting an app on k8s, someone can using an unauthed account to submit, and exception happened. But the app will not exit until 5 minutes later.
the log:
 !screenshot-1.png! 

the trace:
 [^21664.stack] 

I have create a PR:  https://github.com/apache/spark/pull/32101. Maybe help
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 09/Apr/21 01:05;KarlManong;21664.stack;https://issues.apache.org/jira/secure/attachment/13023586/21664.stack, 09/Apr/21 01:05;KarlManong;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13023585/screenshot-1.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Apr 12 05:35:53 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0poxk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Apr/21 01:13;KarlManong;[~dongjoon] please have a look, thanks;;;, 09/Apr/21 01:13;apachespark;User 'KarlManong' has created a pull request for this issue:
https://github.com/apache/spark/pull/32101;;;, 09/Apr/21 01:14;apachespark;User 'KarlManong' has created a pull request for this issue:
https://github.com/apache/spark/pull/32101;;;, 09/Apr/21 03:51;dongjoon;Thank you for filing a new JIRA and making a PR.
BTW, SPARK-34674 seems to fix this case together and be merged to the master. Could you check the master branch once more?;;;, 09/Apr/21 03:54;dongjoon;Oh, is `KubernetesClusterManager` itself not created in this case?;;;, 12/Apr/21 05:35;KarlManong;Got Exception when org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager#createSchedulerBackend.  
https://issues.apache.org/jira/browse/SPARK-34674 is reverted, so I haven't check it.;;;
Affects Version/s.1: 
Comment.1: 09/Apr/21 01:13;apachespark;User 'KarlManong' has created a pull request for this issue:
https://github.com/apache/spark/pull/32101;;;
Comment.2: 09/Apr/21 01:14;apachespark;User 'KarlManong' has created a pull request for this issue:
https://github.com/apache/spark/pull/32101;;;
Comment.3: 09/Apr/21 03:51;dongjoon;Thank you for filing a new JIRA and making a PR.
BTW, SPARK-34674 seems to fix this case together and be merged to the master. Could you check the master branch once more?;;;
Comment.4: 09/Apr/21 03:54;dongjoon;Oh, is `KubernetesClusterManager` itself not created in this case?;;;
Comment.5: 12/Apr/21 05:35;KarlManong;Got Exception when org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager#createSchedulerBackend.  
https://issues.apache.org/jira/browse/SPARK-34674 is reverted, so I haven't check it.;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Spark driver mistakenly classifies OOM error of executor (on K8s pod) as framework error
Issue key: SPARK-35006
Issue id: 13370618
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Nguyen Quang
Creator: Nguyen Quang
Created: 09/Apr/21 09:58
Updated: 09/Apr/21 10:05
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: *Issue Description*

I'm having a Spark application with 1 driver (on bare metal) + 1 executor (on K8s) (please note it's just for testing purpose). The corresponding configuration can be found bellowing.

When I run task for loading & computing on a XML file; due to the size of XML file is large (which I intended to) the executor got OOM error

#Aborting due to java.lang.OutOfMemoryError: Java heap space
 # 
 # A fatal error has been detected by the Java Runtime Environment:
 # 
 # Internal Error (debug.cpp:308), pid=19, tid=0x00007fff765ae700
 # fatal error: OutOfMemory encountered: Java heap space

 

However, the driver doesn't recognize this error as task failure scenario. Instead, it consider this as a framework issue and continue retrying the task

INFO TaskSchedulerImpl:57 - Executor 1 on 10.87.88.44 killed by driver.
 INFO TaskSetManager:57 - task 0.0 in stage 0.0 (TID 0) failed because while it was being computed, its executor exited for a reason unrelated to the task. *{color:#de350b}Not counting this failure towards the maximum number of failures for the task{color}.*
 INFO BlockManagerMasterEndpoint:57 - Trying to remove executor 1 from BlockManagerMaster.

 

This results in the fact that, the Spark application keeps retrying the task forever and locks other tasks from running

 

*Expectation* 

Spark driver should classify OOM on executor pod as a failure due to task and increase the count of max failure time

 

*Configuration*

spark.kubernetes.container.image: "spark_image_path"
 spark.kubernetes.container.image.pullPolicy: "Always"
 spark.kubernetes.namespace: "qa-namespace"
 spark.kubernetes.authenticate.driver.serviceAccountName: "svc-account"
 spark.kubernetes.executor.request.cores: "2"
 spark.kubernetes.executor.limit.cores: "2"
 spark.executorEnv.SPARK_ENV: "dev"
 spark.executor.memoryOverhead: "1G"
 spark.executor.memory: "6g"
 spark.executor.cores: "2"
 spark.executor.instances: "3"
 spark.driver.maxResultSize: "1g"
 spark.driver.memory: "10g"
 spark.driver.cores: "2"
 spark.eventLog.enabled: 'true'
 spark.driver.extraJavaOptions: "-Dcom.sun.management.jmxremote \
 -Dcom.sun.management.jmxremote.authenticate=false \
 -Dcom.sun.management.jmxremote.ssl=false \
 -XX:+UseG1GC \
 -XX:+PrintFlagsFinal \
 -XX:+PrintReferenceGC -verbose:gc \
 -XX:+PrintGCDetails \
 -XX:+PrintGCTimeStamps \
 -XX:+PrintAdaptiveSizePolicy \
 -XX:+UnlockDiagnosticVMOptions \
 -XX:+G1SummarizeConcMark \
 -XX:InitiatingHeapOccupancyPercent=35 \
 -XX:ConcGCThreads=20 \
 -XX:+PrintGCCause \
 -XX:+AlwaysPreTouch \
 -Dlog4j.debug=true -Dlog4j.configuration=[file:///].... "
 spark.sql.session.timeZone: UTC

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Apr 09 10:05:08 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0pplk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Apr/21 10:05;Nguyen Quang;I tried to set the configuration `spark.kubernetes.executor.deleteOnTermination` to False to make sure that spark won't delete pod (which potentially result in [https://github.com/apache/spark/blob/v3.1.1/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/ExecutorPodsLifecycleManager.scala#L205]) 

But this configuration doesn't help.;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Return UDT from Pandas UDF: @pandas_udf(UDT)
Issue key: SPARK-34799
Issue id: 13366229
Parent id: 13361974.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: sadhen
Creator: sadhen
Created: 19/Mar/21 02:51
Updated: 01/Apr/21 06:14
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.2, 3.1.1
Fix Version/s: 
Component/s: PySpark, SQL
Due Date: 
Votes: 0
Labels: 
Description: Focus on a simpler case first: `@pandas_udf(UDT)`.




Because Pandas UDF uses pyarrow to passing data, it does not currently support UserDefinedTypes, as what normal python udf does.

For example:


{code:python}
class BoxType(UserDefinedType):
    @classmethod
    def sqlType(cls) -> StructType:
        return StructType(
            fields=[
                StructField("xmin", DoubleType(), False),
                StructField("ymin", DoubleType(), False),
                StructField("xmax", DoubleType(), False),
                StructField("ymax", DoubleType(), False),
            ]
        )

@pandas_udf(
     returnType=StructType([StructField("boxes", ArrayType(Box()))]
)
def pandas_pf(s: pd.DataFrame) -> pd.DataFrame:
       yield s
{code}

The logs show
{code}
try:
                to_arrow_type(self._returnType_placeholder)
            except TypeError:
>               raise NotImplementedError(
                    "Invalid return type with scalar Pandas UDFs: %s is "
E                   NotImplementedError: Invalid return type with scalar Pandas UDFs: StructType(List(StructField(boxes,ArrayType(Box,true),true))) is not supported
{code}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-03-19 02:51:42.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0oyk8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Allow several properties files
Issue key: SPARK-34345
Issue id: 13356504
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tashoyan
Creator: tashoyan
Created: 03/Feb/21 13:53
Updated: 25/Mar/21 07:52
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.1.1
Fix Version/s: 
Component/s: Kubernetes, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: Example: we have 2 applications A and B. These applications have some common Spark settings and some application-specific settings. The idea is to run them like this:
{code:bash}
spark-submit --properties-files common.properties,a.properties A
spark-submit --properties-files common.properties,b.properties B
{code}
Benefits:
 - Common settings can be extracted to a common file _common.properties_, no need to copy them over _a.properties_ and _b.properties_
 - Applications can override common settings in their respective custom properties files

Currently the following mechanism works in SparkSubmitArguments.scala: console arguments like _--conf key=value_ overwrite settings in the properties file. This is not enough, because console arguments should be specified in the launcher script; de-facto they belong to the binary distribution rather than the configuration.

Consider the following scenario: Spark on Kubernetes, the configuration is provided as a ConfigMap. We could have the following ConfigMaps:
 - _a.properties_ // mount to the Pod with application A
 - _b.properties_ // mount to the Pod with application B
 - _common.properties_ // mount to both Pods with A and B
 Meanwhile the launcher script _app-submit.sh_ is the same for both applications A and B, since it contains none configuration settings:

{code:bash}
spark-submit --properties-files common.properties,${app_name}.properties ...
{code}
*Alternate solution*

Use Typesafe Config for Spark settings instead of properties files. Typesafe Config allows including files.
 For example, settings for the application A - _a.conf_:
{code:yaml}
include required("common.conf")

spark.sql.shuffle.partitions = 240
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 24 08:22:46 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0nbcg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Mar/21 08:22;tashoyan;Typesafe Config seems attractive, as it provides the following features:
 * _include_ directives
 * environment variable injections

This is convenient when running an application in Kubernetes and providing the configuration in ConfigMaps.

As an alternative, a Scala wrapper like [Pureconfig|https://pureconfig.github.io/] could be used.

Implementation details:
 * Deliver within Spark libraries reference files _reference.conf_ containing default (reference) values for all Spark settings.
 ** Different Spark libraries may have their respective files _reference.conf_. For example, _spark-sql/reference.conf_ contains settings specific to Spark SQL and so on.
 * Use Config API to get values
 ** Cleanup default values, coded in Scala or Java
 * Introduce new command-line argument for spark-submit: _--config-file_
 ** When both _\-\-config-file_ and _--properties-file_ specified, ignore the latter and print a warning
 ** When only _--properties-file_ specified, use the legacy way and print a deprecation warning.;;;
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Support TimestampType in UDT
Issue key: SPARK-34835
Issue id: 13366998
Parent id: 13361974.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: sadhen
Creator: sadhen
Created: 23/Mar/21 13:50
Updated: 24/Mar/21 04:50
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.2, 3.1.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: For user defined type with TimestampType, conversion from pandas to pyarrow and from pyarrow to Spark UnsafeRow needs to be carefully handled.


*From pandas to pyarrow*

{code:java}
if isinstance(dt, UserDefinedType):
    s = s.apply(dt.serialize)

t = to_arrow_type(dt)

array = pa.StructArray.from_pandas(s, mask=mask, type=t, safe=self._safecheck)
{code}

The above code may not work. If UDT is like:
{code:java}
class ExamplePointWithTimeUDT(UserDefinedType):
    """
    User-defined type (UDT) for ExamplePointWithTime.
    """

    @classmethod
    def sqlType(self):
        return StructType([
            StructField("x", DoubleType(), False),
            StructField("y", DoubleType(), True),
            StructField("ts", TimestampType(), False),
        ])

    @classmethod
    def module(cls):
        return 'pyspark.sql.tests'

    @classmethod
    def scalaUDT(cls):
        return 'org.apache.spark.sql.test.ExamplePointWithTimeUDT'

    def serialize(self, obj):
        return [obj.x, obj.y, obj.ts]

    def deserialize(self, datum):
        return ExamplePointWithTime(datum[0], datum[1], datum[2])


class ExamplePointWithTime:
    """
    An example class to demonstrate UDT in Scala, Java, and Python.
    """

    __UDT__ = ExamplePointWithTimeUDT()

    def __init__(self, x, y, ts):
        self.x = x
        self.y = y
        self.ts = ts

    def __repr__(self):
        return "ExamplePointWithTime(%s,%s,%s)" % (self.x, self.y, self.ts)

    def __str__(self):
        return "(%s,%s,%s)" % (self.x, self.y, self.ts)

    def __eq__(self, other):
        return isinstance(other, self.__class__) \
            and other.x == self.x and other.y == self.y \
            and other.ts == self.ts
{code}


*From pyarrow to Spark Internal*
Serialize and deserialize will fail.

See the failed PR demo: https://github.com/eddyxu/spark/pull/4

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 24 00:02:35 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0p3b4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Mar/21 00:02;maropu;Please fill the description.;;;
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: DataSourceV2Relation with column filter fails with ClassCastException at collectAsList
Issue key: SPARK-34836
Issue id: 13367004
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: markus.riedl-ehrenle
Creator: markus.riedl-ehrenle
Created: 23/Mar/21 14:05
Updated: 23/Mar/21 14:05
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: After trying to upgrade to 3.1.1. multiple of our test cases fail with a ClassCastException at *DataFrame.collectAsList()*

Produced exception:
{noformat}
java.lang.ClassCastException: class org.apache.spark.sql.catalyst.expressions.GenericInternalRow cannot be cast to class org.apache.spark.sql.catalyst.expressions.UnsafeRow (org.apache.spark.sql.catalyst.expressions.GenericInternalRow and org.apache.spark.sql.catalyst.expressions.UnsafeRow are in unnamed module of loader 'app')
 at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:346) ~[spark-sql_2.12-3.1.1.jar:3.1.1]
 at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898) ~[spark-core_2.12-3.1.1.jar:3.1.1]
 at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898) ~[spark-core_2.12-3.1.1.jar:3.1.1]
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.1.1.jar:3.1.1]
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373) ~[spark-core_2.12-3.1.1.jar:3.1.1]
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:337) ~[spark-core_2.12-3.1.1.jar:3.1.1]
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) ~[spark-core_2.12-3.1.1.jar:3.1.1]
 at org.apache.spark.scheduler.Task.run(Task.scala:131) ~[spark-core_2.12-3.1.1.jar:3.1.1]
 at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497) ~[spark-core_2.12-3.1.1.jar:3.1.1]
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439) ~[spark-core_2.12-3.1.1.jar:3.1.1]
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500) [spark-core_2.12-3.1.1.jar:3.1.1]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]
 at java.lang.Thread.run(Thread.java:834) [?:?]
2021-03-23 13:00:26.974 WARN [org.apache.spark.scheduler.TaskSetManager] Lost task 0.0 in stage 0.0 (TID 0) (192.168.0.6 executor driver): java.lang.ClassCastException: class org.apache.spark.sql.catalyst.expressions.GenericInternalRow cannot be cast to class org.apache.spark.sql.catalyst.expressions.UnsafeRow (org.apache.spark.sql.catalyst.expressions.GenericInternalRow and org.apache.spark.sql.catalyst.expressions.UnsafeRow are in unnamed module of loader 'app')
 at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:346)
 at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
 at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
 at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
 at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
 at org.apache.spark.scheduler.Task.run(Task.scala:131)
 at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
 at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
 at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
 at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
 at java.base/java.lang.Thread.run(Thread.java:834){noformat}
 

All test cases follow this pattern:
{code:java}
StructField aDouble = DataTypes.createStructField("aDouble", DataTypes.DoubleType, true);
StructField aDoubleArray = DataTypes.createStructField("aDoubleArray", DataTypes.createArrayType(DataTypes.DoubleType), true);

StructType schema = DataTypes.createStructType(Arrays.asList(aDouble, aDoubleArray));

Column aDoubleFilter = functions.column("aDouble").equalTo(functions.lit(1d));
Column aDoubleArrayFilter = functions.aggregate(
 functions.col("aDoubleArray"),
 functions.lit(false),
 (seed, column) -> seed.or(column.equalTo(functions.lit(1d))));
Column filter = aDoubleFilter.or(aDoubleArrayFilter);

CaseInsensitiveStringMap options = new CaseInsensitiveStringMap(new HashMap<>());
Dataset<Row> dataset = Dataset.ofRows(sparkSession, DataSourceV2Relation.create(new ReadTable(schema), null, null, options));

Dataset<Row> filtered = dataset.filter(filter);

List<Row> collected = filtered.collectAsList();
{code}
 

ReadTable is a *org.apache.spark.sql.connector.catalog.Table*, with the given schema above. 
 *PartitionReader* returns these rows:
{code:java}
InternalRow.fromSeq(
  JavaConverters.asScalaBuffer(Arrays.asList(1d, ArrayData.toArrayData(new double[]{1d, 2d})))
)
InternalRow.fromSeq(
  JavaConverters.asScalaBuffer(Arrays.asList(3d, ArrayData.toArrayData(new double[]{4d, 5d})))
)
{code}
 

I couldn't reproduce the failure with e.g.,
{code:java}
List<Row> rows = Arrays.asList(
 RowFactory.create(1d, Arrays.asList(1d, 2d)),
 RowFactory.create(2d, Arrays.asList(3d, 4d))
);

Dataset<Row> dataFrame = sparkSession.createDataFrame(rows, schema);{code}
 

The tests succeed with 3.0.2.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-03-23 14:05:09.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0p3cg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: SPARK-34212 issue not fixed if spark.sql.parquet.enableVectorizedReader=true which is default value. Error Parquet column cannot be converted in file.
Issue key: SPARK-34785
Issue id: 13366010
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jobitmathew
Creator: jobitmathew
Created: 18/Mar/21 07:26
Updated: 18/Mar/21 21:14
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: SPARK-34212 issue not fixed if spark.sql.parquet.enableVectorizedReader=true which is default value.

IF spark.sql.parquet.enableVectorizedReader=false below scenario pass but it will reduce the performance.

In Hive, 
{code:java}
create table test_decimal(amt decimal(18,2)) stored as parquet; 
insert into test_decimal select 100;
alter table test_decimal change amt amt decimal(19,3);
{code}
In Spark,
{code:java}
select * from test_decimal;
{code}
{code:java}
+--------+
|    amt |
+--------+
| 100.000 |{code}
but if spark.sql.parquet.enableVectorizedReader=true below error
{code:java}
: jdbc:hive2://10.21.18.161:23040/> select * from test_decimal;
going to print operations logs
printed operations logs
going to print operations logs
printed operations logs
Getting log thread is interrupted, since query is done!
Error: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4) (vm2 executor 2): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file hdfs://hacluster/user/hive/warehouse/test_decimal/000000_0. Column: [amt], Expected: decimal(19,3), Found: FIXED_LEN_BYTE_ARRAY
        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)
        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
        at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:131)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1461)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
        at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:339)
        at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readFixedLenByteArrayBatch(VectorizedColumnReader.java:735)
        at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:312)
        at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:283)
        at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:181)
        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)
        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)
        ... 20 more

Driver stacktrace:
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:368)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:265)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
        at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:45)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:265)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:260)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:274)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4) (vm2 executor 2): org.apache.spark.sql.execution.QueryExecutionException: Parquet column cannot be converted in file hdfs://hacluster/user/hive/warehouse/test_decimal/000000_0. Column: [amt], Expected: decimal(19,3), Found: FIXED_LEN_BYTE_ARRAY
        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:179)
        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
        at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:503)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:131)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1461)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException
        at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.constructConvertNotSupportedException(VectorizedColumnReader.java:339)
        at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readFixedLenByteArrayBatch(VectorizedColumnReader.java:735)
        at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:312)
        at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:283)
        at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:181)
        at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:37)
        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)
        at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)
        ... 20 more

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2205)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2226)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2270)
        at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)
        at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)
        at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2965)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:777)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
        at org.apache.spark.sql.Dataset.collect(Dataset.scala:2965)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:336)

{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Mar 18 21:13:45 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ox7k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Mar/21 07:30;jobitmathew;[~dongjoon] can you check it once.;;;, 18/Mar/21 21:10;dongjoon;SPARK-34212 is complete by itself because it's designed to fix the correctness issue. You will not get incorrect values.
For this specific `Schema evolution` requirement in the PR description, I don't have a bandwidth, [~jobitmathew].;;;, 18/Mar/21 21:13;dongjoon;FYI, each data sources have different schema evolution capabilities. And, Parquet is not the best built-in data source in terms of it. We are tracking it with the following test suite.
- https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/ReadSchemaTest.scala

The recommendation is to use MR code path if you are in that situation.;;;
Affects Version/s.1: 
Comment.1: 18/Mar/21 21:10;dongjoon;SPARK-34212 is complete by itself because it's designed to fix the correctness issue. You will not get incorrect values.
For this specific `Schema evolution` requirement in the PR description, I don't have a bandwidth, [~jobitmathew].;;;
Comment.2: 18/Mar/21 21:13;dongjoon;FYI, each data sources have different schema evolution capabilities. And, Parquet is not the best built-in data source in terms of it. We are tracking it with the following test suite.
- https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/execution/datasources/ReadSchemaTest.scala

The recommendation is to use MR code path if you are in that situation.;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Add tableType for SHOW TABLES to distinguish view and tables
Issue key: SPARK-34710
Issue id: 13363709
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Qin Yao
Creator: Qin Yao
Created: 11/Mar/21 03:05
Updated: 13/Mar/21 16:38
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.2, 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: SHOW TABLES has an output column `isTemporary` which only indicates if a view is a local temp view that is not enough for users if they want pipeline Spark commands, such as SHOW TABLES -> DROP TABLE/VIEW
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Mar 11 03:20:00 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0oj4g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Mar/21 03:19;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31804;;;, 11/Mar/21 03:20;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31804;;;
Affects Version/s.1: 3.1.1
Comment.1: 11/Mar/21 03:20;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/31804;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Checkpointing a union with another checkpoint fails
Issue key: SPARK-34563
Issue id: 13361383
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kamprath
Creator: kamprath
Created: 27/Feb/21 18:40
Updated: 05/Mar/21 18:13
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.2, 3.1.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: I have some PySpark code that periodically checkpoints a data frame  that I am building in pieces by union-ing those pieces together as they are constructed. (Py)Spark fails on the second checkpoint, which would be a union of a new piece of the desired data frame with a previously checkpointed piece. Some simplified PySpark code that will trigger this problem is:

 
{code:java}
RANGE_STEP = 10000
PARTITIONS = 5
COUNT_UNIONS = 20

df = spark.range(1, RANGE_STEP+1, numPartitions=PARTITIONS)

for i in range(1, COUNT_UNIONS+1):
    print('Processing i = {0}'.format(i))
    new_df = spark.range(RANGE_STEP*i + 1, RANGE_STEP*(i+1) + 1, numPartitions=PARTITIONS)
    df = df.union(new_df).checkpoint()

df.count()
{code}
When this code gets to the checkpoint on the second loop iteration (i=2) the job fails with an error:

 
{code:java}
Py4JJavaError: An error occurred while calling o119.checkpoint.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 10.0 failed 4 times, most recent failure: Lost task 9.3 in stage 10.0 (TID 264, 10.20.30.13, executor 0): com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 9062
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:693)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:804)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:296)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1804)
	at org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1227)
	at org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1227)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1227)
	at org.apache.spark.sql.Dataset.$anonfun$checkpoint$1(Dataset.scala:696)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)
	at org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:687)
	at org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:650)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: com.esotericsoftware.kryo.KryoException: Encountered unregistered class ID: 9062
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:693)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:804)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:296)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1804)
	at org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1227)
	at org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1227)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:127)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:462)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:465)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
{code}
 

Note that the checkpoint directory is set, as the first checkpoint does succeed.  Also, if the checkpoint method is removed, the sample code succeeds as expected, so the problems isolated to the use of the checkpoint.

 
Environment: I am running Spark 3.0.2 in stand alone cluster mode, built for Hadoop 2.7, and Scala 2.12.12. I am using QFS 2.2.2 (Quantcast File System) as the underlying DFS. The nodes run on Debian Stretch, and Java is openjdk version "1.8.0_275". 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Mar 05 18:13:15 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0o4sg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Mar/21 18:13;kamprath;I just tested this under Spark 3.1.1 keep everything else in my set up the same, and it fails at the same point. However, the exception thrown looks slightly different:

 
{code:java}
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-2-ea419227c865> in <module>
      8     print('Processing i = {0}'.format(i))
      9     new_df = spark.range(RANGE_STEP*i + 1, RANGE_STEP*(i+1) + 1, numPartitions=PARTITIONS)
---> 10     df = df.union(new_df).checkpoint()
     11 
     12 df.count()

/usr/spark-3.1.1/python/pyspark/sql/dataframe.py in checkpoint(self, eager)
    544         This API is experimental.
    545         """
--> 546         jdf = self._jdf.checkpoint(eager)
    547         return DataFrame(jdf, self.sql_ctx)
    548 

/usr/spark-3.1.1/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1303         answer = self.gateway_client.send_command(command)
   1304         return_value = get_return_value(
-> 1305             answer, self.gateway_client, self.target_id, self.name)
   1306 
   1307         for temp_arg in temp_args:

/usr/spark-3.1.1/python/pyspark/sql/utils.py in deco(*a, **kw)
    109     def deco(*a, **kw):
    110         try:
--> 111             return f(*a, **kw)
    112         except py4j.protocol.Py4JJavaError as e:
    113             converted = convert_exception(e.java_exception)

/usr/spark-3.1.1/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     "An error occurred while calling {0}{1}{2}.\n".
--> 328                     format(target_id, ".", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o65.checkpoint.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 2.0 failed 4 times, most recent failure: Lost task 8.3 in stage 2.0 (TID 50) (10.20.30.17 executor 3): java.lang.IndexOutOfBoundsException: Index: 61, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:659)
	at java.util.ArrayList.get(ArrayList.java:435)
	at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:60)
	at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:857)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:811)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:296)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1866)
	at org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)
	at org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1253)
	at org.apache.spark.sql.Dataset.$anonfun$checkpoint$1(Dataset.scala:697)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
	at org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:688)
	at org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:651)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IndexOutOfBoundsException: Index: 61, Size: 0
	at java.util.ArrayList.rangeCheck(ArrayList.java:659)
	at java.util.ArrayList.get(ArrayList.java:435)
	at com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:60)
	at com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:857)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:811)
	at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:296)
	at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:168)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1866)
	at org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1253)
	at org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1253)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2242)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
{code};;;
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Avoid migrating un-needed shuffle files
Issue key: SPARK-34280
Issue id: 13355269
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 28/Jan/21 20:04
Updated: 31/Jan/21 05:12
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In Spark 3.1 we introduced shuffle migrations. However, it is possible that a shuffle file will still exist after it is no longer needed. I've only observed this in a back port branch with SQL, so I'll do some more digging.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jan 31 05:12:13 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0n3qg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/Jan/21 05:12;attilapiros;It would be interesting to know whether the context cleaner was running for those blocks beforehand.

If you have the log it would be nice to see whether it has any of these: 
 - ["Error cleaning shuffle"|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/ContextCleaner.scala#L223-L237]
 - ["Error deleting data"|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala#L107-L121]

If not re-run it along with the logger "org.apache.spark.ContextCleaner" set to DEBUG level.

(I guess this not much help for you Holden but who knows who else reads this and looks for answers to similar questions.)
;;;
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Create tunable fuzzy logic for dynamic allocation
Issue key: SPARK-34228
Issue id: 13354508
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: holden
Reporter: holden
Creator: holden
Created: 25/Jan/21 18:51
Updated: 25/Jan/21 18:51
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: With default dynamic allocation we may spin-up and down executors relatively frequently. We should allow platform to configure what reasonable levels of fuzzy logic are depending on the relative executor allocation overhead.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-01-25 18:51:31.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0mz1k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, Unknown

Summary: Support complex types in pyspark.sql.functions.lit
Issue key: SPARK-34136
Issue id: 13352674
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zero323
Creator: zero323
Created: 16/Jan/21 13:30
Updated: 16/Jan/21 14:51
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.0
Fix Version/s: 
Component/s: PySpark, SQL
Due Date: 
Votes: 0
Labels: 
Description: At the moment, Python users have to use dedicated function to create complex literal column. For example to create an array:

{code:python}
from pyspark.sql.functions import array, lit

xs = [1, 2, 3]
array(*[lit(x) for x in xs])
{code}

or map

{code:python}
from pyspark.sql.functions import create_map, lit, map_from_arrays
from itertools import chain

kvs = {"a": 1, "b": 2}

create_map(*chain.from_iterable(
    (lit(k), lit(v)) for k, v in kvs.items()
))

# or

map_from_arrays(
    array(*[lit(k) for k in kvs.keys()]),
    array(*[lit(v) for v in kvs.values()])
)
{code}

This is very verbose for such simple task. 

In Scala we have `typedLit` that addresses such cases

{code:scala}
scala> typedLit(Map("a" -> 1, "b" -> 2))
res0: org.apache.spark.sql.Column = keys: [a,b], values: [1,2]

scala> typedLit(Array(1, 2, 3))
res1: org.apache.spark.sql.Column = [1,2,3]

{code}


but its API is not Python-friendly.

It would be nice if {{lit}} could cover at least basic complex types.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Jan 16 14:51:33 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0mnq8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Jan/21 14:51;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/31207;;;
Affects Version/s.1: 3.2.0
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0

Summary: Add support for calling debugCodegen from Python & Java
Issue key: SPARK-35198
Issue id: 13374517
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 22/Apr/21 23:24
Updated: 20/May/24 09:14
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.0.2, 3.1.0, 3.1.1, 3.2.0
Fix Version/s: 
Component/s: PySpark, SQL
Due Date: 
Votes: 0
Labels: pull-request-available, starter
Description: Because it is implimented with an implicit conversion it's a bit complicated to call, we should add a direct method to get debug state for Java & Python users of Dataframes.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed May 10 17:11:41 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qcyw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.2.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Mar/23 15:58;juanvi;[~holden] [~maxgekk] Is this still relevant as we can get to the same result using df.explain("codegen")?

If it is, I can submit a PR.
If not, should the debugCodegen implicit be -removed- deprecated for consistency?;;;, 30/Mar/23 15:07;juanvi;Ended up creating a PR https://github.com/apache/spark/pull/40608;;;, 10/May/23 17:11;ignitetcbot;User 'juanvisoler' has created a pull request for this issue:
https://github.com/apache/spark/pull/40608;;;
Affects Version/s.1: 3.0.2
Comment.1: 30/Mar/23 15:07;juanvi;Ended up creating a PR https://github.com/apache/spark/pull/40608;;;
Comment.2: 10/May/23 17:11;ignitetcbot;User 'juanvisoler' has created a pull request for this issue:
https://github.com/apache/spark/pull/40608;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, Unknown

Summary: inferTimestamp option is missing from the list of options in DataFrameReader.json.
Issue key: SPARK-34679
Issue id: 13363355
Parent id: 
Issue Type: Documentation
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: P7hB
Creator: P7hB
Created: 09/Mar/21 19:40
Updated: 09/May/24 14:04
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.0.2, 3.1.0, 3.1.1
Fix Version/s: 
Component/s: Documentation
Due Date: 
Votes: 0
Labels: documentation, easyfix, newbie, starter
Description: {color:#e01e5a}inferTimestamp{color} option is missing in the list of options in {color:#e01e5a}DataFrameReader.json{color} method in the API docs missing from the [Scaladocs here|[DataFrameReader.json|https://github.com/apache/spark/blob/v3.1.1/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L432-L520]].

Simiarly in the [Pyspark docs|[pyspark.sql.DataFrameReader.json|https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.json.html?highlight=json#pyspark.sql.DataFrameReader.json]] as well.

However we have this blurb in the [migration guide|[Spark 3.0 to 3.0.1 migration guide|https://spark.apache.org/docs/latest/sql-migration-guide.html#upgrading-from-spark-sql-30-to-301]]
 * In Spark 3.0, JSON datasource and JSON function {{schema_of_json}} infer TimestampType from string values if they match to the pattern defined by the JSON option {{timestampFormat}}. Since version 3.0.1, the timestamp type inference is disabled by default. Set the JSON option {{inferTimestamp}} to {{true}} to enable such type inference.

We should add this in the documentation as well as there is a possibility that the Data Engineers might not be aware of this option.
Environment: 
Original Estimate: 1800.0
Remaining Estimate: 1800.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 1800.0
Σ Remaining Estimate: 1800.0
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu May 09 14:04:19 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ogy0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Mar/21 11:39;gurwls223;Are you interested in opening a PR, [~P7hB]?;;;, 11/Mar/21 07:55;P7hB;Thanks [~hyukjin.kwon]. I will create a PR.;;;, 09/May/24 14:04;schilukoori;[~gurwls223] , [~P7hB] I'm interested in contributing to documentation. Is this issue resolved? or could I work on it. 

Also if you have any starter tasks please let me know.

Thank you.;;;
Affects Version/s.1: 3.0.2
Comment.1: 11/Mar/21 07:55;P7hB;Thanks [~hyukjin.kwon]. I will create a PR.;;;
Comment.2: 09/May/24 14:04;schilukoori;[~gurwls223] , [~P7hB] I'm interested in contributing to documentation. Is this issue resolved? or could I work on it. 

Also if you have any starter tasks please let me know.

Thank you.;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: How to specify parameters in spark-sumbit to make HiveDelegationTokenProvider refresh token regularly
Issue key: SPARK-44381
Issue id: 13543209
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jiaoqb
Creator: jiaoqb
Created: 12/Jul/23 03:35
Updated: 12/Jul/23 06:58
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: export KRB5CCNAME=FILE:/tmp/krb5cc_1001
./bin/spark-submit -{-}master yarn --deploy-mode client --proxy-user xxxx --conf spark.app.name=spark-hive-test --conf spark.security.credentials.renewalRatio=0.000058 --conf spark.kerberos.renewal.credentials=ccache{-}  -class org.apache.spark.examples.sql.hive.SparkHiveExample /examples/jars/spark-examples_2.12-3.1.1.jar

spark version 3.1.1，I configured it to refresh every 5 seconds。

--deploy-mode client/cluster wtih/without --proxy-user have all been tried, but none of them will work

Missing any configuration parameters？
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 12 06:48:12 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1j38g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Jul/23 06:48;jiaoqb;[~jshao] please cc ,thanks;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Multi sparkSession should share single SQLAppStatusStore
Issue key: SPARK-41555
Issue id: 13514287
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: lijiahong
Creator: lijiahong
Created: 17/Dec/22 00:49
Updated: 17/Dec/22 03:03
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: In spark , if we create multi sparkSession in the program, we will get multi-SQLTab in UI, 

At the same time, we will get muti-SQLAppStatusListener object, it is waste of memory.

code like this:

 
{code:java}
// code placeholder
def main(args: Array[String]): Unit = {
val sparkConf = new SparkConf()
.setAppName("demo")
.setMaster("local[*]")

val spark = SparkSession.builder()
.config(sparkConf)
.getOrCreate()

setDefaultSession(null)
setActiveSession(null)

val spark2 = SparkSession.builder()
.config(sparkConf)
.getOrCreate()

import spark.implicits._
val testData = spark.sparkContext
.parallelize((1 to 3).map(i => TestData(i, i.toString))).toDF()
testData.createOrReplaceTempView("testTable")
val testData2 = spark.sparkContext.parallelize(
TestData2(1, "1") ::
TestData2(1, "2") ::
TestData2(2, "1") ::
TestData2(2, "2") ::
TestData2(3, "1") ::
TestData2(3, "2") ::
Nil, 2).toDF()

testData2.createOrReplaceTempView("testTable2")
val query = "select ind2,count(*) from ( select * from testTable2 join testTable on testTable.ind = testTable2.ind2 where testTable.name <> '1') group by ind2"
spark.sql(query).collect()

Thread.sleep(500000)
spark.stop()
}
case class TestData(ind: Int, name: String)
case class TestData2(ind2: Int, name: String) {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 17/Dec/22 00:50;lijiahong;muti-SQLStore.png;https://issues.apache.org/jira/secure/attachment/13053934/muti-SQLStore.png, 17/Dec/22 00:50;lijiahong;muti-sqltab.png;https://issues.apache.org/jira/secure/attachment/13053933/muti-sqltab.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Dec 17 01:16:30 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1e5qw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Dec/22 01:08;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39101;;;, 17/Dec/22 01:09;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39101;;;, 17/Dec/22 01:16;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39102;;;
Affects Version/s.1: 3.2.1
Comment.1: 17/Dec/22 01:09;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39101;;;
Comment.2: 17/Dec/22 01:16;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/39102;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Support null in conversions to and from Arrow
Issue key: SPARK-34693
Issue id: 13363589
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: lversluis
Creator: lversluis
Created: 10/Mar/21 15:58
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Looks like a regression from or related to SPARK-33489.

I got the following running spark 3.1.1. with koalas 1.7.0
{code:java}
TypeError                                 Traceback (most recent call last)
/var/scratch/miniconda3/lib/python3.8/site-packages/pyspark/sql/udf.py in returnType(self)
    100             try:
--> 101                 to_arrow_type(self._returnType_placeholder)
    102             except TypeError:

/var/scratch/miniconda3/lib/python3.8/site-packages/pyspark/sql/pandas/types.py in to_arrow_type(dt)
     75     else:
---> 76         raise TypeError("Unsupported type in conversion to Arrow: " + str(dt))
     77     return arrow_type

TypeError: Unsupported type in conversion to Arrow: NullType
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Mar 19 01:57:00 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0oie0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Mar/21 04:24;gurwls223;Which codes did you run? It would be great if you can provide the reproducer;;;, 18/Mar/21 16:30;lversluis;{code:java}
def tps(df) -> pd.DataFrame["workflow_id": int, "task_id": int, "task_slack": int]:
    df.set_index("id", inplace=True)
  
    graph = dict()
    forward_dict = dict()
    finish_times = dict()
    task_runtimes = dict()
    task_arrival_times = dict()
    workflow_id = None
    for row in df.to_records(): # 0: task id, 1: wf id, 2: children, 3: parents, 4: ts submit, 5: runtime
        graph[row[0]] = set(row[3].flatten())
        forward_dict[row[0]] = set(row[2].flatten())
        task_runtimes[row[0]] = row[5]
        task_arrival_times[row[0]] = row[4]
        workflow_id = row[1]
        
    del df
    del row
    
    try:
        groups = list(toposort(graph))
    except CircularDependencyError:
        del forward_dict
        del finish_times
        del task_runtimes
        del task_arrival_times
        del workflow_id
        return pd.DataFrame(columns=["workflow_id", "task_id", "task_slack"])
    
    del graph    if len(groups) < 2:
        del forward_dict
        del finish_times
        del task_runtimes
        del task_arrival_times
        del workflow_id
        del groups
        return pd.DataFrame(columns=["workflow_id", "task_id", "task_slack"])    # Compute all full paths
    max_for_task = dict()
    
    q = deque()
    for i in groups[0]:
        max_for_task[i] = task_arrival_times[i] + task_runtimes[i]
        q.append((max_for_task[i], [i]))
    
    paths = []
    max_path = -1
    
    while len(q) > 0:
        time, path = q.popleft()  # get partial path  # We are are at time t[0] having travelled path t[1]
        if len(forward_dict[path[-1]]) == 0: # End task
            if time < max_path: continue  # smaller path, cannot be a CP
            if time > max_path:  # If we have a new max, clear the list and set it to the max
                max_path = time
                paths.clear()
            paths.append(path)  # Add new and identical length paths
        else:
            for c in forward_dict[path[-1]]:  # Loop over all children of the last task in the path we took
                if c in task_runtimes:
                    # Special case: we find a child that arrives later then the current path.
                    # Start a new path then from this child onwards and do not mark the previous nodes on the critical path
                    # as they can actually delay.
                    if time < task_arrival_times[c]:
                        if c not in max_for_task:
                            max_for_task[c] = task_arrival_times[c] + task_runtimes[c]
                            q.append((max_for_task[c], [c]))
                    else:
                        # If the finishing of one of the children + the runtime causes the same or a new maximum, add a path to the queue
                        # to explore from there on at that time.
                        child_finish = time + task_runtimes[c]
                        if c in max_for_task and child_finish < max_for_task[c]:  continue
                        max_for_task[c] = child_finish
                        l = path.copy()
                        l.append(c)
                        q.append((child_finish, l))
    
    del time
    del max_for_task
    del forward_dict
    del max_path
    del q
    del path
    
    if(len(paths) == 1):
        citical_path_tasks = set(paths[0])
    else:
        citical_path_tasks = _reduce(set.union, [set(cp) for cp in paths])
    del paths
    
    rows = deque()
    for i in range(len(groups)):
        max_in_group = -1
        for task_id in groups[i]:  # Compute max finish time in the group
            if task_id not in finish_times: 
                continue  # Task was not in snapshot of trace
            task_finish = finish_times[task_id]
            if task_finish > max_in_group:
                max_in_group = task_finish
        
        for task_id in groups[i]:
            if task_id in citical_path_tasks:  # Skip if task is on a critical path
                rows.append([workflow_id, task_id, 0])
            else: 
                if task_id not in finish_times: continue  # Task was not in snapshot of trace
                rows.append([workflow_id, task_id, max_in_group - finish_times[task_id]])
    
    del groups
    del citical_path_tasks
    del finish_times
    return pd.DataFrame(rows, columns=["workflow_id", "task_id", "task_slack"])
{code}
and the reading part:
{code:java}
kdf = ks.read_parquet(os.path.join(hdfs_path, folder, "tasks", "schema-1.0"),
 columns=[
 "workflow_id", "id", "task_id", "children", "parents", "ts_submit", "runtime"
 ], pandas_metadata=False, engine='pyarrow')
 
kdf = kdf[((kdf['children'].map(len) > 0) | (kdf['parents'].map(len) > 0))]
kdf = kdf.astype({"ts_submit":'int32', "runtime":'int32'})
kdf.dropna(inplace=True)
grouped_df = kdf.groupby("workflow_id")
grouped_df.apply(tps).to_parquet(output, compression='snappy', engine='pyarrow'){code}
the hdfs path points to [https://doi.org/10.5281/zenodo.3254606] as dataset. I have since changed the implementation of tps as it was too memory hungry (moved to an recursive approach instead of iterative) and I have not seen the crash since.;;;, 19/Mar/21 01:57;gurwls223;Oh, okay. Seems like it's fixed in SPARK-33489 (Spark 3.2.0). It's sort of improvement which isn't backported in general.;;;
Affects Version/s.1: 
Comment.1: 18/Mar/21 16:30;lversluis;{code:java}
def tps(df) -> pd.DataFrame["workflow_id": int, "task_id": int, "task_slack": int]:
    df.set_index("id", inplace=True)
  
    graph = dict()
    forward_dict = dict()
    finish_times = dict()
    task_runtimes = dict()
    task_arrival_times = dict()
    workflow_id = None
    for row in df.to_records(): # 0: task id, 1: wf id, 2: children, 3: parents, 4: ts submit, 5: runtime
        graph[row[0]] = set(row[3].flatten())
        forward_dict[row[0]] = set(row[2].flatten())
        task_runtimes[row[0]] = row[5]
        task_arrival_times[row[0]] = row[4]
        workflow_id = row[1]
        
    del df
    del row
    
    try:
        groups = list(toposort(graph))
    except CircularDependencyError:
        del forward_dict
        del finish_times
        del task_runtimes
        del task_arrival_times
        del workflow_id
        return pd.DataFrame(columns=["workflow_id", "task_id", "task_slack"])
    
    del graph    if len(groups) < 2:
        del forward_dict
        del finish_times
        del task_runtimes
        del task_arrival_times
        del workflow_id
        del groups
        return pd.DataFrame(columns=["workflow_id", "task_id", "task_slack"])    # Compute all full paths
    max_for_task = dict()
    
    q = deque()
    for i in groups[0]:
        max_for_task[i] = task_arrival_times[i] + task_runtimes[i]
        q.append((max_for_task[i], [i]))
    
    paths = []
    max_path = -1
    
    while len(q) > 0:
        time, path = q.popleft()  # get partial path  # We are are at time t[0] having travelled path t[1]
        if len(forward_dict[path[-1]]) == 0: # End task
            if time < max_path: continue  # smaller path, cannot be a CP
            if time > max_path:  # If we have a new max, clear the list and set it to the max
                max_path = time
                paths.clear()
            paths.append(path)  # Add new and identical length paths
        else:
            for c in forward_dict[path[-1]]:  # Loop over all children of the last task in the path we took
                if c in task_runtimes:
                    # Special case: we find a child that arrives later then the current path.
                    # Start a new path then from this child onwards and do not mark the previous nodes on the critical path
                    # as they can actually delay.
                    if time < task_arrival_times[c]:
                        if c not in max_for_task:
                            max_for_task[c] = task_arrival_times[c] + task_runtimes[c]
                            q.append((max_for_task[c], [c]))
                    else:
                        # If the finishing of one of the children + the runtime causes the same or a new maximum, add a path to the queue
                        # to explore from there on at that time.
                        child_finish = time + task_runtimes[c]
                        if c in max_for_task and child_finish < max_for_task[c]:  continue
                        max_for_task[c] = child_finish
                        l = path.copy()
                        l.append(c)
                        q.append((child_finish, l))
    
    del time
    del max_for_task
    del forward_dict
    del max_path
    del q
    del path
    
    if(len(paths) == 1):
        citical_path_tasks = set(paths[0])
    else:
        citical_path_tasks = _reduce(set.union, [set(cp) for cp in paths])
    del paths
    
    rows = deque()
    for i in range(len(groups)):
        max_in_group = -1
        for task_id in groups[i]:  # Compute max finish time in the group
            if task_id not in finish_times: 
                continue  # Task was not in snapshot of trace
            task_finish = finish_times[task_id]
            if task_finish > max_in_group:
                max_in_group = task_finish
        
        for task_id in groups[i]:
            if task_id in citical_path_tasks:  # Skip if task is on a critical path
                rows.append([workflow_id, task_id, 0])
            else: 
                if task_id not in finish_times: continue  # Task was not in snapshot of trace
                rows.append([workflow_id, task_id, max_in_group - finish_times[task_id]])
    
    del groups
    del citical_path_tasks
    del finish_times
    return pd.DataFrame(rows, columns=["workflow_id", "task_id", "task_slack"])
{code}
and the reading part:
{code:java}
kdf = ks.read_parquet(os.path.join(hdfs_path, folder, "tasks", "schema-1.0"),
 columns=[
 "workflow_id", "id", "task_id", "children", "parents", "ts_submit", "runtime"
 ], pandas_metadata=False, engine='pyarrow')
 
kdf = kdf[((kdf['children'].map(len) > 0) | (kdf['parents'].map(len) > 0))]
kdf = kdf.astype({"ts_submit":'int32', "runtime":'int32'})
kdf.dropna(inplace=True)
grouped_df = kdf.groupby("workflow_id")
grouped_df.apply(tps).to_parquet(output, compression='snappy', engine='pyarrow'){code}
the hdfs path points to [https://doi.org/10.5281/zenodo.3254606] as dataset. I have since changed the implementation of tps as it was too memory hungry (moved to an recursive approach instead of iterative) and I have not seen the crash since.;;;
Comment.2: 19/Mar/21 01:57;gurwls223;Oh, okay. Seems like it's fixed in SPARK-33489 (Spark 3.2.0). It's sort of improvement which isn't backported in general.;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Improve Spark SQL Source Filter to allow pushdown of filters span multiple columns
Issue key: SPARK-34694
Issue id: 13363605
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: zinechant
Creator: zinechant
Created: 10/Mar/21 16:38
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.1.0, 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The current org.apache.spark.sql.sources.Filter abstract class only allows pushdown of filters on single column or sum of products of multiple such single-column filters.

Filters on multiple columns cannot be pushed down through this Filter subclass to source, e.g. from TPC-H benchmark on lineitem table:

(l_commitdate#11 < l_receiptdate#12)

(l_shipdate#10 < l_commitdate#11)

 

The current design probably originates from the point that columnar source has a hard time supporting these cross-column filters. But with batching implemented in columnar sources, they can still support cross-column filters.  This issue tries to open up discussion on a more general Filter interface to allow pushing down cross-column filters.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 16 02:39:43 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0oihk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Mar/21 04:23;gurwls223;{code}
(l_commitdate#11 < l_receiptdate#12)
(l_shipdate#10 < l_commitdate#11)
{code}

will be

{{And(LessThen(l_commitdate, l_receiptdate), LessThen(l_shipdate, l_commitdate))}}. I think this seems fine. Do you mind elabourating which kind of design you have in mind?;;;, 15/Mar/21 15:22;zinechant;Hi Hyukjin,

I think the design you described would work.

But the current org.apache.spark.sql.sources.Filter isn't built under the assumption that the 'value' parameter could be a column reference.

e.g. the findReferences member function does not consider value being a column references.


{code:scala}
  protected def findReferences(value: Any): Array[String] = value match {
    case f: Filter => f.references
    case _ => Array.empty
  }

{code}
 

And this is probably why org.apache.spark.sql.execution.datasources.v2.PushDownUtils would not push the cross-column filters down to data sources.

The end result is that cross-column filters don't get pushed down, from stderr of a spark job doing TPC-H Q12:

21/03/10 16:56:16.266 INFO V2ScanRelationPushDown: 
 Pushing operators to lineitem@[file:///blah/blah/lineitem]
 Pushed Filters: Or(EqualTo(l_shipmode,MAIL),EqualTo(l_shipmode,SHIP)), GreaterThanOrEqual(l_receiptdate,1994-01-01), LessThan(l_receiptdate,1995-01-01)
 Post-Scan Filters: (l_commitdate#11 < l_receiptdate#12),(l_shipdate#10 < l_commitdate#11)
 Output: l_orderkey#0, l_shipdate#10, l_commitdate#11, l_receiptdate#12, l_shipmode#14

 

Regards,
 Chen;;;, 16/Mar/21 02:39;gurwls223;Oh, okay. it more proposes to handle column references. Probably the concern is about the type handling of the pushed predicate in the source but sure sounds like a valid issue.;;;
Affects Version/s.1: 3.0.1
Comment.1: 15/Mar/21 15:22;zinechant;Hi Hyukjin,

I think the design you described would work.

But the current org.apache.spark.sql.sources.Filter isn't built under the assumption that the 'value' parameter could be a column reference.

e.g. the findReferences member function does not consider value being a column references.


{code:scala}
  protected def findReferences(value: Any): Array[String] = value match {
    case f: Filter => f.references
    case _ => Array.empty
  }

{code}
 

And this is probably why org.apache.spark.sql.execution.datasources.v2.PushDownUtils would not push the cross-column filters down to data sources.

The end result is that cross-column filters don't get pushed down, from stderr of a spark job doing TPC-H Q12:

21/03/10 16:56:16.266 INFO V2ScanRelationPushDown: 
 Pushing operators to lineitem@[file:///blah/blah/lineitem]
 Pushed Filters: Or(EqualTo(l_shipmode,MAIL),EqualTo(l_shipmode,SHIP)), GreaterThanOrEqual(l_receiptdate,1994-01-01), LessThan(l_receiptdate,1995-01-01)
 Post-Scan Filters: (l_commitdate#11 < l_receiptdate#12),(l_shipdate#10 < l_commitdate#11)
 Output: l_orderkey#0, l_shipdate#10, l_commitdate#11, l_receiptdate#12, l_shipmode#14

 

Regards,
 Chen;;;
Comment.2: 16/Mar/21 02:39;gurwls223;Oh, okay. it more proposes to handle column references. Probably the concern is about the type handling of the pushed predicate in the source but sure sounds like a valid issue.;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: java.io.StreamCorruptedException: invalid stream header: 204356EC when using toPandas() with PySpark
Issue key: SPARK-35401
Issue id: 13378338
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: nafarmer
Creator: nafarmer
Created: 13/May/21 16:51
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Java API
Due Date: 
Votes: 0
Labels: 
Description: Whenever I try to read a Spark dataset using PySpark and convert it to a Pandas df for modeling I get the error: `java.io.StreamCorruptedException: invalid stream header: 204356EC` on the toPandas() step.Whenever I try to read a Spark dataset using PySpark and convert it to a Pandas df for modeling I get the error: `java.io.StreamCorruptedException: invalid stream header: 204356EC` on the toPandas() step.
I am not a Java coder (hence PySpark) and so these errors can be pretty cryptic to me. I tried the following things, but I still have this issue:
 * Made sure my Spark and PySpark versions matched as suggested here: [java.io.StreamCorruptedException when importing a CSV to a Spark DataFrame|https://stackoverflow.com/questions/53286071/java-io-streamcorruptedexception-when-importing-a-csv-to-a-spark-dataframe]
 * Reinstalled Spark using the methods suggested here: [Complete Guide to Installing PySpark on MacOS|https://kevinvecmanis.io/python/pyspark/install/2019/05/31/Installing-Apache-Spark.html]

The logging in the test script below verifies the Spark and PySpark versions are aligned.
test.py:
{code:python}
import logging
from pyspark.sql import SparkSessionfrom pyspark import SparkContext
import findsparkfindspark.init()
logging.basicConfig(    format='%(asctime)s %(levelname)-8s %(message)s',    level=logging.INFO,    datefmt='%Y-%m-%d %H:%M:%S')
sc = SparkContext('local[*]', 'test')spark = SparkSession(sc)logging.info('Spark location: {}'.format(findspark.find()))logging.info('PySpark version: {}'.format(spark.sparkContext.version))
logging.info('Reading spark input dataframe')test_df = spark.read.csv('./data', header=True, sep='|', inferSchema=True)
logging.info('Converting spark DF to pandas DF')pandas_df = test_df.toPandas()logging.info('DF record count: {}'.format(len(pandas_df)))sc.stop()
{code}
Output:
{code:java}
$ python ./test.py   21/05/13 11:54:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.propertiesSetting default log level to "WARN".To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).2021-05-13 11:54:34 INFO     Spark location: /Users/username/server/spark-3.1.1-bin-hadoop2.72021-05-13 11:54:34 INFO     PySpark version: 3.1.12021-05-13 11:54:34 INFO     Reading spark input dataframe2021-05-13 11:54:42 INFO     Converting spark DF to pandas DF                   21/05/13 11:54:42 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.21/05/13 11:54:45 ERROR TaskResultGetter: Exception while getting task result12]java.io.StreamCorruptedException: invalid stream header: 204356EC at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:936) at java.io.ObjectInputStream.<init>(ObjectInputStream.java:394) at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:64) at org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:64) at org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:123) at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108) at org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:97) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996) at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Traceback (most recent call last):  File "./test.py", line 23, in <module>    pandas_df = test_df.toPandas()  File "/Users/username/server/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/pandas/conversion.py", line 141, in toPandas    pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)  File "/Users/username/server/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py", line 677, in collect    sock_info = self._jdf.collectToPython()  File "/Users/username/server/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__  File "/Users/username/server/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py", line 111, in deco    return f(*a, **kw)  File "/Users/username/server/spark-3.1.1-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_valuepy4j.protocol.Py4JJavaError: An error occurred while calling o31.collectToPython.: org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: java.io.StreamCorruptedException: invalid stream header: 204356EC at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253) at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202) at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201) at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201) at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078) at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078) at scala.Option.foreach(Option.scala:407) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2202) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2223) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2242) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267) at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:414) at org.apache.spark.rdd.RDD.collect(RDD.scala:1029) at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390) at org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519) at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685) at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:282) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748){code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun May 16 01:25:51 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0r0i0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/May/21 01:25;gurwls223;[~nafarmer] mind elabourating your enviornment?;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: New PySpark documentation has different URLs
Issue key: SPARK-34606
Issue id: 13362057
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: ondrej
Creator: ondrej
Created: 03/Mar/21 09:21
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Documentation, PySpark
Due Date: 
Votes: 0
Labels: 
Description: The new documentation site moved some subsites to different URLs, notably the PySpark API reference ([see here|https://spark.apache.org/docs/latest/api/python/pyspark.sql.html]). (Note the new `/reference/` bit in the new URL.)

It's the first hit when you google "pyspark sql functions", you'll also get there if you search for individual functions or modules (e.g. "pyspark streaming").

I looked through various JIRA tickets and pull requests, but couldn't find a mention of this. Even the pull request introducing the new documentation site mentions the only visible change to users is the design, not its location.

Possible resolution:
* let the links be refreshed by search engines and live with dead links in various places (stack overflow, emails, bookmarks, ...)
* identify the missing pages and provide a 301 redirects for these (could be found in logs, google analytics, or maybe we can list all assets generated before/now and diff them)
* change sphinx configuration to result in identical links as before

Links to potentially relevant tickets and PRs:
* https://issues.apache.org/jira/browse/SPARK-31851
* https://github.com/apache/spark/pull/29188
* https://issues.apache.org/jira/browse/SPARK-32188
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-36209
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Mar 07 08:37:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0o8y0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Mar/21 09:44;ondrej;I tried building HTML docs for PySpark 2.4.7 and the current master and here's the one-way diff (set(2.4.7) - set(master)).

missing docs
* build/html/pyspark.html
* build/html/pyspark.ml.html
* build/html/pyspark.mllib.html
* build/html/pyspark.sql.html
* build/html/pyspark.streaming.html

other pages not present (module code):
* build/html/_modules/pyspark/profiler.html
* build/html/_modules/pyspark/serializers.html
* build/html/_modules/pyspark/sql/catalog.html
* build/html/_modules/pyspark/sql/context.html
* build/html/_modules/pyspark/sql/udf.html
* build/html/_modules/pyspark/status.html
* build/html/_modules/pyspark/streaming/flume.html
* build/html/_modules/pyspark/streaming/kafka.html
* build/html/_modules/pyspark/streaming/listener.html;;;, 07/Mar/21 05:59;gurwls223;Yeah, I noticed this problem too. One simple solution is to redirect to the root page of new documentation at least. I don't think it's feasible to map each link to the legacy ones.;;;, 07/Mar/21 05:59;gurwls223;[~ondrej], are you working on this? Any PR will be very welcome on this.;;;, 07/Mar/21 08:32;ondrej;[~hyukjin.kwon] gave it a go and [submitted a PR|https://github.com/apache/spark/pull/31770];;;, 07/Mar/21 08:37;apachespark;User 'kokes' has created a pull request for this issue:
https://github.com/apache/spark/pull/31770;;;
Affects Version/s.1: 
Comment.1: 07/Mar/21 05:59;gurwls223;Yeah, I noticed this problem too. One simple solution is to redirect to the root page of new documentation at least. I don't think it's feasible to map each link to the legacy ones.;;;
Comment.2: 07/Mar/21 05:59;gurwls223;[~ondrej], are you working on this? Any PR will be very welcome on this.;;;
Comment.3: 07/Mar/21 08:32;ondrej;[~hyukjin.kwon] gave it a go and [submitted a PR|https://github.com/apache/spark/pull/31770];;;
Comment.4: 07/Mar/21 08:37;apachespark;User 'kokes' has created a pull request for this issue:
https://github.com/apache/spark/pull/31770;;;
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: spark.read.csv is throwing exception ,"lineSep' can contain only 1 character" when parsing windows line feed (CR LF)
Issue key: SPARK-34529
Issue id: 13360748
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: kc.shanmugavel
Creator: kc.shanmugavel
Created: 24/Feb/21 21:14
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.3, 3.1.1, 3.2.0
Fix Version/s: 
Component/s: PySpark, SQL
Due Date: 
Votes: 0
Labels: 
Description: lineSep documentation says - 

`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator that should be used for parsing. Maximum length is 1 character.

Reference: 

 [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader]

When reading csv file using spark

src_df = (spark.read
.option("header", "true")
.option("multiLine","true")
.option("escape", "ǁ")
 .option("lineSep","\r\n")
.schema(materialusetype_Schema)
.option("badRecordsPath","/fh_badfile")
.csv("<path-to-csv>/crlf.csv")
)

Below is the stack trace:

java.lang.IllegalArgumentException: requirement failed: 'lineSep' can contain only 1 character.java.lang.IllegalArgumentException: requirement failed: 'lineSep' can contain only 1 character. at scala.Predef$.require(Predef.scala:281) at org.apache.spark.sql.catalyst.csv.CSVOptions.$anonfun$lineSeparator$1(CSVOptions.scala:209) at scala.Option.map(Option.scala:230) at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:207) at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:58) at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:108) at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:132) at org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:123) at org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:162) at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:510) at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:497) at org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:692) at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:196) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:240) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:236) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192) at org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:79) at org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88) at org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:61) at org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:57) at org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:483) at scala.Option.getOrElse(Option.scala:189) at org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:483) at org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:427) at org.apache.spark.sql.execution.CollectLimitExec.executeCollectResult(limit.scala:58) at org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3013) at org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3004) at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3728) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:248) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:841) at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:198) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3726) at org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3003)
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 26/Aug/21 12:07;Syedhamjath;TestData.csv;https://issues.apache.org/jira/secure/attachment/13032545/TestData.csv, 26/Aug/21 12:04;Syedhamjath;image-2021-08-26-14-04-47-464.png;https://issues.apache.org/jira/secure/attachment/13032543/image-2021-08-26-14-04-47-464.png, 26/Aug/21 12:06;Syedhamjath;image-2021-08-26-14-06-41-055.png;https://issues.apache.org/jira/secure/attachment/13032544/image-2021-08-26-14-06-41-055.png, 26/Aug/21 12:12;Syedhamjath;image-2021-08-26-14-12-30-397.png;https://issues.apache.org/jira/secure/attachment/13032546/image-2021-08-26-14-12-30-397.png, 26/Aug/21 12:42;Syedhamjath;image-2021-08-26-14-42-23-042.png;https://issues.apache.org/jira/secure/attachment/13032547/image-2021-08-26-14-42-23-042.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 5.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 26 12:36:12 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0o1dc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Feb/21 23:09;maropu;Since I think this is not a bug but improvement, I changed the type.;;;, 25/Feb/21 03:39;LuciferYang;There seems to be some discussion [before|https://github.com/apache/spark/pull/23080/files#r272690095];;;, 01/Mar/21 06:00;gurwls223;The problem is that univocity doesn't support 2+ chars properly IIRC.

See also:
https://github.com/apache/spark/pull/18581#issuecomment-314037750
https://github.com/uniVocity/univocity-parsers/issues/170;;;, 26/Aug/21 12:36;Syedhamjath;I'm having similar issue as [~kc.shanmugavel] If we go by documentation \r\n is not a single character it is two characters. Therefore, lineSep option should accept more than one character.

To understand it better I have created a test data - [^TestData.csv]

This TestData.csv is UTF-16 BE BOM and CRLF is used as record delimiter.

!image-2021-08-26-14-06-41-055.png!

Scenario #1

Execute the below command and pray it works well because you cannot enforce \r\n due to max 1 char restriction.

 
{code:java}
from pyspark.sql.functions import *
df = (spark.read
 .format("com.databricks.spark.csv")
 .option("sep", "~")
.option("header", True)
 .option("multiLine", True)
 .option("inferSchema", False)
 .option("encoding", 'utf-16')
 .load("dbfs:/..../TestData.csv"))

{code}
 

 

Result : The result shows that the parsing didn't go well with multiLine option

!image-2021-08-26-14-04-47-464.png!

 

Scenario #2

Since \r\n string has \r character in it. Pass \r to the lineSep

 
{code:java}
from pyspark.sql.functions import *
df = (spark.read
 .format("com.databricks.spark.csv")
 .option("sep", "~")
 .option("lineSep", '\r')
 .option("header", True)
 .option("multiLine", True)
 .option("inferSchema", False)
 .option("encoding", 'utf-16')
 .load("dbfs:/FileStore/shared_uploads/syedhamjath.syedmansur@nl.abnamro.com/TestData.csv"))

display(df.where(col('Col1') != '\n'))

{code}
 

Result : You get records parsed properly but the last character \n in the file creates a blank line. This is the workaround for time being.

!image-2021-08-26-14-42-23-042.png!

 

 ;;;
Affects Version/s.1: 3.1.1
Comment.1: 25/Feb/21 03:39;LuciferYang;There seems to be some discussion [before|https://github.com/apache/spark/pull/23080/files#r272690095];;;
Comment.2: 01/Mar/21 06:00;gurwls223;The problem is that univocity doesn't support 2+ chars properly IIRC.

See also:
https://github.com/apache/spark/pull/18581#issuecomment-314037750
https://github.com/uniVocity/univocity-parsers/issues/170;;;
Comment.3: 26/Aug/21 12:36;Syedhamjath;I'm having similar issue as [~kc.shanmugavel] If we go by documentation \r\n is not a single character it is two characters. Therefore, lineSep option should accept more than one character.

To understand it better I have created a test data - [^TestData.csv]

This TestData.csv is UTF-16 BE BOM and CRLF is used as record delimiter.

!image-2021-08-26-14-06-41-055.png!

Scenario #1

Execute the below command and pray it works well because you cannot enforce \r\n due to max 1 char restriction.

 
{code:java}
from pyspark.sql.functions import *
df = (spark.read
 .format("com.databricks.spark.csv")
 .option("sep", "~")
.option("header", True)
 .option("multiLine", True)
 .option("inferSchema", False)
 .option("encoding", 'utf-16')
 .load("dbfs:/..../TestData.csv"))

{code}
 

 

Result : The result shows that the parsing didn't go well with multiLine option

!image-2021-08-26-14-04-47-464.png!

 

Scenario #2

Since \r\n string has \r character in it. Pass \r to the lineSep

 
{code:java}
from pyspark.sql.functions import *
df = (spark.read
 .format("com.databricks.spark.csv")
 .option("sep", "~")
 .option("lineSep", '\r')
 .option("header", True)
 .option("multiLine", True)
 .option("inferSchema", False)
 .option("encoding", 'utf-16')
 .load("dbfs:/FileStore/shared_uploads/syedhamjath.syedmansur@nl.abnamro.com/TestData.csv"))

display(df.where(col('Col1') != '\n'))

{code}
 

Result : You get records parsed properly but the last character \n in the file creates a blank line. This is the workaround for time being.

!image-2021-08-26-14-42-23-042.png!

 

 ;;;
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, Unknown

Summary: Spark submit should ignore cache for SNAPSHOT dependencies
Issue key: SPARK-34757
Issue id: 13365456
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: apachespark
Reporter: bozhang
Creator: bozhang
Created: 16/Mar/21 09:20
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Deploy, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: When spark-submit is executed with --packages, it will not download the dependency jars when they are available in cache (e.g. ivy cache), even when the dependencies are SNAPSHOTs. 

This might block developers who work on external modules in Spark (e.g. spark-avro), since they need to remove the cache manually every time when they update the code during developments (which generates SNAPSHOT jars). Without knowing this, they could be blocked wondering why their code changes are not reflected in spark-submit executions.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Mar 22 06:07:51 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0otso:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Mar/21 09:45;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/31849;;;, 18/Mar/21 06:32;gurwls223;Issue resolved by pull request 31849
[https://github.com/apache/spark/pull/31849];;;, 22/Mar/21 02:45;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/31918;;;, 22/Mar/21 02:45;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/31918;;;, 22/Mar/21 06:07;gurwls223;Reverted in https://github.com/apache/spark/pull/31918;;;
Affects Version/s.1: 
Comment.1: 18/Mar/21 06:32;gurwls223;Issue resolved by pull request 31849
[https://github.com/apache/spark/pull/31849];;;
Comment.2: 22/Mar/21 02:45;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/31918;;;
Comment.3: 22/Mar/21 02:45;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/31918;;;
Comment.4: 22/Mar/21 06:07;gurwls223;Reverted in https://github.com/apache/spark/pull/31918;;;
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: The result is strange when casting string to date in ORC reading via Schema Evolution
Issue key: SPARK-40289
Issue id: 13479447
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: only1kb
Creator: only1kb
Created: 31/Aug/22 08:31
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Shell
Due Date: 
Votes: 0
Labels: 
Description: I created an ORC file by the code as follows.
{code:java}
val data = Seq(
    ("", "2022-01-32"),  // pay attention to this, null
    ("", "9808-02-30"),  // pay attention to this, 9808-02-29
    ("", "2022-06-31"),  // pay attention to this, 2022-06-30
)
val cols = Seq("str", "date_str")
val df=spark.createDataFrame(data).toDF(cols:_*).repartition(1)
df.printSchema()
df.show(100)
df.write.mode("overwrite").orc("/tmp/orc/data.orc")
{code}
Please note that these three cases are invalid date.
And I read it via:
{code:java}
scala> var df = spark.read.schema("date_str date").orc("/tmp/orc/data.orc"); df.show()
+----------+
|  date_str|
+----------+
|      null|
|9808-02-29|
|2022-06-30|
+----------+{code}
Why is `2022-01-32` converted to `null`, while `9808-02-30` is converted to `9808-02-29`?

Intuitively, they are invalid date, we should return 3 nulls. Is it a bug or a feature?

 

 

*Background*
 * I am working on the project: [https://github.com/NVIDIA/spark-rapids]
 * And I am working on a feature, that is to support reading ORC file as an cuDF (CUDA DataFrame). cuDF is an in-memory data-format of GPU.
 * I need to follow the behaviors of ORC reading in CPU. Otherwise, the users of spark-rapids will feel strange with the results.
 * Therefore I want to know why those happpened.
Environment: * Ubuntu 1804 LTS
 * Spark 311
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Sep 05 04:49:36 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z187eo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Sep/22 04:49;gurwls223;Hm, why don't you read it as a string and cast explicitly? I believe this behaivour is inherited from ORC library itself;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: [SQL] behavior of schema_of_json not same with 2.4.0
Issue key: SPARK-38769
Issue id: 13437364
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: gabry.wu
Creator: gabry.wu
Created: 02/Apr/22 03:32
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When I switch to spark 3.1.1 from spark 2.4.0, I found a built-in function throw errors:
|== Physical Plan == org.apache.spark.sql.AnalysisException: cannot resolve 'schema_of_json(get_json_object(`adtnl_info_txt`, '$.all_model_scores'))' due to data type mismatch: The input json should be a foldable string expression and not null; however, got get_json_object(`adtnl_info_txt`, '$.all_model_scores').; line 3 pos 2; |

But schema_of_json worked well in 2.4.0, So, is it a bug, or a new feature, which doesn't support non-Literal expressions?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Apr 03 02:20:26 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z112y0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Apr/22 01:04;gurwls223;This is meant to work together with {{from_json}} therefore it only allows foldable expression or strings. BTW can I know the usecase of {{schema_of_json(get_json_object(`adtnl_info_txt`, '$.all_model_scores'))}}?;;;, 03/Apr/22 02:20;gabry.wu;[~hyukjin.kwon]  nomatter which UDF to work together, I believe we should not change its behavior, right?

For example, following json contains a field ato_long_v2, however, it will be ato_long_v3, and ato_long_v4, etc. We want to extract the version string as v2,v3,v4, and schema_of_json is used here
{code:java}
{
  "tt_v1": 165
  "tt_long_v2": 474
  "ato_long_v2": 431
  "tt_short_v2": 338
  "ato_v1": 408
  "ato_short_v2": 358
  "sf_long_v3": 400
  "sf_short_v3": 498
}{code};;;
Affects Version/s.1: 
Comment.1: 03/Apr/22 02:20;gabry.wu;[~hyukjin.kwon]  nomatter which UDF to work together, I believe we should not change its behavior, right?

For example, following json contains a field ato_long_v2, however, it will be ato_long_v3, and ato_long_v4, etc. We want to extract the version string as v2,v3,v4, and schema_of_json is used here
{code:java}
{
  "tt_v1": 165
  "tt_long_v2": 474
  "ato_long_v2": 431
  "tt_short_v2": 338
  "ato_v1": 408
  "ato_short_v2": 358
  "sf_long_v3": 400
  "sf_short_v3": 498
}{code};;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: "*" should not throw Exception in SparkGetSchemasOperation
Issue key: SPARK-34146
Issue id: 13352913
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chengpan
Creator: 
Created: 17/Jan/21 16:15
Updated: 24/Nov/22 00:29
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: HiveServer2 treat "*" as list all databases, but spark will throw `Exception` when handle global temp view since "" is not a valid regex.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jan 17 17:18:26 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0mp74:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Jan/21 17:17;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/31217;;;, 17/Jan/21 17:18;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/31217;;;
Affects Version/s.1: 3.1.1
Comment.1: 17/Jan/21 17:18;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/31217;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: PostgresDialect Doesn't handle arrays of custom data types after postgresql driver version 42.2.22
Issue key: SPARK-40024
Issue id: 13476026
Parent id: 
Issue Type: Task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: sjdurfey
Creator: sjdurfey
Created: 09/Aug/22 20:32
Updated: 09/Aug/22 20:32
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Starting in version 42.2.23 (also 42.3.x and 42.4.x), the sql type returned by the postgresql driver is now `ARRAY` with columns with an array of custom data types (e.g. an array of enums). Prior to this version the driver returned the type `CUSTOM`. PostgresDialect can handle custom types and array types, but not array of custom types. Tthe type support within arrays is limited to what is listed in the catalyst type here: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/jdbc/PostgresDialect.scala#L69-L98.] Since a custom type won't match any of those, `None` is returned and eventually `JdbcUtils` throws this exception:

```

java.sql.SQLException: Unsupported type ARRAY
  at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedJdbcTypeError(QueryExecutionErrors.scala:682)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getCatalystType(JdbcUtils.scala:249)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getSchema$1(JdbcUtils.scala:327)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getSchema(JdbcUtils.scala:327)

```

The postgresql driver change was part of [Issue#1948|[https://github.com/pgjdbc/pgjdbc/issues/1948].]

 

I did make a change locally and returned `StringType` instead of `None` for the default case, and that worked fine, but I don't know if that's the desired solution or not.

I created a gist with a code snippet to recreate the issue and run it via spark-shell: https://gist.github.com/sdurfey/f9e73cffaeb90cd9c69dcc771fe59f08
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-08-09 20:32:12.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17mfc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: V2 write for type struct fails to handle case sensitivity on field names during resolution of V2 write command
Issue key: SPARK-39484
Issue id: 13450299
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: erod
Creator: erod
Created: 15/Jun/22 19:34
Updated: 21/Jun/22 15:25
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Summary:

When a V2 write uses an input with a {{struct}} type which contains differences in the casing of field names, the {{caseSensitive}} config is not being honored, always doing a strict case sensitive comparison.

Repro:
{code:java}
CREATE TABLE tmp.test_table_to (key int, object struct<shardId:int>) USING ICEBERG;
CREATE TABLE tmp.test_table_from (key int, object struct<shardid:int>) USING HIVE;
INSERT OVERWRITE tmp.test_table_to SELECT 1 as key, object FROM tmp.test_table_from;{code}
The above results in Exception:
{code:java}
Error in query: unresolved operator 'OverwriteByExpression RelationV2[key#3, object#4] spark_catalog.tmp.test_table_to, true, false;
'OverwriteByExpression RelationV2[key#3, object#4] spark_catalog.tmp.test_table_to, true, false
+- Project [1 AS key#0, object#2]
   +- SubqueryAlias spark_catalog.tmp.test_table_from
      +- HiveTableRelation [`tmp`.`test_table_from`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, Data Cols: [key#1, object#2], Partition Cols: []]{code}
 

If the casing matches in the struct field names, the v2 write works as expected.
Environment: {{{}master{}}}, {{3.1.1}}
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-33136
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jun 15 19:51:24 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z139ww:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Jun/22 19:51;erod;Proposed solution PR: https://github.com/apache/spark/pull/36881;;;, 15/Jun/22 19:51;apachespark;User 'edgarRd' has created a pull request for this issue:
https://github.com/apache/spark/pull/36881;;;
Affects Version/s.1: 3.2.1
Comment.1: 15/Jun/22 19:51;apachespark;User 'edgarRd' has created a pull request for this issue:
https://github.com/apache/spark/pull/36881;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.7.0

Summary: Make stage navigable from max metrics displayed in UI
Issue key: SPARK-38963
Issue id: 13440579
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: pavithraramachandran
Creator: pavithraramachandran
Created: 20/Apr/22 06:10
Updated: 20/Apr/22 06:33
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Web UI
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 20 06:33:27 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11m9k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Apr/22 06:32;pavithraramachandran;working on it;;;, 20/Apr/22 06:33;apachespark;User 'PavithraRamachandran' has created a pull request for this issue:
https://github.com/apache/spark/pull/36278;;;
Affects Version/s.1: 3.2.0
Comment.1: 20/Apr/22 06:33;apachespark;User 'PavithraRamachandran' has created a pull request for this issue:
https://github.com/apache/spark/pull/36278;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Remove an expired indexFilePath from the ESS shuffleIndexCache or the PBS indexCache to save memory.
Issue key: SPARK-38805
Issue id: 13438163
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 06/Apr/22 11:55
Updated: 16/Apr/22 20:02
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: Support to automatically remove an expired indexFilePath from the ESS shuffleIndexCache or the PBS indexCache to save memory.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 06 12:26:03 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z117s8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Apr/22 12:25;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36088;;;, 06/Apr/22 12:26;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36088;;;
Affects Version/s.1: 3.1.1
Comment.1: 06/Apr/22 12:26;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36088;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, Unknown

Summary: Spark master UI behind reverse proxy app storage redirects to master home page
Issue key: SPARK-37725
Issue id: 13419173
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: screwyy
Creator: screwyy
Created: 23/Dec/21 09:43
Updated: 23/Dec/21 09:47
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.1.1
Fix Version/s: 
Component/s: Web UI
Due Date: 
Votes: 0
Labels: 
Description: 1. Using a Spark v3.1.1 Cluster with haproxy as reverse proxy enabled, web UI is at: [http://spark.internal.domain:8080/]
2. Starting an application and enter the application specific web UI (by clicking on the running application name): [http://spark.internal.domain:8080/app/?appId=app-20211202144454-0119]
3. Click on "Application Detail UI" and you will get redirected to [http://spark.internal.domain:8080|http://spark.internal.domain:8080/] [/proxy/app-20211202144454-0119/jobs/|http://sparkm-v3-mlm-op.itn.intraorange/proxy/app-20211202144454-0119/jobs/] (notice the "proxy" in the URL)
4. If you click on any link (e.g. on the "Stages" or "Storage") the Spark master home page UI will be served again, not "stages" not "storage".
5. If you insert the "proxy/app-20211202144454-0119/" and add the stages/ or storage/ in the URL it works.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-23 09:43:30.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xzl4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Driver keep a record of decommission executor
Issue key: SPARK-37432
Issue id: 13413031
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: vanhoale
Creator: vanhoale
Created: 21/Nov/21 18:37
Updated: 23/Nov/21 20:43
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: Hi,

We are running spark on k8s with version 3.1.1. After the spark application running for a while, we are getting the exception below:

On driver: 

 
{code:java}
2021-11-21 18:25:21,859 ERROR Failed to send RPC RPC 6827167497981418905 to /10.1.201.113:58354: java.nio.channels.ClosedChannelException (org.apache.spark.network.client.TransportClient) [rpc-server-4-1]
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Unknown Source)
2021-11-21 18:25:21,864 ERROR Failed to send RPC RPC 7618635518207296341 to /10.1.201.113:58354: java.nio.channels.ClosedChannelException (org.apache.spark.network.client.TransportClient) [rpc-server-4-1]
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Unknown Source)
2021-11-21 18:25:21,868 ERROR Failed to send RPC RPC 5040314884474308699 to /10.1.201.113:58354: java.nio.channels.ClosedChannelException (org.apache.spark.network.client.TransportClient) [rpc-server-4-1]
java.nio.channels.ClosedChannelException
	at io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
	at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
	at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Unknown Source) {code}
 

 

The dead executor (we got the logs exported to persistent storage):

 
{code:java}
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,at java.base/java.lang.Thread.run(Unknown Source)
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.run(BlockManagerDecommissioner.scala:83)
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,at java.base/java.lang.Thread.sleep(Native Method)
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,java.lang.InterruptedException: sleep interrupted
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,"2021-11-21 14:25:47,632 ERROR Error while waiting for block to migrate (org.apache.spark.storage.BlockManagerDecommissioner) [migrate-shuffles-1]"
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,at java.base/java.lang.Thread.run(Unknown Source)
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2021-11-21T14:25:47.745Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.run(BlockManagerDecommissioner.scala:83)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at java.base/java.lang.Thread.sleep(Native Method)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,java.lang.InterruptedException: sleep interrupted
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,"2021-11-21 14:25:47,632 ERROR Error while waiting for block to migrate (org.apache.spark.storage.BlockManagerDecommissioner) [migrate-shuffles-2]"
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at java.base/java.lang.Thread.run(Unknown Source)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at org.apache.spark.storage.BlockManagerDecommissioner$ShuffleMigrationRunnable.run(BlockManagerDecommissioner.scala:83)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,at java.base/java.lang.Thread.sleep(Native Method)
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,java.lang.InterruptedException: sleep interrupted
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,"2021-11-21 14:25:47,632 ERROR Error while waiting for block to migrate (org.apache.spark.storage.BlockManagerDecommissioner) [migrate-shuffles-0]"
2021-11-21T14:25:47.744Z,i-060dc8622c9624dbd,spark,"2021-11-21 14:25:47,618 ERROR Executor self-exiting due to : Finished decommissioning (org.apache.spark.executor.CoarseGrainedExecutorBackend) [wait-for-blocks-to-migrate]"
2021-11-21T14:23:52.722Z,i-060dc8622c9624dbd,spark,"2021-11-21 14:23:52,199 WARN NoSuchMethodException was thrown when disabling normalizeUri. This indicates you are using an old version (< 4.5.8) of Apache http client. It is recommended to use http client version >= 4.5.9 to avoid the breaking change introduced in apache client 4.5.7 and the latency in exception handling. See https://github.com/aws/aws-sdk-java/issues/1919 for more information (com.amazonaws.http.apache.utils.ApacheUtils) [dispatcher-Executor]"
2021-11-21T14:23:51.721Z,i-060dc8622c9624dbd,spark,WARNING: All illegal access operations will be denied in a future release
2021-11-21T14:23:51.721Z,i-060dc8622c9624dbd,spark,WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
2021-11-21T14:23:51.721Z,i-060dc8622c9624dbd,spark,WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
2021-11-21T14:23:51.721Z,i-060dc8622c9624dbd,spark,"WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)"
2021-11-21T14:23:51.721Z,i-060dc8622c9624dbd,spark,WARNING: An illegal reflective access operation has occurred
2021-11-21T14:23:51.721Z,i-060dc8622c9624dbd,spark,+ exec /usr/bin/tini -s -- /usr/local/openjdk-11/bin/java -Dlog4j.configuration=file:/opt/spark/log4j/log4j.properties -javaagent:/prometheus/jmx_prometheus_javaagent-0.16.1.jar=8090:/etc/metrics/conf/prometheus.yaml -Dspark.network.timeout=600s -Dspark.driver.port=7078 -Dspark.driver.blockManager.port=7079 -Xms14336m -Xmx14336m -cp '/opt/spark/conf::/opt/spark/jars/*:' org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url spark://CoarseGrainedScheduler@firehose-ingestion-job-4ace777d3ec0ca06-driver-svc.atlas-spark-apps.svc:7078 --executor-id 699 --cores 3 --app-id spark-be1c315d0c2d49df926455f6d04a50eb --hostname 10.1.201.113 --resourceProfileId 0
2021-11-21T14:23:51.721Z,i-060dc8622c9624dbd,spark,"+ CMD=(${JAVA_HOME}/bin/java ""${SPARK_EXECUTOR_JAVA_OPTS[@]}"" -Xms$SPARK_EXECUTOR_MEMORY -Xmx$SPARK_EXECUTOR_MEMORY -cp ""$SPARK_CLASSPATH:$SPARK_DIST_CLASSPATH"" org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url $SPARK_DRIVER_URL --executor-id $SPARK_EXECUTOR_ID --cores $SPARK_EXECUTOR_CORES --app-id $SPARK_APPLICATION_ID --hostname $SPARK_EXECUTOR_POD_IP --resourceProfileId $SPARK_RESOURCE_PROFILE_ID)" {code}
 

 

The actual spark executor pod was decommissioned but seems like the driver still keeps sending tasks to the driver.

The below (attachment) is the master UI at executor task of the app:

!image-2021-11-21-12-33-56-840.png!

 

and there is no executor pod 699 is running:
{code:java}
$ kubectl get pods |grep firehose
firehose-ingestion-job-driver                             1/1     Running   0          23h
firehoseingestionjob-28d3d27d3ec15aaf-exec-869            1/1     Running   0          18m
firehoseingestionjob-28d3d27d3ec15aaf-exec-874            1/1     Running   0          18m {code}
 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 21/Nov/21 18:40;vanhoale;master_ui_executor_tab.png;https://issues.apache.org/jira/secure/attachment/13036408/master_ui_executor_tab.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Nov 23 19:20:18 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wy7k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Nov/21 19:20;vanhoale;FYI, I had the issue with decommission enabled.

Thoughts if we can have logic like: if a driver doesn't hear any heartbeat from an executor several times then it should remove that executor from its list;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Window function "first" (unboundedFollowing) appears significantly slower than "last" (unboundedPreceding) in identical circumstances
Issue key: SPARK-36844
Issue id: 13403174
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: alainbryden
Creator: alainbryden
Created: 24/Sep/21 15:03
Updated: 29/Oct/21 17:35
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: PySpark, Windows
Due Date: 
Votes: 1
Labels: 
Description: I originally posted a question on SO because I thought perhaps I was doing something wrong:

[https://stackoverflow.com/questions/69308560|https://stackoverflow.com/questions/69308560/spark-first-window-function-is-taking-much-longer-than-last?noredirect=1#comment122505685_69308560]

Perhaps I am, but I'm now fairly convinced that there's something wonky with the implementation of `first` that's causing it to unnecessarily have a much worse complexity than `last`.

 

More or less copy-pasted from the above post:

I was working on a pyspark routine to interpolate the missing values in a configuration table.

Imagine a table of configuration values that go from 0 to 50,000. The user specifies a few data points in between (say at 0, 50, 100, 500, 2000, 500000) and we interpolate the remainder. My solution mostly follows [this blog post|https://walkenho.github.io/interpolating-time-series-p2-spark/] quite closely, except I'm not using any UDFs.

In troubleshooting the performance of this (takes ~3 minutes) I found that one particular window function is taking all of the time, and everything else I'm doing takes mere seconds.

Here is the main area of interest - where I use window functions to fill in the previous and next user-supplied configuration values:
{code:python}
from pyspark.sql import Window, functions as F

# Create partition windows that are required to generate new rows from the ones provided
win_last = Window.partitionBy('PORT_TYPE', 'loss_process').orderBy('rank').rowsBetween(Window.unboundedPreceding, 0)
win_next = Window.partitionBy('PORT_TYPE', 'loss_process').orderBy('rank').rowsBetween(0, Window.unboundedFollowing)

# Join back in the provided config table to populate the "known" scale factors
df_part1 = (df_scale_factors_template
  .join(df_users_config, ['PORT_TYPE', 'loss_process', 'rank'], 'leftouter')
  # Add computed columns that can lookup the prior config and next config for each missing value
  .withColumn('last_rank', F.last( F.col('rank'),         ignorenulls=True).over(win_last))
  .withColumn('last_sf',   F.last( F.col('scale_factor'), ignorenulls=True).over(win_last))
).cache()
debug_log_dataframe(df_part1 , 'df_part1') # Force a .count() and time Part1

df_part2 = (df_part1
  .withColumn('next_rank', F.first(F.col('rank'),         ignorenulls=True).over(win_next))
  .withColumn('next_sf',   F.first(F.col('scale_factor'), ignorenulls=True).over(win_next))
).cache()
debug_log_dataframe(df_part2 , 'df_part2') # Force a .count() and time Part2

df_part3 = (df_part2
  # Implements standard linear interpolation: y = y1 + ((y2-y1)/(x2-x1)) * (x-x1)
  .withColumn('scale_factor', 
              F.when(F.col('last_rank')==F.col('next_rank'), F.col('last_sf')) # Handle div/0 case
              .otherwise(F.col('last_sf') + ((F.col('next_sf')-F.col('last_sf'))/(F.col('next_rank')-F.col('last_rank'))) * (F.col('rank')-F.col('last_rank'))))
  .select('PORT_TYPE', 'loss_process', 'rank', 'scale_factor')
).cache()
debug_log_dataframe(df_part3, 'df_part3', explain: True)
{code}
 

The above used to be a single chained dataframe statement, but I've since split it into 3 parts so that I could isolate the part that's taking so long. The results are:
 * {{Part 1: Generated 8 columns and 300006 rows in 0.65 seconds}}
 * {{Part 2: Generated 10 columns and 300006 rows in 189.55 seconds}}
 * {{Part 3: Generated 4 columns and 300006 rows in 0.24 seconds}}

 

In trying various things to speed up my routine, it occurred to me to try re-rewriting my usages of {{first()}} to just be usages of {{last()}} with a reversed sort order.

So rewriting this:

{code:python}
win_next = (Window.partitionBy('PORT_TYPE', 'loss_process')
  .orderBy('rank').rowsBetween(0, Window.unboundedFollowing))

df_part2 = (df_part1
  .withColumn('next_rank', F.first(F.col('rank'),         ignorenulls=True).over(win_next))
  .withColumn('next_sf',   F.first(F.col('scale_factor'), ignorenulls=True).over(win_next))
)
{code}
 
As this:

{code:python}
win_next = (Window.partitionBy('PORT_TYPE', 'loss_process')
  .orderBy(F.desc('rank')).rowsBetween(Window.unboundedPreceding, 0))

df_part2 = (df_part1
  .withColumn('next_rank', F.last(F.col('rank'),         ignorenulls=True).over(win_next))
  .withColumn('next_sf',   F.last(F.col('scale_factor'), ignorenulls=True).over(win_next))
)
{code}
 
Much to my amazement, this actually solved the performance problem, and now the entire dataframe is generated in just 3 seconds.

I don't know anything about the internals, but conceptually I feel as though the initial solution should be faster, because all 4 columns should be able to take advantage of the same window and sort order by merely look forwards or backwards along the window - re-sorting like this shouldn't be necessary.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 24/Sep/21 15:05;alainbryden;Physical Plan 2 - workaround.png;https://issues.apache.org/jira/secure/attachment/13034118/Physical+Plan+2+-+workaround.png, 24/Sep/21 15:04;alainbryden;Pysical Plan.png;https://issues.apache.org/jira/secure/attachment/13034117/Pysical+Plan.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): https://stackoverflow.com/questions/69308560/
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Oct 29 17:35:19 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v9ko:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Sep/21 15:07;alainbryden;I've attached the physical plan from the initial implementation. `last` ends up using "RunningWindowFunction" whereas `first` just says "Window"

!Pysical Plan.png! 

Here it is after I've re-written things to explicitly reverse the window sort order and use `last` instead of `first`.  Now "RunningWindowFunction" is used in both cases and the dataframe evaluates several orders of magnitude faster (1-2 seconds instead of ~3 minutes):

!Physical Plan 2 - workaround.png! ;;;, 29/Oct/21 17:35;tanelk;Hello,

I also hit this issue a while back and found that, It is a bit explained in this code comment:
https://github.com/apache/spark/blob/abf9675a7559d5666e40f25098334b5edbf8c0c3/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala#L609-L611

So it is not the fault of the first aggregator, but it is the UnboundedFollowing window frame. There are definetly some optimizations, that could be done.

If I followed your code correctly, then I think you would be better of using the [lead|https://spark.apache.org/docs/latest/api/sql/index.html#lead] and [lag|https://spark.apache.org/docs/latest/api/sql/index.html#lag] window functions. With those you can drop the .rowsBetween(...) part from your window specs.;;;
Affects Version/s.1: 
Comment.1: 29/Oct/21 17:35;tanelk;Hello,

I also hit this issue a while back and found that, It is a bit explained in this code comment:
https://github.com/apache/spark/blob/abf9675a7559d5666e40f25098334b5edbf8c0c3/sql/core/src/main/scala/org/apache/spark/sql/execution/window/WindowFunctionFrame.scala#L609-L611

So it is not the fault of the first aggregator, but it is the UnboundedFollowing window frame. There are definetly some optimizations, that could be done.

If I followed your code correctly, then I think you would be better of using the [lead|https://spark.apache.org/docs/latest/api/sql/index.html#lead] and [lag|https://spark.apache.org/docs/latest/api/sql/index.html#lag] window functions. With those you can drop the .rowsBetween(...) part from your window specs.;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Spark uncaught exception handler is using logError
Issue key: SPARK-36756
Issue id: 13401046
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: cchantepie
Creator: cchantepie
Created: 14/Sep/21 15:14
Updated: 15/Sep/21 01:19
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.0.4, 3.1.0, 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark is [setting up an handler|https://github.com/apache/spark/blob/v3.0.1/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L915] to catch any uncaught exception.

This [handler itself catch any subsequent exception|https://github.com/apache/spark/blob/v3.0.1/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala#L64] that can happen while reporting the initially uncaught exception.

Issue is that if the subsequent exception is due to a log4j issue, as the {{catch}} there is also using {{logError}}, it will loop, not display any exception in the logs, and exiting with code 51.

e.g. We got an log4j issue when using logstash JSON logging :

{noformat}
at net.logstash.log4j.JSONEventLayoutV1.format(JSONEventLayoutV1.java:137)
at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310)
at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
at org.apache.log4j.Category.callAppenders(Category.java:206)
at org.apache.log4j.Category.forcedLog(Category.java:391)
at org.apache.log4j.Category.log(Category.java:856)
at org.slf4j.impl.Log4jLoggerAdapter.error(Log4jLoggerAdapter.java:576)
at org.apache.spark.internal.Logging.logError(Logging.scala:94)
at org.apache.spark.internal.Logging.logError$(Logging.scala:93)
at org.apache.spark.util.SparkUncaughtExceptionHandler.logError(SparkUncaughtExceptionHandler.scala:28)
at org.apache.spark.util.SparkUncaughtExceptionHandler.uncaughtException(SparkUncaughtExceptionHandler.scala:37)
at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057)
at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052)
at java.lang.Thread.dispatchUncaughtException(Thread.java:1959)
{noformat}

*Suggested fix:*

Directly using {{println}} and {{printStackTrace}} as safe fallback in this {{catch}}.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): java
Custom field (Last public comment date): 2021-09-14 15:14:37.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uwg8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.0.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Broadcast job is not aborted even the SQL statement canceled
Issue key: SPARK-34064
Issue id: 13351365
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: cltlfcjin
Creator: cltlfcjin
Created: 11/Jan/21 05:37
Updated: 13/Sep/21 22:22
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.1.1, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: SPARK-27036 introduced a runId for BroadcastExchangeExec to resolve the problem that a broadcast job is not aborted when broadcast timeout happens. Since the runId is a random UUID, when a SQL statement is cancelled, these broadcast sub-jobs still not canceled as a whole.

 !Screen Shot 2021-01-11 at 12.03.13 PM.png|width=100%! 

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 11/Jan/21 05:37;cltlfcjin;Screen Shot 2021-01-11 at 12.03.13 PM.png;https://issues.apache.org/jira/secure/attachment/13018476/Screen+Shot+2021-01-11+at+12.03.13+PM.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Sep 13 22:22:33 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0mfnk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Jan/21 06:12;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/31119;;;, 11/Jan/21 06:13;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/31119;;;, 13/Jan/21 12:59;cloud_fan;Issue resolved by pull request 31119
[https://github.com/apache/spark/pull/31119];;;, 18/Jan/21 07:41;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/31227;;;, 07/Apr/21 13:58;inetfuture;May I ask, when will this be resolved?;;;, 13/Sep/21 22:22;holden;[~inetfuture]it's hard to say since the initial fix was reverted, if you want to pick it up your self that's an option.;;;
Affects Version/s.1: 3.1.1
Comment.1: 11/Jan/21 06:13;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/31119;;;
Comment.2: 13/Jan/21 12:59;cloud_fan;Issue resolved by pull request 31119
[https://github.com/apache/spark/pull/31119];;;
Comment.3: 18/Jan/21 07:41;apachespark;User 'LantaoJin' has created a pull request for this issue:
https://github.com/apache/spark/pull/31227;;;
Comment.4: 07/Apr/21 13:58;inetfuture;May I ask, when will this be resolved?;;;
Comment.5: 13/Sep/21 22:22;holden;[~inetfuture]it's hard to say since the initial fix was reverted, if you want to pick it up your self that's an option.;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, Unknown

Summary: Volcano resource manager for Spark on Kubernetes
Issue key: SPARK-35623
Issue id: 13381844
Parent id: 
Issue Type: Brainstorming
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: dipanjanK
Creator: dipanjanK
Created: 03/Jun/21 07:15
Updated: 02/Sep/21 11:56
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 2
Labels: kubernetes, resourcemanager
Description: Dear Spark Developers, 
  
 Hello from the Netherlands! Posting this here as I still haven't gotten accepted to post in the spark dev mailing list.
  
 My team is planning to use spark with Kubernetes support on our shared (multi-tenant) on premise Kubernetes cluster. However we would like to have certain scheduling features like fair-share and preemption which as we understand are not built into the current spark-kubernetes resource manager yet. We have been working on and are close to a first successful prototype integration with Volcano ([https://volcano.sh/en/docs/]). Briefly this means a new resource manager component with lots in common with existing spark-kubernetes resource manager, but instead of pods it launches Volcano jobs which delegate the driver and executor pod creation and lifecycle management to Volcano. We are interested in contributing this to open source, either directly in spark or as a separate project.
  
 So, two questions: 
  
 1. Do the spark maintainers see this as a valuable contribution to the mainline spark codebase? If so, can we have some guidance on how to publish the changes? 
  
 2. Are any other developers / organizations interested to contribute to this effort? If so, please get in touch.
  
 Best,
 Dipanjan
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 02 11:27:40 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rm2g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): holden
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Jun/21 03:42;pingsutw;[~dipanjanK] Thanks for proposing this feature.

I'm interested to contribute this feature.;;;, 21/Jun/21 11:48;dipanjanK;Hi [~pingsutw], thank you for expressing your interest! We are in the process of publishing a first draft. In the meantime, how can we contact you, to maybe give you a more detailed overview? Do you have a preferred email address?;;;, 21/Jun/21 23:48;k82cn;That's interesting, I'd like to help on that :);;;, 23/Jun/21 22:13;holden;I'm also interested in this. I sent a message to the dev list back on Jun 17th about this (or more generally adding support for batch schedulers in general).

 

I know some groups inside of Spark have had a working group format where they sync periodically and write reports back to the mailing list. Since it seems like there are a few folks  interested maybe we could try that?;;;, 24/Jun/21 02:11;pingsutw;[~dipanjanK] Here is my email address. pingsutw@gmail.com;;;, 24/Jun/21 12:15;dipanjanK;I added our WIP code on Github: [https://github.com/spark-volcano-wip/spark-3-volcano. |https://github.com/spark-volcano-wip/spark-3-volcano/blob/main/README.md#how-it-works]

Please feel free to take a look and comment.

High level overview in [https://github.com/spark-volcano-wip/spark-3-volcano/blob/main/README.md#how-it-works]

Please let me know if you'd like to be added as a collaborator. 

[~holden] - sounds like a great idea. Certainly me and my colleagues on working on this project would be interested in it.  What format of collaboration are you thinking? More face to face such as Slack or Google Meets, or something more async like Google Groups? 

 

 ;;;, 24/Jun/21 12:26;dipanjanK;[~pingsutw] - I sent you an invite as a collaborator.;;;, 02/Sep/21 11:27;senthh;[~dipanjanK] Include me too pls.

mail id: senthissenthh@gmail.com;;;
Affects Version/s.1: 3.1.2
Comment.1: 21/Jun/21 11:48;dipanjanK;Hi [~pingsutw], thank you for expressing your interest! We are in the process of publishing a first draft. In the meantime, how can we contact you, to maybe give you a more detailed overview? Do you have a preferred email address?;;;
Comment.2: 21/Jun/21 23:48;k82cn;That's interesting, I'd like to help on that :);;;
Comment.3: 23/Jun/21 22:13;holden;I'm also interested in this. I sent a message to the dev list back on Jun 17th about this (or more generally adding support for batch schedulers in general).

 

I know some groups inside of Spark have had a working group format where they sync periodically and write reports back to the mailing list. Since it seems like there are a few folks  interested maybe we could try that?;;;
Comment.4: 24/Jun/21 02:11;pingsutw;[~dipanjanK] Here is my email address. pingsutw@gmail.com;;;
Comment.5: 24/Jun/21 12:15;dipanjanK;I added our WIP code on Github: [https://github.com/spark-volcano-wip/spark-3-volcano. |https://github.com/spark-volcano-wip/spark-3-volcano/blob/main/README.md#how-it-works]

Please feel free to take a look and comment.

High level overview in [https://github.com/spark-volcano-wip/spark-3-volcano/blob/main/README.md#how-it-works]

Please let me know if you'd like to be added as a collaborator. 

[~holden] - sounds like a great idea. Certainly me and my colleagues on working on this project would be interested in it.  What format of collaboration are you thinking? More face to face such as Slack or Google Meets, or something more async like Google Groups? 

 

 ;;;
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0

Summary: MinHashLSH.approxSimilarityJoin should not required inputCol if output exist
Issue key: SPARK-36458
Issue id: 13394178
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: tthien
Creator: tthien
Created: 09/Aug/21 11:02
Updated: 10/Aug/21 05:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: ML
Due Date: 
Votes: 0
Labels: 
Description: Refer to documents and example code in MinHashLSH 
 [https://spark.apache.org/docs/latest/ml-features#minhash-for-jaccard-distance]

The example written that:

We could avoid computing hashes by passing in the already-transformed dataset, e.g. `model.approxSimilarityJoin(transformedA, transformedB, 0.6)`

However, inputCol still required in transformedA and transformedB even if they already have outputCol.

A code that should work but it doesn't

 

 
{code:java}
from pyspark.ml.feature import MinHashLSH
 from pyspark.ml.linalg import Vectors
 from pyspark.sql.functions import col
dataA = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),
 (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),
 (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]
 dfA = spark.createDataFrame(dataA, ["id", "features"])
dataB = [(3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),
 (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),
 (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]
 dfB = spark.createDataFrame(dataB, ["id", "features"])
key = Vectors.sparse(6, [1, 3], [1.0, 1.0])
mh = MinHashLSH(inputCol="features", outputCol="hashes", numHashTables=5)
 model = mh.fit(dfA)
transformedA = model.transform(dfA).select("id", "hashes")
 transformedB = model.transform(dfB).select("id", "hashes")
model.approxSimilarityJoin(transformedA, transformedB, 0.6, distCol="JaccardDistance")\
 .select(col("datasetA.id").alias("idA"),
 col("datasetB.id").alias("idB"),
 col("JaccardDistance")).show()
{code}
As in the code I give, I discard columns `features` but keep column `hashes` which is output data. 
 approxSimilarityJoin should only work on `hashes` (the outputCol), which is exist and ignore the lack of `features` (the inputCol).

Be able to transform the data beforehand and remove inputCol can make input data much smaller and prevent confusion about the tip "_We could avoid computing hashes by passing in the already-transformed dataset_".
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-09 11:02:12.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tq2o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Select filter query on table with struct complex type fails
Issue key: SPARK-35835
Issue id: 13384775
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: chetdb
Creator: chetdb
Created: 21/Jun/21 05:35
Updated: 21/Jun/21 07:39
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: [Steps]:-

From Spark beeline create a parquet or ORC table having complex type data. Load data in the table and execute select filter query.

0: jdbc:hive2://vm2:22550/> create table Struct_com (CUST_ID string, YEAR int, MONTH int, AGE int, GENDER string, EDUCATED string, IS_MARRIED string, STRUCT_INT_DOUBLE_STRING_DATE struct<ID:int,SALARY:double,COUNTRY:STRING,CHECK_DATE:string>,CARD_COUNT int,DEBIT_COUNT int, CREDIT_COUNT int, DEPOSIT double, HQ_DEPOSIT double) stored as parquet;
+---------+
| Result |
+---------+
+---------+
No rows selected (0.161 seconds)
0: jdbc:hive2://vm2:22550/> LOAD DATA INPATH 'hdfs://hacluster/chetan/Struct.csv' OVERWRITE INTO TABLE Struct_com;
+---------+
| Result |
+---------+
+---------+
No rows selected (1.09 seconds)
0: jdbc:hive2://vm2:22550/> SELECT struct_int_double_string_date.COUNTRY,struct_int_double_string_date.CHECK_DATE,struct_int_double_string_date.CHECK_DATE,struct_in t_double_string_date.Country, SUM(struct_int_double_string_date.id) AS Sum FROM (select * from Struct_com) SUB_QRY WHERE struct_int_double_string_date.id > 5700 GRO UP BY struct_int_double_string_date.COUNTRY,struct_int_double_string_date.CHECK_DATE,struct_int_double_string_date.CHECK_DATE,struct_int_double_string_date.Country ORDER BY struct_int_double_string_date.COUNTRY asc,struct_int_double_string_date.CHECK_DATE asc,struct_int_double_string_date.CHECK_DATE asc, struct_int_double_stri ng_date.Country asc;

 

[Actual Issue] : - Select filter query on table with struct complex type fails

0: jdbc:hive2://vm2:22550/> SELECT struct_int_double_string_date.COUNTRY,struct_int_double_string_date.CHECK_DATE,struct_int_double_string_date.CHECK_DATE,struct_in t_double_string_date.Country, SUM(struct_int_double_string_date.id) AS Sum FROM (select * from Struct_com) SUB_QRY WHERE struct_int_double_string_date.id > 5700 GRO UP BY struct_int_double_string_date.COUNTRY,struct_int_double_string_date.CHECK_DATE,struct_int_double_string_date.CHECK_DATE,struct_int_double_string_date.Country ORDER BY struct_int_double_string_date.COUNTRY asc,struct_int_double_string_date.CHECK_DATE asc,struct_int_double_string_date.CHECK_DATE asc, struct_int_double_stri ng_date.Country asc;
Error: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange rangepartitioning(COUNTRY#139896 ASC NULLS FIRST, CHECK_DATE#139897 ASC NULLS FIRST, CHECK_DATE#139897 ASC NULLS FIRST, COUNTRY#139896 ASC NULLS FIRST, 200 ), ENSURE_REQUIREMENTS, [id=#17161]
+- *(2) HashAggregate(keys=[_gen_alias_139928#139928, _gen_alias_139929#139929], functions=[sum(cast(_gen_alias_139931#139931 as bigint))], output=[COUNTRY#139896, CHECK_DATE#139897, CHECK_DATE#139898, Country#139899, Sum#139877L])
+- Exchange hashpartitioning(_gen_alias_139928#139928, _gen_alias_139929#139929, 200), ENSURE_REQUIREMENTS, [id=#17157]
+- *(1) HashAggregate(keys=[_gen_alias_139928#139928, _gen_alias_139929#139929], functions=[partial_sum(cast(_gen_alias_139931#139931 as bigint))], output=[_g en_alias_139928#139928, _gen_alias_139929#139929, sum#139934L])
+- *(1) Project [STRUCT_INT_DOUBLE_STRING_DATE#139885.COUNTRY AS _gen_alias_139928#139928, STRUCT_INT_DOUBLE_STRING_DATE#139885.CHECK_DATE AS _gen_alias_13 9929#139929, STRUCT_INT_DOUBLE_STRING_DATE#139885.COUNTRY AS _gen_alias_139930#139930, STRUCT_INT_DOUBLE_STRING_DATE#139885.ID AS _gen_alias_139931#139931]
+- *(1) Filter (isnotnull(STRUCT_INT_DOUBLE_STRING_DATE#139885) AND (STRUCT_INT_DOUBLE_STRING_DATE#139885.ID > 5700))
+- FileScan parquet default.struct_com[STRUCT_INT_DOUBLE_STRING_DATE#139885] Batched: false, DataFilters: [isnotnull(STRUCT_INT_DOUBLE_STRING_DATE#13 9885), (STRUCT_INT_DOUBLE_STRING_DATE#139885.ID > 5700)], Format: Parquet, Location: InMemoryFileIndex[hdfs://hacluster/user/hive/warehouse/struct_com], PartitionFi lters: [], PushedFilters: [IsNotNull(STRUCT_INT_DOUBLE_STRING_DATE), GreaterThan(STRUCT_INT_DOUBLE_STRING_DATE.ID,5700)], ReadSchema: struct<STRUCT_INT_DOUBLE_STRIN G_DATE:struct<ID:int,COUNTRY:string,CHECK_DATE:string>>

at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(Spar kExecuteStatementOperation.scala:396)
at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$3(SparkExecuteStatementOperation.scala:281)
at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:46)
at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:281)
at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:268)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1761)
at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:295)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
at java.util.concurrent.FutureTask.run(FutureTask.java:266)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange rangepartitioning(COUNTRY#139896 ASC NULLS FIRST, CHECK_DATE#139897 ASC NULLS FIRST, CHECK_DATE#139897 ASC NULLS FIRST, COUNTRY#139896 ASC NULLS FIRST, 200 ), ENSURE_REQUIREMENTS, [id=#17161]
+- *(2) HashAggregate(keys=[_gen_alias_139928#139928, _gen_alias_139929#139929], functions=[sum(cast(_gen_alias_139931#139931 as bigint))], output=[COUNTRY#139896, CHECK_DATE#139897, CHECK_DATE#139898, Country#139899, Sum#139877L])
+- Exchange hashpartitioning(_gen_alias_139928#139928, _gen_alias_139929#139929, 200), ENSURE_REQUIREMENTS, [id=#17157]
+- *(1) HashAggregate(keys=[_gen_alias_139928#139928, _gen_alias_139929#139929], functions=[partial_sum(cast(_gen_alias_139931#139931 as bigint))], output=[_g en_alias_139928#139928, _gen_alias_139929#139929, sum#139934L])
+- *(1) Project [STRUCT_INT_DOUBLE_STRING_DATE#139885.COUNTRY AS _gen_alias_139928#139928, STRUCT_INT_DOUBLE_STRING_DATE#139885.CHECK_DATE AS _gen_alias_13 9929#139929, STRUCT_INT_DOUBLE_STRING_DATE#139885.COUNTRY AS _gen_alias_139930#139930, STRUCT_INT_DOUBLE_STRING_DATE#139885.ID AS _gen_alias_139931#139931]
+- *(1) Filter (isnotnull(STRUCT_INT_DOUBLE_STRING_DATE#139885) AND (STRUCT_INT_DOUBLE_STRING_DATE#139885.ID > 5700))
+- FileScan parquet default.struct_com[STRUCT_INT_DOUBLE_STRING_DATE#139885] Batched: false, DataFilters: [isnotnull(STRUCT_INT_DOUBLE_STRING_DATE#13 9885), (STRUCT_INT_DOUBLE_STRING_DATE#139885.ID > 5700)], Format: Parquet, Location: InMemoryFileIndex[hdfs://hacluster/user/hive/warehouse/struct_com], PartitionFi lters: [], PushedFilters: [IsNotNull(STRUCT_INT_DOUBLE_STRING_DATE), GreaterThan(STRUCT_INT_DOUBLE_STRING_DATE.ID,5700)], ReadSchema: struct<STRUCT_INT_DOUBLE_STRIN G_DATE:struct<ID:int,COUNTRY:string,CHECK_DATE:string>>

at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)
at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:321)
at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:387)
at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3706)
at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2968)
at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3697)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:108)
at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:170)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:91)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:777)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3695)
at org.apache.spark.sql.Dataset.collect(Dataset.scala:2968)
at org.apache.spark.sql.hive.thriftserver.SparkExecuteProxyStatementOperation.processResults(SparkExecuteProxyStatementOperation.scala:221)
at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(Spar kExecuteStatementOperation.scala:365)
... 16 more
Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: _gen_alias_139930#139930
at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)
at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)
at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:317)
at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:73)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:317)
at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:322)
at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:407)
at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243)
at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:405)
at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:322)
at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:306)
at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)
at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:96)
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
at scala.collection.immutable.List.foreach(List.scala:392)
at scala.collection.TraversableLike.map(TraversableLike.scala:238)
at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
at scala.collection.immutable.List.map(List.scala:298)
at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:96)
at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:554)
at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:741)
at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:148)
at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)
at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)
at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:47)
at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)
at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)
at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)
at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
... 47 more
Caused by: java.lang.RuntimeException: Couldn't find _gen_alias_139930#139930 in [_gen_alias_139928#139928,_gen_alias_139929#139929,sum(cast(_gen_alias_139931#13993 1 as bigint))#139901L]
at scala.sys.package$.error(package.scala:30)
at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)
at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
... 90 more (state=,code=0)

 

[Expected Result] :- Select filter query on table with struct complex type should be success
Environment: Spark 3.1.1
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 21 07:39:40 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s44w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Jun/21 05:36;pavithraramachandran;i shall raise a PR soon;;;, 21/Jun/21 07:39;apachespark;User 'PavithraRamachandran' has created a pull request for this issue:
https://github.com/apache/spark/pull/32996;;;
Affects Version/s.1: 
Comment.1: 21/Jun/21 07:39;apachespark;User 'PavithraRamachandran' has created a pull request for this issue:
https://github.com/apache/spark/pull/32996;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: TimestampType: OverflowError: mktime argument out of range 
Issue key: SPARK-35515
Issue id: 13380324
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: mstuder
Creator: mstuder
Created: 25/May/21 11:54
Updated: 25/May/21 12:05
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: This issue occurs, for example, when trying to create a data frame from Python {{datetime}} objects that are "out of range" where "out of range" is platform-dependent due to the use of [{{time.mktime}}|https://docs.python.org/3/library/time.html#time.mktime] in {{TimestampType.toInternal}}:

{code}
import datetime
spark_session.createDataFrame([(datetime.datetime(9999, 12, 31, 0, 0),)])
{code}

A more direct way to reproduce the issue is by invoking {{TimestampType.toInternal}} directly:
{code}
import datetime
from pyspark.sql.types import TimestampType
dt = datetime.datetime(9999, 12, 31, 0, 0)
TimestampType().toInternal(dt)
{code}

The suggested improvement is to avoid using {{time.mktime}} to increase the range of {{datetime}} values. A possible implementation may look as follows:

{code}
import datetime
import pytz

EPOCH_UTC = datetime.datetime(1970, 1, 1).replace(tzinfo=pytz.utc)
LOCAL_TZ = datetime.datetime.now().astimezone().tzinfo

def toInternal(dt):
	if dt is not None:
		dt = dt if dt.tzinfo else dt.replace(tzinfo=LOCAL_TZ)
		dt_utc = dt.astimezone(pytz.utc)
		td = dt_utc - EPOCH_UTC
		return (td.days * 86400 + td.seconds) * 10 ** 6 + td.microseconds
{code}

This relies on the ability to derive the local timezone. Other mechanisms may be used to what is suggested above.

Test cases include:
{code}
dt1 = datetime.datetime(2021, 5, 25, 12, 23)
dt2 = dt1.replace(tzinfo=pytz.timezone('Europe/Zurich'))
dt3 = datetime.datetime(9999, 12, 31, 0, 0)
dt4 = dt3.replace(tzinfo=pytz.timezone('Europe/Zurich'))

toInternal(dt1) == TimestampType().toInternal(dt1)
toInternal(dt2) == TimestampType().toInternal(dt2)
toInternal(dt3) # TimestampType().toInternal(dt3) fails
toInternal(dt4) == TimestampType().toInternal(dt4)
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue May 25 12:05:54 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rcqw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/May/21 12:05;mstuder;I'm happy to provide a PR if this seems like a sensible improvement.;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: _SUCCESS file not written when using partitionOverwriteMode=dynamic
Issue key: SPARK-35279
Issue id: 13375935
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: gorankl
Creator: gorankl
Created: 30/Apr/21 03:59
Updated: 24/May/21 11:27
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Steps to reproduce:

 
{code:java}
case class A(a: String, b:String)
def df = List(A("a", "b")).toDF
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")
val writer = df.write.mode(SaveMode.Overwrite).partitionBy("a") 
writer.parquet("s3a://some_bucket/test/")
{code}

when spark.sql.sources.partitionOverwriteMode is set to dynamic, the output written doesn't have _SUCCESS file updated.

(I have checked different versions of hadoop from 3.1.4 to 3.22 and they all behave the same, so the issue is with spark)

This is working in spark 3.0.2
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon May 24 11:27:29 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qlpk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/May/21 18:54;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/32437;;;, 04/May/21 18:54;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/32437;;;, 15/May/21 17:46;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/32558;;;, 24/May/21 11:27;stevel@apache.org;Any reason for not using the S3A committers here? i.e the ones which are safe to use on S3?;;;
Affects Version/s.1: 
Comment.1: 04/May/21 18:54;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/32437;;;
Comment.2: 15/May/21 17:46;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/32558;;;
Comment.3: 24/May/21 11:27;stevel@apache.org;Any reason for not using the S3A committers here? i.e the ones which are safe to use on S3?;;;
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Subexpression elimination leading to a performance regression
Issue key: SPARK-35256
Issue id: 13375609
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: ondrej
Creator: ondrej
Created: 28/Apr/21 14:07
Updated: 20/May/21 14:50
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I'm seeing almost double the runtime between 3.0.1 and 3.1.1 in my pipeline that does mostly str_to_map, split and a few other operations - all projections, no joins or aggregations (it's here only to trigger the pipeline). I cut it down to the simplest reproducible example I could - anything I remove from this changes the runtime difference quite dramatically. (even moving those two expressions from f.when to standalone columns makes the difference disappear)
{code:java}
import time
import os

import pyspark  
from pyspark.sql import SparkSession

import pyspark.sql.functions as f

if __name__ == '__main__':
    print(pyspark.__version__)
    spark = SparkSession.builder.getOrCreate()

    filename = 'regression.csv'
    if not os.path.isfile(filename):
        with open(filename, 'wt') as fw:
            fw.write('foo\n')
            for _ in range(10_000_000):
                fw.write('foo=bar&baz=bak&bar=f,o,1:2:3\n')

    df = spark.read.option('header', True).csv(filename)
    t = time.time()
    dd = (df
            .withColumn('my_map', f.expr('str_to_map(foo, "&", "=")'))
            .withColumn('extracted',
                            # without this top level split it is only 50% slower, with it
                            # the runtime almost doubles
                            f.split(f.split(f.col("my_map")["bar"], ",")[2], ":")[0]
                       )
            .select(
                f.when(
                    f.col("extracted").startswith("foo"), f.col("extracted")
                ).otherwise(
                    f.concat(f.lit("foo"), f.col("extracted"))
                ).alias("foo")
            )
        )
    # dd.explain(True)
    _ = dd.groupby("foo").count().count()
    print("elapsed", time.time() - t)
{code}
Running this in 3.0.1 and 3.1.1 respectively (both installed from PyPI, on my local macOS)
{code:java}
3.0.1
elapsed 21.262351036071777
3.1.1
elapsed 40.26582884788513
{code}
(Meaning the transformation took 21 seconds in 3.0.1 and 40 seconds in 3.1.1)

Feel free to make the CSV smaller to get a quicker feedback loop - it scales linearly (I developed this with 2M rows).

It might be related to my previous issue - SPARK-32989 - there are similar operations, nesting etc. (splitting on the original column, not on a map, makes the difference disappear)

I tried dissecting the queries in SparkUI and via explain, but both 3.0.1 and 3.1.1 produced identical plans.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 13/May/21 15:14;ondrej;bisect_log.txt;https://issues.apache.org/jira/secure/attachment/13025429/bisect_log.txt, 13/May/21 15:14;ondrej;bisect_timing.csv;https://issues.apache.org/jira/secure/attachment/13025430/bisect_timing.csv
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu May 20 14:50:59 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qjp4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/May/21 15:17;ondrej;I ran git bisect with the setup mentioned in this ticket and found this to be the offending commit

[https://github.com/apache/spark/commit/6fa80ed1dd43c2ecd092c10933330b501641c51b]

I attached some logs to this issue - first a log of the actual bisection and then a CSV with all the timings - that is, I would build Spark (Ubuntu 20.x, OpenJDK 8) and run the script and if it was below 50 seconds, it was good, above it meant bad. I first timed the script using 3.0.1 and 3.1.1 downloaded from PyPI to get reference numbers for the good/bad threshold.;;;, 19/May/21 23:11;kimahriman;See https://issues.apache.org/jira/browse/SPARK-35448 for a few related issues for subexpression elimination being worked. Specifically https://issues.apache.org/jira/browse/SPARK-35410 is probably the main issue you're facing where `when` expressions can cause a lot of extra, unused subexpressions to be included in the codegen;;;, 20/May/21 14:50;ondrej;[~Kimahriman] I think you're right - I've just built the PR linked in 35410 and it brought the runtime to less than half of what it was under 3.1.1 (15.45s vs 35s) and it's also faster than 2.4.x, which is nice. So if merged, I'll close this as a dupe - but for now I'll subscribe to that issue and PR and wait for its resolution.

Strangely enough, my original pipeline (which was simplified into the repro linked in this issue) is only 10% faster than under 3.1.1 (so way way slower than 2.4.x), so there are more things at play. I'll investigate more once this is merged.

Thanks for the links!;;;
Affects Version/s.1: 
Comment.1: 19/May/21 23:11;kimahriman;See https://issues.apache.org/jira/browse/SPARK-35448 for a few related issues for subexpression elimination being worked. Specifically https://issues.apache.org/jira/browse/SPARK-35410 is probably the main issue you're facing where `when` expressions can cause a lot of extra, unused subexpressions to be included in the codegen;;;
Comment.2: 20/May/21 14:50;ondrej;[~Kimahriman] I think you're right - I've just built the PR linked in 35410 and it brought the runtime to less than half of what it was under 3.1.1 (15.45s vs 35s) and it's also faster than 2.4.x, which is nice. So if merged, I'll close this as a dupe - but for now I'll subscribe to that issue and PR and wait for its resolution.

Strangely enough, my original pipeline (which was simplified into the repro linked in this issue) is only 10% faster than under 3.1.1 (so way way slower than 2.4.x), so there are more things at play. I'll investigate more once this is merged.

Thanks for the links!;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: inferSchema for type date 
Issue key: SPARK-34953
Issue id: 13369475
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: xhudik
Creator: xhudik
Created: 04/Apr/21 11:30
Updated: 16/May/21 11:10
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core, SQL
Due Date: 
Votes: 0
Labels: 
Description: Reading a csv file with `option({color:#6a8759}"inferSchema"{color}{color:#cc7832},{color}{color:#6a8759}"true"{color})` doesnt work with `date` type.  E.g. [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/csv/CSVInferSchema.scala#L101:L119] - can process only `Timestamp` not a `Date`

 

Datasets often contain `Date` type therefore reading a file to Spark should be able to infer `Date` type to a column.

For now, only work-arounds (e.g. [https://stackoverflow.com/a/46595057/1408096] , or [https://stackoverflow.com/questions/66935214/spark-reading-csv-with-specified-date-format] ) are possible/
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun May 16 11:10:54 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0pijk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/May/21 17:44;saurabhc100;This seems to the be the problem when there are multiple columns in the file which are of DateType and there is need to change it from StringType to DateType. 

[~xhudik]- Thanks for raising this Jira.

I have raised the PR for this change

https://github.com/apache/spark/pull/32558;;;, 15/May/21 17:46;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/32558;;;, 16/May/21 11:10;xhudik;Hi [~saurabhc100] , thanks for PR.

"when there are multiple columns" - i think this is general problem not related to the number of columns (it can be 1, 2 10,...). The problem is that there is no way how to create a schema for `date` type;;;
Affects Version/s.1: 
Comment.1: 15/May/21 17:46;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/32558;;;
Comment.2: 16/May/21 11:10;xhudik;Hi [~saurabhc100] , thanks for PR.

"when there are multiple columns" - i think this is general problem not related to the number of columns (it can be 1, 2 10,...). The problem is that there is no way how to create a schema for `date` type;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Add string-to-number conversion support to JacksonParser
Issue key: SPARK-35374
Issue id: 13377917
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: sarutak
Reporter: sarutak
Creator: sarutak
Created: 11/May/21 16:46
Updated: 11/May/21 16:56
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.4.7, 3.1.1, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: In the current implementation, spark.read.json doesn't convert numbers represented as string even though types are specified by schema.
Here is anexample.
{code}
$ cat test.json
{ "value": "foo" }                                                                                                                                
{ "value": "100" }                                                                                                                                
{ "value": "257" }                                                                                                                                
{ "value": "32768" }                                                                                                                              
{ "value": "2147483648" }                                                                                                                         
{ "value": "2.71f" }                                                                                                                              
{ "value": "1.0E100" }  

$ bin/spark-shell
import org.apache.spark.sql.types._
val df = spark.read.
  schema(StructType(StructField("value", DoubleType)::Nil)).json("test.json")
df.show
+-----+
|value|
+-----+
| null|
| null|
| null|
| null|
| null|
| null|
| null|
+-----+
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue May 11 16:56:09 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qxwo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/May/21 16:55;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32506;;;, 11/May/21 16:56;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32506;;;
Affects Version/s.1: 3.1.1
Comment.1: 11/May/21 16:56;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/32506;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.6.0, Unknown

Summary: DynamicFilter pushdown not working
Issue key: SPARK-35245
Issue id: 13375398
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jccote
Creator: jccote
Created: 27/Apr/21 17:04
Updated: 01/May/21 14:06
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description:  

The pushed filters is always empty. `PushedFilters: []`

I was expecting the filters to be pushed down on the probe side of the join.

Not sure how to properly configure this to work. For example how to set fallbackFilterRatio ?


spark = ( SparkSession.builder
 .master('local')
 .config("spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio", 100)
 .getOrCreate()
 )
 
df = spark.read.parquet('abfss://warehouse@<domain>/iceberg/opensource/<table>/data/timeperiod=2021-04-25/00000-0-929b48ef-7ec3-47bd-b0a1-e9172c2dca6a-00001.parquet')
df.createOrReplaceTempView('TMP')
 
spark.sql('''
explain cost
select 
   timeperiod, rrname
from
   TMP
where
   timeperiod in (
     select
       TO_DATE(d, 'MM-dd-yyyy') AS timeperiod
     from
     values
       ('01-01-2021'),
       ('01-01-2021'),
       ('01-01-2021') tbl(d)
 )
group by
   timeperiod, rrname
''').show(truncate=False)

|== Optimized Logical Plan ==
Aggregate [timeperiod#597, rrname#594], [timeperiod#597, rrname#594], Statistics(sizeInBytes=69.0 MiB)
+- Join LeftSemi, (timeperiod#597 = timeperiod#669), Statistics(sizeInBytes=69.0 MiB)
 :- Project [rrname#594, timeperiod#597], Statistics(sizeInBytes=69.0 MiB)
 : +- Relation[count#591,time_first#592L,time_last#593L,rrname#594,rrtype#595,rdata#596,timeperiod#597] parquet, Statistics(sizeInBytes=198.5 MiB)
 +- LocalRelation [timeperiod#669], Statistics(sizeInBytes=36.0 B)

== Physical Plan ==
*(2) HashAggregate(keys=[timeperiod#597, rrname#594], functions=[], output=[timeperiod#597, rrname#594])
+- Exchange hashpartitioning(timeperiod#597, rrname#594, 200), ENSURE_REQUIREMENTS, [id=#839]
 +- *(1) HashAggregate(keys=[timeperiod#597, rrname#594], functions=[], output=[timeperiod#597, rrname#594])
 +- *(1) BroadcastHashJoin [timeperiod#597], [timeperiod#669], LeftSemi, BuildRight, false
 :- *(1) ColumnarToRow
 : +- FileScan parquet [rrname#594,timeperiod#597] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[abfss://warehouse@<domain>/iceberg/opensource/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<rrname:string,timeperiod:date>
 +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, date, true]),false), [id=#822]
 +- LocalTableScan [timeperiod#669]
 
```
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat May 01 14:06:27 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0qie8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/May/21 14:06;yumwang;This is because filtering side do not has selective predicate : https://github.com/apache/spark/blob/19c7d2f3d8cda8d9bc5dfc1a0bf5d46845b1bc2f/sql/core/src/main/scala/org/apache/spark/sql/execution/dynamicpruning/PartitionPruning.scala#L193-L201;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: support other numeric types besides Double in  QueryTest#checkAggregatesWithTol
Issue key: SPARK-34936
Issue id: 13369077
Parent id: 
Issue Type: Test
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: KevinPis
Creator: KevinPis
Created: 01/Apr/21 09:57
Updated: 01/Apr/21 10:03
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL, Tests
Due Date: 
Votes: 0
Labels: 
Description: This is a followup of SPARK-10641

it noly support Double type in QueryTest#checkAggregatesWithTol  and this pr will  support other numeric types.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Apr 01 10:03:54 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0pg34:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Apr/21 10:03;KevinPis;i'm working on;;;
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: consider markdownlint
Issue key: SPARK-34838
Issue id: 13367027
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jsoref
Creator: jsoref
Created: 23/Mar/21 15:11
Updated: 24/Mar/21 14:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: Build, Documentation
Due Date: 
Votes: 0
Labels: 
Description: There's apparently a tool called markdownlint which seems to be integrated w/ Visual Studio Code. It suggests, among other things, to use {{*}} instead of {{-}} for top level bullets:

https://github.com/DavidAnson/markdownlint/blob/v0.23.1/doc/Rules.md#md004 
https://github.com/apache/spark/pull/30679#issuecomment-804971370
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-03-23 15:11:49.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0p3hk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: registerFunction shouldnt be logging warning message for same function being re-registered
Issue key: SPARK-34804
Issue id: 13366393
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: sumeetmi2
Creator: sumeetmi2
Created: 19/Mar/21 16:12
Updated: 22/Mar/21 08:47
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: {code:java}
test("function registry warning") {
 implicit val ss = spark
 import ss.implicits._
 val dd = Seq((1),(2)).toDF("a")
 val dd1 = udf((i: Int) => i*2)
 (1 to 4).foreach {
   _ =>
     dd.sparkSession.udf.register("function", dd1)
     Thread.sleep(1000)
 }
 dd.withColumn("aa", expr("function(a)")).show(10)
}{code}
logs:

21/03/19 22:39:39 WARN SparkSession$Builder : Using an existing SparkSession; some spark core configurations may not take effect.

21/03/19 22:39:39 WARN SparkSession$Builder : Using an existing SparkSession; the static sql configurations will not take effect.
 21/03/19 22:39:39 WARN SparkSession$Builder : Using an existing SparkSession; some spark core configurations may not take effect.
 21/03/19 22:39:43 WARN SimpleFunctionRegistry : The function function replaced a previously registered function.
 21/03/19 22:39:44 WARN SimpleFunctionRegistry : The function function replaced a previously registered function.
 21/03/19 22:39:45 WARN SimpleFunctionRegistry : The function function replaced a previously registered function.
 +----+--+
|a|aa|

+----+--+
|1|2|
|2|4|

+----+--+


Basically in the FunctionRegistry implementation
{code:java}
override def registerFunction(
 name: FunctionIdentifier,
 info: ExpressionInfo,
 builder: FunctionBuilder): Unit = synchronized {
 val normalizedName = normalizeFuncName(name)
 val newFunction = (info, builder)
 functionBuilders.put(normalizedName, newFunction) match {
 case Some(previousFunction) if previousFunction != newFunction =>
 logWarning(s"The function $normalizedName replaced a previously registered function.")
 case _ =>
 }
}{code}
The *previousFunction != newFunction* equality comparison is incorrect
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Mar 22 08:47:07 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ozko:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Mar/21 17:33;apachespark;User 'sumeetmi2' has created a pull request for this issue:
https://github.com/apache/spark/pull/31903;;;, 19/Mar/21 17:34;sumeetmi2;Proposed changes: [https://github.com/apache/spark/pull/31903] ;;;, 19/Mar/21 17:34;apachespark;User 'sumeetmi2' has created a pull request for this issue:
https://github.com/apache/spark/pull/31903;;;, 22/Mar/21 08:47;sumeetmi2;I need some pointers in how to check the Seq[Expression] to Expression for equality. Any comments are appreciated.;;;
Affects Version/s.1: 
Comment.1: 19/Mar/21 17:34;sumeetmi2;Proposed changes: [https://github.com/apache/spark/pull/31903] ;;;
Comment.2: 19/Mar/21 17:34;apachespark;User 'sumeetmi2' has created a pull request for this issue:
https://github.com/apache/spark/pull/31903;;;
Comment.3: 22/Mar/21 08:47;sumeetmi2;I need some pointers in how to check the Seq[Expression] to Expression for equality. Any comments are appreciated.;;;
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Add JobId to Stage page in web UI
Issue key: SPARK-34656
Issue id: 13362869
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: gabrielelnizzoli
Creator: gabrielelnizzoli
Created: 07/Mar/21 19:40
Updated: 09/Mar/21 08:14
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 2.4.7, 3.0.2, 3.1.1
Fix Version/s: 
Component/s: Web UI
Due Date: 
Votes: 0
Labels: 
Description: In the stage page, there is no indication to which job a stage belongs to. Adding a *Job Id* column (with link) would be very helpful to visually understand what is going on.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): SPARK-34653, SPARK-34655, SPARK-34654
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 07/Mar/21 19:42;gabrielelnizzoli;Screenshot from 2021-03-07 11-34-55.png;https://issues.apache.org/jira/secure/attachment/13021780/Screenshot+from+2021-03-07+11-34-55.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 09 08:13:54 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ody8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Mar/21 19:43;gabrielelnizzoli;How it will look:

---

!Screenshot from 2021-03-07 11-34-55.png|width=456,height=413!;;;, 09/Mar/21 08:13;apachespark;User 'gabrielenizzoli' has created a pull request for this issue:
https://github.com/apache/spark/pull/31786;;;
Affects Version/s.1: 3.0.2
Comment.1: 09/Mar/21 08:13;apachespark;User 'gabrielenizzoli' has created a pull request for this issue:
https://github.com/apache/spark/pull/31786;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Provide silhouette score for each sample when using ClusteringEvaluator
Issue key: SPARK-34664
Issue id: 13363015
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: JJorczik
Creator: JJorczik
Created: 08/Mar/21 13:45
Updated: 08/Mar/21 13:45
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1
Fix Version/s: 
Component/s: ML
Due Date: 
Votes: 0
Labels: 
Description: Computing the average silhouette score is already implemented when using ClusteringEvaluator. When looking at the [source code|https://gitlab.com/mark91/SparkClusteringEvaluationMetrics/-/blob/master/src/main/scala/org/apache/spark/ml/evaluation/SquaredEuclideanSilhouetteEvaluator.scala] of ClusteringEvaluator, I think it would be easy to provide not only the average silhouette score but also the silhouette score for each sample, as they are already computed (Line 95-99).
 The silhouette score for each sample can be helpful to generate a silhouette plot for instance as described in [this scikit-learn article|https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html]. The resulting feature would be equivalent to the silhouette_samples function implemented in scikit-learn.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-03-08 13:45:32.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0oeuo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1

Summary: Prevent incomplete master URLs for Spark on Kubernetes early
Issue key: SPARK-34264
Issue id: 13354946
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jlaskowski
Creator: jlaskowski
Created: 27/Jan/21 13:40
Updated: 27/Jan/21 13:40
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.1.1
Fix Version/s: 
Component/s: Kubernetes, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: It turns out that {{--master k8s://}} is accepted and although leads to termination displays stacktraces that don't really tell what the real cause is.

This may happen when the Kubernetes API server(s) are described by an environment variable that's not initialized in the current terminal.

{code}
$ ./bin/spark-shell --master k8s:// --verbose
...
Spark config:
(spark.jars,)
(spark.app.name,Spark shell)
(spark.submit.pyFiles,)
(spark.ui.showConsoleProgress,true)
(spark.submit.deployMode,client)
(spark.master,k8s://https://)
...
21/01/27 14:29:44 ERROR Main: Failed to initialize Spark session.
io.fabric8.kubernetes.client.KubernetesClientException: Failed to start websocket
	at io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager$1.onFailure(WatchConnectionManager.java:208)
...
Caused by: java.net.UnknownHostException: api: nodename nor servname provided, or not known
	at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929)
	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1519)
	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1509)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1368)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1302)
	at okhttp3.Dns$1.lookup(Dns.java:40)
	at okhttp3.internal.connection.RouteSelector.resetNextInetSocketAddress(RouteSelector.java:185)
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-01-27 13:40:38.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0n1qw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.1
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, Unknown

Summary: Misleading Spark Streaming source documentation
Issue key: SPARK-36984
Issue id: 13406102
Parent id: 
Issue Type: Documentation
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: Novotný
Creator: Novotný
Created: 12/Oct/21 08:16
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Documentation, PySpark, Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: The documentation at [https://spark.apache.org/docs/latest/streaming-programming-guide.html#advanced-sources] clearly states that *Kafka* (and Kinesis) are available in the Python API v 3.1.2 in *Spark Streaming (DStreams)*. (see attachments for highlight)

However, there is no way to create DStream from Kafka in PySpark >= 3.0.0, as the `kafka.py` file is missing in [https://github.com/apache/spark/tree/master/python/pyspark/streaming]. I'm coming from PySpark 2.4.4 where this was possible. _Should Kafka be excluded as advanced source for spark streaming in Python API in the docs?_

 

Note that I'm aware of this Kafka integration guide [https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html] but I'm not interested in Structured Streaming as it doesn't support arbitrary stateful operations in Python. DStreams support this functionality with `updateStateByKey`.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 12/Oct/21 08:17;Novotný;docs_highlight.png;https://issues.apache.org/jira/secure/attachment/13034825/docs_highlight.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Oct 14 02:14:07 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vrmo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Oct/21 02:14;gurwls223;Yeah we should fix the docs. feel free to edit -  we don't have the support anymore in Dstream. Users should better migrate to Structured Streaming.;;;
Affects Version/s.1: 3.0.2
Comment.1: 
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Decommissioning executors get killed before transferring their data because of the hardcoded timeout of 60 secs
Issue key: SPARK-36872
Issue id: 13403690
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: shkhrgpt
Creator: shkhrgpt
Created: 28/Sep/21 02:41
Updated: 28/Jan/22 03:27
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: During the graceful decommissioning phase, executors need to transfer all of their shuffle and cache data to the peer executors. However, they get killed before transferring all the data because of the hardcoded timeout value of 60 secs in the decommissioning script. As a result of executors dying prematurely, the spark tasks on other executors fail which causes application failures, and it is hard to debug those failures. To fix the issue, we ended up writing a custom script with a different timeout and rebuilt the spark image but we would prefer an easier solution that does not require rebuilding the image. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jan 28 03:27:20 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vcr4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Jan/22 03:58;abhisrao;[~shkhrgpt], could you please share more details on which script you are referring to? We're facing similar issues and we're looking for options to fix this.;;;, 27/Jan/22 17:18;shkhrgpt;[~abhisrao] I am referring to the following script:

 

[https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/decom.sh]

 

 ;;;, 28/Jan/22 03:27;abhisrao;Thanks. We'll have a look at this.;;;
Affects Version/s.1: 3.1.2
Comment.1: 27/Jan/22 17:18;shkhrgpt;[~abhisrao] I am referring to the following script:

 

[https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/decom.sh]

 

 ;;;
Comment.2: 28/Jan/22 03:27;abhisrao;Thanks. We'll have a look at this.;;;
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Document change for Spark sql jdbc
Issue key: SPARK-36801
Issue id: 13402069
Parent id: 
Issue Type: Documentation
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: senthh
Creator: senthh
Created: 19/Sep/21 18:19
Updated: 19/Sep/21 18:59
Last Viewed: 26/Jul/24 00:25
Resolved: 
Affects Version/s: 3.0.0, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Documentation
Due Date: 
Votes: 0
Labels: 
Description: Reading using Spark SQL jdbc DataSource  does not maintain nullable type and changes "non nullable" columns to "nullable".

 

For example:

mysql> CREATE TABLE Persons(Id int NOT NULL, FirstName varchar(255), LastName varchar(255), Age int);
Query OK, 0 rows affected (0.04 sec)

mysql> show tables;
+-------------------+
| Tables_in_test_db |
+-------------------+
| Persons |
+-------------------+
1 row in set (0.00 sec)

mysql> desc Persons;
+-----------+--------------+------+-----+---------+-------+
| Field | Type | Null | Key | Default | Extra |
+-----------+--------------+------+-----+---------+-------+
| Id | int | NO | | NULL | |
| FirstName | varchar(255) | YES | | NULL | |
| LastName | varchar(255) | YES | | NULL | |
| Age | int | YES | | NULL | |
+-----------+--------------+------+-----+---------+-------+

 

 

{color:#cc7832}val {color}df = spark.read.format({color:#6a8759}"jdbc"{color})
 .option({color:#6a8759}"database"{color}{color:#cc7832},{color}{color:#6a8759}"Test_DB"{color})
 .option({color:#6a8759}"user"{color}{color:#cc7832}, {color}{color:#6a8759}"root"{color})
 .option({color:#6a8759}"password"{color}{color:#cc7832}, {color}{color:#6a8759}""{color})
 .option({color:#6a8759}"driver"{color}{color:#cc7832}, {color}{color:#6a8759}"com.mysql.cj.jdbc.Driver"{color})
 .option({color:#6a8759}"url"{color}{color:#cc7832}, {color}{color:#6a8759}"jdbc:mysql://localhost:3306/Test_DB"{color})
 .option({color:#6a8759}"query"{color}{color:#cc7832}, {color}{color:#6a8759}"(select * from Persons)"{color})
 .load()
df.printSchema()

 

*output:*

 

root
 |-- Id: integer (nullable = true)
 |-- FirstName: string (nullable = true)
 |-- LastName: string (nullable = true)
 |-- Age: integer (nullable = true)

 

 

So we need to add a note, in Documentation[1], "All columns are automatically converted to be nullable for compatibility reasons."

 Ref:

[1 ][https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases]

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Inward issue link (Child-Issue): 
Inward issue link (Container): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Sep 19 18:59:52 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v2rc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Sep/21 18:59;apachespark;User 'senthh' has created a pull request for this issue:
https://github.com/apache/spark/pull/34042;;;, 19/Sep/21 18:59;apachespark;User 'senthh' has created a pull request for this issue:
https://github.com/apache/spark/pull/34042;;;
Affects Version/s.1: 3.0.2
Comment.1: 19/Sep/21 18:59;apachespark;User 'senthh' has created a pull request for this issue:
https://github.com/apache/spark/pull/34042;;;
Comment.2: 
Comment.3: 
Comment.4: 
Comment.5: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

