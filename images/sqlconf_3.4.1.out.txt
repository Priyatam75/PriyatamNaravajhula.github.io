Spark 3.4.1
EMR 6.15, EMR 6.13
          "https://maven-central.storage-download.googleapis.com/maven2/"
spark.sql.adaptive.advisoryPartitionSizeInBytes = true
spark.sql.adaptive.advisoryShuffledHashJoinStreamPartitionSizeThreshold = true
spark.sql.adaptive.coalescePartitions.minPartitionSize = true
spark.sql.adaptive.coalescePartitions.parallelismFirst = true
spark.sql.adaptive.customCostEvaluatorClass = true
spark.sql.adaptive.datasetToRdd.enabled = true
spark.sql.adaptive.failOnIncompatiblePlan = false
spark.sql.adaptive.forceApply = false
spark.sql.adaptive.forceOptimizeSkewedJoin = false
spark.sql.adaptive.localShuffleReader.enabled = true
spark.sql.adaptive.logLevel = "debug"
spark.sql.adaptive.optimizer.excludedRules = true
spark.sql.adaptive.rebalancePartitionsSmallPartitionFactor = 0.2
spark.sql.adaptive.reoptimizationWithPartialAggregate.enabled = true
spark.sql.adaptive.shuffle.mapOutputStats.useDataSize = true
spark.sql.adaptive.shuffle.targetPostShuffleInputSize = true
spark.sql.adaptive.skewJoin.enabled = true
spark.sql.adaptive.skewJoin.skewedPartitionFactor = 5.0
spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes = 0.2
spark.sql.adaptive.stats.enabled = true
spark.sql.adaptive.stats.ndv.maxError = 0.05
spark.sql.adaptivePartialAggregation.aggressivePassthrough.force = false
spark.sql.adaptivePartialAggregation.enabled = true
spark.sql.adaptivePartialAggregation.minProcessedRowsCount = 100000
spark.sql.alwaysAddPartialWindowTopK.enabled = false
spark.sql.alwaysWindowSort.enabled = false
spark.sql.alwaysWindowTopKSort.enabled = false
spark.sql.analyzer.canonicalization.multiCommutativeOpMemoryOptThreshold = 3
spark.sql.analyzer.failAmbiguousSelfJoin = true
spark.sql.analyzer.maxIterations = 100
spark.sql.ansi.doubleQuotedIdentifiers = false
spark.sql.ansi.enabled = sys.env.get("SPARK_ANSI_SQL_MODE"
spark.sql.ansi.enforceReservedKeywords = false
spark.sql.ansi.relationPrecedence = false
spark.sql.autoBroadcastJoinThreshold = 3
spark.sql.avro.compression.codec = "snappy"
spark.sql.avro.datetimeRebaseModeInRead = LegacyBehaviorPolicy.EXCEPTION.toString
spark.sql.avro.datetimeRebaseModeInWrite = LegacyBehaviorPolicy.EXCEPTION.toString
spark.sql.avro.deflate.level = Deflater.DEFAULT_COMPRESSION
spark.sql.avro.filterPushdown.enabled = true
spark.sql.bloomFilterJoin.adaptiveBloomFilterPassthrough.enabled = true
spark.sql.bloomFilterJoin.adaptiveBloomFilterPassthrough.sampleSizePercentage = 0.01
spark.sql.bloomFilterJoin.afterDynamicPruning.broadeningApplicability.enabled = true
spark.sql.bloomFilterJoin.afterDynamicPruning.enabled = true
spark.sql.bloomFilterJoin.afterDynamicPruning.maxFilterBytes = 20L * 1024 * 1024
spark.sql.bloomFilterJoin.enabled = true
spark.sql.bloomFilterJoin.maxFilterBytes = 2L * 1024 * 1024
spark.sql.bloomFilterJoin.targetFalsePositivePercentage = .01
spark.sql.broadcastTimeout = false
spark.sql.bucketing.coalesceBucketsInJoin.enabled = false
spark.sql.bucketing.coalesceBucketsInJoin.maxBucketRatio = 4
spark.sql.cachePrunedCatalogFileIndex.enabled = true
spark.sql.cartesianProductExec.buffer.in.memory.threshold = 4096
spark.sql.cartesianProductExec.buffer.spill.threshold = SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD.defaultValue.get
spark.sql.caseSensitive = false
spark.sql.catalog.optimizePartitionLocationListing = true
spark.sql.cbo.enabled = false
spark.sql.cbo.joinReorder.card.weight = 0.7
spark.sql.cbo.joinReorder.dp.star.filter = false
spark.sql.cbo.joinReorder.dp.threshold = 12
spark.sql.cbo.joinReorder.enabled = false
spark.sql.cbo.planStats.enabled = false
spark.sql.cbo.starJoinFTRatio = 0.9
spark.sql.cbo.starSchemaDetection = false
spark.sql.charAsVarchar = false
spark.sql.cli.print.header = false
spark.sql.codegen.aggregate.fastHashMap.capacityBit = 16
spark.sql.codegen.aggregate.map.twolevel.enabled = true
spark.sql.codegen.aggregate.map.twolevel.partialOnly = true
spark.sql.codegen.aggregate.map.vectorized.enable = false
spark.sql.codegen.aggregate.sortAggregate.enabled = true
spark.sql.codegen.aggregate.splitAggregateFunc.enabled = true
spark.sql.codegen.factoryMode = CodegenObjectFactoryMode.FALLBACK.toString
spark.sql.codegen.fallback = true
spark.sql.codegen.hugeMethodLimit = 65535
spark.sql.codegen.join.existenceSortMergeJoin.enabled = true
spark.sql.codegen.join.fullOuterShuffledHashJoin.enabled = true
spark.sql.codegen.join.fullOuterSortMergeJoin.enabled = true
spark.sql.codegen.logging.maxLines = 1000
spark.sql.codegen.maxFields = 100
spark.sql.codegen.methodSplitThreshold = 1024
spark.sql.codegen.splitConsumeFuncByOperator = true
spark.sql.codegen.useIdInClassName = true
spark.sql.codegen.wholeStage = true
spark.sql.columnNameOfCorruptRecord = "_corrupt_record"
spark.sql.columnVector.offheap.enabled = false
spark.sql.constraintPropagation.enabled = true
spark.sql.crossJoin.enabled = true
spark.sql.csv.filterPushdown.enabled = true
spark.sql.csv.parser.columnPruning.enabled = true
spark.sql.csv.parser.inputBufferSize = false
spark.sql.dataPrefetch.enabled = true
spark.sql.dataprefetch.filescan.maxParallelismPerTask = 4
spark.sql.dataprefetch.filescan.schemes = "s3,s3n,s3a"
spark.sql.datetime.java8API.enabled = false
spark.sql.debug.maxToStringFields = 25
spark.sql.decimalOperations.allowPrecisionLoss = true
spark.sql.decimalOperations.optimizeMultiplication = true
spark.sql.decimalOperations.removeRedundantMakeDecimal = true
spark.sql.decimalOperations.rewriteDecimalAddSubtractOptimization = true
spark.sql.decimalOperations.unscaledDecimalSums.enabled = true
spark.sql.deduplicateAggregateWithSkewedJoin.enabled = true
spark.sql.deduplicateAggregateWithSkewedJoin.restrictJoinKeys = true
spark.sql.deduplicateAggregateWithSkewedJoin.sizeChecks.enabled = true
spark.sql.deduplicateAggregateWithSkewedJoin.sizeChecks.threshold = 5.0
spark.sql.defaultCatalog = SESSION_CATALOG_NAME
spark.sql.defaultColumn.allowedProviders = "csv,json,orc,parquet"
spark.sql.defaultColumn.enabled = true
spark.sql.defaultColumn.useNullsForMissingDefaultValues = true
spark.sql.defaultSizeInBytes = Long.MaxValue
spark.sql.error.messageFormat = ErrorMessageFormat.PRETTY.toString
spark.sql.exchange.reuse = true
spark.sql.execution.arrow.enabled = false
spark.sql.execution.arrow.fallback.enabled = true
spark.sql.execution.arrow.localRelationThreshold = Utils.isTesting
spark.sql.execution.arrow.pyspark.enabled = false
spark.sql.execution.arrow.pyspark.fallback.enabled = 10000
spark.sql.execution.arrow.sparkr.enabled = false
spark.sql.execution.broadcastHashJoin.outputPartitioningExpandLimit = 8
spark.sql.execution.extendOperatorCanonicalization = true
spark.sql.execution.extendedOperatorReuse.enabled = true
spark.sql.execution.fastDecimalAvg.enabled = true
spark.sql.execution.fastFailOnFileFormatOutput = false
spark.sql.execution.optimizePrefixGroupingSetsSequence.enabled = true
spark.sql.execution.optimizePrefixGroupingSetsSequence.maxGroupingSets = 16
spark.sql.execution.pandas.convertToArrowArraySafely = false
spark.sql.execution.pandas.udf.buffer.size = !Utils.isTesting
spark.sql.execution.pythonUDF.arrow.enabled = false
spark.sql.execution.rangeExchange.sampleSizePerPartition = 100
spark.sql.execution.removeRedundantBroadcastHashJoins.enabled = true
spark.sql.execution.removeRedundantProjects = true
spark.sql.execution.removeRedundantSorts = true
spark.sql.execution.replaceHashWithSortAgg = false
spark.sql.execution.reuseSubquery = true
spark.sql.execution.sortBeforeRepartition = true
spark.sql.execution.topKSortFallbackThreshold = ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH
spark.sql.execution.useObjectHashAggregateExec = true
spark.sql.extendedEventInfo = true
spark.sql.files.ignoreMissingFiles = false
spark.sql.files.maxPartitionBytes = false
spark.sql.files.maxRecordsPerFile = 0
spark.sql.footer.cachedFooterEnabled = true
spark.sql.function.eltOutputAsString = false
spark.sql.groupByAliases = true
spark.sql.groupByOrdinal = true
spark.sql.hive.advancedPartitionPredicatePushdown.enabled = true
spark.sql.hive.caseSensitiveInferenceMode = HiveCaseSensitiveInferenceMode.NEVER_INFER.toString
spark.sql.hive.convertCTAS = false
spark.sql.hive.dropPartitionByName.enabled = false
spark.sql.hive.filesourcePartitionFileCacheSize = 250 * 1024 * 1024
spark.sql.hive.gatherFastStats = true
spark.sql.hive.manageFilesourcePartitions = true
spark.sql.hive.metastorePartitionPruning = true
spark.sql.hive.metastorePartitionPruningFallbackOnException = false
spark.sql.hive.metastorePartitionPruningFastFallback = false
spark.sql.hive.metastorePartitionPruningInSetThreshold = 1000
spark.sql.hive.stringLikePartitionPredicatePushdown.enabled = true
spark.sql.hive.tablePropertyLengthThreshold = false
spark.sql.hive.verifyPartitionPath = false
spark.sql.inMemoryColumnarStorage.batchSize = 10000
spark.sql.inMemoryColumnarStorage.compressed = true
spark.sql.inMemoryColumnarStorage.enableVectorizedReader = true
spark.sql.inMemoryColumnarStorage.partitionPruning = true
spark.sql.inMemoryTableScanStatistics.enable = false
spark.sql.join.preferSortMergeJoin = true
spark.sql.json.enablePartialResults = true
spark.sql.json.filterPushdown.enabled = true
spark.sql.jsonGenerator.ignoreNullFields = true
spark.sql.jsonGenerator.writeNullIfWithDefaultValue = true
spark.sql.lateralColumnAlias.enableImplicitResolution = true
spark.sql.leafNodeDefaultParallelism = 200
spark.sql.leftExistenceJoinDeduplication.enabled = true
spark.sql.legacy.addSingleFileInAddFile = false
spark.sql.legacy.allowAutoGeneratedAliasForView = false
spark.sql.legacy.allowEmptySchemaWrite = false
spark.sql.legacy.allowHashOnMapType = false
spark.sql.legacy.allowNegativeScaleOfDecimal = false
spark.sql.legacy.allowNonEmptyLocationInCTAS = false
spark.sql.legacy.allowNullComparisonResultInArraySort = false
spark.sql.legacy.allowParameterlessCount = false
spark.sql.legacy.allowStarWithSingleTableIdentifierInCount = false
spark.sql.legacy.allowTempViewCreationWithMultipleNameparts = false
spark.sql.legacy.allowUntypedScalaUDF = false
spark.sql.legacy.allowZeroIndexInFormatString = false
spark.sql.legacy.bucketedTableScan.outputOrdering = false
spark.sql.legacy.castComplexTypesToString.enabled = false
spark.sql.legacy.charVarcharAsString = false
spark.sql.legacy.createEmptyCollectionUsingStringType = false
spark.sql.legacy.createHiveTableByDefault = true
spark.sql.legacy.csv.enableDateTimeParsingFallback = 100
spark.sql.legacy.ctePrecedencePolicy = LegacyBehaviorPolicy.EXCEPTION.toString
spark.sql.legacy.dataset.nameNonStructGroupingKeyAsValue = false
spark.sql.legacy.doLooseUpcast = false
spark.sql.legacy.emptyCurrentDBInCli = false
spark.sql.legacy.execution.pandas.groupedMap.assignColumnsByName = true
spark.sql.legacy.exponentLiteralAsDecimal.enabled = false
spark.sql.legacy.extraOptionsBehavior.enabled = false
spark.sql.legacy.followThreeValuedLogicInArrayExists = true
spark.sql.legacy.fromDayTimeString.enabled = false
spark.sql.legacy.groupingIdWithAppendedUserGroupBy = false
spark.sql.legacy.histogramNumericPropagateInputType = true
spark.sql.legacy.integerGroupingId = false
spark.sql.legacy.interval.enabled = false
spark.sql.legacy.json.allowEmptyString.enabled = false
spark.sql.legacy.keepCommandOutputSchema = false
spark.sql.legacy.keepPartitionSpecAsStringLiteral = false
spark.sql.legacy.literal.pickMinimumPrecision = true
spark.sql.legacy.lpadRpadAlwaysReturnString = false
spark.sql.legacy.mssqlserver.numericMapping.enabled = false
spark.sql.legacy.notReserveProperties = false
spark.sql.legacy.nullValueWrittenAsQuotedEmptyStringCsv = false
spark.sql.legacy.parquet.nanosAsLong = false
spark.sql.legacy.parseNullPartitionSpecAsStringLiteral = false
spark.sql.legacy.parser.havingWithoutGroupByAsWhere = false
spark.sql.legacy.pathOptionBehavior.enabled = false
spark.sql.legacy.replaceDatabricksSparkAvro.enabled = true
spark.sql.legacy.setCommandRejectsSparkCoreConfs = true
spark.sql.legacy.setopsPrecedence.enabled = false
spark.sql.legacy.sizeOfNull = true
spark.sql.legacy.skipTypeValidationOnAlterPartition = false
spark.sql.legacy.statisticalAggregate = false
spark.sql.legacy.storeAnalyzedPlanForView = false
spark.sql.legacy.timeParserPolicy = LegacyBehaviorPolicy.EXCEPTION.toString
spark.sql.legacy.typeCoercion.datetimeToString.enabled = false
spark.sql.legacy.useCurrentConfigsForView = false
spark.sql.legacy.useV1Command = false
spark.sql.legacy.v1IdentifierNoCatalog = false
spark.sql.limit.initialNumPartitions = 1
spark.sql.limit.scaleUpFactor = 4
spark.sql.logPerQueryExecutionMetrics.enabled = true
spark.sql.mapKeyDedupPolicy = MapKeyDedupPolicy.EXCEPTION.toString
spark.sql.maven.additionalRemoteRepositories = sys.env.getOrElse("DEFAULT_ARTIFACT_REPOSITORY",
spark.sql.maxConcurrentOutputFileWriters = 0
spark.sql.maxPlanStringLength = 100
spark.sql.maxSinglePartitionBytes = Long.MaxValue
spark.sql.objectHashAggregate.sortBased.fallbackThreshold = 128
spark.sql.optimizeNullAwareAntiJoin = true
spark.sql.optimizedUnsafeRowSerializers.enabled = true
spark.sql.optimizer.allowSplittingAggregatesInLogicalPlan = true
spark.sql.optimizer.canChangeCachedPlanOutputPartitioning = false
spark.sql.optimizer.collapseProjectAlwaysInline = false
spark.sql.optimizer.decorrelateInnerQuery.enabled = true
spark.sql.optimizer.decorrelateSetOps.enabled = true
spark.sql.optimizer.decorrelateSubqueryLegacyIncorrectCountHandling.enabled = false
spark.sql.optimizer.disableHints = false
spark.sql.optimizer.distinctBeforeIntersect.enabled = true
spark.sql.optimizer.dynamicDataPruning.enabled = true
spark.sql.optimizer.dynamicDataPruning.pruningSideThreshold = true
spark.sql.optimizer.dynamicPartitionPruning.enabled = true
spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio = 0.5
spark.sql.optimizer.dynamicPartitionPruning.reuseBroadcastOnly = true
spark.sql.optimizer.dynamicPartitionPruning.useStats = true
spark.sql.optimizer.eliminateInnerJoin.enabled = true
spark.sql.optimizer.eliminateLeftOuterJoin.enabled = true
spark.sql.optimizer.enableCsvExpressionOptimization = true
spark.sql.optimizer.enableJsonExpressionOptimization = true
spark.sql.optimizer.excludedRules = 100
spark.sql.optimizer.expression.nestedPruning.enabled = true
spark.sql.optimizer.expressionProjectionCandidateLimit = 100
spark.sql.optimizer.flattenCrossJoinedAggregates.enabled = true
spark.sql.optimizer.flattenCrossJoinedAggregates.maxDistinctAggGroups = 10
spark.sql.optimizer.flattenScalarSubqueriesWithAggregates.enabled = true
spark.sql.optimizer.inSetConversionThreshold = 10
spark.sql.optimizer.inSetSwitchThreshold = 400
spark.sql.optimizer.nestedPredicatePushdown.supportedFileSources = "parquet,orc"
spark.sql.optimizer.nestedSchemaPruning.enabled = true
spark.sql.optimizer.optimizeOneRowRelationSubquery = true
spark.sql.optimizer.optimizeOneRowRelationSubquery.alwaysInline = true
spark.sql.optimizer.plannedWrite.enabled = true
spark.sql.optimizer.propagateDistinctKeys.enabled = true
spark.sql.optimizer.pullHintsIntoSubqueries = true
spark.sql.optimizer.pushDownAggregateThroughShuffledJoin.enabled = true
spark.sql.optimizer.pushDownDataSourceAggregate.enabled = true
spark.sql.optimizer.pushDownLeftSemiAntiJoin.smallAggregateThreshold = true
spark.sql.optimizer.removeRedundantGroupKeys.enabled = true
spark.sql.optimizer.removeRedundantJoinWithinInSubquery.enabled = true
spark.sql.optimizer.replaceExceptWithFilter = true
spark.sql.optimizer.rewriteDivisionWithinSum.enabled = true
spark.sql.optimizer.runtime.bloomFilter.creationSideThreshold = 1000000L
spark.sql.optimizer.runtime.bloomFilter.enabled = false
spark.sql.optimizer.runtime.bloomFilter.maxNumBits = 67108864L
spark.sql.optimizer.runtime.bloomFilter.maxNumItems = 4000000L
spark.sql.optimizer.runtime.bloomFilter.numBits = 8388608L
spark.sql.optimizer.runtime.rowLevelOperationGroupFilter.enabled = true
spark.sql.optimizer.runtimeFilter.number.threshold = 10
spark.sql.optimizer.runtimeFilter.semiJoinReduction.enabled = false
spark.sql.optimizer.serializer.nestedSchemaPruning.enabled = true
spark.sql.optimizer.sizeBasedJoinReorder.enabled = true
spark.sql.optimizer.transitiveDynamicFilterPruning.enabled = true
spark.sql.orc.aggregatePushdown = false
spark.sql.orc.columnarReaderBatchSize = 4096
spark.sql.orc.columnarWriterBatchSize = 1024
spark.sql.orc.compression.codec = "snappy"
spark.sql.orc.enableNestedColumnVectorizedReader = true
spark.sql.orc.enableVectorizedReader = true
spark.sql.orc.filterPushdown = true
spark.sql.orc.impl = "native"
spark.sql.orc.mergeSchema = false
spark.sql.orderByOrdinal = true
spark.sql.parquet.aggregatePushdown = false
spark.sql.parquet.binaryAsString = false
spark.sql.parquet.columnarReaderBatchSize = 4096
spark.sql.parquet.compression.codec = "snappy"
spark.sql.parquet.datetimeRebaseModeInRead = LegacyBehaviorPolicy.EXCEPTION.toString
spark.sql.parquet.datetimeRebaseModeInWrite = LegacyBehaviorPolicy.EXCEPTION.toString
spark.sql.parquet.enableNestedColumnVectorizedReader = true
spark.sql.parquet.enableVectorizedReader = true
spark.sql.parquet.fieldId.read.enabled = false
spark.sql.parquet.fieldId.read.ignoreMissing = false
spark.sql.parquet.fieldId.write.enabled = true
spark.sql.parquet.filterPushdown = true
spark.sql.parquet.filterPushdown.date = true
spark.sql.parquet.filterPushdown.decimal = true
spark.sql.parquet.filterPushdown.string.startsWith = true
spark.sql.parquet.filterPushdown.stringPredicate = 10
spark.sql.parquet.filterPushdown.timestamp = true
spark.sql.parquet.inferTimestampNTZ.enabled = true
spark.sql.parquet.int96AsTimestamp = true
spark.sql.parquet.int96RebaseModeInRead = LegacyBehaviorPolicy.EXCEPTION.toString
spark.sql.parquet.int96RebaseModeInWrite = LegacyBehaviorPolicy.EXCEPTION.toString
spark.sql.parquet.int96TimestampConversion = false
spark.sql.parquet.mergeSchema = false
spark.sql.parquet.output.committer.class = "org.apache.parquet.hadoop.ParquetOutputCommitter"
spark.sql.parquet.outputTimestampType = ParquetOutputTimestampType.INT96.toString
spark.sql.parquet.passthroughFileStatus.enabled = true
spark.sql.parquet.passthroughFileStatus.retryCount = 1
spark.sql.parquet.recordLevelFilter.enabled = false
spark.sql.parquet.respectSummaryFiles = false
spark.sql.parquet.writeLegacyFormat = false
spark.sql.parser.escapedStringLiterals = false
spark.sql.parser.quotedRegexColumnNames = false
spark.sql.pivotMaxValues = 10000
spark.sql.planChangeLog.level = "trace"
spark.sql.planChangeLog.rules = false
spark.sql.pyspark.inferNestedDictAsStruct.enabled = false
spark.sql.pyspark.legacy.inferArrayTypeFromFirstElement.enabled = false
spark.sql.readSideCharPadding = true
spark.sql.redaction.options.regex = "(?i
spark.sql.redaction.string.regex = false
spark.sql.repl.eagerEval.enabled = false
spark.sql.repl.eagerEval.maxNumRows = 20
spark.sql.repl.eagerEval.truncate = 20
spark.sql.requireAllClusterKeysForCoPartition = true
spark.sql.requireAllClusterKeysForDistribution = false
spark.sql.retainGroupColumns = true
spark.sql.runSQLOnFiles = true
spark.sql.scriptTransformation.exitTimeoutInSeconds = 10L
spark.sql.selfJoinAutoResolveAmbiguity = true
spark.sql.session.timeZone = 4096
spark.sql.sessionWindow.buffer.in.memory.threshold = 4096
spark.sql.sessionWindow.buffer.spill.threshold = SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD.defaultValue.get
spark.sql.shuffledHashJoin.optimizeHashedRelation.buildSideThreshold = 10.0
spark.sql.shuffledHashJoin.optimizeHashedRelation.enabled = true
spark.sql.shuffledHashJoin.optimizeHashedRelation.numValuesToKeysThreshold = 2.0
spark.sql.shuffledHashJoin.optimizeLongHashedRelation.enabled = true
spark.sql.shuffledHashJoin.optimizeUnsafeHashedRelation.enabled = true
spark.sql.sort.enableRadixSort = true
spark.sql.sortMergeJoinExec.buffer.in.memory.threshold = ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH
spark.sql.sortMergeJoinExec.buffer.spill.threshold = SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD.defaultValue.get
spark.sql.sources.binaryFile.maxLength = Int.MaxValue
spark.sql.sources.bucketing.autoBucketedScan.enabled = true
spark.sql.sources.bucketing.enabled = true
spark.sql.sources.bucketing.maxBuckets = 100000
spark.sql.sources.checkEarlyStatsAccessDuringTesting = true
spark.sql.sources.default = "parquet"
spark.sql.sources.fastS3PartitionDiscovery.enabled = true
spark.sql.sources.fastS3PartitionDiscovery.maxNumPages = 10
spark.sql.sources.fastS3PartitionDiscovery.maxTargetNumPartitionsPerPage = 400
spark.sql.sources.fastS3PartitionDiscovery.parallelism = 10
spark.sql.sources.fileCompressionFactor = 1.0
spark.sql.sources.ignoreDataLocality = false
spark.sql.sources.outputCommitterClass = "org.apache.spark.sql.execution.datasources.SQLEmrOptimizedCommitProtocol"
spark.sql.sources.parallelPartitionDiscovery.parallelism = 10000
spark.sql.sources.parallelPartitionDiscovery.threshold = 32
spark.sql.sources.partitionColumnTypeInference.enabled = true
spark.sql.sources.readSubdirectory.enabled = false
spark.sql.sources.relationCache.excludedProviders = ""
spark.sql.sources.useV1SourceList = "avro,csv,json,kafka,orc,parquet,text"
spark.sql.sources.v2.bucketing.enabled = false
spark.sql.sources.v2.bucketing.partiallyClusteredDistribution.enabled = false
spark.sql.sources.v2.bucketing.pushPartValues.enabled = false
spark.sql.sources.validatePartitionColumns = true
spark.sql.statistics.fallBackToHdfs = false
spark.sql.statistics.histogram.enabled = false
spark.sql.statistics.histogram.numBins = 254
spark.sql.statistics.ndv.maxError = 0.05
spark.sql.statistics.parallelFileListingInStatsComputation.enabled = true
spark.sql.statistics.percentile.accuracy = 10000
spark.sql.statistics.size.autoUpdate.enabled = false
spark.sql.statsBasedPartialWindowTopK.enabled = true
spark.sql.statsBasedWindowSort.numRowsPerWindowThreshold = 10000
spark.sql.statsBasedWindowSort.partitionsSortersRatioThreshold = 4.0
spark.sql.statsBasedWindowTopKSort.limitUpperBoundThreshold = 100000
spark.sql.statsImprovements.enabled = true
spark.sql.storeAssignmentPolicy = StoreAssignmentPolicy.ANSI.toString
spark.sql.streaming.aggregation.stateFormatVersion = 2
spark.sql.streaming.asyncLogPurge.enabled = true
spark.sql.streaming.checkpoint.renamedFileCheck.enabled = false
spark.sql.streaming.checkpointFileManagerClass = true
spark.sql.streaming.checkpointLocation = false
spark.sql.streaming.commitProtocolClass = "org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol"
spark.sql.streaming.continuous.epochBacklogQueueSize = 10000
spark.sql.streaming.continuous.executorPollIntervalMs = 100
spark.sql.streaming.continuous.executorQueueSize = 1024
spark.sql.streaming.disabledV2MicroBatchReaders = ""
spark.sql.streaming.disabledV2Writers = ""
spark.sql.streaming.fileSink.log.cleanupDelay = TimeUnit.MINUTES.toMillis(10
spark.sql.streaming.fileSink.log.compactInterval = 10
spark.sql.streaming.fileSink.log.deletion = true
spark.sql.streaming.fileSource.cleaner.numThreads = 1
spark.sql.streaming.fileSource.log.cleanupDelay = TimeUnit.MINUTES.toMillis(10
spark.sql.streaming.fileSource.log.compactInterval = 10
spark.sql.streaming.fileSource.log.deletion = true
spark.sql.streaming.fileSource.schema.forceNullable = true
spark.sql.streaming.fileStreamSink.ignoreMetadata = false
spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion = 2
spark.sql.streaming.join.stateFormatVersion = 2
spark.sql.streaming.kafka.useDeprecatedOffsetFetching = false
spark.sql.streaming.maxBatchesToRetainInMemory = 2
spark.sql.streaming.metadataCache.enabled = true
spark.sql.streaming.metricsEnabled = false
spark.sql.streaming.minBatchesToRetain = 100
spark.sql.streaming.multipleWatermarkPolicy = "min"
spark.sql.streaming.noDataMicroBatches.enabled = true
spark.sql.streaming.numRecentProgressUpdates = 100
spark.sql.streaming.pollingDelay = 10L
spark.sql.streaming.schemaInference = false
spark.sql.streaming.sessionWindow.merge.sessions.in.local.partition = false
spark.sql.streaming.sessionWindow.stateFormatVersion = 1
spark.sql.streaming.stateStore.compression.codec = "lz4"
spark.sql.streaming.stateStore.formatValidation.enabled = true
spark.sql.streaming.stateStore.maintenanceInterval = TimeUnit.MINUTES.toMillis(1
spark.sql.streaming.stateStore.minDeltasForSnapshot = 10
spark.sql.streaming.stateStore.providerClass = "org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider"
spark.sql.streaming.stateStore.rocksdb.formatVersion = 5
spark.sql.streaming.stateStore.skipNullsForStreamStreamJoins.enabled = false
spark.sql.streaming.stateStore.stateSchemaCheck = true
spark.sql.streaming.statefulOperator.allowMultiple = true
spark.sql.streaming.statefulOperator.checkCorrectness.enabled = true
spark.sql.streaming.statefulOperator.useStrictDistribution = true
spark.sql.streaming.stopActiveRunOnRestart = true
spark.sql.streaming.stopTimeout = 10000L
spark.sql.streaming.unsupportedOperationCheck = true
spark.sql.stringHashComputationWithoutCopyMemory.enabled = true
spark.sql.subResultCache.batchSize = 1000
spark.sql.subResultCache.maxBufferSize = 25
spark.sql.subResultCache.partition.requiredMinSize = TimeUnit.HOURS.toMillis(4
spark.sql.subResultCache.reductionRatioThreshold = 8.0
spark.sql.subResultCache.write.timeout = false
spark.sql.subexpressionElimination.cache.maxEntries = SUBEXPRESSION_ELIMINATION_CACHE_MAX_ENTRIES_DEFAULT
spark.sql.thriftServer.interruptOnCancel = false
spark.sql.thriftServer.queryTimeout = 0L
spark.sql.thriftserver.ui.retainedSessions = 200
spark.sql.thriftserver.ui.retainedStatements = 200
spark.sql.timestampType = TimestampTypes.TIMESTAMP_LTZ.toString
spark.sql.truncateTable.ignorePermissionAcl.enabled = false
spark.sql.ui.explainMode = "extended"
spark.sql.ui.extendedInfo = true
spark.sql.ui.pruneCachedInMemoryRelation = true
spark.sql.variable.substitute = true
spark.sql.view.maxNestedViewDepth = 100
spark.sql.windowExec.buffer.spill.threshold = SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD.defaultValue.get
spark.sql.windowTopKSorter.filterRatioCheckBasicNumRows = 10000
spark.sql.windowTopKSorter.filterRatioCheckInterval = 1000
spark.sql.windowTopKSorter.inefficientFilterRatioThreshold = 0.7
spark.sql.windowTopKSorter.removalCountThreshold = 10000
