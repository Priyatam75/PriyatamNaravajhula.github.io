Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Fix Version/s,Component/s,Due Date,Votes,Labels,Description,Environment,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocked),Outward issue link (Blocked),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Completes),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id).1,Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Colour),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity).1,Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Affects Version/s.1,Affects Version/s.2,Affects Version/s.3,Affects Version/s.4,Comment.1
Spark images are based of Python 3.8 which is soon EOL,SPARK-48092,13577964,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,,,mayurmadnani,mayurmadnani,5/2/24 15:29,5/10/24 14:11,7/17/24 20:45,,"3.4.0, 3.4.1, 3.4.2, 3.5.0, 3.5.1",,Spark Docker,,0,,"Python 3.8 will be EOL in Oct 2024 and all the Spark docker images are based on Python 3.8 as of now.

I am proposing to use Python 3.10 as default. Let me know if I can pick this up to make the changes in [spark-docker|[https://github.com/apache/spark-docker]]
",,,,,,,,,,,,,,,,,,,,,,,,,"02/May/24 15:31;mayurmadnani;Screenshot 2024-05-02 at 21.00.18.png;https://issues.apache.org/jira/secure/attachment/13068601/Screenshot+2024-05-02+at+21.00.18.png, 02/May/24 15:31;mayurmadnani;Screenshot 2024-05-02 at 21.00.48.png;https://issues.apache.org/jira/secure/attachment/13068602/Screenshot+2024-05-02+at+21.00.48.png",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 10 14:11:12 UTC 2024,,,,,,,,,,0|z1ozyg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"10/May/24 14:11;schilukoori;[~mayurmadnani] I'm looking for opportunities to start contributing to Spark, could I assist you in updating the images?

Thank you.;;;",3.4.1,3.4.2,3.5.0,3.5.1,
Inaccurate Decimal multiplication and division results,SPARK-45786,13556766,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kazuyukitanimura,kazuyukitanimura,kazuyukitanimura,11/4/23 0:35,2/2/24 17:07,7/17/24 20:45,11/7/23 17:07,"3.2.4, 3.3.3, 3.4.1, 3.5.0, 4.0.0","3.4.2, 3.5.1, 4.0.0",SQL,,0,"correctness, pull-request-available","Decimal multiplication and division results may be inaccurate due to rounding issues.
h2. Multiplication:
{code:scala}
scala> sql(""select  -14120025096157587712113961295153.858047 * -0.4652"").show(truncate=false)
+----------------------------------------------------+                          
|(-14120025096157587712113961295153.858047 * -0.4652)|
+----------------------------------------------------+
|6568635674732509803675414794505.574764              |
+----------------------------------------------------+
{code}
The correct answer is
{quote}6568635674732509803675414794505.574763
{quote}

Please note that the last digit is 3 instead of 4 as

 
{code:scala}
scala> java.math.BigDecimal(""-14120025096157587712113961295153.858047"").multiply(java.math.BigDecimal(""-0.4652""))
val res21: java.math.BigDecimal = 6568635674732509803675414794505.5747634644
{code}
Since the factional part .574763 is followed by 4644, it should not be rounded up.
h2. Division:
{code:scala}
scala> sql(""select -0.172787979 / 533704665545018957788294905796.5"").show(truncate=false)
+-------------------------------------------------+
|(-0.172787979 / 533704665545018957788294905796.5)|
+-------------------------------------------------+
|-3.237521E-31                                    |
+-------------------------------------------------+
{code}
The correct answer is
{quote}-3.237520E-31
{quote}

Please note that the last digit is 0 instead of 1 as

 
{code:scala}
scala> java.math.BigDecimal(""-0.172787979"").divide(java.math.BigDecimal(""533704665545018957788294905796.5""), 100, java.math.RoundingMode.DOWN)
val res22: java.math.BigDecimal = -3.237520489418037889998826491401059986665344697406144511563561222578738E-31
{code}
Since the factional part .237520 is followed by 4894..., it should not be rounded up.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 02 17:07:58 UTC 2024,,,,,,,,,,0|z1legw:,9223372036854775807,,,,,,,,,,,,,3.4.2,,,,,,,,"07/Nov/23 17:05;dongjoon;Thank you, [~kazuyukitanimura] .

As a release manager of Apache Spark 3.4.2, I added a `correctness` label and raises the priority to `Blocker` for Apache Spark 3.4.2.;;;, 07/Nov/23 17:07;dongjoon;This is resolved via [https://github.com/apache/spark/pull/43678];;;, 02/Feb/24 17:07;nchammas;[~kazuyukitanimura] - I'm just curious: How did you find this bug? Was it something you stumbled on by accident or did you search for it using something like a fuzzer?;;;",3.3.3,3.4.1,3.5.0,4.0.0,07/Nov/23 17:07;dongjoon;This is resolved via [https://github.com/apache/spark/pull/43678];;;
Join loses records for cached datasets,SPARK-45282,13551692,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,eejbyfeldt,koert,koert,9/22/23 17:13,1/18/24 23:04,7/17/24 20:45,11/12/23 21:54,"3.4.1, 3.5.0",3.4.2,SQL,,0,"correctness, CorrectnessBug, pull-request-available","we observed this issue on spark 3.4.1 but it is also present on 3.5.0. it is not present on spark 3.3.1.

it only shows up in distributed environment. i cannot replicate in unit test. however i did get it to show up on hadoop cluster, kubernetes, and on databricks 13.3

the issue is that records are dropped when two cached dataframes are joined. it seems in spark 3.4.1 in queryplan some Exchanges are dropped as an optimization while in spark 3.3.1 these Exhanges are still present. it seems to be an issue with AQE with canChangeCachedPlanOutputPartitioning=true.

to reproduce on distributed cluster these settings needed are:
{code:java}
spark.sql.adaptive.advisoryPartitionSizeInBytes 33554432
spark.sql.adaptive.coalescePartitions.parallelismFirst false
spark.sql.adaptive.enabled true
spark.sql.optimizer.canChangeCachedPlanOutputPartitioning true {code}
code using scala to reproduce is:
{code:java}
import java.util.UUID
import org.apache.spark.sql.functions.col

import spark.implicits._

val data = (1 to 1000000).toDS().map(i => UUID.randomUUID().toString).persist()

val left = data.map(k => (k, 1))
val right = data.map(k => (k, k)) // if i change this to k => (k, 1) it works!
println(""number of left "" + left.count())
println(""number of right "" + right.count())
println(""number of (left join right) "" +
  left.toDF(""key"", ""value1"").join(right.toDF(""key"", ""value2""), ""key"").count()
)

val left1 = left
  .toDF(""key"", ""value1"")
  .repartition(col(""key"")) // comment out this line to make it work
  .persist()
println(""number of left1 "" + left1.count())

val right1 = right
  .toDF(""key"", ""value2"")
  .repartition(col(""key"")) // comment out this line to make it work
  .persist()
println(""number of right1 "" + right1.count())

println(""number of (left1 join right1) "" +  left1.join(right1, ""key"").count()) // this gives incorrect result{code}
this produces the following output:
{code:java}
number of left 1000000
number of right 1000000
number of (left join right) 1000000
number of left1 1000000
number of right1 1000000
number of (left1 join right1) 859531 {code}
note that the last number (the incorrect one) actually varies depending on settings and cluster size etc.

 ",spark 3.4.1 on apache hadoop 3.3.6 or kubernetes 1.26 or databricks 13.3,,,,,,,,,,,,,,,,,,,SPARK-41048,,SPARK-45592,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 18 23:04:12 UTC 2024,,,,,,,,,,0|z1kj74:,9223372036854775807,,,,,,,,,,,,,"3.4.2, 3.5.1",,,,,,,,"22/Sep/23 18:40;koert;after reverting SPARK-41048 the issue went away.;;;, 24/Sep/23 14:46;yumwang;cc [~ulysses] [~cloud_fan];;;, 27/Sep/23 01:36;ulysses;I can not re-produce this issue in master branch (4.0.0), [~koert] have you tried master branch ?;;;, 28/Sep/23 04:05;koert;yes i can reproduce it.

master branch on commit:
{code:java}
commit 7e8aafd2c0f1f6fcd03a69afe2b85fd3fda95d20 (HEAD -> master, upstream/master)
Author: lanmengran1 <lanmengran1@jd.com>
Date:   Tue Sep 26 21:01:02 2023 -0500    [SPARK-45334][SQL] Remove misleading comment in parquetSchemaConverter {code}
i build spark for k8s using:
{code:java}
$ dev/make-distribution.sh --name kubernetes --tgz -Pkubernetes -Phadoop-cloud {code}
created docker container using Dockerfile provided in resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile

launch pod and shell inside:
{code:java}
185@proxy:~/work-dir$ export SPARK_LOCAL_HOSTNAME=$(hostname -i
185@proxy:~/work-dir$ export SPARK_PUBLIC_DNS=$(hostname -i)                                                                                              185@proxy:~/work-dir$ /opt/spark/bin/spark-shell --master k8s://https://kubernetes.default:443 --deploy-mode client --num-executors 4 --executor-memory 2G --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kubernetes.namespace=default --conf spark.sql.adaptive.coalescePartitions.parallelismFirst=false --conf spark.sql.adaptive.enabled=true --conf spark.sql.adaptive.advisoryPartitionSizeInBytes=33554432 --conf spark.sql.optimizer.canChangeCachedPlanOutputPartitioning=true --conf spark.kubernetes.container.image=111111111111.dkr.ecr.us-east-1.amazonaws.com/spark:4.0.0-SNAPSHOT
23/09/28 03:44:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 4.0.0-SNAPSHOT
      /_/
         
Using Scala version 2.13.11 (OpenJDK 64-Bit Server VM, Java 21)
Type in expressions to have them evaluated.
Type :help for more information.
Spark context Web UI available at http://10.177.71.94:4040
Spark context available as 'sc' (master = k8s://https://kubernetes.default:443, app id = spark-5ab0957571944828866a2f23068ff180).
Spark session available as 'spark'.scala> :paste
// Entering paste mode (ctrl-D to finish)import java.util.UUID
import org.apache.spark.sql.functions.col
import spark.implicits._

val data = (1 to 1000000).toDS().map(i => UUID.randomUUID().toString).persist()
val left = data.map(k => (k, 1))
val right = data.map(k => (k, k)) // if i change this to k => (k, 1) it works!
println(""number of left "" + left.count())
println(""number of right "" + right.count())
println(""number of (left join right) "" +
  left.toDF(""key"", ""vertex"").join(right.toDF(""key"", ""state""), ""key"").count()
)

val left1 = left
  .toDF(""key"", ""vertex"")
  .repartition(col(""key"")) // comment out this line to make it work
  .persist()
println(""number of left1 "" + left1.count())
val right1 = right
  .toDF(""key"", ""state"")
  .repartition(col(""key"")) // comment out this line to make it work
  .persist()
println(""number of right1 "" + right1.count())
println(""number of (left1 join right1) "" +  left1.join(right1, ""key"").count()) // this gives incorrect result
// Exiting paste mode, now interpreting.
23/09/28 03:45:30 WARN TaskSetManager: Stage 0 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
23/09/28 03:45:34 WARN TaskSetManager: Stage 1 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
number of left 1000000                                                          
23/09/28 03:45:36 WARN TaskSetManager: Stage 4 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
number of right 1000000
23/09/28 03:45:39 WARN TaskSetManager: Stage 7 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
23/09/28 03:45:40 WARN TaskSetManager: Stage 8 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
number of (left join right) 1000000                                             
23/09/28 03:45:45 WARN TaskSetManager: Stage 16 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
number of left1 1000000                                                         
23/09/28 03:45:48 WARN TaskSetManager: Stage 24 contains a task of very large size (6631 KiB). The maximum recommended task size is 1000 KiB.
number of right1 1000000                                                        
number of (left1 join right1) 850735                                            
import java.util.UUID
import org.apache.spark.sql.functions.col
import spark.implicits._
val data: org.apache.spark.sql.Dataset[String] = [value: string]
val left: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int]
val right: org.apache.spark.sql.Dataset[(String, String)] = [_1: string, _2: string]
val left1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [key: string, vertex: int]
val right1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [key: string, state: string]

scala>   {code}
 ;;;, 31/Oct/23 00:00;dongjoon;Hi, All
Is this correctness issue still valid in branch-3.4/3.5/master? Or, I'm wondering if this exists in the reported platform, Databricks 13.3?;;;, 31/Oct/23 00:01;dongjoon;Let me increase the priority to `Blocker` for now.;;;, 31/Oct/23 00:26;koert;last time i checked this issue was still present in 3.4/3.5/master


;;;, 31/Oct/23 16:47;dongjoon;Thank you for sharing, [~koert].;;;, 08/Nov/23 16:09;eejbyfeldt;The code reproducing the bug looks quite similar to https://issues.apache.org/jira/browse/SPARK-45592 I wonder if the fix for that might also have solved this bug as I could not reproduce this issue on a build from the master branch.;;;, 08/Nov/23 16:20;koert;it does look like same issue

and partitioning being the cause makes sense too


;;;, 09/Nov/23 08:40;eejbyfeldt;Created this [https://github.com/apache/spark/pull/43729] to backport the fix to 3.4 from my manual test it solved the reproduction in this ticket.;;;, 12/Nov/23 21:54;dongjoon;Issue resolved by pull request 43729
[https://github.com/apache/spark/pull/43729];;;, 18/Jan/24 23:04;rrusso2007;Is it possible that this also affects spark 3.3.2? I have an application that has been running on spark 3.3.2 and with AQE enabled. When I upgraded to 3.5.0 I immediately ran into the issue in this ticket. However when I started looking more closely I found that for 1 particular type of report the issue was still present even after rolling back to 3.3.2 with AQE enabled.

Either way on 3.3.2 or 3.5.0, disabling AQE fixed the problem.;;;",3.5.0,,,,"24/Sep/23 14:46;yumwang;cc [~ulysses] [~cloud_fan];;;, 09/Nov/23 08:40;eejbyfeldt;Created this [https://github.com/apache/spark/pull/43729] to backport the fix to 3.4 from my manual test it solved the reproduction in this ticket.;;;, 12/Nov/23 21:54;dongjoon;Issue resolved by pull request 43729
[https://github.com/apache/spark/pull/43729];;;, 18/Jan/24 23:04;rrusso2007;Is it possible that this also affects spark 3.3.2? I have an application that has been running on spark 3.3.2 and with AQE enabled. When I upgraded to 3.5.0 I immediately ran into the issue in this ticket. However when I started looking more closely I found that for 1 particular type of report the issue was still present even after rolling back to 3.3.2 with AQE enabled.

Either way on 3.3.2 or 3.5.0, disabling AQE fixed the problem.;;;"
AQE and InMemoryTableScanExec correctness bug,SPARK-45592,13554580,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,10/18/23 13:34,1/12/24 5:39,7/17/24 20:45,10/31/23 3:23,"3.4.1, 3.5.0","3.4.2, 3.5.1, 4.0.0",SQL,,0,"correctness, pull-request-available","The following query should return 1000000
{code:java}
import org.apache.spark.storage.StorageLevel

val df = spark.range(0, 1000000, 1, 5).map(l => (l, l))
val ee = df.select($""_1"".as(""src""), $""_2"".as(""dst""))
  .persist(StorageLevel.MEMORY_AND_DISK)

ee.count()
val minNbrs1 = ee
  .groupBy(""src"").agg(min(col(""dst"")).as(""min_number""))
  .persist(StorageLevel.MEMORY_AND_DISK)
val join = ee.join(minNbrs1, ""src"")
join.count(){code}
but on spark 3.5.0 there is a correctness bug causing it to return `104800` or some other smaller value.",,,,,,,,,,,,,,,,,,,,,,,SPARK-45282,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 31 03:23:45 UTC 2023,,,,,,,,,,0|z1l0zk:,9223372036854775807,,,,,,,,,,,,,3.4.2,,,,,,,,"30/Oct/23 05:48;dongjoon;Thank you, [~eejbyfeldt]. I added a label, `correctness`, and raised the priority to `Blocker`.;;;, 31/Oct/23 03:23;cloud_fan;Issue resolved by pull request 43435
[https://github.com/apache/spark/pull/43435];;;",3.5.0,,,,"31/Oct/23 03:23;cloud_fan;Issue resolved by pull request 43435
[https://github.com/apache/spark/pull/43435];;;"
Subquery changes the output schema of the outer query,SPARK-45580,13554466,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,bersprockets,bersprockets,bersprockets,10/17/23 21:13,12/7/23 3:24,7/17/24 20:45,12/6/23 19:02,"3.3.3, 3.4.1, 3.5.0","3.3.4, 3.4.3, 3.5.1, 4.0.0",SQL,,0,"correctness, pull-request-available","A query can have an incorrect output schema because of a subquery.

Assume this data:
{noformat}
create or replace temp view t1(a) as values (1), (2), (3), (7);
create or replace temp view t2(c1) as values (1), (2), (3);
create or replace temp view t3(col1) as values (3), (9);
cache table t1;
cache table t2;
cache table t3;
{noformat}
When run in {{spark-sql}}, the following query has a superfluous boolean column:
{noformat}
select *
from t1
where exists (
  select c1
  from t2
  where a = c1
  or a in (select col1 from t3)
);

1	false
2	false
3	true
{noformat}
The result should be:
{noformat}
1
2
3
{noformat}
When executed via the {{Dataset}} API, you don't see the incorrect result, because the Dataset API truncates the right-side of the rows based on the analyzed plan's schema (it's the optimized plan's schema that goes wrong).

However, even with the {{Dataset}} API, this query goes wrong:
{noformat}
select (
  select *
  from t1
  where exists (
    select c1
    from t2
    where a = c1
    or a in (select col1 from t3)
  )
  limit 1
)
from range(1);

java.lang.AssertionError: assertion failed: Expects 1 field, but got 2; something went wrong in analysis
	at scala.Predef$.assert(Predef.scala:279)
	at org.apache.spark.sql.execution.ScalarSubquery.updateResult(subquery.scala:88)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1(SparkPlan.scala:276)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1$adapted(SparkPlan.scala:275)
	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:576)
	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:574)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:933)
        ...
{noformat}
Other queries that have the wrong schema:
{noformat}
select *
from t1
where a in (
  select c1
  from t2
  where a in (select col1 from t3)
);
{noformat}
and
{noformat}
select *
from t1
where not exists (
  select c1
  from t2
  where a = c1
  or a in (select col1 from t3)
);
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 06 18:53:53 UTC 2023,,,,,,,,,,0|z1l0a8:,9223372036854775807,,,,,,,,,,,,,3.3.4,,,,,,,,"17/Oct/23 21:38;bersprockets;I'll make a PR in the coming days.;;;, 06/Dec/23 18:53;dongjoon;Thank you, [~bersprockets].;;;, 06/Dec/23 18:53;dongjoon;I raised this issue to the blocker for Apache Spark 3.3.4.;;;",3.4.1,3.5.0,,,"06/Dec/23 18:53;dongjoon;Thank you, [~bersprockets].;;;"
SPJ: Results duplicated when SPJ partial-cluster and pushdown enabled but conditions unmet,SPARK-44641,13545904,13412655,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,csun,szehon,szehon,8/2/23 18:53,11/24/23 23:05,7/17/24 20:45,8/8/23 2:21,"3.4.0, 3.4.1","3.4.2, 3.5.0",SQL,,0,correctness,"Adding the following test case in KeyGroupedPartitionSuite demonstrates the problem.

 
{code:java}
test(""test join key is the second partition key and a transform"") {
  val items_partitions = Array(bucket(8, ""id""), days(""arrive_time""))
  createTable(items, items_schema, items_partitions)
  sql(s""INSERT INTO testcat.ns.$items VALUES "" +
    s""(1, 'aa', 40.0, cast('2020-01-01' as timestamp)), "" +
    s""(1, 'aa', 41.0, cast('2020-01-15' as timestamp)), "" +
    s""(2, 'bb', 10.0, cast('2020-01-01' as timestamp)), "" +
    s""(2, 'bb', 10.5, cast('2020-01-01' as timestamp)), "" +
    s""(3, 'cc', 15.5, cast('2020-02-01' as timestamp))"")

  val purchases_partitions = Array(bucket(8, ""item_id""), days(""time""))
  createTable(purchases, purchases_schema, purchases_partitions)
  sql(s""INSERT INTO testcat.ns.$purchases VALUES "" +
    s""(1, 42.0, cast('2020-01-01' as timestamp)), "" +
    s""(1, 44.0, cast('2020-01-15' as timestamp)), "" +
    s""(1, 45.0, cast('2020-01-15' as timestamp)), "" +
    s""(2, 11.0, cast('2020-01-01' as timestamp)), "" +
    s""(3, 19.5, cast('2020-02-01' as timestamp))"")

  withSQLConf(
    SQLConf.REQUIRE_ALL_CLUSTER_KEYS_FOR_CO_PARTITION.key -> ""false"",
    SQLConf.V2_BUCKETING_PUSH_PART_VALUES_ENABLED.key -> ""true"",
    SQLConf.V2_BUCKETING_PARTIALLY_CLUSTERED_DISTRIBUTION_ENABLED.key ->
      ""true"") {
    val df = sql(""SELECT id, name, i.price as purchase_price, "" +
      ""p.item_id, p.price as sale_price "" +
      s""FROM testcat.ns.$items i JOIN testcat.ns.$purchases p "" +
      ""ON i.arrive_time = p.time "" +
      ""ORDER BY id, purchase_price, p.item_id, sale_price"")

    val shuffles = collectShuffles(df.queryExecution.executedPlan)
    assert(!shuffles.isEmpty, ""should not perform SPJ as not all join keys are partition keys"")
    checkAnswer(df,
      Seq(
        Row(1, ""aa"", 40.0, 1, 42.0),
        Row(1, ""aa"", 40.0, 2, 11.0),
        Row(1, ""aa"", 41.0, 1, 44.0),
        Row(1, ""aa"", 41.0, 1, 45.0),
        Row(2, ""bb"", 10.0, 1, 42.0),
        Row(2, ""bb"", 10.0, 2, 11.0),
        Row(2, ""bb"", 10.5, 1, 42.0),
        Row(2, ""bb"", 10.5, 2, 11.0),
        Row(3, ""cc"", 15.5, 3, 19.5)
      )
    )
  }
}{code}
 

Note: this tests has setup the datasourceV2 to return multiple splits for same partition.

In this case, SPJ is not triggered (because join key does not match partition key), but the following code in DSV2Scan:

[https://github.com/apache/spark/blob/v3.4.1/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/BatchScanExec.scala#L194]

intended to fill the empty partition for 'pushdown-vallue' will still iterate through non-grouped partition and lookup from grouped partition to fill the map, resulting in some duplicate input data fed into the join.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,53:49.0,,,,,,,,,,0|z1jjug:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,,,,
Numerical output of MulticlassClassificationEvaluator does not coincide with expected output,SPARK-45910,13557796,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,airwoz,airwoz,11/13/23 22:02,7/6/24 20:16,7/17/24 20:45,,"3.4.1, 3.5.0",,ML,,0,,"To show an example of MulticlassClassificationEvaluator generating a numerical output, which does not coincide with the expected output consider the following code:
{code:java}
from pyspark.ml.classification import LinearSVC
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

train_data = [(0, 1.0, 2.0, 3.0), (1, 4.0, 5.0, 6.0), (0, 7.0, 8.0, 9.0)]
valid_data = [(1, 2.0, 3.0, 4.0), (0, 5.0, 6.0, 7.0), (1, 8.0, 9.0, 10.0)]

schema = [""label"", ""feature1"", ""feature2"", ""feature3""]

train = spark.createDataFrame(train_data, schema=schema)
valid = spark.createDataFrame(valid_data, schema=schema)

feature_columns = [""feature1"", ""feature2"", ""feature3""]
assembler = VectorAssembler(inputCols=feature_columns, outputCol=""features"")
train = assembler.transform(train)
valid = assembler.transform(valid)

svm = LinearSVC(maxIter=10, regParam=0.1)
model = svm.fit(train)
predictions = model.transform(valid)

recallByLabel = MulticlassClassificationEvaluator(metricName=""recallByLabel"")
weightedRecall = MulticlassClassificationEvaluator(metricName=""weightedRecall"")

print(f""Recall by label: {recallByLabel.evaluate(predictions)}"")
print(f""Weighted recall: {weightedRecall.evaluate(predictions)}"") {code}
It produces:
{code:java}
Recall by label: 1.0
Weighted recall: 0.3333333333333333{code}
but predictions.show() implies the following hand calculated confusion matrix:
{code:java}
 -----------
|  0  |  0  |
|  2  |  1  |
 -----------{code}
where the recall is 0, i.e., 0 / (0 + 2).

What is the nature of this discrepancy? Also, note that it is not restricted to recall; and other classifiers, which include a probability column in predictions, behave similarly.

 

Furthermore, the translation of the example to Scala, namely:
{code:java}
import org.apache.spark.ml.classification.LinearSVC
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
import org.apache.spark.sql.DataFrame

val trainData = Seq((0, 1.0, 2.0, 3.0), (1, 4.0, 5.0, 6.0), (0, 7.0, 8.0, 9.0))
val validData = Seq((1, 2.0, 3.0, 4.0), (0, 5.0, 6.0, 7.0), (1, 8.0, 9.0, 10.0))

val schema = Seq(""label"", ""feature1"", ""feature2"", ""feature3"")

val train: DataFrame = spark.createDataFrame(trainData).toDF(schema: _*)
val valid: DataFrame = spark.createDataFrame(validData).toDF(schema: _*)

val featureColumns = Array(""feature1"", ""feature2"", ""feature3"")
val assembler = new VectorAssembler()
  .setInputCols(featureColumns)
  .setOutputCol(""features"")

val trainAssembled = assembler.transform(train)
val validAssembled = assembler.transform(valid)

val svm = new LinearSVC()
  .setMaxIter(10)
  .setRegParam(0.1)

val model = svm.fit(trainAssembled)
val predictions = model.transform(validAssembled)

val recallByLabel = new MulticlassClassificationEvaluator()
  .setMetricName(""recallByLabel"")
val weightedRecall = new MulticlassClassificationEvaluator()
  .setMetricName(""weightedRecall"")

println(s""Recall by label: ${recallByLabel.evaluate(predictions)}"")
println(s""Weighted recall: ${weightedRecall.evaluate(predictions)}""){code}
produces the same recall by label and weighted recall, as described above.",,,,,,,,,,,,,,,,,,,,,,,,,13/Nov/23 22:03;airwoz;predictions_dot_show.png;https://issues.apache.org/jira/secure/attachment/13064376/predictions_dot_show.png,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,"pyspark, Python3, Scala",Sat Jul 06 20:16:16 UTC 2024,,,,,,,,,,0|z1lktk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,06/Jul/24 20:16;psyren99;Would like to take on this issue if there is no one else working on it.;;;,3.5.0,,,,
Spark UI: A stage is still active even when all of it's tasks are succeeded,SPARK-45101,13549936,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,rickyma,rickyma,9/7/23 14:00,6/4/24 11:57,7/17/24 20:45,,"3.4.1, 3.5.0, 4.0.0",,Spark Core,,0,,"In the stage UI, we can see all the tasks' statuses are SUCCESS.

But the stage is still marked as active.",,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/23 14:03;rickyma;1.png;https://issues.apache.org/jira/secure/attachment/13062748/1.png, 07/Sep/23 14:03;rickyma;2.png;https://issues.apache.org/jira/secure/attachment/13062747/2.png, 07/Sep/23 14:03;rickyma;3.png;https://issues.apache.org/jira/secure/attachment/13062749/3.png",,3,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 04 11:57:07 UTC 2024,,,,,,,,,,0|z1k8d4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Feb/24 19:22;bjornjorgensen;did you use spark.stop()  ;;;, 04/Jun/24 11:57;rickyma;No. It's just a Spark SQL.;;;",3.5.0,4.0.0,,,04/Jun/24 11:57;rickyma;No. It's just a Spark SQL.;;;
Data is silently lost in Tab separated CSV with empty (whitespace) rows,SPARK-46876,13566285,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,martinitus,martinitus,1/26/24 10:31,6/2/24 0:22,7/17/24 20:45,,3.4.1,,Input/Output,,0,pull-request-available,"When reading a tab separated file that contains lines that only contain tabs (i.e. empty strings as values of the columns for that row), then these rows will silently be skipped (as empty lines) and the resulting dataframe will have less rows than expected.

This behavior is inconsistent with the behavior for e.g. semicolon separated files, where the resulting dataframe will have a row with only empty string values.

A minimal reproducible example would look like:

A minimal reproducible example: A file containing this
{code:java}
a\tb\tc\r\n
\t\t\r\n
1\t2\t3{code}
will create a dataframe with one row (a=1,b=2,c=3)
whereas this
{code:java}
a;b;c\r\n
;;\r\n
1;2;3{code}
will read as two rows (first row contains empty strings)

I used the following pyspark command to read the dataframes
{code:java}
 spark.read.option(""header"",""true"").option(""sep"",""\t"").csv(""<tabseparated file>"").collect()
spark.read.option(""header"",""true"").option(""sep"","";"").csv(""<semicolon file>"").collect()
{code}
I ran into this particularly on databricks (I assume they use the same reader), but [this stack overflow post|https://stackoverflow.com/questions/47823858/replacing-empty-lines-with-characters-when-reading-csv-using-spark#comment137288546_47823858] indicates, that this is an old issue that may have been taken over from databricks when their csv reader was adopted in SPARK-12420

I recommend to at least add a respective test case to the CSV reader.

 

Why is this behaviour a problem:
 * It violates some of the core assumptions
 ** a properly configured roundtrip via csv write/read should result in the same set of rows
 ** changing the csv separator (when everything is properly esacped) should have no effect

Potential resolutions:
 * When the configured delimiter consists of only whitespace
 ** deactivate the ""skip empty line feature""
 ** or skip only lines that are completely empty (only a (carriage return) newline)
 * Change the skip empty line feature to only skip if the line is completely empty (only contains a newlin)
 ** this may break some user code that relies on the current behaviour",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 15:16:22 UTC 2024,,,,,,,,,,0|z1n0uw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Jan/24 10:43;martinitus;Any resolution of this should probaly consider the mode. I.e. when using FAILFAST, i would assume, that a line that is only a newline could be ignored, but a line with only three tabs where the header indicates 5 columns should cause an error.;;;, 30/Jan/24 01:24;doki;{{The reason is that before parsing the csv lines spark calls `CSVExprUtils.filterCommentAndEmpty` to filter `empty` lines which only contains characters those <= ' '. I doubt that if it's neccessary to do this, because they may be exactly data itself. I've learnt that apache/commons-csv does trim for every column instead of whole line before parsing and trim is an option.}};;;, 15/Feb/24 11:35;martinitus;ping.

Can someone review the PR? I had a quick look and it looks ok, but I don't really understand scala....;;;, 12/Mar/24 13:53;martinitus;ping. The PR is open, the issue could be closed since 2 weeks already. Can someone with commit rights please have a look at the PR and either provide feedback or merge it?;;;, 14/Mar/24 15:16;martinitus;[~doki] any chance to make progress on this?;;;",,,,,"30/Jan/24 01:24;doki;{{The reason is that before parsing the csv lines spark calls `CSVExprUtils.filterCommentAndEmpty` to filter `empty` lines which only contains characters those <= ' '. I doubt that if it's neccessary to do this, because they may be exactly data itself. I've learnt that apache/commons-csv does trim for every column instead of whole line before parsing and trim is an option.}};;;"
DeepSpeed Distributor,SPARK-44264,13542146,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,lu.DB,lu.DB,6/30/23 20:01,5/2/24 6:56,7/17/24 20:45,7/11/23 22:37,3.4.1,3.5.0,"ML, PySpark",,0,pull-request-available,To make it easier for Pyspark users to run distributed training and inference with DeepSpeed on spark clusters using PySpark. This was a project determined by the Databricks ML Training Team.,,,,,,,,,,,,,,,,,,,,,,,,,19/Jul/23 02:13;erithwik;Trying to Run Deepspeed Funcs.html;https://issues.apache.org/jira/secure/attachment/13061411/Trying+to+Run+Deepspeed+Funcs.html,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 26 17:59:44 UTC 2023,,,,,,,,,,0|z1iwq8:,9223372036854775807,,,,,gurwls223,,,,,,,,,,,,,,,,"11/Jul/23 22:37;gurwls223;Issue resolved by pull request 41770
[https://github.com/apache/spark/pull/41770];;;, 14/Jul/23 21:19;XinrongM;Issue resolved by pull request https://github.com/apache/spark/pull/41946;;;, 19/Jul/23 02:18;hudson;User 'mathewjacob1002' has created a pull request for this issue:
https://github.com/apache/spark/pull/42067;;;, 26/Jul/23 17:59;ignitetcbot;User 'mathewjacob1002' has created a pull request for this issue:
https://github.com/apache/spark/pull/42118;;;",,,,,14/Jul/23 21:19;XinrongM;Issue resolved by pull request https://github.com/apache/spark/pull/41946;;;
Got fetch failed exception when new executor reused same ip address from a previously killed executor,SPARK-47678,13574203,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,bobyangbo,bobyangbo,4/2/24 0:40,4/15/24 22:46,7/17/24 20:45,,"3.4.0, 3.4.1, 3.4.2, 3.5.0, 3.5.1",,Shuffle,,0,pull-request-available,"This is an edge case which caused Spark on Kubernetes getting fetch failed exception when new executor reused same ip address from a previously killed executor.

The new executor checks shuffle block ip address and compares it with its own host address. If the two ip addresses are the same, the new executor will assume the block on its own local disk and try to read it locally. This causes failure since the block is actually on the previously killed executor which happened to have same ip address.","This only happens on Kubernetes, where same ip address can be re-used for new executor pod.",,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,40:14.0,,,,,,,,,,0|z1od80:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,3.4.2,3.5.0,3.5.1,
Recover -1 and 0 case for spark.sql.execution.arrow.maxRecordsPerBatch,SPARK-47068,13568665,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,gurwls223,2/16/24 1:16,4/2/24 3:48,7/17/24 20:45,4/2/24 3:48,"3.4.1, 3.5.0, 4.0.0","3.4.3, 3.5.2, 4.0.0",PySpark,,0,pull-request-available,"{code}
import pandas as pd
spark.conf.set(""spark.sql.execution.arrow.pyspark.enabled"", ""true"")
spark.conf.set(""spark.sql.execution.arrow.maxRecordsPerBatch"", 0)
spark.conf.set(""spark.sql.execution.arrow.pyspark.fallback.enabled"", False)
spark.createDataFrame(pd.DataFrame({'a': [123]})).toPandas()

spark.conf.set(""spark.sql.execution.arrow.maxRecordsPerBatch"", -1)
spark.createDataFrame(pd.DataFrame({'a': [123]})).toPandas()
{code}

{code}
/.../spark/python/pyspark/sql/pandas/conversion.py:371: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and will not continue because automatic fallback with 'spark.sql.execution.arrow.pyspark.fallback.enabled' has been set to false.
  range() arg 3 must not be zero
  warn(msg)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/.../spark/python/pyspark/sql/session.py"", line 1483, in createDataFrame
    return super(SparkSession, self).createDataFrame(  # type: ignore[call-overload]
  File ""/.../spark/python/pyspark/sql/pandas/conversion.py"", line 351, in createDataFrame
    return self._create_from_pandas_with_arrow(data, schema, timezone)
  File ""/.../spark/python/pyspark/sql/pandas/conversion.py"", line 633, in _create_from_pandas_with_arrow
    pdf_slices = (pdf.iloc[start : start + step] for start in range(0, len(pdf), step))
ValueError: range() arg 3 must not be zero
{code}

{code}
Empty DataFrame
Columns: [a]
Index: []
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 03:48:55 UTC 2024,,,,,,,,,,0|z1nfjk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,02/Apr/24 03:48;joshrosen;Marking this issue as fixed.;;;,3.5.0,4.0.0,,,
PruneFilters incorrectly tags isStreaming flag when replacing child of Filter with LocalRelation,SPARK-47305,13570978,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kabhwan,kabhwan,kabhwan,3/6/24 12:59,3/8/24 8:46,7/17/24 20:45,3/7/24 6:12,"3.4.0, 3.4.1, 3.4.2, 3.5.0, 3.5.1, 4.0.0","3.4.3, 3.5.2, 4.0.0",Structured Streaming,,0,pull-request-available,"This seems to be a very old bug in optimizer. Related ticket:  https://issues.apache.org/jira/browse/SPARK-21765

When filter is evaluated to be always false, PruneFilters replaces the filter with empty LocalRelation, which effectively prunes filter. The logic cares about migration of the isStreaming flag, but incorrectly migrated in some case, via picking up the value of isStreaming flag from root node rather than filter (or child).

isStreaming flag is true if the value of isStreaming flag from any of children is true. Flipping the coin, some children might have isStreaming flag as ""false"". If the filter being pruned is a descendant to such children (in other word, ancestor of streaming node), LocalRelation is incorrectly tagged as streaming where it should be batch.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 07 06:12:09 UTC 2024,,,,,,,,,,0|z1ntbs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"07/Mar/24 06:12;kabhwan;Issue resolved by pull request 45406
[https://github.com/apache/spark/pull/45406];;;",3.4.1,3.4.2,3.5.0,3.5.1,
Spark Container doesn't have spark group or spark user created,SPARK-47105,13569235,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,albertatcelerdata,albertatcelerdata,2/20/24 18:08,2/20/24 19:59,7/17/24 20:45,,3.4.1,,"Kubernetes, Spark Docker",,0,,"I see that [https://github.com/apache/spark-docker/blob/431aa516ba58985c902bf2d2a07bf0eaa1df6740/3.4.1/scala2.12-java11-ubuntu/Dockerfile#L19] is supposed to have a spark user and spark group created but checking the container, it doesn't have those uid and gid created.  Both should have 185 uid and 185 gid.

I have no name!@spark-hudi:/opt/spark/bin$ cat /etc/group
root:x:0:
daemon:x:1:
bin:x:2:
sys:x:3:
adm:x:4:
tty:x:5:
disk:x:6:
lp:x:7:
mail:x:8:
news:x:9:
uucp:x:10:
man:x:12:
proxy:x:13:
kmem:x:15:
dialout:x:20:
fax:x:21:
voice:x:22:
cdrom:x:24:
floppy:x:25:
tape:x:26:
sudo:x:27:
audio:x:29:
dip:x:30:
www-data:x:33:
backup:x:34:
operator:x:37:
list:x:38:
irc:x:39:
src:x:40:
gnats:x:41:
shadow:x:42:
utmp:x:43:
video:x:44:
sasl:x:45:
plugdev:x:46:
staff:x:50:
games:x:60:
users:x:100:
nogroup:x:65534:
I have no name!@spark-hudi:/opt/spark/bin$ cat /etc/passwd
root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
bin:x:2:2:bin:/bin:/usr/sbin/nologin
sys:x:3:3:sys:/dev:/usr/sbin/nologin
sync:x:4:65534:sync:/bin:/bin/sync
games:x:5:60:games:/usr/games:/usr/sbin/nologin
man:x:6:12:man:/var/cache/man:/usr/sbin/nologin
lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin
mail:x:8:8:mail:/var/mail:/usr/sbin/nologin
news:x:9:9:news:/var/spool/news:/usr/sbin/nologin
uucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin
proxy:x:13:13:proxy:/bin:/usr/sbin/nologin
www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin
backup:x:34:34:backup:/var/backups:/usr/sbin/nologin
list:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin
irc:x:39:39:ircd:/run/ircd:/usr/sbin/nologin
gnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin
nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin
_apt:x:100:65534::/nonexistent:/usr/sbin/nologin

I have no name!@spark-hudi:/opt/spark/bin$ cat /etc/passwd
root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
bin:x:2:2:bin:/bin:/usr/sbin/nologin
sys:x:3:3:sys:/dev:/usr/sbin/nologin
sync:x:4:65534:sync:/bin:/bin/sync
games:x:5:60:games:/usr/games:/usr/sbin/nologin
man:x:6:12:man:/var/cache/man:/usr/sbin/nologin
lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin
mail:x:8:8:mail:/var/mail:/usr/sbin/nologin
news:x:9:9:news:/var/spool/news:/usr/sbin/nologin
uucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin
proxy:x:13:13:proxy:/bin:/usr/sbin/nologin
www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin
backup:x:34:34:backup:/var/backups:/usr/sbin/nologin
list:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin
irc:x:39:39:ircd:/run/ircd:/usr/sbin/nologin
gnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin
nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin
_apt:x:100:65534::/nonexistent:/usr/sbin/nologin",Using container apache/spark-py:latest,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 19:59:31 UTC 2024,,,,,,,,,,0|z1nikg:,9223372036854775807,,,,,gurwls223,,,,,,,,,,,,,,,,20/Feb/24 19:59;albertatcelerdata;Related: https://issues.apache.org/jira/browse/SPARK-45557;;;,,,,,
CSV reader reads data inconsistently depending on column position,SPARK-46959,13567182,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,martinitus,martinitus,2/2/24 12:09,2/5/24 12:45,7/17/24 20:45,,3.4.1,,Spark Core,,0,,"Reading the following CSV
{code:java}
""a"";""b"";""c"";""d""
10;100,00;""Some;String"";""ok""
20;200,00;"""";""still ok""
30;300,00;""also ok"";""""
40;400,00;"""";"""" {code}
with these options
{code:java}
spark.read
        .option(""header"",""true"")
        .option(""sep"","";"")
        .option(""encoding"",""ISO-8859-1"")
        .option(""lineSep"",""\r\n"")
        .option(""nullValue"","""")
        .option(""quote"",'""')
        .option(""escape"","""") {code}
results in the followin inconsistent dataframe

 
||a||b||c||d||
|10|100,00|Some;String|ok|
|20|200,00|<null>|still ok|
|30|300,00|also ok|""|
|40|400,00|<null>|""|

As one can see, the quoted empty fields of the last column are not correctly read as null but instead contain a single double quote. It works for column c.

If I recall correctly, this only happens when the ""escape"" option is set to an empty string. Not setting it to """" (defaults to ""\"") seems to not cause this bug.

I observed this on databricks spark runtime 13.2 (think that is spark 3.4.1).",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 05 12:45:57 UTC 2024,,,,,,,,,,0|z1n6e8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"05/Feb/24 12:45;martinitus;Is there an option to disable escaping?

A potential fix for this would be to not allow an empty string as escape: raise exception when this option value is specified.

Quietly reading in garbage is kind of the worst case scenario (at least when mode=""FAILFAST"").;;;",,,,,
Unable to read from JDBC data sources when using custom schema containing varchar,SPARK-44638,13545889,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,michaelsaid,michaelsaid,8/2/23 16:33,1/11/24 11:03,7/17/24 20:45,,"3.1.0, 3.2.4, 3.3.2, 3.4.1",,SQL,,0,,"When querying the data from JDBC databases with custom schema containing varchar I got this error :
{code:java}
[23/07/14 06:12:19 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) ( executor 1): java.sql.SQLException: Unsupported type varchar(100) at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedJdbcTypeError(QueryExecutionErrors.scala:818) 23/07/14 06:12:21 INFO TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2) on , executor 0: java.sql.SQLException (Unsupported type varchar(100)){code}
Code example: 
{code:java}
CUSTOM_SCHEMA=""ID Integer, NAME VARCHAR(100)""
df = spark.read.format(""jdbc"")
.option(""url"", ""jdbc:oracle:thin:@0.0.0.0:1521:db"")
.option(""driver"", ""oracle.jdbc.OracleDriver"")
.option(""dbtable"", ""table"")
.option(""customSchema"", CUSTOM_SCHEMA)
.option(""user"", ""user"")
.option(""password"", ""password"")
.load()
df.show(){code}
I tried to set {{spark.sql.legacy.charVarcharAsString = true}} to restore the behavior before Spark 3.1 but it doesn't help.
The issue occurs in version 3.1.0 and above. I believe that this issue is caused by https://issues.apache.org/jira/browse/SPARK-33480",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 11 11:03:19 UTC 2024,,,,,,,,,,0|z1jjr4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,11/Jan/24 11:03;yao;Can you reproduce this issue on 3.5.0 or master branch?;;;,3.2.4,3.3.2,3.4.1,,
Pyspark DecisionTreeClassifier: results and tree structure in spark3 very different from that of the spark2 version on the same data and with the same hyperparameters.,SPARK-45154,13550538,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,oumarnour,oumarnour,9/13/23 13:39,12/12/23 12:34,7/17/24 20:45,,"3.0.0, 3.2.4, 3.3.1, 3.3.2, 3.3.3, 3.4.0, 3.4.1",,"ML, MLlib, PySpark, Spark Core",,0,"decisiontree, pyspark3, spark2, spark3","Hello,
I have an engine running on spark2 using a DecisionTreeClassifier model using the CrossValidator. 

 
{code:java}
dt  = DecisionTreeClassifier(maxBins=10000, seed=0)   
cv_dt_evaluator = BinaryClassificationEvaluator(
            metricName="""", 
            rawPredictionCol=""probability"")

# Create param grid and cross validator for model selection
dt_grid = ParamGridBuilder()\
            .addGrid(
                dt.minInstancesPerNode, [100]
        )\
            .addGrid(
                dt.maxDepth, [10]
        )\
            .build()
cv = CrossValidator(
            estimator=dt, estimatorParamMaps=dt_grid, evaluator=cv_dt_evaluator,
            parallelism=4
            numFolds=4
        ){code}
 

I want to {*}migrate from spark2  to spark3{*}. I've run *DecisionTreeClassifier* on the same data with the same parameter values. But unfortunately my results are {*}completely different, especially in terms of tree structure{*}. I have trees with less depth and fewer splits on spark3. I've tried to read the documentation but I haven't found an answer to my question.

 

Can you help me find a solution to this problem?

Thanks in advance for your help 

        

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,"pyspark, Python3, Spark2, spark3",Tue Dec 12 12:34:22 UTC 2023,,,,,,,,,,0|z1kc2o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,12/Dec/23 12:34;apeng;[~oumarnour] I think you need to set the _seed_ param of CrossValidator.;;;,3.2.4,3.3.1,3.3.2,3.3.3,
Fix PERCENTILE_DISC behaviour,SPARK-44871,13547866,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,petertoth,petertoth,petertoth,8/18/23 12:54,11/24/23 22:24,7/17/24 20:45,8/22/23 16:27,"3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.4.0, 3.4.1","3.3.4, 3.4.2, 3.5.0, 4.0.0",SQL,,0,correctness,"Currently {{percentile_disc()}} returns incorrect results in some cases:

E.g.:
{code:java}
SELECT
  percentile_disc(0.0) WITHIN GROUP (ORDER BY a) as p0,
  percentile_disc(0.1) WITHIN GROUP (ORDER BY a) as p1,
  percentile_disc(0.2) WITHIN GROUP (ORDER BY a) as p2,
  percentile_disc(0.3) WITHIN GROUP (ORDER BY a) as p3,
  percentile_disc(0.4) WITHIN GROUP (ORDER BY a) as p4,
  percentile_disc(0.5) WITHIN GROUP (ORDER BY a) as p5,
  percentile_disc(0.6) WITHIN GROUP (ORDER BY a) as p6,
  percentile_disc(0.7) WITHIN GROUP (ORDER BY a) as p7,
  percentile_disc(0.8) WITHIN GROUP (ORDER BY a) as p8,
  percentile_disc(0.9) WITHIN GROUP (ORDER BY a) as p9,
  percentile_disc(1.0) WITHIN GROUP (ORDER BY a) as p10
FROM VALUES (0), (1), (2), (3), (4) AS v(a)
{code}
returns:
{code:java}
+---+---+---+---+---+---+---+---+---+---+---+
| p0| p1| p2| p3| p4| p5| p6| p7| p8| p9|p10|
+---+---+---+---+---+---+---+---+---+---+---+
|0.0|0.0|0.0|1.0|1.0|2.0|2.0|2.0|3.0|3.0|4.0|
+---+---+---+---+---+---+---+---+---+---+---+
{code}
but it should return:
{noformat}
+---+---+---+---+---+---+---+---+---+---+---+
| p0| p1| p2| p3| p4| p5| p6| p7| p8| p9|p10|
+---+---+---+---+---+---+---+---+---+---+---+
|0.0|0.0|0.0|1.0|1.0|2.0|2.0|3.0|3.0|4.0|4.0|
+---+---+---+---+---+---+---+---+---+---+---+
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 22 16:27:35 UTC 2023,,,,,,,,,,0|z1jvlc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Aug/23 17:44;tgraves;Can you add a description to this please;;;, 18/Aug/23 18:37;petertoth;[~tgraves], sure, I've just updated it.

It looks like my PR didn't get linked here automatically, so here it is: https://github.com/apache/spark/pull/42559;;;, 22/Aug/23 16:27;maxgekk;Issue resolved by pull request 42610
[https://github.com/apache/spark/pull/42610];;;",3.3.1,3.3.2,3.3.3,3.4.0,"18/Aug/23 18:37;petertoth;[~tgraves], sure, I've just updated it.

It looks like my PR didn't get linked here automatically, so here it is: https://github.com/apache/spark/pull/42559;;;"
"Parquet INT64 (TIMESTAMP(NANOS,false)) throwing Illegal Parquet type",SPARK-44988,13548859,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,,,flavioodas,flavioodas,8/28/23 15:47,10/9/23 9:56,7/17/24 20:45,,"3.4.0, 3.4.1",,SQL,,4,,"This bug seems similar to https://issues.apache.org/jira/browse/SPARK-40819, except that it's a problem with INT64 (TIMESTAMP(NANOS,false)), instead of INT64 (TIMESTAMP(NANOS,true)).

The error happens whenever I'm trying to read:
{code:java}
org.apache.spark.sql.AnalysisException: Illegal Parquet type: INT64 (TIMESTAMP(NANOS,false)).
	at org.apache.spark.sql.errors.QueryCompilationErrors$.illegalParquetTypeError(QueryCompilationErrors.scala:1762)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.illegalType$1(ParquetSchemaConverter.scala:206)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertPrimitiveField$2(ParquetSchemaConverter.scala:283)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertPrimitiveField(ParquetSchemaConverter.scala:224)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertField(ParquetSchemaConverter.scala:187)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3(ParquetSchemaConverter.scala:147)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3$adapted(ParquetSchemaConverter.scala:117)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.immutable.Range.foreach(Range.scala:158)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertInternal(ParquetSchemaConverter.scala:117)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convert(ParquetSchemaConverter.scala:87)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readSchemaFromFooter$2(ParquetFileFormat.scala:493)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readSchemaFromFooter(ParquetFileFormat.scala:493)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$2(ParquetFileFormat.scala:473)
	at scala.collection.immutable.Stream.map(Stream.scala:418)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:473)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:464)
	at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:79)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 09 09:56:14 UTC 2023,,,,,,,,,,0|z1k1ps:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"31/Aug/23 13:27;fanjia;Have you tried setting spark.sql.legacy.parquet.nanosAsLong to true？;;;, 09/Oct/23 09:56;milesgranger;[~fanjia]that ""worked"" for me, but then of course need to cast the resulting bigint to a timestamp, which I feel is error prone. Would be nice if spark supported timestamp[ns] though.;;;",3.4.1,,,,"09/Oct/23 09:56;milesgranger;[~fanjia]that ""worked"" for me, but then of course need to cast the resulting bigint to a timestamp, which I feel is error prone. Would be nice if spark supported timestamp[ns] though.;;;"
Migrated shuffle blocks are encrypted multiple times when io.encryption is enabled ,SPARK-44588,13545325,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,henrymai,henrymai,henrymai,7/28/23 15:55,8/2/23 4:08,7/17/24 20:45,8/1/23 21:41,"3.1.0, 3.1.1, 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1","3.3.3, 3.4.2, 3.5.0",Spark Core,,0,,"Shuffle blocks upon migration are wrapped for encryption again when being written out to a file on the receiver side.

 

Pull request to fix this: https://github.com/apache/spark/pull/42214

 

Details:

Sender/Read side:

BlockManagerDecommissioner:run()
    blocks = bm.migratableResolver.getMigrationBlocks()
        *dataFile = IndexShuffleBlockResolver:getDataFile(...)*
       buffer = FileSegmentManagedBuffer(..., dataFile)
                       *^ This reads straight from disk without decryption*
    blocks.foreach((blockId, buffer) => bm.blockTransferService.uploadBlockSync(..., buffer, ...))
        -> uploadBlockSync() -> uploadBlock(..., buffer, ...)
            -> client.uploadStream(UploadBlockStream, buffer, ...)
 - Notice that there is no decryption here on the sender/read side.

Receiver/Write side:

NettyBlockRpcServer:receiveStream() <--- This is the UploadBlockStream handler
    putBlockDataAsStream()
        migratableResolver.putShuffleBlockAsStream()
            *-> file = IndexShuffleBlockResolver:getDataFile(...)*
            -> tmpFile = (file + .<uuid> extension)
            *-> Creates an encrypting writable channel to a tmpFile using serializerManager.wrapStream()*
            -> onData() writes the data into the channel
            -> onComplete() renames the tmpFile to the file
 - Notice:

 * Both getMigrationBlocks()[read] and putShuffleBlockAsStream()[write] target IndexShuffleBlockResolver:getDataFile()
 * The read path does not decrypt but the write path encrypts.
 * As a thought exercise: if this cycle happens more than once (where this receiver is now a sender) even if we assume that the shuffle blocks are initially unencrypted*, then bytes in the file will just have more and more layers of encryption applied to it each time it gets migrated.
 * *In practice, the shuffle blocks are encrypted on disk to begin with, this is just a thought exercise",,,,,,,,,,,,,,,,,,,,SPARK-20629,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,55:04.0,,,,,,,,,,0|z1jg9s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"3.1.1, 3.3.1, 3.3.2, 3.4.0, 3.4.1",3.1.2,3.1.3,3.2.0,
Fix catalog.listCatalogs in PySpark,SPARK-43527,13536410,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,podongfeng,podongfeng,podongfeng,5/16/23 13:27,5/16/23 23:31,7/17/24 20:45,5/16/23 23:31,"3.4.0, 3.4.1, 3.5.0","3.4.1, 3.5.0",PySpark,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 23:31:48 UTC 2023,,,,,,,,,,0|z1hxfk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"16/May/23 23:31;gurwls223;Issue resolved by pull request 41186
[https://github.com/apache/spark/pull/41186];;;",3.4.1,3.5.0,,,
High On-heap memory usage is detected while doing parquet-file reading with Off-Heap memory mode enabled on spark,SPARK-44718,13546525,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,majdyz,majdyz,majdyz,8/8/23 11:55,7/2/24 17:06,7/17/24 20:45,8/15/23 10:10,3.4.1,4.0.0,"Spark Core, SQL",,0,pull-request-available,"I see the high use of on-heap memory usage while doing the parquet file reading when the off-heap memory mode is enabled. This is caused by the memory-mode for the column vector for the vectorized reader is configured by different flag, and the default value is always set to On-Heap.

Conf to reproduce the issue:

{{spark.memory.offHeap.size 1000000}}
{{spark.memory.offHeap.enabled true}}

Enabling these configurations only will not change the memory mode used for parquet-reading by the vectorized reader to Off-Heap.

 

Proposed PR: https://github.com/apache/spark/pull/42394",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Tue Aug 15 10:10:39 UTC 2023,,,,,,,,,,0|z1jnog:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"09/Aug/23 04:18;snoot;User 'majdyz' has created a pull request for this issue:
https://github.com/apache/spark/pull/42394;;;, 15/Aug/23 10:10;cloud_fan;Issue resolved by pull request 42394
[https://github.com/apache/spark/pull/42394];;;",,,,,"15/Aug/23 10:10;cloud_fan;Issue resolved by pull request 42394
[https://github.com/apache/spark/pull/42394];;;"
Writing to JDBC Temporary View Failed,SPARK-48562,13581892,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jackylee,jackylee,6/7/24 7:15,6/12/24 3:00,7/17/24 20:45,,"3.4.0, 3.4.1, 3.4.2, 3.4.3, 3.5.0, 3.5.1, 4.0.0",,SQL,,0,,"When creating a JDBC temporary view, *ApplyCharTypePadding* would add a Project before LogicalRelation if CHAR/VARCHAR column exists and Spark would save it as a view plan. Then if we try to write this view, Spark would put this view plan to *InsertintoStatement* in *ResolveRelations* which would fall {*}PrewriteCheck{*}.

Adding the following code to *JDBCTableCatalogSuite* would meet this problem.
{code:java}
test(""test writing temporary jdbc view"") {
    withConnection { conn =>
      conn.prepareStatement(""""""CREATE TABLE ""test"".""to_drop"" (id CHAR)"""""").executeUpdate()
    }
    sql(
      s""""""
        CREATE TEMPORARY TABLE jdbcTable
        USING jdbc
        OPTIONS (
          url='$url',
          dbtable='""test"".""to_drop""');"""""")
    sql(""INSERT INTO jdbcTable values(1),(2)"")
    sql(""select * from test.to_drop"").show()
    withConnection { conn =>
      conn.prepareStatement(""""""DROP TABLE ""test"".""to_drop"""""""").executeUpdate()
    }
  } {code}
 

Then we would get the following error.
{code:java}
[UNSUPPORTED_INSERT.RDD_BASED] Can't insert into the target. An RDD-based table is not allowed. SQLSTATE: 42809;
'InsertIntoStatement Project [staticinvoke(class org.apache.spark.sql.catalyst.util.CharVarcharCodegenUtils, StringType, readSidePadding, ID#0, 1, true, false, true) AS ID#1], false, false, false
+- LocalRelation [col1#3] {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 12 03:00:10 UTC 2024,,,,,,,,,,0|z1po54:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"11/Jun/24 07:02;jackylee;[~cloud_fan] Maybe we need to move *{color:#172b4d}ApplyCharTypePaddin{color}{{{}g{}}}* from *Anlyzer* to *Planer* to solve this bug.

The *ApplyCharTypePadding* is currently defined in the *Analyzer* layer, it conflicts with other ResolveRule rules in the {*}Analyzer{*}, as other ResolveRule rules are one-to-one modification rules, while *ApplyCharTypePadding* exhibits different behavior.

Therefore, we need to consider refactoring the *ApplyCharTypePadding* rule and moving it to the *Planner* layer. This can avoid inconsistent behavior in the Analyzer layer without affecting other logic.

 

Correct me if I'm wrong. Or maybe any better idea to solve this problem?

also cc [~yao] ;;;, 11/Jun/24 15:19;cloud_fan;writing to a temp view is an ill pattern IMO...;;;, 12/Jun/24 03:00;jackylee;Thanks for your reply. Since we do not restrict writes to views, users have the flexibility to utilize them in their scenarios. Additionally, temporary tables serve various purposes, such as mitigating data conflicts and minimizing metadata impact. Therefore, I believe it is essential to maintain support for this behavior unless a specific rule is implemented to explicitly prohibit such operations.;;;",3.4.1,3.4.2,3.4.3,3.5.0,11/Jun/24 15:19;cloud_fan;writing to a temp view is an ill pattern IMO...;;;
Overwriting the same partition of a partitioned table multiple times with empty data yields non-idempotent results,SPARK-44473,13543899,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ychris,ychris,7/18/23 8:03,6/5/24 2:59,7/17/24 20:45,,"3.1.3, 3.2.4, 3.3.2, 3.4.1",,SQL,,0,pull-request-available," 

Preparation:
Create a simple partition table using spark version 3.x, for example:

 
{code:java}
spark-sql> create table test1 (a int) partitioned by (dt string);
Time taken: 0.219 seconds{code}
 

 
 * Overwrite a new partition with empty data, and you can see that the partition information and the corresponding HDFS path are generated , for example:

{code:java}

spark-sql> insert overwrite table test1 partition(dt='20230702') select 2 where 1 <> 1;
Time taken: 0.992 seconds
spark-sql> dfs -ls /user/hive/warehouse/test1;
Found 2 items
-rw-r--r-- 2 hadoop hadoop 0 2023-07-18 14:41 /user/hive/warehouse/test1/_SUCCESS
drwxrwxrwx- hadoop hadoop 0 2023-07-18 14:41 /user/hive/warehouse/test1/dt=20230702
spark-sql> show partitions test1;
dt=20230702
Time taken: 0.162 seconds, Fetched 1 row(s)
{code}
 * When re-running the insert overwrite statement, you can see that the HDFS path corresponding to this partition does not exist.

 
{code:java}
spark-sql> insert overwrite table test1 partition(dt='20230702') select 2 where 1 <> 1;
Time taken: 0.706 seconds
spark-sql> dfs -ls /user/hive/warehouse/test1;
Found 1 items
-rw-r--r--   2 hadoop hadoop          0 2023-07-18 14:45 /user/hive/warehouse/test1/_SUCCESS
spark-sql> show partitions test1;
dt=20230702
Time taken: 0.183 seconds, Fetched 1 row(s){code}
For subsequent tasks that need to use this HDFS path, an exception that the path does not exist will be thrown, which caused us trouble.

 

I was expecting to execute the same statement multiple times to get the same result, {*}not non-idempotent{*}. thanks.",spark : 3.x,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,03:07.0,,,,,,,,,,0|z1j7hk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.2.4,3.3.2,3.4.1,,
batch-read parquet files written by streaming returns non-nullable fields in schema,SPARK-48492,13581205,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,JulienPeloton,JulienPeloton,5/31/24 19:16,6/3/24 11:03,7/17/24 20:45,,3.4.1,,Structured Streaming,,0,,"Hello,

In the documentation, it is stated that

> When reading Parquet files, all columns are automatically converted to be nullable for compatibility reasons.

While this seems correct for static DataFrames, I have a counter example for streaming ones:

 
{code:java}
from pyspark.sql import SparkSession
from pyspark.sql import Row
import pyspark.sql.functions as F

spark = SparkSession.builder.getOrCreate()
spark.sparkContext.setLogLevel(""WARN"")

df = spark.createDataFrame(
    [
        Row(a=1, b=2.0, c=""toto""),
        Row(a=3, b=4.0, c=""titi""),
        Row(a=10, b=11.0, c=""tutu""),
    ]
)

# add a non-nullable column
df = df.withColumn('d', F.lit(1.0))

print(""Original dataframe"")
df.printSchema()

# Write this on disk
df.write.parquet('static.parquet')

# Now load a stream
df_stream = (
    spark.readStream.format(""parquet"")
    .schema(df.schema)
    .option(""path"", ""static.parquet"")
    .option(""latestFirst"", False)
    .load()
)

# add a non-nullable column
df_stream = df_stream.withColumn('e', F.lit(""error""))

print(""Streaming dataframe"")
df_stream.printSchema()

# Now write the dataframe using writestream
query = (
    df_stream.writeStream.outputMode(""append"")
    .format(""parquet"")
    .option(""checkpointLocation"", 'test_parquet_checkpoint')
    .option(""path"", 'test_parquet')
    .trigger(availableNow=True)
    .start()
)

spark.streams.awaitAnyTermination()

# Now read back
df_stream_2 = spark.read.format(""parquet"").load(""test_parquet"")

print(""Static dataframe from the streaming job (read)"")
df_stream_2.printSchema() 

# Now load a stream
df_stream_3 = (
    spark.readStream.format(""parquet"")
    .schema(df_stream_2.schema)
    .option(""path"", ""test_parquet"")
    .option(""latestFirst"", False)
    .load()
)

print(""Streaming dataframe from the streaming job (readStream)"")
df_stream_3.printSchema(){code}
 

 

which outputs:
{noformat}
Original dataframe
root
 |-- a: long (nullable = true)
 |-- b: double (nullable = true)
 |-- c: string (nullable = true)
 |-- d: double (nullable = false)

Streaming dataframe
root
 |-- a: long (nullable = true)
 |-- b: double (nullable = true)
 |-- c: string (nullable = true)
 |-- d: double (nullable = true)
 |-- e: string (nullable = false)

Static dataframe from the streaming job (read)
root
 |-- a: long (nullable = true)
 |-- b: double (nullable = true)
 |-- c: string (nullable = true)
 |-- d: double (nullable = true)
 |-- e: string (nullable = false)

Streaming dataframe from the streaming job (readStream)
root
 |-- a: long (nullable = true)
 |-- b: double (nullable = true)
 |-- c: string (nullable = true)
 |-- d: double (nullable = true)
 |-- e: string (nullable = true){noformat}
 

So the column `d` is correctly set to `nullable = true` (expected), but in the case of the column `e`, it stays non-nullable if it is read using the `read` method and it is correctly set to `nullable = true` if read with `readStream`. Is that expected? According to this old issue, https://issues.apache.org/jira/browse/SPARK-28651, it was supposed to be resolved. Any ideas?","python --version
Python 3.9.13

 

spark-submit --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.4.1
      /_/
                        
Using Scala version 2.12.17, OpenJDK 64-Bit Server VM, 1.8.0_302
Branch HEAD
Compiled by user centos on 2023-06-19T23:01:01Z
Revision 6b1ff22dde1ead51cbf370be6e48a802daae58b6",,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,16:35.0,,,,,,,,,,0|z1pjwo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Add to_varchar alias for to_char SQL function,SPARK-43815,13537827,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ryu796,ryu796,ryu796,5/26/23 15:44,5/24/24 2:37,7/17/24 20:45,5/30/23 18:48,3.4.1,3.5.0,SQL,,0,pull-request-available,"We want to add the alias to_varchar for the function to_char. 

For users who are migrating to Spark SQL such that the SQL engine they formerly used supported to_varchar instead of to_char, this change would minimize the number of changes to their application to ensure it is compatible with Spark SQL syntax and support.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 30 18:48:22 UTC 2023,,,,,,,,,,0|z1i660:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/May/23 18:06;maxgekk;[~ryu796] Let's focus on the first item, and create separate JIRAs for other items.;;;, 30/May/23 18:48;maxgekk;Issue resolved by pull request 41319
[https://github.com/apache/spark/pull/41319];;;",,,,,"30/May/23 18:48;maxgekk;Issue resolved by pull request 41319
[https://github.com/apache/spark/pull/41319];;;"
Kryo serialization issue with push-based shuffle,SPARK-48043,13577567,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,romainardiet,romainardiet,4/29/24 11:02,5/13/24 13:06,7/17/24 20:45,,3.4.1,,Shuffle,,0,,"I'm running a spark job on AWS EMR. I wanted to test the new push-based shuffle introduced in Spark 3.2 but it's failing with a kryo exception when I'm enabling it.

The issue is happening when Executor starts, during 
KryoSerializerInstance.getAutoReset() check:
{code:java}
24/04/24 15:36:22 ERROR YarnCoarseGrainedExecutorBackend: Executor self-exiting due to : Unable to create executor due to Failed to register classes with Kryo
org.apache.spark.SparkException: Failed to register classes with Kryo
    at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:186) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
    at org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:241) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:174) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:105) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48) ~[kryo-shaded-4.0.2.jar:?]
    at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:112) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:352) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.serializer.KryoSerializerInstance.getAutoReset(KryoSerializer.scala:452) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects$lzycompute(KryoSerializer.scala:259) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.serializer.KryoSerializer.supportsRelocationOfSerializedObjects(KryoSerializer.scala:255) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.util.Utils$.serializerIsSupported$lzycompute$1(Utils.scala:2721) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.util.Utils$.serializerIsSupported$1(Utils.scala:2716) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.util.Utils$.isPushBasedShuffleEnabled(Utils.scala:2730) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.storage.BlockManager.initialize(BlockManager.scala:554) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.executor.Executor.<init>(Executor.scala:143) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1.applyOrElse(CoarseGrainedExecutorBackend.scala:190) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_402]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_402]
    at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_402]
Caused by: java.lang.ClassNotFoundException: com.analytics.AnalyticsEventWrapper
    at java.net.URLClassLoader.findClass(URLClassLoader.java:387) ~[?:1.8.0_402]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:418) ~[?:1.8.0_402]
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352) ~[?:1.8.0_402]
    at java.lang.ClassLoader.loadClass(ClassLoader.java:351) ~[?:1.8.0_402]
    at java.lang.Class.forName0(Native Method) ~[?:1.8.0_402]
    at java.lang.Class.forName(Class.java:348) ~[?:1.8.0_402]
    at org.apache.spark.util.Utils$.classForName(Utils.scala:228) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$6(KryoSerializer.scala:177) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.15.jar:?]
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.15.jar:?]
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.15.jar:?]
    at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$5(KryoSerializer.scala:176) ~[spark-core_2.12-3.4.1-amzn-1.jar:3.4.1-amzn-1]
    ... 24 more
24/04/24 15:36:22 INFO YarnCoarseGrainedExecutorBackend: Driver commanded a shutdown
24/04/24 15:36:22 ERROR Utils: Uncaught exception in thread shutdown-hook-0
 {code}
My job code is the following:
{code:java}
package com.analytics.spark;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import com.analytics.archive.AnalyticsEventWrapperRowMapper;
import com.analytics.AnalyticsEventWrapper;

public class ProcessAnalyticsEventJob {
  public static void main(String[] args) throws Exception {
    SparkConf sparkConf = new SparkConf()
        .setMaster(""yarn"")
        .registerKryoClasses(AnalyticsEventWrapper.class);
        
    SparkSession spark = SparkSession.builder().config(sparkConf).getOrCreate();
        
    MapFunction<Row, AnalyticsEventWrapper> mapAsAnalyticsEventWrapper = AnalyticsEventWrapperRowMapper::map;
    Dataset<AnalyticsEventWrapper> inputDataset = spark.read()
        .parquet(""s3://bucket/path/to/events"")
        .map(mapAsAnalyticsEventWrapper, Encoders.kryo(AnalyticsEventWrapper.class));
        
    // rest of the job (shuffle aggregation and output write)
  }
} {code}
AnalyticsEventWrapperRowMapper.java
{code:java}
package com.analytics.archive;

import org.apache.spark.sql.Row;
import com.analytics.AnalyticsEventWrapper;

public class AnalyticsEventWrapperRowMapper {
    public static AnalyticsEventWrapper map(Row r) {
        AnalyticsEventWrapper analyticsEventWrapper = new AnalyticsEventWrapper();
        analyticsEventWrapper.setId(r.getAs(""id""));
        analyticsEventWrapper.setTimestamp(r.getAs(""timestamp""));
        analyticsEventWrapper.setType(r.getAs(""type""));
        analyticsEventWrapper.setTopic(r.getAs(""topic""));
        return analyticsEventWrapper;
    }
} {code}
AnalyticsEventWrapper.java
{code:java}
package com.analytics;

public class AnalyticsEventWrapper {
    private String id;
    private Long timestamp;
    private String type;
    private String topic;
    
    public String getId() {
        return id;
    }
    
    public void setId(String id) {
        this.id = id;
    }
    
    public Long getTimestamp() {
        return timestamp;
    }
    
    public void setTimestamp(Long timestamp) {
        this.timestamp = timestamp;
    }
    
    public String getType() {
        return type;
    }
    
    public void setType(String type) {
        this.type = type;
    }
    
    public String getTopic() {
        return topic;
    }
    
    public void setTopic(String topic) {
        this.topic = topic;
    }
} {code}
the job is launched on EMR with spark-submit command (all application code is packaged in application.jar with maven shade plugin):
{code:java}
// work
spark-submit --class com.analytics.spark.ProcessAnalyticsEventJob \
  /tmp/application.jar \
  --deploy-mode cluster \
  --verbose

// kryo issue
spark-submit --class com.analytics.spark.ProcessAnalyticsEventJob \
  /tmp/application.jar \
  --deploy-mode cluster \
  --verbose \
  --conf ""spark.shuffle.push.enabled=true""

// yarn-site.xml of Node Managers
<property>
  <name>spark.shuffle.push.server.mergedShuffleFileManagerImpl</name>
  <value>org.apache.spark.network.shuffle.RemoteBlockPushResolver</value>
</property>  {code}",AWS EMR 6.14 (Spark 3.4.1),,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,02:44.0,,,,,,,,,,0|z1oxio:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid previous reader checks in Vectorized DELTA_BYTE_ARRAY parquet decoder,SPARK-48234,13578869,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,yutsareva,yutsareva,5/10/24 12:31,5/10/24 14:57,7/17/24 20:45,,"3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0, 3.4.1, 3.4.2, 3.4.3, 3.5.0, 3.5.1",,Spark Core,,0,pull-request-available,"The vectorized DELTA_BYTE_ARRAY Parquet decoder can cause read failures when reading columns with varying page encodings and if some pages are encoded using DELTA_BYTE_ARRAY.

Same bug existed in parquet-mr reader but was fixed 3 months ago. There is no separate bug fix commit, it was silently fixed along with other changes. https://github.com/apache/parquet-mr/blob/c241170d9bc2cd8415b04e06ecea40ed3d80f64d/parquet-column/src/main/java/org/apache/parquet/column/impl/ColumnReaderBase.java#L732",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,31:28.0,,,,,,,,,,0|z1p5iw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"3.2.1, 3.4.0, 3.4.1, 3.4.2, 3.4.3, 3.5.0, 3.5.1",3.2.2,3.2.3,3.2.4,
ExecutorPodsAllocator doesn't create new executors if no pod snapshot captured pod creation,SPARK-44609,13545545,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,alibiyeslambek,alibiyeslambek,7/31/23 13:01,5/10/24 0:19,7/17/24 20:45,,3.4.1,,"Kubernetes, Scheduler",,0,pull-request-available,"There’s a following race condition in ExecutorPodsAllocator when running a spark application with static allocation on kubernetes with numExecutors >= 1:
 * Driver requests an executor
 * exec-1 gets created and registers with driver
 * exec-1 is moved from {{newlyCreatedExecutors}} to {{schedulerKnownNewlyCreatedExecs}}
 * exec-1 got deleted very quickly (~1-30 sec) after registration
 * {{ExecutorPodsWatchSnapshotSource}} fails to catch the creation of the pod (e.g. websocket connection was reset, k8s-apiserver was down, etc.)
 * {{ExecutorPodsPollingSnapshotSource}} fails to catch the creation because it runs every 30 secs, but executor was removed much quicker after creation
 * exec-1 is never removed from {{schedulerKnownNewlyCreatedExecs}}
 * {{ExecutorPodsAllocator}} will never request new executor because it’s slot is occupied by exec-1, due to {{schedulerKnownNewlyCreatedExecs}} never being cleared.

 

Put up a fix here https://github.com/apache/spark/pull/42297",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,01:56.0,,,,,,,,,,0|z1jhmo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ProducedRowCount to SparkListenerConnectOperationFinished,SPARK-44776,13546987,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lingkai2,lingkai2,lingkai2,8/11/23 14:58,5/7/24 4:24,7/17/24 20:45,8/22/23 1:07,3.4.1,"3.5.0, 4.0.0",Connect,,0,pull-request-available,As title,,,,,,,,,,,,,,,,,,,,,SPARK-48163,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 22 01:07:12 UTC 2023,,,,,,,,,,0|z1jqj4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"22/Aug/23 01:07;gurwls223;Issue resolved by pull request 42454
[https://github.com/apache/spark/pull/42454];;;",,,,,
TableChange.updateColumnComment does not remove comment entry,SPARK-48130,13578168,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,pgrandjean,pgrandjean,5/4/24 21:52,5/4/24 22:01,7/17/24 20:45,,3.4.1,,SQL,,0,,"The function {{TableChange.updateColumnComment}} can be used to add a comment to a column, change it, but not remove the comment entry from the column's metadata.

Moreover, trying to update the column with value {{null}} risks raising a NPE if the function {{TableChange.UpdateColumnComment.equals}} is called.

Examples in Scala:
{code:scala}
TableChange.updateColumnComment(Array(""foo""), ""foo comment"") // adds or changes comment => OK
TableChange.updateColumnComment(Array(""foo""), """") // adds or changes comment to empty string, instead of removing comment from metadata => KO
TableChange.updateColumnComment(Array(""foo""), null) // adds or changes comment to null, instead of removing comment from metadata => KO
TableChange.updateColumnComment(Array(""foo""), ""foo comment"") == TableChange.updateColumnComment(Array(""foo""), null) // raises an NPE => KO
{code}

Solution would be to accept empty string or null as values that remove the {{""comment""}} entry from the metadata, or create a new case {{TableChange.RemoveColumnComment}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,52:16.0,,,,,,,,,,0|z1p17k:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrameWriterV2.overwrite fails with invalid plan,SPARK-47828,13575589,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,4/12/24 3:43,4/19/24 15:24,7/17/24 20:45,4/15/24 4:21,"3.4.0, 3.4.1, 3.4.2, 3.4.3, 3.5.1, 4.0.0","3.4.4, 3.5.2, 4.0.0",Connect,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 15 04:21:26 UTC 2024,,,,,,,,,,0|z1oldc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"15/Apr/24 04:21;podongfeng;Issue resolved by pull request 46023
[https://github.com/apache/spark/pull/46023];;;",3.4.1,3.4.2,3.4.3,3.5.1,
Enable Process Isolation for streaming python worker,SPARK-44461,13543836,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,rangadi,rangadi,7/17/23 19:18,4/18/24 23:53,7/17/24 20:45,1/23/24 10:30,3.4.1,,"Connect, Structured Streaming",,0,pull-request-available,Enable PI for Python worker used for foreachBatch() & streaming listener in Connect.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,SPARK-42938,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 23:50:16 UTC 2023,,,,,,,,,,0|z1j73k:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 03:08;gurwls223;Issue resolved by pull request 42421
[https://github.com/apache/spark/pull/42421];;;, 10/Aug/23 03:55;rangadi;[~gurwls223] could you reopen this? This is about Process isolation for python processes and mistakenly used for another PR. ;;;, 11/Aug/23 03:38;snoot;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/42443;;;, 11/Aug/23 23:50;gurwls223;[~rangadi] can we switch the JIRA by switching the description and title?;;;",,,,,10/Aug/23 03:55;rangadi;[~gurwls223] could you reopen this? This is about Process isolation for python processes and mistakenly used for another PR. ;;;
Connect generated proots can't be pickled,SPARK-47862,13575934,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,grundprinzip-db,grundprinzip-db,grundprinzip-db,4/15/24 21:14,4/16/24 4:10,7/17/24 20:45,4/16/24 4:10,3.4.1,4.0.0,Connect,,0,pull-request-available,"When Spark Connect generates the protobuf files, they're manually adjusted and moved to the right folder. However, we did not fix the package for the descriptor. This breaks serializing them to proto.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 16 04:10:06 UTC 2024,,,,,,,,,,0|z1oni0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"16/Apr/24 04:10;gurwls223;Issue resolved by pull request 46068
[https://github.com/apache/spark/pull/46068];;;",,,,,
[PYTHON] Avoid shadowing python built-ins in python function variable naming,SPARK-47854,13575815,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,liucao,liucao,4/15/24 7:25,4/15/24 7:29,7/17/24 20:45,,"3.3.4, 3.4.1, 3.5.0, 3.5.1",,PySpark,,0,,"Given that spark 4.0.0 is upcoming I wonder if we should at least consider renaming certain function variable naming in python. Otherwise, we may need to wait another 4 years to do so.

Example

[https://github.com/apache/spark/blob/e6b7950f553cff5adc02b8b5195e79cffff3c97c/python/pyspark/sql/functions/builtin.py#L12768]

There are 8 uses of `len` and 35 `str` as variable names, both of which are python built-ins. Shadowing `str` is somewhat dangerous in that the following would be nonsensical:
{code:java}
def foo(str: ""ColumnOrName"", bar: ""ColumnOrName""):
    # str is variable now, cannot be used as type
    bar = if lit(bar) if isinstance(bar, str) else bar
{code}
 

Now obviously this would be breaking change for user code if the function is called with kwargs style. If we rename `str` to `src` or `col`, certain old code using kwargs would break:
{code:java}
# breaks:
foo(str=""x"", bar=""y"")

# okay:
foo(""x"", bar=""y""){code}
Is this change a possibility for 4.0? Or are we thinking that the kwargs breaking change is too big to make compared to the benefit?

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,"pyspark, python, Python",25:26.0,,,,,,,,,,0|z1omrk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,3.5.1,,
"jdbc connect to duckdb with error Unrecognized configuration property ""path""",SPARK-47853,13575808,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ZhaoWeiNan,ZhaoWeiNan,4/15/24 7:03,4/15/24 7:07,7/17/24 20:45,,"3.4.0, 3.4.1, 3.5.0, 3.5.1",,"Spark Core, SQL",,0,,"Link issue: _[https://github.com/duckdb/duckdb/issues/11651_]
 # reproduce python code 

```python

from pyspark.sql import SparkSession

if {_}{{_}}name{{_}}{_} == '{_}{{_}}main{{_}}{_}':
    spark = SparkSession.builder \
        .appName(""Example Application"") \
        .config(""spark.master"", ""local"") \
        .config(""spark.jars.packages"",
                ""io.delta:delta-core_2.12:2.4.0,org.xerial:sqlite-jdbc:3.45.2.0,org.duckdb:duckdb_jdbc:0.9.2"") \
        .getOrCreate()

    spark.sql(
        f""""""
            create table default.movies  
            using jdbc
            options (url ""jdbc:duckdb:database/duckdb.db"" , driver ""org.duckdb.DuckDBDriver"" , dbtable ""duckdb.main.test"");
            """"""
    )

    spark.sql(""select * from default.movies"").show()

    spark.stop()

```

2. error log

```

16:28:57    Runtime Error in model movies (models/sources/movies.sql)
  An error occurred while calling o40.sql.
  : java.sql.SQLException: Invalid Input Error: Unrecognized configuration property ""path""
        at org.duckdb.DuckDBNative.duckdb_jdbc_startup(Native Method)
        at org.duckdb.DuckDBConnection.newConnection(DuckDBConnection.java:48)
        at org.duckdb.DuckDBDriver.connect(DuckDBDriver.java:41)
        at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)
    ```

3.  relative spark code

[https://github.com/apache/spark/blob/e6b7950f553cff5adc02b8b5195e79cffff3c97c/sql/core/src/main/scala/org/apache/spark/sql/execution/command/createDataSourceTables.scala#L64]

Why do we need to replicate the `path` into the JDBC connection?

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,03:23.0,,,,,,,,,,0|z1omq0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,3.5.1,,
pyspark.pandas read_excel implementation at version 3.4.1,SPARK-46143,13559770,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,mpavanetti,mpavanetti,11/28/23 20:19,4/8/24 6:56,7/17/24 20:45,,3.4.1,,Build,,2,,"Hello, 

I would like to report an issue with pyspark.pandas implementation on read_excel function.

Microsoft Fabric spark environment 1.2 (runtime) uses pyspark 3.4.1 which potentially uses an older version of pandas on it's implementations of pyspark.pandas.

The function read_excel from pandas doesn't expect a parameter called ""squeeze"" however it's implemented as part of pyspark.pandas and the parameter ""squeeze"" is being passed to the pandas function.

 

!image-2023-11-28-13-20-40-275.png!

 

I've been digging into it for further investigation into pyspark 3.4.1 documentation

[https://spark.apache.org/docs/3.4.1/api/python/_modules/pyspark/pandas/namespace.html#read_excel|https://mcas-proxyweb.mcas.ms/certificate-checker?login=false&originalUrl=https%3A%2F%2Fspark.apache.org.mcas.ms%2Fdocs%2F3.4.1%2Fapi%2Fpython%2F_modules%2Fpyspark%2Fpandas%2Fnamespace.html%3FMcasTsid%3D20893%23read_excel&McasCSRF=92c0f0a0811f59386edd92fd5f3fcb0ac451ce363b3f2e01ed076f45e2b20500]

 

This is the point I found that ""squeeze"" parameter is being passed to pandas read_excel function which is not expected.

It seems like it was deprecated as part of pyspark 3.4.0 but still being used in the implementation.

 

!image-2023-11-28-13-20-51-291.png!

 

I believe this is an issue with pyspark implementation 3.4.1 not necessaily with fabric. However fabric uses this version as its 1.2 build.

 

I am able to work around that for now by download the excel from the one lake to the spark driver, loading that to the memory with pandas and then converting to a spark dataframe etc or I made it work downgrading the build

I downloaded the pyspark build 20230713 to my local, made the changes and re-compiled it and it worked locally. So it means that is related to the implementation and they would have to fix or I do a downgrade to older version like 3.3.3 or try the latest 3.5.0 which is not the case for fabric

 

 ","pyspark 3.4.1.5.3 build 20230713.

Running on Microsoft Fabric workspace at runtime 1.2.

Tested the same scenario on a spark 3.4.1 standalone deployment on docker documented at https://github.com/mpavanetti/sparkenv

 

 ",,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/23 20:20;mpavanetti;MicrosoftTeams-image.png;https://issues.apache.org/jira/secure/attachment/13064781/MicrosoftTeams-image.png, 28/Nov/23 20:20;mpavanetti;image-2023-11-28-13-20-40-275.png;https://issues.apache.org/jira/secure/attachment/13064782/image-2023-11-28-13-20-40-275.png, 28/Nov/23 20:20;mpavanetti;image-2023-11-28-13-20-51-291.png;https://issues.apache.org/jira/secure/attachment/13064783/image-2023-11-28-13-20-51-291.png",,3,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Mon Apr 08 06:56:06 UTC 2024,,,,,,,,,,0|z1lwo0:,9223372036854775807,,,,,,,,,,,,,3.3.3,,,,,,,,"29/Nov/23 15:59;mpavanetti;Tested the same scenario on a spark 3.4.1 standalone deployment on docker documented at [https://github.com/mpavanetti/sparkenv]

Got the same error.;;;, 05/Apr/24 13:38;ckarras-ext-exo;I have the same issue too. The problem is because the squeeze parameter of read_excel has been deprecated since pandas version 1.4, and has been completely removed in pandas 2.0. But Pyspark's implementation of read_excel keeps passing the squeeze parameter, even though the parameter is also deprecated since pyspark 3.4.

Since there's no version constraint in Pyspark that indicates Pandas 2.0 is not supported, a fix to avoid the need to stay with pandas 1.x could be to detect the pandas version and decide if the squeeze parameter should be passed depending on the pandas version. Also, if the squeeze parameter is passed in the Pyspark function, raise an error if a version of pandas that no longer supports this parameter is installed. This would be a transition solution until the squeeze parameter is also removed completely from Pyspark.

 

Modify pyspark\pandas\namespace.py:
 * Change the squeeze parameter of the read_excel function to be ""squeeze: Optional[bool] = None""
 * Modify the nested pd_read_excel function to check for the pandas version and build a dict of arguments it will pass to pd.read_excel based on that version. Also consider if the squeeze parameter was passed by the caller or not. And if the caller specified that argument but a newer version of pandas that doesn't support it, raise an exception

 

Sample code that could implement this solution:
def pd_read_excel(
    io_or_bin: Any,
    sn: Union[str, int, List[Union[str, int]], None], sq: bool   
) -> pd.DataFrame:

    read_excel_args: dict =

{         ""io"":BytesIO(io_or_bin) if isinstance(io_or_bin, (bytes, bytearray)) else io_or_bin,         ""sheet_name"":sn,         ""header"":header,         # TODO other args...,         **kwds     }

    if squeeze is not None:
        pandas_version_major = int(pd.__version__.split(""."")[0])
        if pandas_version_major >= 2:
            raise Exception(""The squeeze parameter for read_excel is no longer available in pandas 2.x"")
       
        read_excel_args[""squeeze""] = squeeze
    return pd.read_excel(**read_excel_args)
 ;;;, 08/Apr/24 06:56;comet;voted for this issue.;;;",,,,,"05/Apr/24 13:38;ckarras-ext-exo;I have the same issue too. The problem is because the squeeze parameter of read_excel has been deprecated since pandas version 1.4, and has been completely removed in pandas 2.0. But Pyspark's implementation of read_excel keeps passing the squeeze parameter, even though the parameter is also deprecated since pyspark 3.4.

Since there's no version constraint in Pyspark that indicates Pandas 2.0 is not supported, a fix to avoid the need to stay with pandas 1.x could be to detect the pandas version and decide if the squeeze parameter should be passed depending on the pandas version. Also, if the squeeze parameter is passed in the Pyspark function, raise an error if a version of pandas that no longer supports this parameter is installed. This would be a transition solution until the squeeze parameter is also removed completely from Pyspark.

 

Modify pyspark\pandas\namespace.py:
 * Change the squeeze parameter of the read_excel function to be ""squeeze: Optional[bool] = None""
 * Modify the nested pd_read_excel function to check for the pandas version and build a dict of arguments it will pass to pd.read_excel based on that version. Also consider if the squeeze parameter was passed by the caller or not. And if the caller specified that argument but a newer version of pandas that doesn't support it, raise an exception

 

Sample code that could implement this solution:
def pd_read_excel(
    io_or_bin: Any,
    sn: Union[str, int, List[Union[str, int]], None], sq: bool   
) -> pd.DataFrame:

    read_excel_args: dict =

{         ""io"":BytesIO(io_or_bin) if isinstance(io_or_bin, (bytes, bytearray)) else io_or_bin,         ""sheet_name"":sn,         ""header"":header,         # TODO other args...,         **kwds     }

    if squeeze is not None:
        pandas_version_major = int(pd.__version__.split(""."")[0])
        if pandas_version_major >= 2:
            raise Exception(""The squeeze parameter for read_excel is no longer available in pandas 2.x"")
       
        read_excel_args[""squeeze""] = squeeze
    return pd.read_excel(**read_excel_args)
 ;;;"
Support Hive tables as a streaming source and sink,SPARK-47717,13574490,,New Feature,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,asuresh9,asuresh9,4/3/24 15:08,4/3/24 15:08,7/17/24 20:45,,"3.3.2, 3.4.1, 3.5.1","3.3.2, 3.4.1, 3.5.1",SQL,,0,,"People have data stored in Hive tables. Currently these tables do not support Spark streaming, so customers do not have a good way to natively stream this data in Spark. The current solutions involve an intermediary to track which data has been read and periodically execute batch jobs. This use case should be supported by Spark's in-built streaming mechanism.

 

From doing some research, Hive supports streaming [https://cwiki.apache.org/confluence/display/Hive/Streaming+Data+Ingest+V2] but Spark does not support streaming on tables in Hive format. I don't think it makes sense to start copying Hive server-side code into Spark, but we could copy the relevant logic and wrap it in the DataSourceV2 APIs to enable this feature. To not break backwards compatibility, we would probably want to gate this behind a new Spark property.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,08:21.0,,,,,,,,,,0|z1oezk:,9223372036854775807,,,,,asuresh9,,,,,,,,"3.3.2, 3.4.1, 3.5.1",,,,,,,,,3.4.1,3.5.1,,,
Fix NPE when reading mysql bit array as LongType,SPARK-47666,13574118,13571633,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,4/1/24 8:21,4/2/24 12:49,7/17/24 20:45,4/1/24 10:28,"3.4.1, 3.5.1, 4.0.0","3.4.3, 3.5.2, 4.0.0",SQL,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 01 10:29:00 UTC 2024,,,,,,,,,,0|z1ocp4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,01/Apr/24 10:29;yao;resolved by https://github.com/apache/spark/pull/45790;;;,3.5.1,4.0.0,,,
Enforce Window partitionSpec is orderable.,SPARK-47572,13573383,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mashplant,mashplant,mashplant,3/26/24 18:41,3/29/24 8:19,7/17/24 20:45,3/29/24 8:19,"3.3.4, 3.4.1, 3.5.1",4.0.0,SQL,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 29 08:19:26 UTC 2024,,,,,,,,,,0|z1o86o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"29/Mar/24 08:19;cloud_fan;Issue resolved by pull request 45730
[https://github.com/apache/spark/pull/45730];;;",3.4.1,3.5.1,,,
SPJ: Handle empty input partitions after dynamic filtering,SPARK-45652,13555374,13412655,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,10/24/23 16:14,3/28/24 13:22,7/17/24 20:45,10/26/23 15:52,3.4.1,"3.4.2, 3.5.1, 4.0.0",SQL,,0,pull-request-available,"When the number of input partitions become 0 after dynamic filtering, in {{BatchScanExec}}, currently SPJ will fail with error:
{code}
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:529)
	at scala.None$.get(Option.scala:527)
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.filteredPartitions$lzycompute(BatchScanExec.scala:108)
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.filteredPartitions(BatchScanExec.scala:65)
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.inputRDD$lzycompute(BatchScanExec.scala:136)
	at org.apache.spark.sql.execution.datasources.v2.BatchScanExec.inputRDD(BatchScanExec.scala:135)
{code}

This is because {{groupPartitions}} will return {{None}} for this case.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 26 15:52:31 UTC 2023,,,,,,,,,,0|z1l5w0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Oct/23 15:52;csun;Issue resolved by pull request 43531
[https://github.com/apache/spark/pull/43531];;;",,,,,
Aggregate in not causes ,SPARK-47287,13570844,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tedjenks,tedjenks,3/5/24 13:58,3/28/24 5:21,7/17/24 20:45,,3.4.1,,SQL,,0,," 

The below snippet is confirmed working with Spark 3.2.1 and broken Spark 3.4.1. i believe this is a bug. 
{code:java}
       Dataset<Row> ds = dummyDataset
                .withColumn(""flag"", functions.not(functions.coalesce(functions.col(""bool1""), functions.lit(false)).equalTo(true)))
                .groupBy(""code"")
                .agg(functions.max(functions.col(""flag"")).alias(""flag""));
        ds.show(); {code}
It fails with:
{code:java}
Caused by: java.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:208)
	at org.apache.spark.sql.catalyst.util.V2ExpressionBuilder.$anonfun$generateExpression$7(V2ExpressionBuilder.scala:185)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.catalyst.util.V2ExpressionBuilder.generateExpression(V2ExpressionBuilder.scala:184)
	at org.apache.spark.sql.catalyst.util.V2ExpressionBuilder.build(V2ExpressionBuilder.scala:33)
	at org.apache.spark.sql.execution.datasources.PushableExpression$.unapply(DataSourceStrategy.scala:803)
	at org.apache.spark.sql.catalyst.util.V2ExpressionBuilder.generateAggregateFunc(V2ExpressionBuilder.scala:293)
	at org.apache.spark.sql.catalyst.util.V2ExpressionBuilder.generateExpression(V2ExpressionBuilder.scala:98)
	at org.apache.spark.sql.catalyst.util.V2ExpressionBuilder.build(V2ExpressionBuilder.scala:33)
	at org.apache.spark.sql.execution.datasources.PushableExpression$.unapply(DataSourceStrategy.scala:803)
	at org.apache.spark.sql.execution.datasources.DataSourceStrategy$.translate$1(DataSourceStrategy.scala:700){code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 28 05:21:19 UTC 2024,,,,,,,,,,0|z1nsi0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"28/Mar/24 05:21;juefeiyan;I tried the code on 3.4 branch, cannot reproduce this problem;;;",,,,,
Aggregate + First() Function - ArrayIndexOutOfBoundsException - ColumnPruning?,SPARK-47615,13573499,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,schreiber,schreiber,3/27/24 9:36,3/27/24 9:36,7/17/24 20:45,,"3.4.1, 3.5.0",,Optimizer,,0,,"Currently i`m investigating in upgrade our code base from spark 3.3.0 to 3.5.0 (embedded in dedicated aws emr cluster).
 
I got the following exception if i execute my code on the cluster, if i run local unit tests the code runs as expected without exception.
 
 
{code:java}
24/03/26 19:32:19 INFO RecordServerQueryListener: Cleaning up temp directory - /user/KKQI7VHKTMNQZJQNMMZXKH5KYNRPOHXG/application_1711468652551_0023 Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 4 times, most recent failure: Lost task 0.3 in stage 12.0 (TID 186) (ip-10-1-1-6.eu-central-1.compute.internal executor 2): java.lang.ArrayIndexOutOfBoundsException: Index -1 out of bounds for length 3 at org.apache.spark.sql.vectorized.ColumnarBatch.column(ColumnarBatch.java:95) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnaraggregatetorow_parquetMax_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnaraggregatetorow_nextBatch_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source) at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142) at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45) at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:68) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161) at org.apache.spark.scheduler.Task.run(Task.scala:143) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:629) at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:95) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:632) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:840)   Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3067) at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3003) at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3002) at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3002) at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1318) at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1318) at scala.Option.foreach(Option.scala:407) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1318) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3271) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3205) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3194) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66) at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:277) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:276) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:558) at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:520) at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4411) at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3370) at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4401) at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:625) at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4399) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:255) at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:129) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:165) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:108) at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:255) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:165) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:276) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:164) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:70) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4399) at org.apache.spark.sql.Dataset.head(Dataset.scala:3370) at org.apache.spark.sql.Dataset.head(Dataset.scala:3377) at org.apache.spark.sql.Dataset.first(Dataset.scala:3384) at de.my.maintained.code.business.transformer.MyMaintainedClass$BusinessForecastNonRecurringTransformer.determineMaxDateId(MyMaintainedClass.scala:56) at de.my.maintained.code.business.transformer.MyMaintainedClass$BusinessForecastNonRecurringTransformer.processInternal(MyMaintainedClass.scala:25) at de.my.maintained.code.business.transformer.MyMaintainedClass$BusinessForecastNonRecurringTransformer.processInternal$(MyMaintainedClass.scala:22) at de.my.maintained.code.common.app.FqmApp$$anon$35.processInternal(FqmApp.scala:112) at de.my.maintained.code.common.transformer.Transformer.process(Transformer.scala:26) at de.my.maintained.code.common.transformer.Transformer.process$(Transformer.scala:24) at de.my.maintained.code.common.app.FqmApp$$anon$35.process(FqmApp.scala:112) at de.my.maintained.code.business.transformer.BusinessForecastTransformerComponent$BusinessForecastTransformer.processInternal(BusinessForecastTransformerComponent.scala:35) at de.my.maintained.code.business.transformer.BusinessForecastTransformerComponent$BusinessForecastTransformer.processInternal$(BusinessForecastTransformerComponent.scala:26) at de.my.maintained.code.common.app.FqmApp$$anon$33.processInternal(FqmApp.scala:110) at de.my.maintained.code.common.transformer.Transformer.process(Transformer.scala:26) at de.my.maintained.code.common.transformer.Transformer.process$(Transformer.scala:24) at de.my.maintained.code.common.app.FqmApp$$anon$33.process(FqmApp.scala:110) at de.my.maintained.code.business.aws.BusinessStage2Forecast$.main(BusinessStage2Forecast.scala:10) at de.my.maintained.code.business.aws.BusinessStage2Forecast.main(BusinessStage2Forecast.scala) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:568) at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1075) at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194) at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217) at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91) at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1167) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1176) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) Caused by: java.lang.ArrayIndexOutOfBoundsException: Index -1 out of bounds for length 3 at org.apache.spark.sql.vectorized.ColumnarBatch.column(ColumnarBatch.java:95) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnaraggregatetorow_parquetMax_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnaraggregatetorow_nextBatch_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source) at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43) at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142) at org.apache.spark.shuffle.ShuffleWriteProcessor.doWrite(ShuffleWriteProcessor.scala:45) at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:68) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54) at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161) at org.apache.spark.scheduler.Task.run(Task.scala:143) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:629) at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:95) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:632) at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) at java.base/java.lang.Thread.run(Thread.java:840){code}
 
 
A little earlier in logfile i found the following:
 
24/03/26 19:32:18 INFO DAGScheduler: Submitting 16 missing tasks from ShuffleMapStage 12 (MapPartitionsRDD[28] at first at MyMaintainedClass.scala:56) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
 
 
if i jump to the code i found this:
 
 
{code:java}
52: val first_row =
53:  df_usage_current_and_next_month
54:   .filter(args.billingDate.filter_current_month($""year"", $""month""))
55:   .withColumn(""DATE_ID"", $""year"" * 10000 + $""month"" * 100 + date_format(dateColumn, ""dd"").cast(IntegerType))
56:   .agg(max(""DATE_ID"")).first()  {code}
 
The Problem seems to be occur in the last row "".agg(max(""DATE_ID"")).first()""
 
So next i have identified all placed with this exception. All of them use aggregation (min/max/count) with the call of first() after that.
 
After that i searched all code placed with the first() or head() function in our code base and i found one example without occuring an ArrayIndexOutOfBoundException. The StackOverFlow (link at the bottom), post gave me an hint. Every time we do an spark.read (parquet) with an aggregation and calling first function AND using the same DataFrame after that for other calculations & filterings & writing we got an ArrayIndexOutOfBounds Exception. If we still only do agg and first there is no problem.
 
So there must be something mutable while reading the DataSource and split the executing plan into two ways. (don`t know what there happen exactly). In my opinion, an optimization mechanism intervenes there, which removes certain columns that are supposedly not needed but needed. Unfortunately i`m not able to reproduce it locally, only in AWS EMR Cluster. (maybe there is a different Configuration)
 
Workaround not getting the ArrayIndexOutOfBoundException
 
1. using an explicit spark.read for every agg an first function
OR
2. using an persist() between agg and first function
 
 
#similar problem mentioned on stackoverflow
https://stackoverflow.com/questions/53483406/spark-sql-dataframe-count-gives-java-lang-arrayindexoutofboundsexception
 
 
Versions Tested:
 
Spark
3.3.0 (no problem) (emr 6.11.1)
3.4.1 (ArrayIndexOutOfBoundsException) (emr 6.15.1)
3.5.0 (ArrayIndexOutOfBoundsException) (emr 7.0.0)
 ","Amazon EMR version
emr-7.0.0
Installed applications
Tez 0.10.2, Spark 3.5.0
Amazon Linux release
2023.3.20240312.0
 
1 Master Node m6g.xlarge
2 Core Nodes m6g.2xlarge
 
 ",,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,36:08.0,,,,,,,,,,0|z1o8wg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.5.0,,,,
Issue with Spark Connect on Python 3.12,SPARK-47613,13573479,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,KaiRoesner,KaiRoesner,3/27/24 7:19,3/27/24 7:19,7/17/24 20:45,,"3.4.1, 3.5.0",,PySpark,,0,,When trying to create a remote Spark session with PySpark on Python 3.12 a {{ModuleNotFoundError: No module named 'distutils'}} excpetion is thrown. In Python 3.12 {{distutils}} was removed from the stdlib. As a workaround we can {{import setuptools}} before creating the session. See also [this question on SOF|https://stackoverflow.com/questions/78207291] and the [answer|https://stackoverflow.com/a/78212125/11474852] by Anderson Bravalheri.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,19:20.0,,,,,,,,,,0|z1o8s0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.5.0,,,,
Use MySQL Connector/J for MySQL DB instead of MariaDB Connector/J ,SPARK-47537,13573146,13571633,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,3/25/24 3:37,3/25/24 15:52,7/17/24 20:45,3/25/24 6:05,"3.4.1, 3.5.1, 4.0.0","3.4.3, 3.5.2, 4.0.0",SQL,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 25 06:05:00 UTC 2024,,,,,,,,,,0|z1o6q0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"25/Mar/24 06:05;dongjoon;Issue resolved by pull request 45689
[https://github.com/apache/spark/pull/45689];;;",3.5.1,4.0.0,,,
Use `Utils.tryWithResource` during reading shuffle data from external storage,SPARK-47521,13572928,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maheshk114,maheshk114,maheshk114,3/22/24 9:23,3/23/24 5:53,7/17/24 20:45,3/22/24 17:45,"3.3.4, 3.4.1, 3.5.0","3.4.3, 3.5.2, 4.0.0",Spark Core,,0,pull-request-available,"In method FallbackStorage.read method, the file handle is not closed if there is a failure during read operation.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 22 17:52:24 UTC 2024,,,,,,,,,,0|z1o5dk:,9223372036854775807,,,,,dongjoon,,,,,,,,,,,,,,,,"22/Mar/24 17:45;dongjoon;Issue resolved by pull request 45663
[https://github.com/apache/spark/pull/45663];;;, 22/Mar/24 17:52;dongjoon;FYI, [~maheshk114], `Target Version` field is reserved for the Apache Spark committers. So, please don't set it with your aspiration.
- https://spark.apache.org/contributing.html

{quote}
Do not set the following fields:
Fix Version. This is assigned by committers only when resolved.
Target Version. This is assigned by committers to indicate a PR has been accepted for possible fix by the target version.
{quote};;;",3.4.1,3.5.0,,,"22/Mar/24 17:52;dongjoon;FYI, [~maheshk114], `Target Version` field is reserved for the Apache Spark committers. So, please don't set it with your aspiration.
- https://spark.apache.org/contributing.html

{quote}
Do not set the following fields:
Fix Version. This is assigned by committers only when resolved.
Target Version. This is assigned by committers to indicate a PR has been accepted for possible fix by the target version.
{quote};;;"
Task fraction resource request is not expected,SPARK-45527,13553942,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wbo4958,Ngone51,Ngone51,10/13/23 1:56,3/19/24 13:50,7/17/24 20:45,1/4/24 15:21,"3.2.1, 3.3.3, 3.4.1, 3.5.0",4.0.0,Spark Core,,0,pull-request-available," 
{code:java}
test(""SPARK-XXX"") {
  import org.apache.spark.resource.{ResourceProfileBuilder, TaskResourceRequests}

  withTempDir { dir =>
    val scriptPath = createTempScriptWithExpectedOutput(dir, ""gpuDiscoveryScript"",
      """"""{""name"": ""gpu"",""addresses"":[""0""]}"""""")

    val conf = new SparkConf()
      .setAppName(""test"")
      .setMaster(""local-cluster[1, 12, 1024]"")
      .set(""spark.executor.cores"", ""12"")
    conf.set(TASK_GPU_ID.amountConf, ""0.08"")
    conf.set(WORKER_GPU_ID.amountConf, ""1"")
    conf.set(WORKER_GPU_ID.discoveryScriptConf, scriptPath)
    conf.set(EXECUTOR_GPU_ID.amountConf, ""1"")
    sc = new SparkContext(conf)
    val rdd = sc.range(0, 100, 1, 4)
    var rdd1 = rdd.repartition(3)
    val treqs = new TaskResourceRequests().cpus(1).resource(""gpu"", 1.0)
    val rp = new ResourceProfileBuilder().require(treqs).build
    rdd1 = rdd1.withResources(rp)
    assert(rdd1.collect().size === 100)
  }
} {code}
In the above test, the 3 tasks generated by rdd1 are expected to be executed in sequence as we expect ""new TaskResourceRequests().cpus(1).resource(""gpu"", 1.0)"" should override ""conf.set(TASK_GPU_ID.amountConf, ""0.08"")"". However, those 3 tasks are run in parallel in fact.

The root cause is that ExecutorData#ExecutorResourceInfo#numParts is static. In this case, the ""gpu.numParts"" is initialized with 12 (1/0.08) and won't change even if there's a new task resource request (e.g., resource(""gpu"", 1.0) in this case). Thus, those 3 tasks are able to be executed in parallel.
 ",,,,,,,,,,,,,,,,,,,,,,SPARK-47458,,SPARK-39853,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 27 14:55:15 UTC 2024,,,,,,,,,,0|z1kx1s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"13/Oct/23 01:58;Ngone51;cc [~wbo4958]   [~tgraves] ;;;, 13/Oct/23 15:30;tgraves;thanks for filing and digging into this. I assume this is only with the TaskResourceRequests and using the default ExecutorResourceRequests.  seems a bug since that functionality was added.  Either way when we fix should add tests similar if we can.;;;, 16/Oct/23 23:16;wbo4958;Working on a PR to fix it.;;;, 27/Feb/24 14:55;tgraves;Note that this is related to SPARK-39853 which was supposed to implement stage level scheduling with dynamic allocation disabled.  That pr did not properly handle resources (gpu, fpga, etc);;;",3.3.3,3.4.1,3.5.0,,13/Oct/23 15:30;tgraves;thanks for filing and digging into this. I assume this is only with the TaskResourceRequests and using the default ExecutorResourceRequests.  seems a bug since that functionality was added.  Either way when we fix should add tests similar if we can.;;;
Preserve full principal user name on executor side,SPARK-44976,13548705,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,eub,eub,8/26/23 2:55,3/19/24 0:18,7/17/24 20:45,,"3.2.3, 3.3.3, 3.4.1",,Spark Core,,0,pull-request-available,"SPARK-6558 changes the behavior of {{Utils.getCurrentUserName()}} to use shortname instead of full principal name.
Due to this, it doesn't respect {{hadoop.security.auth_to_local}} rule on the side of non-kerberized hdfs namenode.
For example, I use 2 hdfs cluster. One is kerberized, the other one is not kerberized.
I make a rule to add some prefix to username on the non-kerberized cluster if some one access it from the kerberized cluster.


{code}
  <property>
    <name>hadoop.security.auth_to_local</name>
    <value xml:space=""preserve"">
RULE:[1:$1@$0](.*@EXAMPLE.COM)s/(.+)@.*/_ex_$1/
RULE:[2:$1@$0](.*@EXAMPLE.COM)s/(.+)@.*/_ex_$1/
DEFAULT</value>
  </property>
{code}

However, if I submit spark job with keytab & principal option, hdfs directory and files ownership is not coherent.

(I change some words for privacy.)

{code}
$ hdfs dfs -ls hdfs:///user/eub/some/path/20230510/23
Found 52 items
-rw-rw-rw-   3 _ex_eub hdfs          0 2023-05-11 00:16 hdfs:///user/eub/some/path/20230510/23/_SUCCESS
-rw-r--r--   3 eub      hdfs  134418857 2023-05-11 00:15 hdfs:///user/eub/some/path/20230510/23/part-00000-b781be38-9dbc-41da-8d0e-597a7f343649-c000.txt.gz
-rw-r--r--   3 eub      hdfs  153410049 2023-05-11 00:16 hdfs:///user/eub/some/path/20230510/23/part-00001-b781be38-9dbc-41da-8d0e-597a7f343649-c000.txt.gz
-rw-r--r--   3 eub      hdfs  157260989 2023-05-11 00:16 hdfs:///user/eub/some/path/20230510/23/part-00002-b781be38-9dbc-41da-8d0e-597a7f343649-c000.txt.gz
-rw-r--r--   3 eub      hdfs  156222760 2023-05-11 00:16 hdfs:///user/eub/some/path/20230510/23/part-00003-b781be38-9dbc-41da-8d0e-597a7f343649-c000.txt.gz
{code}

Another interesting point is that if I submit spark job without keytab and principal option but with kerberos authentication with {{kinit}}, it will not follow {{hadoop.security.auth_to_local}} rule completely.

{code}
$ hdfs dfs -ls  hdfs:///user/eub/output/
Found 3 items
-rw-rw-r--+  3 eub hdfs          0 2023-08-25 12:31 hdfs:///user/eub/output/_SUCCESS
-rw-rw-r--+  3 eub hdfs        512 2023-08-25 12:31 hdfs:///user/eub/output/part-00000.gz
-rw-rw-r--+  3 eub hdfs        574 2023-08-25 12:31 hdfs:///user/eub/output/part-00001.gz
{code}


I finally found that if I submit spark job with {{--principal}} and {{--keytab}} option, ugi will be different.
(refer to https://github.com/apache/spark/blob/2583bd2c16a335747895c0843f438d0966f47ecd/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L905).

Only file ({{_SUCCESS}}) and output directory created by driver (application master side) will respect {{hadoop.security.auth_to_local}} on the non-kerberized namenode only if {{--principal}} and {{--keytab}] options are provided.

No matter how hdfs files or directory are created by executor or driver, those should respect {{hadoop.security.auth_to_local}} rule and should be the same.


Workaround is to pass additional argument to change {{SPARK_USER}} on the executor side.
e.g. {{--conf spark.executorEnv.SPARK_USER=_ex_eub}}

{{--conf spark.yarn.appMasterEnv.SPARK_USER=_ex_eub}} will make an error. There are some logics to append environment value with {{:}} (colon) as a separator.

- https://github.com/apache/spark/blob/4748d858b4478ea7503b792050d4735eae83b3cd/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L893
- https://github.com/apache/spark/blob/4748d858b4478ea7503b792050d4735eae83b3cd/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnSparkHadoopUtil.scala#L52
",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 26 14:25:40 UTC 2023,,,,,,,,,,0|z1k0rk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Aug/23 04:54;eub;[https://github.com/apache/spark/pull/44244];;;, 26/Aug/23 14:25;eub;I think it is also related to https://issues.apache.org/jira/browse/SPARK-31551.;;;",3.3.3,3.4.1,,,26/Aug/23 14:25;eub;I think it is also related to https://issues.apache.org/jira/browse/SPARK-31551.;;;
Bloom filter is not added for left outer join if the left side table is smaller than broadcast threshold.,SPARK-44307,13542476,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,maheshk114,maheshk114,7/5/23 5:11,3/17/24 0:20,7/17/24 20:45,,3.4.1,,Optimizer,,0,pull-request-available,"In case of left outer join, even if the left side table is small enough to be broadcasted, shuffle join is used. This is because of the property of the left outer join. If the left side is broadcasted in left outer join, the result generated will be wrong. But this is not taken care of in bloom filter. While injecting the bloom filter, if lest side is smaller than broadcast threshold, bloom filter is not added. It assumes that the left side will be broadcast and there is no need for a bloom filter. This causes bloom filter optimization to be missed in case of left outer join with small left side and huge right-side table.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 11:50:31 UTC 2023,,,,,,,,,,0|z1iyqw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"06/Jul/23 11:50;awsthni;User 'maheshk114' has created a pull request for this issue:
https://github.com/apache/spark/pull/41860;;;",,,,,
spark-sql does not recognize expressions in repartition hint,SPARK-47425,13572101,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Resolved,,mrbrahman,mrbrahman,3/15/24 21:02,3/16/24 1:36,7/17/24 20:45,3/16/24 1:36,3.4.1,,SQL,,0,,"In Scala, it is possible to do this, to create a bucketed table to not have many small files.

 
{code:scala}
df.repartition(expr(""pmod(hash(user_id), 200)""))
  .write
  .mode(SaveMode.Overwrite)
  .bucketBy(200, ""user_id"")
  .option(""path"", output_path)
  .saveAsTable(""bucketed_table"")
{code}
Found [this small trick|https://towardsdatascience.com/best-practices-for-bucketing-in-spark-sql-ea9f23f7dd53] to have the same # files as buckets.

However, the equivalent does not work in spark-sql (using repartition hint)
{code:sql}
create table bucketed_table stored as parquet
clustered by (user_id) into 200 buckets
select /*+ repartition (pmod(hash(user_id),200)) */ * from df_table
{code}
{{REPARTITION Hint parameter should include columns, but 'pmod('hash('user_id), 200) found.}}

When I instead make a virtual column and use that, Spark is not respecting the repartition anymore
{code:sql}
create table bucketed_table stored as parquet
clustered by (user_id) into 200 buckets
select /*+ repartition (bkt) */
*, pmod(hash(user_id),200) as bkt
from df_table
{code}
{code:bash}
$ hdfs dfs -ls -h /user/spark/warehouse/bucket_test.db/bucketed_table| head
Found 101601 items
...
{code}

The same behavior is seen even with DISTRIBUTE BY clause

{code:sql}
create table bucketed_table stored as parquet
clustered by (user_id) into 200 buckets
select *
from df_table
distribute by pmod(hash(user_id),200)
{code}

Can the behavior of repartition hint be changed to work like the Scala/Python equivalent?

Thank you",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 16 01:36:56 UTC 2024,,,,,,,,,,0|z1o09s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"16/Mar/24 01:36;mrbrahman;Nevermind, the problem was in storage specification. I was using the Hive storage clause, while this needed the Spark storage clause.

The below works!
{code:sql}
create table bucketed_table USING parquet
clustered by (user_id) into 200 buckets
select *
from df_table
distribute by pmod(hash(user_id),200) 
{code}

Simple and clean solution without the need for an extra column for the repartition hint.;;;",,,,,
Spark 3.3.3 tuple encoders built using Encoders.tuple do not correctly cast null into None for Option values,SPARK-46251,13560552,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,willbo,willbo,12/4/23 22:28,3/14/24 5:40,7/17/24 20:45,,"3.3.3, 3.4.0, 3.4.1, 3.4.2, 3.5.0",,SQL,,0,,"In Spark {{3.3.2}} encoders created using {{Encoders.tuple(encoder1, encoder2, ..)}} correctly handle casting {{null}} into {{None}} when the target type is an Option. 

In Spark {{{}3.3.3{}}}, this behaviour has changed and the Option value comes through as {{null}} which is likely to cause a {{NullPointerException}} for most Scala code that operates on the Option. The change seems to be related to the following commit:

[https://github.com/apache/spark/commit/9110c05d54c392e55693eba4509be37c571d610a]

I have made a reproduction with a couple of examples in a public Github repo here:

[https://github.com/q-willboulter/spark-tuple-encoders-bug] 

The common use case where this is likely to be encountered is while doing any joins that can return null, e.g. left or outer joins. When casting the result of a left join it is sensible to wrap the right-hand side in an Option to handle the case where there is no match. Since 3.3.3 this would fail if the encoder is derived manually using {{Encoders.tuple(leftEncoder, rightEncoder).}}

If the entire tuple encoder {{Encoder[(Left, Option[Right]])}} is derived at once using reflection, the encoder works as expected. The bug appears to be in the following function inside {{ExpressionEncoder.scala}}
{code:java}
def tuple(encoders: Seq[ExpressionEncoder[_]]): ExpressionEncoder[_] = ...{code}
 ",,,,,,,,,,,,,,,,,,SPARK-47385,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 14 05:34:27 UTC 2024,,,,,,,,,,0|z1m1hs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"14/Mar/24 05:34;joshrosen;FYI, this looks like it was duplicated by https://issues.apache.org/jira/browse/SPARK-47385 which now has a PR open to fix it.;;;",3.4.0,3.4.1,3.4.2,3.5.0,
Fix performance regression in JDK 17 caused from RocksDB logging,SPARK-47369,13571674,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,neilramaswamy,neilramaswamy,3/12/24 21:04,3/13/24 17:48,7/17/24 20:45,,"3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0, 3.4.1, 3.4.2, 3.5.0, 3.5.1",,Structured Streaming,,0,,"JDK 17 has a performance regression in the JNI's AttachCurrentThread and DetachCurrentThread calls, as reported here: [https://bugs.openjdk.org/browse/JDK-8314859]. You can find a minimal reproduction of the JDK issue in that bug report. I have marked as affected versions 3.3.0^ since that is when JDK 17 started being offered in Spark.

For context, every time RocksDB logs, it currently [attaches itself to the JVM|https://github.com/facebook/rocksdb/blob/main/java/rocksjni/loggerjnicallback.cc#L140], invokes the RocksDB [logging callback that we specify|https://github.com/apache/spark/blob/8fcef1657a02189f91d5485eabb5b165706cdce9/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala#L839], and then [detaches itself from the JVM|https://github.com/facebook/rocksdb/blob/main/java/rocksjni/loggerjnicallback.cc#L170]. These attach/detach calls regressed, causing JDK 17 SS queries to run up to 10-15% slower than their respective JDK 8 queries.

For example, a 100K record/second dropDuplicates had a p95 latency regression of 12%. A regression of 12% and 21% (at the p95) was observed for a query with 1M record/second, 100K keys, 10 second windows, and 0 second watermark.

Because the Hotspot folks marked this as ""Won't fix,"" one way to fix this is to avoid the JNI entirely and write the RocksDB to stderr. RocksDB [8.11.3 natively supports this|https://github.com/facebook/rocksdb/wiki/Logging-in-RocksJava#configuring-a-native-logger] (I implemented that feature in RocksJava). We can configure our RocksDB logger to do its logging this way.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 13 17:48:32 UTC 2024,,,,,,,,,,0|z1nxm8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Mar/24 21:05;neilramaswamy;cc: [~dongjoon] and [~LuciferYang], who were involved on the JDK 17 upgrade to 8.11.3. Maybe you would be interested in discussing this issue and the proposed fix.;;;, 12/Mar/24 22:00;dongjoon;Thank you for reporting, [~neilramaswamy]  Could you provide a reproducible Spark example for the further discussion?;;;, 13/Mar/24 07:40;LuciferYang;From the current Spark code, it appears that the {{Logger}} is only set for the {{RocksDB}} instance built for external shuffle db(inRocksDBProvider), and not for other parts. However, it seems that the Spark code does not actively print RocksDB-related logs (perhaps my confirmation method is incorrect, could you provide a way to confirm it? [~neilramaswamy] );;;, 13/Mar/24 17:48;neilramaswamy;[~dongjoon], I will create a minimal Spark repro and paste it in here shortly.

[~LuciferYang], I believe that you're looking at the `DBProvider`, which invokes the`RocksDBProvider`. Indeed, this is used for the external shuffle. However, Structured Streaming uses the RocksDB class, which [_always_ creates a logger invoked over the JNI|https://github.com/apache/spark/blob/b75325ccefa67b0c2daee317264808c67d76854f/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/state/RocksDB.scala#L132]. It's possible that the External shuffle service's use of RocksDB logging also suffers from a JDK 17 JNI attach/detach performance regression but I have never observed this in practice. If you are able to create a regression repro for the ESS, we should address it in a separate ticket.;;;",3.3.1,3.3.2,3.3.3,3.3.4,"12/Mar/24 22:00;dongjoon;Thank you for reporting, [~neilramaswamy]  Could you provide a reproducible Spark example for the further discussion?;;;"
Spark wrongly map the BOOLEAN Type to BIT(1) in Snowflake,SPARK-44866,13547843,13571633,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hayssam,hayssam,hayssam,8/18/23 9:20,3/12/24 14:44,7/17/24 20:45,9/8/23 22:14,3.4.1,4.0.0,SQL,,0,pull-request-available,"In Snowflake the Boolean type is represented by the Boolean data type ([https://docs.snowflake.com/en/sql-reference/data-types-logical]), but Spark rely on the default JdbcDialect to generate the mapping which maps _Boolean_ to _BIT(1)_

This should be probably handled by a dialect specific to Snowflake.",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 08 22:14:37 UTC 2023,,,,,,,,,,0|z1jvg8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Aug/23 13:26;hayssam;Created PR [https://github.com/apache/spark/pull/42545];;;, 08/Sep/23 22:14;dongjoon;Issue resolved by pull request 42545
[https://github.com/apache/spark/pull/42545];;;",,,,,"08/Sep/23 22:14;dongjoon;Issue resolved by pull request 42545
[https://github.com/apache/spark/pull/42545];;;"
Too Many Shared Locks due to PostgresDialect.getTableExistsQuery - LIMIT 1,SPARK-46747,13565089,13571633,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,bbellam,bbellam,1/17/24 13:19,3/12/24 14:39,7/17/24 20:45,1/30/24 12:41,"2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.4.5, 2.4.6, 2.4.7, 2.4.8, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0, 3.4.1, 3.4.2, 3.5.0","3.4.3, 3.5.1, 4.0.0",SQL,,0,pull-request-available,"+*Background:*+
PostgresDialect.getTableExistsQuery is using LIMIT 1 query to check the table existence in the database by overriding the default JdbcDialect.getTableExistsQuery which has WHERE 1 = 0.

+*Issue:*+
Due to LIMIT 1 query pattern, we are seeing high number of shared locks in the PostgreSQL installations where there are many partitions under a table that's being written to. Hence resorting to the default JdbcDialect which does WHERE 1 = 0 is proven to be more optimal as it doesn't scan any of the partitions and effectively checks for table existence.

The SELECT 1 FROM table LIMIT 1 query can indeed be heavier in certain scenarios, especially with partitioned tables or tables with a lot of data, as it may take shared locks on all partitions or involve more planner and execution time to determine the quickest way to get a single row.

On the other hand, SELECT 1 FROM table WHERE 1=0 doesn't actually try to read any data due to the always false WHERE condition. This makes it a lighter operation, as it typically only involves checking the table's metadata to validate the table's existence without taking locks on the table's data or partitions.

So, considering performance and minimizing locks, SELECT 1 FROM table WHERE 1=0 would be a better choice if we're strictly looking to check for a table's existence and want to avoid potentially heavier operations like taking shared locks on partitions.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Scala,Tue Jan 30 14:45:17 UTC 2024,,,,,,,,,,0|z1mth4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"30/Jan/24 03:41;yao;It would be better if you could provide the stats of # of shared locks before and after.;;;, 30/Jan/24 12:41;yao;Issue resolved by pull request 44948
[https://github.com/apache/spark/pull/44948];;;, 30/Jan/24 13:08;bbellam;Thank you very much [~yao] . Sure, I can provide those number of shared locks as soon as I can.

Currently we are using older versions of Spark (2.3). Does this PR update the older versions as well?;;;, 30/Jan/24 14:38;yao;according to the release policies of Spark，patches never get merged to EOL versions. FYI, Spark 3.2 and before are EOL.

if you stay in 2.3 for some reason, consider backport the patch to it
;;;, 30/Jan/24 14:45;bbellam;Thanks [~yao] . So, does this get released to 3.3 & higher? ;;;","2.0.1, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3","2.0.2, 2.4.4, 2.4.5, 2.4.6, 2.4.7, 2.4.8, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0","2.1.0, 3.1.1, 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1","2.1.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0, 3.4.1, 3.4.2, 3.5.0","30/Jan/24 12:41;yao;Issue resolved by pull request 44948
[https://github.com/apache/spark/pull/44948];;;"
spark.catalog.listTables fails with ParseException after upgrading to Spark 3.4.1 from 3.3.1,SPARK-45854,13557342,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,orolesko,orolesko,11/9/23 14:36,3/12/24 4:26,7/17/24 20:45,,"3.4.0, 3.4.1",,"PySpark, Spark Core, Spark Submit",,0,,"After upgrading to Spark 3.4.1, the listTables() method in PySpark now throws a ParseException with the message ""Syntax error at or near end of input."". This did not occur in previous versions of Spark, such as 3.3.1.

Install Spark version 3.4.1.
 
Run pyspark
```bash
{{pyspark --packages io.delta:delta-core_2.12:2.4.0 --conf ""spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension"" --conf ""spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog""}}
```
 
Attempt to list tables using
```console
{{spark.range(1).createTempView(""test_view"")}}
{{spark.catalog.listTables()}}
```
Expected result: The listTables() method should return a list of tables without throwing any exceptions.

Actual result: 
{{Traceback (most recent call last):}}
{{File ""<stdin>"", line 1, in <module>}}
{{File "".venv/lib/python3.10/site-packages/pyspark/sql/catalog.py"", line 302, in listTables}}
{{iter = self._jcatalog.listTables(dbName).toLocalIterator()}}
{{File "".venv/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py"", line 1322, in _{_}call{_}_}}
{{File "".venv/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py"", line 175, in deco}}
{{raise converted from None}}
{{pyspark.errors.exceptions.captured.ParseException:}}
{{[PARSE_SYNTAX_ERROR] Syntax error at or near end of input.(line 1, pos 0)}}

== SQL ==

^^^

>>>

The same code worked correctly in Spark version 3.3.1.
No changes were made to the code aside from upgrading Spark.

Thank you for considering this issue! Any assistance in resolving it would be greatly appreciated.

Best regards,
Andrej",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 12 04:26:18 UTC 2024,,,,,,,,,,0|z1li0w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Mar/24 04:26;lee@chungmin.dev;This is probably a bug in ShowTablesExec (isTempView always returns false if the catalog is not V2SessionCatalog) but it could be an intended behavior. Anyway, you can set spark.sql.legacy.useV1Command to true to workaround the issue.;;;",3.4.1,,,,
Spark shell log filter should be applied to all AbstractAppender,SPARK-46510,13562908,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,angerszhuuuu,angerszhuuuu,12/26/23 6:47,3/8/24 6:19,7/17/24 20:45,,"3.2.1, 3.3.4, 3.4.1",,Spark Core,,0,pull-request-available,"When we set async appender and refer to console, spark shell log filter won't work.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,47:45.0,,,,,,,,,,0|z1mg0w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.3.4,3.4.1,,,
DataType __repr__ change breaks datatype checking (anit-)pattern,SPARK-47288,13570866,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tedjenks,tedjenks,3/5/24 17:26,3/8/24 0:10,7/17/24 20:45,,3.4.1,,SQL,,0,,"This pr: [https://github.com/apache/spark/pull/34320]

Made reprs for datatype eval-able. This is kind of nice, but we have a ton of users doing stuff like:

 
{code:java}
if str(data_type) == ""StringType"":
   ...
{code}
 

Which breaks.

 

What would people think of adding a __str__ to the base class that returns the old behaviour so we can have the best of both worlds.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 08 00:10:34 UTC 2024,,,,,,,,,,0|z1nsmw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"05/Mar/24 17:26;tedjenks;[~gurwls223] I saw you on the original PR, curious for you thoughts.;;;, 08/Mar/24 00:10;gurwls223;I agree in principle but my concern is more about backward compatibility.;;;",,,,,08/Mar/24 00:10;gurwls223;I agree in principle but my concern is more about backward compatibility.;;;
get_json_object flattens wildcard queries that match a single value,SPARK-46778,13565481,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,revans2,revans2,1/19/24 18:47,3/4/24 22:14,7/17/24 20:45,,3.4.1,,SQL,,0,,"I think this impacts all versions of {{{}get_json_object{}}}, but I am not 100% sure.

The unit test for [$.store.book[*].reader|https://github.com/apache/spark/blob/39f8e1a5953b5897f893151d24dc585a80c0c8a0/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/JsonExpressionsSuite.scala#L142-L146] verifies that the output of this query is a single level JSON array, but when I put the same JSON and JSON path into [http://jsonpath.com/] I get a result with multiple levels of nesting. It looks like Apache Spark tries to flatten lists for {{[*]}} matches when there is only a single element that matches.
{code:java}
scala> Seq(""""""[{""a"":""A""},{""b"":""B""}]"""""",""""""[{""a"":""A""},{""a"":""B""}]"""""").toDF(""jsonStr"").selectExpr(""""""get_json_object(jsonStr,""$[*].a"")"""""").show(false)
+--------------------------------+
|get_json_object(jsonStr, $[*].a)|
+--------------------------------+
|""A""                             |
|[""A"",""B""]                       |
+--------------------------------+ {code}
But this has problems in that I no longer have a consistent schema returned, even if the input schema is known to be consistent. For example if I wanted to know how many elements matched this query I could wrap it in a {{json_array_length}} but that does not work in the generic case.
{code:java}
scala> Seq(""""""[{""a"":""A""},{""b"":""B""}]"""""",""""""[{""a"":""A""},{""a"":""B""}]"""""").toDF(""jsonStr"").selectExpr(""""""json_array_length(get_json_object(jsonStr,""$[*].a""))"""""").show(false)
+---------------------------------------------------+
|json_array_length(get_json_object(jsonStr, $[*].a))|
+---------------------------------------------------+
|null                                               |
|2                                                  |
+---------------------------------------------------+ {code}
If the value returned might be a JSON array, and then I would get a number, but it is wrong.
{code:java}
scala> Seq(""""""[{""a"":[1,2,3,4,5]},{""b"":""B""}]"""""",""""""[{""a"":[1,2,3,4,5]},{""a"":[1,2,3,4,5]}]"""""").toDF(""jsonStr"").selectExpr(""""""json_array_length(get_json_object(jsonStr,""$[*].a""))"""""").show(false)
+---------------------------------------------------+
|json_array_length(get_json_object(jsonStr, $[*].a))|
+---------------------------------------------------+
|5                                                  |
|2                                                  |
+---------------------------------------------------+ {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 04 22:14:06 UTC 2024,,,,,,,,,,0|z1mvw8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"04/Mar/24 22:14;planga82;I was looking at it and I found a comment in the code that explain why this behavior ([https://github.com/apache/spark/blob/35bced42474e3221cf61d13a142c3c5470df1f22/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/jsonExpressions.scala#L377]) 

There are some tests around the code that test it and I reproduced it in hive 3.1.3 and it still maintains this behavior so I don't know if we can change it.;;;",,,,,
Fix `core` module to succeed SBT tests,SPARK-47196,13570075,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,2/27/24 21:23,2/28/24 2:23,7/17/24 20:45,2/28/24 2:22,"3.4.0, 3.4.1, 3.4.2",3.4.3,Spark Core,,0,pull-request-available,"This happens at branch-3.4 only. branch-3.3/branch-3.5/master are okay.
{code:java}
$ build/sbt ""core/testOnly *.DAGSchedulerSuite""
[info] DAGSchedulerSuite:
[info] - [SPARK-3353] parent stage should have lower stage id *** FAILED *** (439 milliseconds)
[info]   java.lang.IllegalStateException: Could not initialize plugin: interface org.mockito.plugins.MockMaker (alternate: null)
...
[info] *** 1 SUITE ABORTED ***
[info] *** 118 TESTS FAILED ***
[error] Error during tests:
[error] 	org.apache.spark.scheduler.DAGSchedulerSuite
[error] (core / Test / testOnly) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 48 s, completed Feb 27, 2024, 1:26:27 PM {code}
 

MAVEN
{code:java}
$ build/mvn dependency:tree -pl core | grep byte-buddy
...
[INFO] |  +- net.bytebuddy:byte-buddy:jar:1.12.10:test
[INFO] |  +- net.bytebuddy:byte-buddy-agent:jar:1.12.10:test
{code}
SBT
{code:java}
$ build/sbt ""core/test:dependencyTree"" | grep byte-buddy
[info]   | | | | +-net.bytebuddy:byte-buddy:1.12.10 (evicted by: 1.12.18)
[info]   | | | | +-net.bytebuddy:byte-buddy:1.12.18
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 02:22:22 UTC 2024,,,,,,,,,,0|z1nnr4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"28/Feb/24 02:22;dongjoon;Issue resolved by pull request 45295
[https://github.com/apache/spark/pull/45295];;;",3.4.1,3.4.2,,,
In the spark driver pod. Failed to access the krb5 file,SPARK-47114,13569290,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Resolved,,melin,melin,2/21/24 6:31,2/28/24 1:17,7/17/24 20:45,2/28/24 1:17,3.4.1,,Kubernetes,,0,,"spark runs in kubernetes and accesses an external hdfs cluster (kerberos)，pod error logs
{code:java}
Caused by: java.lang.IllegalArgumentException: KrbException: krb5.conf loading failed{code}
This error generally occurs when the krb5 file cannot be found

[~yao] [~Qin Yao] 
{code:java}
./bin/spark-submit \
    --master k8s://https://172.18.5.44:6443 \
    --deploy-mode cluster \
    --name spark-pi \
    --class org.apache.spark.examples.SparkPi \
    --conf spark.executor.instances=1 \
    --conf spark.kubernetes.submission.waitAppCompletion=true \
    --conf spark.kubernetes.driver.pod.name=spark-xxxxxxx \
    --conf spark.kubernetes.executor.podNamePrefix=spark-executor-xxxxxxx \
    --conf spark.kubernetes.driver.label.profile=production \
    --conf spark.kubernetes.executor.label.profile=production \
    --conf spark.kubernetes.namespace=superior \
    --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
    --conf spark.kubernetes.container.image=registry.cn-hangzhou.aliyuncs.com/melin1204/spark-jobserver:3.4.0 \
    --conf spark.kubernetes.file.upload.path=hdfs://cdh1:8020/user/superior/kubernetes/ \
    --conf spark.kubernetes.container.image.pullPolicy=Always \
    --conf spark.kubernetes.container.image.pullSecrets=docker-reg-demos \
    --conf spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf  \
    --conf spark.kerberos.principal=superior/admin@DATACYBER.COM  \
    --conf spark.kerberos.keytab=/root/superior.keytab  \
    file:///root/spark-3.4.2-bin-hadoop3/examples/jars/spark-examples_2.12-3.4.2.jar  5{code}
{code:java}
(base) [root@cdh1 ~]# kubectl logs spark-xxxxxxx -n superior
Exception in thread ""main"" java.lang.IllegalArgumentException: Can't get Kerberos realm
        at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:71)
        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315)
        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
        at org.apache.hadoop.security.UserGroupInformation.isAuthenticationMethodEnabled(UserGroupInformation.java:395)
        at org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled(UserGroupInformation.java:389)
        at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:1119)
        at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:385)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.IllegalArgumentException: KrbException: krb5.conf loading failed
        at java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(Unknown Source)
        at org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120)
        at org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69)
        ... 13 more
(base) [root@cdh1 ~]# kubectl describe pod spark-xxxxxxx -n superior
Name:             spark-xxxxxxx
Namespace:        superior
Priority:         0
Service Account:  spark
Node:             cdh2/172.18.5.45
Start Time:       Wed, 21 Feb 2024 15:48:08 +0800
Labels:           profile=production
                  spark-app-name=spark-pi
                  spark-app-selector=spark-728e24e49f9040fa86b04c521463020b
                  spark-role=driver
                  spark-version=3.4.2
Annotations:      <none>
Status:           Failed
IP:               10.244.1.4
IPs:
  IP:  10.244.1.4
Containers:
  spark-kubernetes-driver:
    Container ID:  containerd://cceaf13b70cc5f21a639e71cb8663989ec73e122380844624d4bfac3946bae15
    Image:         spark:3.4.1
    Image ID:      docker.io/library/spark@sha256:69fb485a0bcad88f9a2bf066e1b5d555f818126dc9df5a0b7e6a3b6d364bc694
    Ports:         7078/TCP, 7079/TCP, 4040/TCP
    Host Ports:    0/TCP, 0/TCP, 0/TCP
    Args:
      driver
      --properties-file
      /opt/spark/conf/spark.properties
      --class
      org.apache.spark.examples.SparkPi
      spark-internal
      5
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Wed, 21 Feb 2024 15:49:54 +0800
      Finished:     Wed, 21 Feb 2024 15:49:56 +0800
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  1408Mi
    Requests:
      cpu:     1
      memory:  1408Mi
    Environment:
      SPARK_USER:                 superior
      SPARK_APPLICATION_ID:       spark-728e24e49f9040fa86b04c521463020b
      SPARK_DRIVER_BIND_ADDRESS:   (v1:status.podIP)
      HADOOP_CONF_DIR:            /opt/hadoop/conf
      SPARK_LOCAL_DIRS:           /var/data/spark-5e734880-8e00-4349-a88e-e6062ecee6f8
      SPARK_CONF_DIR:             /opt/spark/conf
    Mounts:
      /etc/krb5.conf from krb5-file (rw,path=""krb5.conf"")
      /mnt/secrets/kerberos-keytab from kerberos-keytab (rw)
      /opt/hadoop/conf from hadoop-properties (rw)
      /opt/spark/conf from spark-conf-volume-driver (rw)
      /var/data/spark-5e734880-8e00-4349-a88e-e6062ecee6f8 from spark-local-dir-1 (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-mn8dm (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  hadoop-properties:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      spark-pi-ea209a8dcaa2d678-hadoop-config
    Optional:  false
  krb5-file:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      spark-pi-ea209a8dcaa2d678-krb5-file
    Optional:  false
  kerberos-keytab:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  spark-pi-ea209a8dcaa2d678-kerberos-keytab
    Optional:    false
  spark-local-dir-1:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  spark-conf-volume-driver:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      spark-drv-0a84c78dcaa2de11-conf-map
    Optional:  false
  kube-api-access-mn8dm:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age    From               Message
  ----     ------       ----   ----               -------
  Normal   Scheduled    2m46s  default-scheduler  Successfully assigned superior/spark-xxxxxxx to cdh2
  Warning  FailedMount  2m46s  kubelet            MountVolume.SetUp failed for volume ""krb5-file"" : configmap ""spark-pi-ea209a8dcaa2d678-krb5-file"" not found
  Warning  FailedMount  2m46s  kubelet            MountVolume.SetUp failed for volume ""hadoop-properties"" : configmap ""spark-pi-ea209a8dcaa2d678-hadoop-config"" not found
  Warning  FailedMount  2m46s  kubelet            MountVolume.SetUp failed for volume ""kerberos-keytab"" : secret ""spark-pi-ea209a8dcaa2d678-kerberos-keytab"" not found
  Warning  FailedMount  2m46s  kubelet            MountVolume.SetUp failed for volume ""spark-conf-volume-driver"" : configmap ""spark-drv-0a84c78dcaa2de11-conf-map"" not found
  Normal   Pulling      2m45s  kubelet            Pulling image ""spark:3.4.1""
  Normal   Pulled       60s    kubelet            Successfully pulled image ""spark:3.4.1"" in 1m44.871s (1m44.871s including waiting)
  Normal   Created      60s    kubelet            Created container spark-kubernetes-driver
  Normal   Started      60s    kubelet            Started container spark-kubernetes-driver{code}
 

 cm:  spark-pi-ea209a8dcaa2d678-kerberos-keytab not exists
{code:java}
(base) [root@cdh1 ~]# kubectl get cm -n superior
NAME                                      DATA   AGE
kube-root-ca.crt                          1      161m
spark-drv-0a84c78dcaa2de11-conf-map       2      8m43s
spark-pi-ea209a8dcaa2d678-hadoop-config   11     8m43s
spark-pi-ea209a8dcaa2d678-krb5-file       1      8m43s {code}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 28 01:16:42 UTC 2024,,,,,,,,,,0|z1niwo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,28/Feb/24 01:16;melin;默认jre17 不支持kerberos，换成jdk ;;;,,,,,
Preformance issue on thrift API,SPARK-47085,13568966,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,igreenfi,igreenfi,igreenfi,2/18/24 8:10,2/26/24 5:34,7/17/24 20:45,2/19/24 7:57,"3.4.1, 3.5.0","3.4.3, 3.5.2, 4.0.0",SQL,,0,pull-request-available,"This new complexity was introduced in SPARK-39041.

before this PR the code was:
{code:java}
while (curRow < maxRows && iter.hasNext) {
  val sparkRow = iter.next()
  val row = ArrayBuffer[Any]()
  var curCol = 0
  while (curCol < sparkRow.length) {
    if (sparkRow.isNullAt(curCol)) {
      row += null
    } else {
      addNonNullColumnValue(sparkRow, row, curCol, timeFormatters)
    }
    curCol += 1
  }
  resultRowSet.addRow(row.toArray.asInstanceOf[Array[Object]])
  curRow += 1
}{code}
 foreach without the _*O(n^2)*_ complexity so this change just return the state to what it was before.

 

In class `RowSetUtils` there is a loop that has _*O(n^2)*_ complexity:
{code:scala}
...
 while (i < rowSize) {
          val row = rows(I)
          ...
{code}
It can be easily converted back into _*O( n )*_ complexity.

 

 ",,,,,,,,,,,,,,,,,,,,SPARK-39041,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 20:37:45 UTC 2024,,,,,,,,,,0|z1ngww:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Feb/24 08:30;igreenfi;https://github.com/apache/spark/pull/45155;;;, 19/Feb/24 07:57;yao;resolved by https://github.com/apache/spark/pull/45155;;;, 20/Feb/24 15:53;dongjoon;Hi, [~igreenfi]and [~yao]. 
Could you provide some background why this is a regression at 3.4.1 and 3.5.0? If this is not a regression at that version, we should change `Affected Versions` to `4.0.0` because this is an improvement.;;;, 20/Feb/24 18:23;igreenfi;[~dongjoon] I updated the details....;;;, 20/Feb/24 20:37;dongjoon;Thank you. I added SPARK-39041 as a link `is caused by`.;;;",3.5.0,,,,19/Feb/24 07:57;yao;resolved by https://github.com/apache/spark/pull/45155;;;
Migrate Log4j 2.x in Spark 3.4.1 to Logback,SPARK-44646,13545925,,Brainstorming,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ytian,ytian,8/2/23 22:39,2/24/24 13:54,7/17/24 20:45,,3.4.1,,Build,,0,,"Hi,

We are working on the spark 3.4.1 upgrade from spark 3.1.3, in our logging system, we are using logback framework, it is working with spark 3.1.3 since it is using log4j 1.x. However, when we upgrade spark to 3.4.1, based on the [release notes|https://spark.apache.org/docs/latest/core-migration-guide.html], spark is migrating from log4j 2.x from log4j 1.x, the way we are replacing the log4j with logback is causing build failures in spark master start process.

Error: Unable to initialize main class org.apache.spark.deploy.master.Master
Caused by: java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/Filter

In our current approach, we are using log4j-over-slf4j to replace the log4j-core, it is only applicable to log4j 1.x library. And there is no log4j-over-slf4j for log4j 2.x out there yet. (please correct me if I am wrong). 

I am also curious that why spark choose to use log4j 2.x instead of using SPI, which gives the users less flexibility to choose whatever logger implementation they want to use.

I want to share this issue and see if anyone else has been reported this and if there is any work-around or alternative solutions for it. Any suggestions are appreciated, thanks.",,,,,,,,,,,,,,,,,,,,,,,,,09/Aug/23 21:40;ytian;Screenshot 2023-08-09 at 2.40.12 PM.png;https://issues.apache.org/jira/secure/attachment/13062013/Screenshot+2023-08-09+at+2.40.12+PM.png,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 24 13:54:09 UTC 2024,,,,,,,,,,0|z1jjz4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Aug/23 21:33;ytian;Hi [~viirya] 

Could you please check this thread? It is a question related to https://issues.apache.org/jira/browse/SPARK-37814

Thanks.;;;, 03/Aug/23 21:51;viirya;I have not used it but maybe you can try https://logging.apache.org/log4j/log4j-2.2/log4j-to-slf4j/index.html.;;;, 09/Aug/23 21:42;ytian;Hi [~viirya] 

Thanks for the suggestion, we spent some time evaluate the log4j-to-slf4j approach, unfortunately, it seems not working.

Compared to log4j-over-slf4j for log4j 1.x, log4j-to-slf4j is mainly an adapter for log4j-core, which means we still need the dependency of log4j-core. Since logback and log4j-core are 2 implementations, slf4j will complain about it. Below is the diagram we have tried:

!Screenshot 2023-08-09 at 2.40.12 PM.png!

If there are no better solutions, we may need to rewrite the existing logging logics with log4j 2.x. Thanks.;;;, 24/Aug/23 12:19;filtzsche;Hi all,  
we are running into the same problem, and I believe the problem is not in the forwarding library but in Spark itself.  When initializing the logging in [Logging.scala|[https://github.com/apache/spark/blob/v3.3.1/core/src/main/scala/org/apache/spark/internal/Logging.scala#L232C22-L232C22]], the log4j implementation (dependency org.apache.logging.log4j.log4j-slf4j-impl) is required.
However, this dependency also needs to be excluded in order to successfully use logging via logback. 
I was able to run our program with logback logging in debug mode, by skipping the code line liked above. Wrapping the call to “StaticLoggerBinder.getSingleton.getLoggerFactoryClassStr” in a try block and return an empty string if the class is not known would probably solve the problem.
It would be great if this issue could be fixed soon, as it blocks us from upgrading to Spark version 3.3.0 and newer.
Thanks.;;;, 24/Feb/24 13:54;rmucha-c;Hi [~viirya] ,

Do you have any plans to somehow address this issue? It is quite serious problem, since it force using log4j2 implementation (log4j-core) with Spark. Solution is either quite naive approach like [here|https://github.com/apache/spark/pull/45001] , or on the other hand, creating separate interfaces in Spark core and developing packages that offer implementations utilising different logging frameworks - it is spring approach. Have you had any discussions about that?

Best;;;",,,,,03/Aug/23 21:51;viirya;I have not used it but maybe you can try https://logging.apache.org/log4j/log4j-2.2/log4j-to-slf4j/index.html.;;;
Use AnalyzeTableCommand overwrite statistics information incorrectly,SPARK-46996,13567672,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,davidxdh,davidxdh,2/7/24 2:31,2/7/24 12:05,7/17/24 20:45,,"2.4.4, 3.4.1",,SQL,,0,pull-request-available,"When the size of the table changes but the total number of rows in the table does not change, I use the sql statement ""analyze table student compute statistics"" to analyze the external table statistics. The Statistics information returned by the sql statement ""desc extended student"" only contains the table Size information, excluding rowCounts information.

Specific operating instructions:
{code:sql}
Create external table

create table student(id int, name string, age int) row format delimited fields terminated by ','
lines terminated by '\n' location 'hdfs://nameservice/spark/student';


The contents of the external table file are as follows:
class1.txt:

1,'Jack',25
2,'Thompson',28


class2.txt:

3,'Davy',30
4,'Thompson',35


class3.txt：

5,'Curry',40
6,'Morgan',20


Import external table data

hdfs dfs -put 1.txt /spark/student
hdfs dfs -put 2.txt /spark/student


Analyze external table statistics

analyze table student compute statistics;
desc extended student;

Return results

Type EXTERNAL
Provider hive
Table Properties [transient_lastDdlTime=1707265554]
Statistics 56 bytes, 4 rows
Location hdfs://nameservice/spark/student
Serde Library org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat org.apache.hadoop.mapred.TextInputFormat
OutputFormat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Storage Properties [serialization.format=,, line.delim=
, field.delim=,]
Partition Provider Catalog


Modify external table

hdfs dfs -rm /spark/student/student2.txt
hdfs dfs -put student3.txt /spark/student


Analyze the external table again

analyze table student compute statistics;
desc extended student;

Return results

Type EXTERNAL
Provider hive
Table Properties [transient_lastDdlTime=1707265719]
Statistics 55 bytes
Location hdfs://nameservice/spark/student
Serde Library org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat org.apache.hadoop.mapred.TextInputFormat
OutputFormat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
Storage Properties [serialization.format=,, line.delim=
, field.delim=,]
Partition Provider Catalog
{code}
Through the above operation results, I found that when the table size changes but the number of rows does not change, the statistics should include the new table size and the total number of rows. I don’t know if it is correct to only display the statistics of the table size.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,31:08.0,,,,,,,,,,0|z1n9ew:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,,,,
failed to insert the table using the default value of union,SPARK-46192,13560141,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,zengxl,zengxl,12/1/23 3:01,1/31/24 1:17,7/17/24 20:45,1/31/24 1:17,"3.4.0, 3.4.1",,SQL,,0,," 

Obtain the following tables and data
{code:java}
create table test_spark(k string default null,v int default null) stored as orc;
create table test_spark_1(k string default null,v int default null) stored as orc;
insert into table test_spark_1 values('k1',1),('k2',2),('k3',3);
create table test_spark_2(k string default null,v int default null) stored as orc; 
insert into table test_spark_2 values('k3',3),('k4',4),('k5',5);

{code}
Execute the following SQL
{code:java}
insert into table test_spark (k) 
select k from test_spark_1
union
select k from test_spark_2 

{code}
exception:
{code:java}
23/12/01 10:44:25 INFO HiveSessionStateBuilder$$anon$1: here is CatalogAndIdentifier
23/12/01 10:44:25 INFO HiveSessionStateBuilder$$anon$1: here is CatalogAndIdentifier
23/12/01 10:44:25 INFO HiveSessionStateBuilder$$anon$1: here is CatalogAndIdentifier
23/12/01 10:44:26 INFO Analyzer$ResolveUserSpecifiedColumns: i.userSpecifiedCols.size is 1
23/12/01 10:44:26 INFO Analyzer$ResolveUserSpecifiedColumns: i.userSpecifiedCols.size is 1
23/12/01 10:44:26 INFO Analyzer$ResolveUserSpecifiedColumns: i.table.output 2 ,resolved :1 , i.query 1
23/12/01 10:44:26 INFO Analyzer$ResolveUserSpecifiedColumns: here is ResolveUserSpecifiedColumns tableOutoyt: 2---nameToQueryExpr : 1Error in query: `default`.`test_spark` requires that the data to be inserted have the same number of columns as the target table: target table has 2 column(s) but the inserted data has 1 column(s), including 0 partition column(s) having constant value(s). {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 31 01:12:25 UTC 2024,,,,,,,,,,0|z1lyyg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Dec/23 11:16;zengxl;{code:java}
create table test_spark_3(k string default null,v int default null,m string default null) stored as orc; 

insert into table test_spark_3(k,v) select k,sum(v) v from test_spark_1 group by k;

insert into table test_spark_3(k,v) select distinct a.k,a.v from test_spark a left join test_spark_1 b on a.k=b.k  limit 2;{code}
The above SQL has the same exception

 ;;;, 28/Dec/23 13:36;raciniewska;I am working on it;;;, 31/Jan/24 01:12;zengxl;This patch solves all of these problems

https://issues.apache.org/jira/browse/SPARK-43742;;;",3.4.1,,,,28/Dec/23 13:36;raciniewska;I am working on it;;;
Remove inline scripts from UI descriptions,SPARK-46893,13566440,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rshkv,rshkv,rshkv,1/28/24 17:59,1/30/24 6:44,7/17/24 20:45,1/30/24 6:43,3.4.1,"3.4.3, 3.5.1, 4.0.0","UI, Web UI",,0,pull-request-available,"Users can inject inline scripts (e.g. {{onclick}} or {{onmouseover}} handlers) in the UI job and stage descriptions.

The UI already has precaution to treat, e.g., {{<script>}} tags as plain-text. But that doesn't extend to inline scripts.

Example:

{code:title=Bad job descriptions}
scala> sc.setJobDescription(""""""<a href=""/link"" onmouseover=""alert('oops');"">onmouseover</a>"""""")

scala> spark.sql(""SELECT 1"").show()
...

scala> sc.setJobDescription(""""""<a href=""/link"" onclick=""alert('oops');"">onclick</a>"""""")

scala> spark.sql(""SELECT 1"").show()
...
{code}

 !Screenshot 2024-01-29 at 09.06.34.png|width=600! ",,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/24 17:59;rshkv;Screen Recording 2024-01-28 at 17.51.47.mov;https://issues.apache.org/jira/secure/attachment/13066301/Screen+Recording+2024-01-28+at+17.51.47.mov, 29/Jan/24 09:07;rshkv;Screenshot 2024-01-29 at 09.06.34.png;https://issues.apache.org/jira/secure/attachment/13066306/Screenshot+2024-01-29+at+09.06.34.png",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 30 06:43:52 UTC 2024,,,,,,,,,,0|z1n1tc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"29/Jan/24 12:15;rshkv;cc [~dongjoon], for your awareness as PMC who's recently touched the UI.

I'm wondering if we should file a CVE for this.;;;, 30/Jan/24 06:36;dongjoon;Thank you for pinging me, [~rshkv].;;;, 30/Jan/24 06:43;dongjoon;Issue resolved by pull request 44933
[https://github.com/apache/spark/pull/44933];;;",,,,,"30/Jan/24 06:36;dongjoon;Thank you for pinging me, [~rshkv].;;;"
RocksDB versionID Mismatch in SST files,SPARK-46796,13565713,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bhuwan.sahni,bhuwan.sahni,bhuwan.sahni,1/22/24 20:28,1/24/24 12:49,7/17/24 20:45,1/24/24 12:49,"3.4.1, 3.4.2, 3.5.0, 3.5.1, 3.5.2, 4.0.0","3.5.1, 4.0.0",Structured Streaming,,0,pull-request-available,"We need to ensure that the correct SST files are used on executor during RocksDB load as per mapping in metadata.zip. With current implementation, its possible that the executor uses a SST file (with a different UUID) from a older version which is not the exact file mapped in the metadata.zip. This can cause version Id mismatch errors while loading RocksDB leading to streaming query failures.

Few scenarios in which such a situation can occur are:

**Scenario 1 - Distributed file system does not support overwrite functionality**
 # A task T1 on executor A commits Rocks Db snapshot for version X.
 # Another task T2 on executor A loads version X-1, and tries to commit X. During commit, SST files are copied but metadata file is not overwritten.
 # Task T3 is scheduled on A, this task reuses previously loaded X (loaded in (2) above) and commits X+1.
 # Task T4 is scheduled on A again for state store version X. The executor deletes SST files corresponding to commit X+1, downloads the metadata for version X (which was committed in task T1), and loads RocksDB. This would fail because the metadata in (1) is not compatible with SST files in (2).

 

**Scenario 2 - Multiple older State versions have different DFS files for a particular SST file.**


In the current logic, we look at all the versions older than X to find if a local SST file can be reused. The reuse logic only ensures that the local SST file was present in any of the previous version. However, its possible that 2 different older versions had a different SST file (`0001-uuid1.sst` and `0001-uuid2.sst`) uploaded on DFS. These SST files will have the same local name (with UUID truncated) and size, but are not compatible due to different RocksDB Version Ids. We need to ensure that the correct SST file (as per UUID) is picked as mentioned in the metadata.zip.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 24 12:49:23 UTC 2024,,,,,,,,,,0|z1mxbs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"22/Jan/24 20:29;bhuwan.sahni;Working on a PR for the fix.;;;, 22/Jan/24 21:54;bhuwan.sahni;PR created - [https://github.com/apache/spark/pull/44837];;;, 24/Jan/24 12:49;kabhwan;Issue resolved by pull request 44837
[https://github.com/apache/spark/pull/44837];;;",3.4.2,3.5.0,3.5.1,3.5.2,22/Jan/24 21:54;bhuwan.sahni;PR created - [https://github.com/apache/spark/pull/44837];;;
Coalesce partiton assert error after skew join optimization,SPARK-46590,13563546,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Jackey Lee,Jackey Lee,Jackey Lee,1/4/24 8:36,1/24/24 7:44,7/17/24 20:45,1/23/24 8:12,"3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0, 3.4.1, 3.5.0, 3.5.1, 4.0.0","3.5.1, 4.0.0",SQL,,0,pull-request-available,"Recently when we were testing TPCDS Q71, we found that if `spark.sql.shuffle.partitions` and `spark.sql.adaptive.coalescePartitions.initialPartitionNum` are both set to the number of executor cores, an AssertError may be reported in coalescePartition due to the partitionSpecs of joins after skew are different.",,,,,,,,,,,,,,,,,,,,,,,,,04/Jan/24 08:37;Jackey Lee;problem.log;https://issues.apache.org/jira/secure/attachment/13065728/problem.log,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 23 08:12:09 UTC 2024,,,,,,,,,,0|z1mjyg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Jan/24 08:12;cloud_fan;Issue resolved by pull request 44661
[https://github.com/apache/spark/pull/44661];;;",3.3.2,3.3.3,3.3.4,3.4.0,
Relax constraint for columnar shuffle check in AQE,SPARK-44660,13546050,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,csun,csun,8/3/23 19:27,1/21/24 2:36,7/17/24 20:45,,3.4.1,,SQL,,0,,"Currently in AQE, after evaluating the columnar rules, Spark will check if the top operator of the stage is still a shuffle operator, and throw exception if it doesn't.

{code}
        val optimized = e.withNewChildren(Seq(optimizeQueryStage(e.child, isFinalStage = false)))
        val newPlan = applyPhysicalRules(
          optimized,
          postStageCreationRules(outputsColumnar = plan.supportsColumnar),
          Some((planChangeLogger, ""AQE Post Stage Creation"")))
        if (e.isInstanceOf[ShuffleExchangeLike]) {
          if (!newPlan.isInstanceOf[ShuffleExchangeLike]) {
            throw SparkException.internalError(
              ""Custom columnar rules cannot transform shuffle node to something else."")
          }
{code}

However, once a shuffle operator is transformed into a custom columnar shuffle operator, the {{supportsColumnar}} of the new shuffle operator will return true, and therefore the columnar rules will insert {{ColumnarToRow}} on top of it. This means the {{newPlan}} is likely no longer a {{ShuffleExchangeLike}} but a {{ColumnarToRow}}, and exception will be thrown, even though the use case is valid.

This JIRA proposes to relax the check by allowing the above case.



",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 21 02:36:13 UTC 2024,,,,,,,,,,0|z1jkqw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Aug/23 20:37;csun;In fact the check is necessary, but it seems 
{code}
postStageCreationRules(outputsColumnar = plan.supportsColumnar)
{code}

can be relaxed: if the new shuffle operator supports columnar, then maybe we shouldn't insert {{ColumnarToRow}} to this stage. This is assuming the following stage knows the shuffle output is columnar and has corresponding {{ColumnarToRow}} if necessary.;;;, 21/Jan/24 02:36;Suraj Naik;[~csun] Did you manage to fix this? ;;;",,,,,21/Jan/24 02:36;Suraj Naik;[~csun] Did you manage to fix this? ;;;
Avro connector: convert a union of a single primitive type to a StructType,SPARK-44919,13548294,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,adrianhu96,adrianhu96,8/22/23 22:19,1/19/24 0:19,7/17/24 20:45,,3.4.1,,Spark Core,,0,pull-request-available,"Spark Avro data source schema converter currently converts union with a single primitive type to a Spark primitive type instead of a StructType.

While for more complex union types that consists of multiple primitive types, the schema converter translate them into StructTypes.

For example, 
import scala.collection.JavaConverters._
import org.apache.avro._
import org.apache.spark.sql.avro._

// [""string"", ""null""]
SchemaConverters.toSqlType(
  Schema.createUnion(Seq(Schema.create(Schema.Type.STRING), Schema.create(Schema.Type.NULL)).asJava)
).dataType

// [""string"", ""int"", ""null""]
SchemaConverters.toSqlType(
  Schema.createUnion(Seq(Schema.create(Schema.Type.STRING), Schema.create(Schema.Type.INT), Schema.create(Schema.Type.NULL)).asJava)
).dataType
The first one would return StringType, the second would return StructType(StringType, IntegerType).
 
We hope to add a new configuration to control the conversion behavior. The default behavior would still be the same. When the config is altered, a union with single primitive type would be translated into StructType.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,19:37.0,,,,,,,,,,0|z1jy88:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
The return status is incorrect in standalone mode,SPARK-45290,13551761,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,changhu,changhu,9/24/23 1:53,1/16/24 0:19,7/17/24 20:45,,"3.4.0, 3.4.1, 3.5.0",,Block Manager,,0,pull-request-available," When a job is submitted in standalone mode using spark launch, sparkLancher returns a successful execution and then a failed execution. Examples are as follows

log: Spark App Id [app-20230922160022-0006] State Changed.  State [FINISHED]
23/09/22 16:01:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/09/22 16:01:10 INFO MemoryStore: MemoryStore cleared
23/09/22 16:01:10 INFO BlockManager: BlockManager stopped
23/09/22 16:01:10 INFO BlockManagerMaster: BlockManagerMaster stopped
23/09/22 16:01:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/09/22 16:01:10 INFO SparkContext: Successfully stopped SparkContext
23/09/22 16:01:10 INFO ShutdownHookManager: Shutdown hook called
23/09/22 16:01:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-e015933f-a220-45a8-9d73-650b8bd8a337
23/09/22 16:01:10 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-8879f1d8-1f21-4b52-bca1-d3c66af6f754
23/09/22 16:01:11 INFO log: Spark App Id [app-20230922160022-0006] State Changed.  State [FAILED]",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,53:21.0,,,,,,,,,,0|z1kjmg:,9223372036854775807,,,,,,,,,,,,,"3.4.0, 3.4.1, 3.5.0",,,,,,,,,3.4.1,3.5.0,,,
Fix `MasterPage` to sort `Running Drivers` table by `Duration` column correctly,SPARK-46704,13564544,13557406,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,1/12/24 20:06,1/12/24 20:54,7/17/24 20:45,1/12/24 20:54,"3.0.0, 3.1.3, 3.2.4, 3.3.4, 3.4.1, 3.5.0, 4.0.0","3.4.3, 3.5.1, 4.0.0","Spark Core, Web UI",,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 20:54:58 UTC 2024,,,,,,,,,,0|z1mq48:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Jan/24 20:54;dongjoon;Issue resolved by pull request 44711
[https://github.com/apache/spark/pull/44711];;;",3.1.3,3.2.4,3.3.4,3.4.1,
Remove unnecessary TaskScheduler.killAllTaskAttempts,SPARK-46052,13558991,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,11/22/23 8:28,1/12/24 9:18,7/17/24 20:45,1/12/24 9:18,"3.0.3, 3.1.3, 3.2.4, 3.3.3, 3.4.1, 3.5.0",4.0.0,Spark Core,,0,pull-request-available,"Spark has two functions to kill all tasks in a Stage:
* `cancelTasks`: Not only kill all the running tasks in all the stage attempts but also abort all the stage attempts
*  `killAllTaskAttempts`: Only kill all the running tasks in all the stage attemtps but won't abort the attempts.


However, there's no use case in Spark that a stage would launch new tasks after its all tasks get killed. So I think we can replace `killAllTaskAttempts` with `cancelTasks` directly.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 12 09:18:10 UTC 2024,,,,,,,,,,0|z1lrvc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Jan/24 09:18;cloud_fan;Issue resolved by pull request 43954
[https://github.com/apache/spark/pull/43954];;;",3.1.3,3.2.4,3.3.3,3.4.1,
Pyspark throwing TypeError while collecting a RDD,SPARK-46636,13564074,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,skyrim233,skyrim233,1/9/24 15:32,1/9/24 15:33,7/17/24 20:45,,3.4.1,,PySpark,,0,,"Im trying to collect a RDD after applying a filter on it but its throwing an error.

 

Error can be reproduced from below code
{code:java}
from pyspark.sql import SparkSession
spark = SparkSession.builder.master(""local[*]"").appName(""Practice"").getOrCreate()

sc = spark.sparkContext

data = [1,2,3,4,5,6,7,8,9,10,11,12]
dataRdd = sc.parallelize(data)

dataRdd = dataRdd.filter(lambda a: a%2==0)

dataRdd.collect() {code}
Below is the error that its throwing:

 
{code:java}
--------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[18], line 1 ----> 1 dataRdd.collect() 
File ~\anaconda3\envs\spark_latest\Lib\site-packages\pyspark\rdd.py:
1814, in RDD.collect(self)  
1812 with SCCallSiteSync(self.context):  
1813 assert self.ctx._jvm is not None 
-> 1814 sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())  
1815 return list(_load_from_socket(sock_info, self._jrdd_deserializer)) 
File ~\anaconda3\envs\spark_latest\Lib\site-packages\pyspark\rdd.py:
5441, in PipelinedRDD._jrdd(self)  
438 else:  
5439 profiler = None 
-> 5441 wrapped_func = _wrap_function(  
5442 self.ctx, self.func, self._prev_jrdd_deserializer, self._jrdd_deserializer, profiler  
5443 )  
5445 assert self.ctx._jvm is not None  
5446 python_rdd = self.ctx._jvm.PythonRDD(  
5447 self._prev_jrdd.rdd(), wrapped_func, self.preservesPartitioning, self.is_barrier  
5448 ) 
File ~\anaconda3\envs\spark_latest\Lib\site-packages\pyspark\rdd.py:
5243, in _wrap_function(sc, func, deserializer, serializer, profiler)  
5241 pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)  
5242 assert sc._jvm is not None 
-> 5243 return sc._jvm.SimplePythonFunction(  
5244 bytearray(pickled_command),  
5245 env,  
5246 includes,  
5247 sc.pythonExec,  
5248 sc.pythonVer,  
5249 broadcast_vars,  
5250 sc._javaAccumulator,  
5251 ) 

TypeError: 'JavaPackage' object is not callable{code}
 

 ","Running this in anaconda jupyter notebook

Python== 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) 

[MSC v.1916 64 bit (AMD64)]

Spark== 3.3.4

pyspark== 3.4.1",,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,32:26.0,,,,,,,,,,0|z1mn7s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
OrcColumnarBatchReader should respect the memory mode when creating column vectors for the missing column,SPARK-46598,13563593,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,1/4/24 14:15,1/6/24 20:48,7/17/24 20:45,1/6/24 20:48,"3.4.1, 3.5.0, 4.0.0","3.4.3, 3.5.1, 4.0.0",SQL,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 06 20:48:09 UTC 2024,,,,,,,,,,0|z1mk8w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"06/Jan/24 20:48;dongjoon;Issue resolved by pull request 44598
[https://github.com/apache/spark/pull/44598];;;",3.5.0,4.0.0,,,
critical CVE vulnerability with a fix in Derby,SPARK-46267,13560637,,Dependency upgrade,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,julienlau,julienlau,12/5/23 11:06,1/2/24 11:52,7/17/24 20:45,,3.4.1,,Build,,1,security," 

It would be necessary to upgrade Derby dependency in order to solve a critical vulnerability that was fixed in the latest release of Derby in November:

[https://db.apache.org/derby/releases/release-10_17_1_0.cgi]

https://issues.apache.org/jira/browse/DERBY-7147?focusedCommentId=17799544&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17799544

 

 

The vuln:

```

│                   Library                    │ Vulnerability  │ Severity │ Status │ Installed Version │ Fixed Version │                            Title                             │

│ org.apache.derby:derby (derby-10.14.2.0.jar) │ CVE-2022-46337 │ CRITICAL │ fixed  │ 10.14.2.0         │ 10.17.1.0     │ A cleverly devised username might bypass LDAP authentication │

```","I know it is in spark 3.4.1 that is the last version released by canonical charmed spark.

Since the fix was released on Nov 10 on derby side it probably affects all versions of spark.",,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,06:01.0,,,,,,,,,,0|z1m20o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Push filters through intersect,SPARK-44812,13547331,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,HectorZhang,HectorZhang,8/15/23 4:42,12/30/23 0:18,7/17/24 20:45,,3.4.1,,SQL,,0,pull-request-available,"For following SQL
{code:sql}
select a from (select a from tl intersect select x from tr) where a > 123 {code}
The physical plan is
{code:bash}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- HashAggregate(keys=[a#8L], functions=[])
   +- Exchange hashpartitioning(a#8L, 200), ENSURE_REQUIREMENTS, [plan_id=133]
      +- HashAggregate(keys=[a#8L], functions=[])
         +- BroadcastHashJoin [coalesce(a#8L, 0), isnull(a#8L)], [coalesce(x#24L, 0), isnull(x#24L)], LeftSemi, BuildRight, false
            :- Filter (isnotnull(a#8L) AND (a#8L > 123))
            :  +- FileScan json [a#8L] ...
            +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, bigint, true], 0), isnull(input[0, bigint, true])),false), [plan_id=129]
               +- FileScan json [x#24L] ... {code}
We can find the filter {color:#ff8b00}_a > 123_{color} is not pushed to right table.

 

 

Further more, for following SQL
{code:sql}
select a from (select a from tl intersect select x from tr) join trr on a = y{code}
The physical plan is
{code:bash}
*(3) Project [a#8L]
+- *(3) BroadcastHashJoin [a#8L], [y#114L], Inner, BuildRight, false
   :- *(3) HashAggregate(keys=[a#8L], functions=[])
   :  +- Exchange hashpartitioning(a#8L, 200), ENSURE_REQUIREMENTS, [plan_id=506]
   :     +- *(1) HashAggregate(keys=[a#8L], functions=[])
   :        +- *(1) BroadcastHashJoin [coalesce(a#8L, 0), isnull(a#8L)], [coalesce(x#24L, 0), isnull(x#24L)], LeftSemi, BuildRight, false
   :           :- *(1) Filter isnotnull(a#8L)
   :           :  +- FileScan json [a#8L] ...
   :           +- BroadcastExchange HashedRelationBroadcastMode(List(coalesce(input[0, bigint, true], 0), isnull(input[0, bigint, true])),false), [plan_id=490]
   :              +- FileScan json [x#24L] ...
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=512]
      +- *(2) Filter isnotnull(y#114L)
         +- FileScan json [y#114L] ...{code}
There should be a filter _{color:#ff8b00}isnotnull( x ){color}_ for table tr, while it's not pushed down.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,42:47.0,,,,,,,,,,0|z1jsag:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Support keyword columns on filters that interact with HMS,SPARK-45102,13549947,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,scarlin,scarlin,9/7/23 16:18,12/25/23 0:19,7/17/24 20:45,,3.4.1,,Connect,,0,pull-request-available,"Recently, https://issues.apache.org/jira/browse/HIVE-27665 was pushed on Hive. This will allow HMS to handle columns that are surrounded by backticks in filters.  An example of a customer who hit this problem had a filter in Spark like this:

where date='2015-01-06'

This didn't work because the word ""date"" is a keyword.  In order for the customer to work, the where clause should be changed to this:

where `date`='2015-01-06'

Spark strips out the backticks before passing the filter to HMS.  We need to no longer strip the backticks as a configurable flag.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,18:22.0,,,,,,,,,,0|z1k8fk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not transpose windows if they conflict on ORDER BY / PROJECT clauses,SPARK-45055,13549439,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,gubichev,gubichev,9/1/23 18:59,12/23/23 0:18,7/17/24 20:45,,3.4.1,,SQL,,0,pull-request-available,"TransposeWindows rule reorders parent and child window functions. Currently it incorrectly reorders the window functions in cases where the top window function orders by on the result of the bottom window function, e.g. {{sum1}} in the following example:

{{SELECT ROW_NUMBER() OVER (PARTITION BY C ORDER BY sum1)  FROM (SELECT ROW_NUMBER() OVER (PARTITION BY C) as sum1 FROM T) }}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 06 09:39:50 UTC 2023,,,,,,,,,,0|z1k5ao:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"06/Sep/23 09:39;aparna.garg;User 'agubichev' has created a pull request for this issue:
https://github.com/apache/spark/pull/42778;;;",,,,,
Shutdown hook timeouts during ui stop,SPARK-46456,13562302,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,12/19/23 9:57,12/21/23 2:10,7/17/24 20:45,12/20/23 23:05,"3.4.1, 3.5.0, 4.0.0",4.0.0,Spark Core,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 20 23:05:01 UTC 2023,,,,,,,,,,0|z1mcag:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"20/Dec/23 23:05;dongjoon;Issue resolved by pull request 44413
[https://github.com/apache/spark/pull/44413];;;",3.5.0,4.0.0,,,
Scala None shows up as null for Aggregator BUF or OUT  ,SPARK-44323,13542727,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,koert,koert,7/6/23 17:08,12/19/23 0:19,7/17/24 20:45,,3.4.1,,SQL,,0,pull-request-available,"when doing an upgrade from spark 3.3.1 to spark 3.4.1 we suddenly started getting null pointer exceptions in Aggregators (classes extending org.apache.spark.sql.expressions.Aggregator) that use scala Option for BUF and/or OUT. basically None is now showing up as null.

after adding a simple test case and doing a binary search on commits we landed on SPARK-37829 being the cause.

we observed the issue at first with NPE inside Aggregator.merge because None was null. i am having a hard time replicating that in a spark unit test, but i did manage to get a None become null in the output. simple test that now fails:

 
{code:java}
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/DatasetAggregatorSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/DatasetAggregatorSuite.scala
index e9daa825dd4..a1959d7065d 100644
--- a/sql/core/src/test/scala/org/apache/spark/sql/DatasetAggregatorSuite.scala
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DatasetAggregatorSuite.scala
@@ -228,6 +228,16 @@ case class FooAgg(s: Int) extends Aggregator[Row, Int, Int] {
   def outputEncoder: Encoder[Int] = Encoders.scalaInt
 }
 
+object OptionStringAgg extends Aggregator[Option[String], Option[String], Option[String]] {
+  override def zero: Option[String] = None
+  override def reduce(b: Option[String], a: Option[String]): Option[String] = merge(b, a)
+  override def finish(reduction: Option[String]): Option[String] = reduction
+  override def merge(b1: Option[String], b2: Option[String]): Option[String] =
+    b1.map{ b1v => b2.map{ b2v => b1v ++ b2v }.getOrElse(b1v) }.orElse(b2)
+  override def bufferEncoder: Encoder[Option[String]] = ExpressionEncoder()
+  override def outputEncoder: Encoder[Option[String]] = ExpressionEncoder()
+}
+
 class DatasetAggregatorSuite extends QueryTest with SharedSparkSession {
   import testImplicits._
 
@@ -432,4 +442,15 @@ class DatasetAggregatorSuite extends QueryTest with SharedSparkSession {
     val agg = df.select(mode(col(""a""))).as[String]
     checkDataset(agg, ""3"")
   }
+
+  test(""typed aggregation: option string"") {
+    val ds = Seq((1, Some(""a"")), (1, None), (1, Some(""c"")), (2, None)).toDS()
+
+    checkDataset(
+      ds.groupByKey(_._1).mapValues(_._2).agg(
+        OptionStringAgg.toColumn
+      ),
+      (1, Some(""ac"")), (2, None)
+    )
+  }
 }
 {code}",,,,,,,,,,,,,,,,,,,,SPARK-37829,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 09 15:35:37 UTC 2023,,,,,,,,,,0|z1j09s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"06/Jul/23 18:28;koert;i think the issue is that Nones inside Tuples now become nulls.

so its the usage of nullSafe inside the childrenDeserializers for tuples introduced in [https://github.com/apache/spark/pull/40755];;;, 09/Jul/23 15:35;koert;not sure why pullreq isnt getting linked automatically but its here:

https://github.com/apache/spark/pull/41903;;;",,,,,"09/Jul/23 15:35;koert;not sure why pullreq isnt getting linked automatically but its here:

https://github.com/apache/spark/pull/41903;;;"
Encoders.bean does no longer work with read-only properties,SPARK-45081,13549685,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,gbloisi-openaire,gbloisi-openaire,9/5/23 14:42,12/15/23 4:52,7/17/24 20:45,9/12/23 14:17,3.4.1,"3.4.2, 3.5.1, 4.0.0",SQL,,0,pull-request-available,"Since Spark 3.4.x an exception is thrown when Encoders.bean is called providing a bean having read-only properties, such as:

 
{code:java}
public static class ReadOnlyPropertyBean implements Serializable {
    public boolean isEmpty() {
      return true;
    }
} {code}
 

 
Encoders.bean(ReadOnlyPropertyBean.class) will throw:
{code:java}
java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:529)
        at scala.None$.get(Option.scala:527)
        at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$deserializerFor$8(ScalaReflection.scala:359)
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
        at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
        at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
        at scala.collection.TraversableLike.map(TraversableLike.scala:286)
        at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
        at scala.collection.AbstractTraversable.map(Traversable.scala:108)
        at org.apache.spark.sql.catalyst.ScalaReflection$.deserializerFor(ScalaReflection.scala:348)
        at org.apache.spark.sql.catalyst.ScalaReflection$.deserializerFor(ScalaReflection.scala:183)
        at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:56)
        at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.javaBean(ExpressionEncoder.scala:62)
        at org.apache.spark.sql.Encoders$.bean(Encoders.scala:179)
        at org.apache.spark.sql.Encoders.bean(Encoders.scala) {code}
This problem is described also in [link Encoders.bean doesn't work anymore on a Java POJO, with Spark 3.4.0|https://stackoverflow.com/questions/76036349/encoders-bean-doesnt-work-anymore-on-a-java-pojo-with-spark-3-4-0]",,,,,,,,,,,,SPARK-45311,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 18 20:40:37 UTC 2023,,,,,,,,,,0|z1k6tc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,18/Sep/23 20:40;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/42913;;;,,,,,
Encoders.bean does not support superclasses with generic type arguments,SPARK-44910,13548200,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gbloisi-openaire,gbloisi-openaire,gbloisi-openaire,8/22/23 11:54,12/15/23 4:52,7/17/24 20:45,9/19/23 4:37,"3.4.1, 3.5.0, 4.0.0","3.4.2, 3.5.1, 4.0.0",SQL,,0,pull-request-available,"As per SPARK-44634 another unsupported feature of bean encoder is when the superclass of the bean has generic type arguments. For example:
{code:java}
class JavaBeanWithGenericsA<T> {
    public T getPropertyA() {
        return null;
    }

    public void setPropertyA(T a) {

    }
}

class JavaBeanWithGenericBase extends JavaBeanWithGenericsA<String> {
}

Encoders.bean(JavaBeanWithGenericBase.class); // Exception

{code}",,,,,,,,,,,,SPARK-45311,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 19 04:39:19 UTC 2023,,,,,,,,,,0|z1jxnk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"31/Aug/23 16:52;ignitetcbot;User 'gbloisi-openaire' has created a pull request for this issue:
https://github.com/apache/spark/pull/42634;;;, 19/Sep/23 04:37;dongjoon;Issue resolved by pull request 42634
[https://github.com/apache/spark/pull/42634];;;, 19/Sep/23 04:39;dongjoon;This landed at branch-3.4 via https://github.com/apache/spark/pull/42914;;;",3.5.0,4.0.0,,,"19/Sep/23 04:37;dongjoon;Issue resolved by pull request 42634
[https://github.com/apache/spark/pull/42634];;;"
Encoders.bean does no longer support nested beans with type arguments,SPARK-44634,13545823,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,gbloisi-openaire,gbloisi-openaire,8/2/23 9:13,12/15/23 4:52,7/17/24 20:45,8/7/23 14:14,"3.4.1, 3.5.0, 4.0.0","3.4.2, 3.5.0, 4.0.0",SQL,,0,,"Hi,

  while upgrading a project from spark 2.4.0 to 3.4.1 version, I have encountered the same problem described in [java - Encoders.bean attempts to check the validity of a return type considering its generic type and not its concrete class, with Spark 3.4.0 - Stack Overflow|https://stackoverflow.com/questions/76045255/encoders-bean-attempts-to-check-the-validity-of-a-return-type-considering-its-ge].

Put it short, starting from Spark 3.4.x Encoders.bean throws an exception when the passed class contains a field whose type is a nested bean with type arguments:

 
{code:java}
class A<T> {
   T value;
   // value getter and setter
}

class B {
   A<String> stringHolder;
   // stringHolder getter and setter
}

Encoders.bean(B.class); // throws ""SparkUnsupportedOperationException: [ENCODER_NOT_FOUND]...""{code}
 

 

It looks like this is a regression introduced with [SPARK-42093 SQL Move JavaTypeInference to AgnosticEncoders|https://github.com/apache/spark/commit/18672003513d5a4aa610b6b94dbbc15c33185d3#diff-1191737b908340a2f4c22b71b1c40ebaa0da9d8b40c958089c346a3bda26943b] while getting rid of TypeToken, that somehow managed that case.",,,,,,,,,,,,SPARK-45311,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 07 14:14:33 UTC 2023,,,,,,,,,,0|z1jjcg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,07/Aug/23 14:14;gbloisi-openaire;PR has been merged;;;,3.5.0,4.0.0,,,
"Encoder fails on many ""NoSuchElementException: None.get"" since 3.4.x, search for an encoder for a generic type, and since 3.5.x isn't ""an expression encoder""",SPARK-45311,13551860,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,mlebihan,mlebihan,9/25/23 8:46,12/15/23 4:52,7/17/24 20:45,12/15/23 4:52,"3.4.0, 3.4.1, 3.5.0","3.4.2, 3.5.1, 4.0.0",Spark Core,,0,,"If you find it convenient, you might clone the [https://gitlab.com/territoirevif/minimal-tests-spark-issue] project (that does many operations around cities, local authorities and accounting with open data) where I've extracted from my work what's necessary to make a set of 35 tests that run correctly with Spark 3.3.x, and show the troubles encountered with 3.4.x and 3.5.x.

 

It is working well with Spark 3.2.x, 3.3.x. But as soon as I selec{*}t Spark 3.4.x{*}, where the encoder seems to have deeply changed, the encoder fails with two problems:

 

*1)* It throws *java.util.NoSuchElementException: None.get* messages everywhere.

Asking over the Internet, I wasn't alone facing this problem. Reading it, you'll see that I've attempted a debug but my Scala skills are low.

[https://stackoverflow.com/questions/76036349/encoders-bean-doesnt-work-anymore-on-a-java-pojo-with-spark-3-4-0]

{color:#172b4d}by the way, if possible, the encoder and decoder functions should forward a parameter as soon as the name of the field being handled is known, and then all the long of their process, so that when the encoder is at any point where it has to throw an exception, it knows the field it is handling in its specific call and can send a message like:{color}
{color:#00875a}_java.util.NoSuchElementException: None.get when encoding [the method or field it was targeting]_{color}

 

*2)* *Not found an encoder of the type RS to Spark SQL internal representation.* Consider to change the input type to one of supported at (...)
Or : Not found an encoder of the type *OMI_ID* to Spark SQL internal representation (...)

 
where *RS* and *OMI_ID* are generic types.
This is strange.
[https://stackoverflow.com/questions/76045255/encoders-bean-attempts-to-check-the-validity-of-a-return-type-considering-its-ge]

 

*3)* When I switch to the *Spark 3.5.0* version, the same problems remain, but another add itself to the list:
""{*}Only expression encoders are supported for now{*}"" on what was accepted and working before.
 ","Debian 12

Java 17

Underlying Spring-Boot 2.7.14",,,,,,,,,,"SPARK-44634, SPARK-44910, SPARK-45081",,,,,,,,,,,,,,"24/Nov/23 05:21;mlebihan;JavaTypeInference_116.png;https://issues.apache.org/jira/secure/attachment/13064654/JavaTypeInference_116.png, 24/Nov/23 05:26;mlebihan;sparkIssue_02.png;https://issues.apache.org/jira/secure/attachment/13064655/sparkIssue_02.png",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Fri Dec 15 04:52:21 UTC 2023,,,,,,,,,,0|z1kk8g:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Nov/23 10:55;mlebihan;The problem *java.util.NoSuchElementException: None.get*   happens in `{color:#0033b3} {color}{color:#000000}JavaBeanEncoder{color}({color:#000000}tag{color}, {color:#000000}fields{color})` case  of 
`deserializerFor` method in `ScalaReflexion`

(sorry for the indentation that I had to change due to a bad copy-paste)

 
{code:java}
case JavaBeanEncoder(tag, fields) =>
      val setters = fields.map { f =>
            val newTypePath = walkedTypePath.recordField(
                f.enc.clsTag.runtimeClass.getName,
                f.name)
    val setter = expressionWithNullSafety(
        deserializerFor(
            f.enc,
            addToPath(path, f.name, f.enc.dataType, newTypePath),
            newTypePath),
          nullable = f.nullable,
          newTypePath)
     f.writeMethod.get -> setter
}
{code}
 
`f.writeMethod` is valued with `None` if it happens that the method observed, a `getSomething()` or `isSomething()` doesn't have a corresponding setter method `setSomething(...)`.

I cannot tell if it's the new wished behavior for Spark after the `3.4.x` or if it's a regression.

But until `3.3.x`, it was possible to have class having `isSomething()` methods, in order to do some checkings, without the need of having a setter.
----
 
The workaround is to rename a method not having a setter to `something()` only, without `get` or `is` prefix.
Then Spark will skip it.;;;, 20/Nov/23 09:52;gbloisi-openaire;This is likely a duplicate of https://issues.apache.org/jira/browse/SPARK-45081 a fix has already been applied to 3.4 branch and will be part of 3.4.2 release. There are also other issues about backward compatibility of java beans support https://issues.apache.org/jira/browse/SPARK-44910 and https://issues.apache.org/jira/browse/SPARK-44634 that have been recently fixed and will be part of next releases.;;;, 21/Nov/23 06:29;mlebihan;Thanks, that's a good news. I'll check if this 3.4.2 succeed in resolving the two first issues mentioned (I think it will) from the SNAPSHOT in preparation, and will close them as duplicate.

For the last one,

*3)* When I switch to the *Spark 3.5.0* version, the same problems remain, but another add itself to the list:
""{*}Only expression encoders are supported for now{*}"" on what was accepted and working before.

it's a new one, I should keep it, open, isn't it?
 ;;;, 22/Nov/23 20:25;gbloisi-openaire;I agree point 3) is a different problem that remains valid to report. Please note the fix here is 
to use Encoders.row(schema) instead of RowEncoder.encoderFor(schema). This is likely caused by the refactoring required for implementing ""Spark Connect"". ;;;, 22/Nov/23 21:22;mlebihan;I'll do a try for 3.5.0. I believe that I was forced to change from _{color:#000000}RowEncoder{color}.apply({color:#000000}schema{color}) (3.3.x and 3.4.x)_ to {color:#000000}_RowEncoder.encoderFor(schema)_ by the wish of the 3.5.0 version.{color}because _{color:#000000}RowEncoder{color}.apply_ doesn't exist anymore.

I didn't found an {{Encoders.row(...)}} method, one {{new Encoder(schema)}} was possible, but I found difficult to extract a {{{}ClassInfo<Row> getCls(){}}}.

I've found and changed for this, below, and it seems to reach the workaround you wrote about in 3.5.x. (I'll will continue to check, later) :
{code:java}
ExpressionEncoder<Row> encoder = ExpressionEncoder.apply(schema);

cible = cible.mapPartitions((MapPartitionsFunction<Row, Row>) it -> {
   List<Row> rows = new LinkedList<>();

   while (it.hasNext()) {
      [...]
      rows.add(RowFactory.create(valeurs.toArray()));
   }

   return rows.iterator();
}, encoder); {code}
 
----
I've experienced the latest 3.4.2-SNAPSHOT version available (refreshed my fork 19h ago)
to check for the problems related to  java.util.NoSuchElementException: None.get and the generic types. And it improves the execution greatly.

From 22 (or around) failing tests, for the 3.4.0 or 3.4.1,  the 3.4.2-SNAPSHOT faces 4 failures only:  but they look different than before.
{code:java}
-------------------------------------------------------------------------------
Test set: fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT
-------------------------------------------------------------------------------
Tests run: 6, Failures: 1, Errors: 3, Skipped: 0, Time elapsed: 8.709 s <<< FAILURE! - in fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT
catalogueJeuxDeDonneesEtRessources Time elapsed: 1.472 s <<< ERROR!
java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.lang.reflect.TypeVariable; ([Ljava.lang.Object; and [Ljava.lang.reflect.TypeVariable; are in module java.base of loader 'bootstrap')
at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.catalogueJeuxDeDonneesEtRessources(CatalogueDatagouvIT.java:161)

catalogueDatasetsObjetsMetiersPagines Time elapsed: 1.03 s <<< ERROR!
java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.lang.reflect.TypeVariable; ([Ljava.lang.Object; and [Ljava.lang.reflect.TypeVariable; are in module java.base of loader 'bootstrap')
at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.lambda$catalogueDatasetsObjetsMetiersPagines$0(CatalogueDatagouvIT.java:105)
at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.catalogueDatasetsObjetsMetiersPagines(CatalogueDatagouvIT.java:105)

catalogueJeuxDeDonnees Time elapsed: 0.043 s <<< ERROR!
java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.lang.reflect.TypeVariable; ([Ljava.lang.Object; and [Ljava.lang.reflect.TypeVariable; are in module java.base of loader 'bootstrap')
at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.catalogueJeuxDeDonnees(CatalogueDatagouvIT.java:143)

catalogueDatasetsObjetsMetiersPaginesStreames Time elapsed: 0.534 s <<< FAILURE!
org.opentest4j.AssertionFailedError: Unexpected exception thrown: java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.lang.reflect.TypeVariable; ([Ljava.lang.Object; and [Ljava.lang.reflect.TypeVariable; are in module java.base of loader 'bootstrap')
at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.catalogueDatasetsObjetsMetiersPaginesStreames(CatalogueDatagouvIT.java:126)
Caused by: java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.lang.reflect.TypeVariable; ([Ljava.lang.Object; and [Ljava.lang.reflect.TypeVariable; are in module java.base of loader 'bootstrap')
at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.lambda$catalogueDatasetsObjetsMetiersPaginesStreames$3(CatalogueDatagouvIT.java:127)
at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.lambda$catalogueDatasetsObjetsMetiersPaginesStreames$4(CatalogueDatagouvIT.java:127)
at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.catalogueDatasetsObjetsMetiersPaginesStreames(CatalogueDatagouvIT.java:126){code};;;, 23/Nov/23 10:43;gbloisi-openaire;Could you disable failsafe trimStackTrace (as explained in [https://stackoverflow.com/questions/42248856/how-to-get-the-full-stacktrace-of-failed-tests-in-failsafe] ) to get the full stack of the error and report it here? ;;;, 24/Nov/23 04:36;mlebihan;I've updated the  [https://gitlab.com/territoirevif/minimal-tests-spark-issue] testing project accordingly.
{code:java}
-------------------------------------------------------------------------------
Test set: fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT
-------------------------------------------------------------------------------
Tests run: 6, Failures: 1, Errors: 3, Skipped: 0, Time elapsed: 8.715 s <<< FAILURE! - in fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT
catalogueJeuxDeDonneesEtRessources  Time elapsed: 1.498 s  <<< ERROR!
java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.lang.reflect.TypeVariable; ([Ljava.lang.Object; and [Ljava.lang.reflect.TypeVariable; are in module java.base of loader 'bootstrap')
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:116)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.$anonfun$encoderFor$1(JavaTypeInference.scala:140)
    at scala.collection.ArrayOps$.map$extension(ArrayOps.scala:929)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:138)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:60)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:53)
    at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.javaBean(ExpressionEncoder.scala:62)
    at org.apache.spark.sql.Encoders$.bean(Encoders.scala:179)
    at org.apache.spark.sql.Encoders.bean(Encoders.scala)
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvJeuxDeDonneesDataset.catalogueDataset(CatalogueDatagouvJeuxDeDonneesDataset.java:100)
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvJeuxDeDonneesDataset.catalogueDataset(CatalogueDatagouvJeuxDeDonneesDataset.java:88)
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.catalogueJeuxDeDonneesEtRessources(CatalogueDatagouvIT.java:161)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
    at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
    at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)catalogueDatasetsObjetsMetiersPagines  Time elapsed: 0.954 s  <<< ERROR!
java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.lang.reflect.TypeVariable; ([Ljava.lang.Object; and [Ljava.lang.reflect.TypeVariable; are in module java.base of loader 'bootstrap')
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:116)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.$anonfun$encoderFor$1(JavaTypeInference.scala:140)
    at scala.collection.ArrayOps$.map$extension(ArrayOps.scala:929)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:138)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:60)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:53)
    at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.javaBean(ExpressionEncoder.scala:62)
    at org.apache.spark.sql.Encoders$.bean(Encoders.scala:179)
    at org.apache.spark.sql.Encoders.bean(Encoders.scala)
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvJeuxDeDonneesDataset.catalogueDataset(CatalogueDatagouvJeuxDeDonneesDataset.java:100)
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.lambda$catalogueDatasetsObjetsMetiersPagines$0(CatalogueDatagouvIT.java:105)
    at fr.ecoemploi.adapters.outbound.spark.dataset.core.Paginateur.toObjetMetierDataset(Paginateur.java:73)
    at fr.ecoemploi.adapters.outbound.spark.dataset.core.Paginateur.paginer(Paginateur.java:59)
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.catalogueDatasetsObjetsMetiersPagines(CatalogueDatagouvIT.java:105)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220)
    at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188)
    at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181)
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150)
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124)
    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)catalogueJeuxDeDonnees  Time elapsed: 0.059 s  <<< ERROR!
java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.lang.reflect.TypeVariable; ([Ljava.lang.Object; and [Ljava.lang.reflect.TypeVariable; are in module java.base of loader 'bootstrap')
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:116)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.$anonfun$encoderFor$1(JavaTypeInference.scala:140)
    at scala.collection.ArrayOps$.map$extension(ArrayOps.scala:929)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:138)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:60)
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:53)
    at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.javaBean(ExpressionEncoder.scala:62)
    at org.apache.spark.sql.Encoders$.bean(Encoders.scala:179)
    at org.apache.spark.sql.Encoders.bean(Encoders.scala)
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvJeuxDeDonneesDataset.catalogueDataset(CatalogueDatagouvJeuxDeDonneesDataset.java:100)
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvJeuxDeDonneesDataset.catalogueDataset(CatalogueDatagouvJeuxDeDonneesDataset.java:88)
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.catalogueJeuxDeDonnees(CatalogueDatagouvIT.java:143)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511)
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
    at org.junit.platform.engine.support.hierarchical",,,,,
ThrowableCollector.execute(ThrowableCollector.java:73),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)catalogueDatasetsObjetsMetiersPaginesStreames  Time elapsed: 0.529 s  <<< FAILURE!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
org.opentest4j.AssertionFailedError: Unexpected exception thrown: java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.lang.reflect.TypeVariable; ([Ljava.lang.Object; and [Ljava.lang.reflect.TypeVariable; are in module java.base of loader 'bootstrap'),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.api.AssertDoesNotThrow.createAssertionFailedError(AssertDoesNotThrow.java:83),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:54),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.catalogueDatasetsObjetsMetiersPaginesStreames(CatalogueDatagouvIT.java:126),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at java.base/java.lang.reflect.Method.invoke(Method.java:568),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at java.base/java.util.ArrayList.forEach(ArrayList.java:1511),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:220),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.launcher.core.DefaultLauncher.lambda$execute$6(DefaultLauncher.java:188),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.launcher.core.DefaultLauncher.withInterceptedStreams(DefaultLauncher.java:202),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:181),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:128),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:150),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:124),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Caused by: java.lang.ClassCastException: class [Ljava.lang.Object; cannot be cast to class [Ljava.lang.reflect.TypeVariable; ([Ljava.lang.Object; and [Ljava.lang.reflect.TypeVariable; are in module java.base of loader 'bootstrap'),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:116),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.spark.sql.catalyst.JavaTypeInference$.$anonfun$encoderFor$1(JavaTypeInference.scala:140),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at scala.collection.ArrayOps$.map$extension(ArrayOps.scala:929),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:138),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:60),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:53),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.javaBean(ExpressionEncoder.scala:62),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.spark.sql.Encoders$.bean(Encoders.scala:179),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.apache.spark.sql.Encoders.bean(Encoders.scala),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvJeuxDeDonneesDataset.catalogueDataset(CatalogueDatagouvJeuxDeDonneesDataset.java:100),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.lambda$catalogueDatasetsObjetsMetiersPaginesStreames$3(CatalogueDatagouvIT.java:127),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at fr.ecoemploi.adapters.outbound.spark.dataset.core.Paginateur.toObjetMetierDataset(Paginateur.java:73),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at fr.ecoemploi.adapters.outbound.spark.dataset.core.Paginateur.paginer(Paginateur.java:59),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at fr.ecoemploi.adapters.outbound.spark.dataset.core.Paginateur.paginerEnStream(Paginateur.java:42),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at fr.ecoemploi.adapters.outbound.spark.dataset.datagouv.CatalogueDatagouvIT.lambda$catalogueDatasetsObjetsMetiersPaginesStreames$4(CatalogueDatagouvIT.java:127),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
    ... 68 more,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 {code};;;, 24/Nov/23 05:29;mlebihan;A breakpoint in {{catalogueJeuxDeDonnees()}} test,  at :,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
{{org.apache.spark.sql.catalyst.JavaTypeInference$.encoderFor(JavaTypeInference.scala:116}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
!JavaTypeInference_116.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The caller of it :,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
!sparkIssue_02.png!,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
{{OMD_ID}} is a generic, compatible with {{{}CatalogueId{}}}.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ;;;, 26/Nov/23 23:38;gbloisi-openaire;The issue arise while Encoders.bean is inferring the schema for JeuDeDonnees class. This class has a field of type Ressources.class which extends a LinkedHashMap<RessourceJeuDeDonneesId, Ressource>.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A simple work-around to let the tests pass is to modify the JeuDeDonnees and declare ressources as a Map:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
{code:java},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
private Map<RessourceJeuDeDonneesId, Ressource> ressources; ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
//...,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
public Map<RessourceJeuDeDonneesId, Ressource> getRessources() {,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
//...{code},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
and, when required, iterate the values explicitly:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
{code:java},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
jeuDeDonnees.getRessources().values().forEach {code},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The exception is thrown because the code assumes (wrongly in that case) that if a class (such as Ressources.class) is a Map, then it has generic type information attached to it, here instead the information is available in the base/super class.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
There is a wider problem behind this. There are cases where mapping to a Spark schema would be ambigous, for example:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 * Ressources could have also getters and setters, should it be mapped as a map or a struct?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 * A class could implement both List and Map interfaces. should it be mapped as an array or a map?,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IMO the workaround is also a good idiomatic way to structure beans to be used with Spark, as it makes the mapping explicit and removes the possibility of ambiguities. ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ;;;, 27/Nov/23 04:39;mlebihan;Thanks.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I only had to change the :,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
{{{color:#0033b3}public{color} Ressources getRessources()}} getter to,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
{{{color:#0033b3}public {color}{color:#000000}Map{color}<{color:#000000}RessourceJeuDeDonneesId{color}, {color:#000000}Ressource{color}> getRessources()}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
to make it working. And change a test from {{jeuDeDonnees.getRessources().forEach((ressource) -> {color:#871094}LOGGER{color}.info(...))}} to {{jeuDeDonnees.getRessources().forEach((id, ressource) -> {color:#871094}LOGGER{color}.info(...))}},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
and now, all the troubles I had in this issue are solved or have found a workaround.;;;," 15/Dec/23 04:52;mlebihan;Resolved through the resolution of linked issues;;;""",3.4.1,3.5.0,,,"20/Nov/23 09:52;gbloisi-openaire;This is likely a duplicate of https://issues.apache.org/jira/browse/SPARK-45081 a fix has already been applied to 3.4 branch and will be part of 3.4.2 release. There are also other issues about backward compatibility of java beans support https://issues.apache.org/jira/browse/SPARK-44910 and https://issues.apache.org/jira/browse/SPARK-44634 that have been recently fixed and will be part of next releases.;;;, 15/Dec/23 04:52;mlebihan;Resolved through the resolution of linked issues;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
calculation error ,SPARK-46362,13561359,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,paridee,paridee,12/11/23 10:47,12/12/23 13:23,7/17/24 20:45,,3.4.1,,SQL,,0,,"Hi,

I had wrong values while using the POW function in SPARK SQL, please see this example (in this example I multiply (10^-2)*25051 and the expected result is 250.51

 

from pyspark.sql.functions import lit
df = spark.range(1)
df.createOrReplaceTempView(""TEST"")
spark.sql(""SELECT POWER(10, -2)*25051 FROM TEST"").show()

 

but I got 250.51000000000002

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 12 13:23:46 UTC 2023,,,,,,,,,,0|z1m6h4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Dec/23 13:23;apeng;[~paridee] This is not a defect of Spark, it's related to the representation of floating-point numbers.

More details [here|https://stackoverflow.com/questions/70404164/the-calculation-accuracy-of-floating-point-numbers-float-double-in-java-ieee].;;;",,,,,
Return null when overflowing during casting from timestamp to integers,SPARK-45816,13556981,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,11/7/23 5:52,12/11/23 20:49,7/17/24 20:45,11/8/23 12:23,"3.3.3, 3.4.1, 3.5.0",4.0.0,SQL,,0,pull-request-available,"Spark cast works in two modes: ansi and non-ansi. When overflowing during casting, the common behavior under non-ansi mode is to return null. However, casting from Timestamp to Int/Short/Byte returns a wrapping value now. The behavior to silently overflow doesn't make sense.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 08 12:23:54 UTC 2023,,,,,,,,,,0|z1lfso:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"08/Nov/23 12:23;beliefer;Issue resolved by pull request 43694
[https://github.com/apache/spark/pull/43694];;;",3.4.1,3.5.0,,,
Cannot deploy Spark application using VolcanoFeatureStep to specify podGroupTemplate file ,SPARK-46310,13561009,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,lsbx96_,lsbx96_,12/7/23 14:59,12/11/23 12:35,7/17/24 20:45,,3.4.1,,Kubernetes,,0,,"I'm trying to deploy a Spark application (version 3.4.1) on Kubernetes using Volcano as the scheduler. I define a [VolcanoJob|https://volcano.sh/en/docs/vcjob/] that represents the Spark driver - it has only one task, whose pod specification includes the driver container, which invokes the spark-submit command.

Following the official Spark documentation (available on ""[Using Volcano as Customized Scheduler for Spark on Kubernetes|https://spark.apache.org/docs/latest/running-on-kubernetes.html#using-volcano-as-customized-scheduler-for-spark-on-kubernetes]""), I define the necessary configuration parameters to make use of Volcano as the scheduler for my Spark workload:
{code:java}
/opt/spark/bin/spark-submit --name ""volcano-spark-1"" --deploy-mode=""client"" \
--class ""org.apache.spark.examples.SparkPi"" \
--conf spark.executor.instances=""1"" \
--conf spark.kubernetes.driver.pod.featureSteps=""org.apache.spark.deploy.k8s.features.VolcanoFeatureStep"" \
--conf spark.kubernetes.executor.pod.featureSteps=""org.apache.spark.deploy.k8s.features.VolcanoFeatureStep"" \
--conf spark.kubernetes.scheduler.volcano.podGroupTemplateFile=""/var/template/podgroup.yaml"" \
file:///opt/spark/examples/jars/spark-examples_2.12-3.4.1.jar
{code}
In the block above, I omitted some Kubernetes configuration parameters that aren't important for this example. The parameter *{{spark.kubernetes.scheduler.volcano.podGroupTemplateFile}}* points to a file mounted in the driver container. It has a content just as the following example (cpu / memory values may vary):
{code:yaml}
apiVersion: scheduling.volcano.sh/v1beta1
kind: PodGroup
metadata: 
  name: pod-group-test
spec: 
  minResources: 
    cpu: ""2""
    memory: ""2Gi""
  queue: some-existing-queue
{code}
I manually verified that this file ""/var/template/podgroup.yaml"" exists in the container before the ""spark-submit"" command is issued. I also granted all the necessary RBAC permissions so that the driver pod can interact with Kubernetes objects (pods, VolcanoJobs, podgroups, queues, etc.).

When I execute this VolcanoJob, I see only the driver pod being created, and when inspecting its logs, I see the following error:
{code:java}
io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://api.<masked-environment-endpoint>/api/v1/namespaces/04522055-15b3-40d8-ba07-22b1a2a5ffcc/pods. Message: admission webhook ""validatepod.volcano.sh"" denied the request: failed to get PodGroup for pod <04522055-15b3-40d8-ba07-22b1a2a5ffcc/volcano-spark-1-driver-0-exec-789>: podgroups.scheduling.volcano.sh ""spark-5ad570e340934d3997065fa6d504910e-podgroup"" not found. Received status: Status(apiVersion=v1, code=400, details=null, kind=Status, message=admission webhook ""validatepod.volcano.sh"" denied the request: failed to get PodGroup for pod <04522055-15b3-40d8-ba07-22b1a2a5ffcc/volcano-spark-1-driver-0-exec-789>: podgroups.scheduling.volcano.sh ""spark-5ad570e340934d3997065fa6d504910e-podgroup"" not found, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=null, status=Failure, additionalProperties={}).
	at io.fabric8.kubernetes.client.KubernetesClientException.copyAsCause(KubernetesClientException.java:238)
	at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:538)
	at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:558)
	at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleCreate(OperationSupport.java:349)
	at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:711)
	at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:93)
	at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)
	at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:1113)
	at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:93)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:440)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:417)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:370)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:363)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:363)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:134)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:134)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:143)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:131)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl.$anonfun$addSubscriber$1(ExecutorPodsSnapshotsStoreImpl.scala:85)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:182)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:296)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:838)
{code}
The error seems to be triggered when the driver attempts to deploy the executors of my Spark application. In the error message, it says that the podGroup ""spark-5ad570e340934d3997065fa6d504910e-podgroup"" cannot be found (pointed out by the Volcano admission hook).

I was expecting that the driver and executors would be assigned to the same PodGroup object, created by the VolcanoFeatureStep using the template file that I provided through the configuration parameter ""{*}{{spark.kubernetes.scheduler.volcano.podGroupTemplateFile}}{*}"". With that, I would have a proper batch scheduling of my Spark application, as driver and executor pods would reside in the same pod group, and would be scheduled together by Volcano. But instead, only the driver pod is deployed, and the error seen above is found on its logs.

The documentation ""[Using Volcano as Customized Scheduler for Spark on Kubernetes|https://spark.apache.org/docs/latest/running-on-kubernetes.html#using-volcano-as-customized-scheduler-for-spark-on-kubernetes]"" leads me to understand that by providing the PodGroup template file, my Spark application (i.e., driver and executors) would be allocated in the same PodGroup object, following the specification I provided. That doesn't seem to be the case, and it looks like the PodGroup isn't created following the provided template, nor can the executors be created.

Some more details about the environment I used:
 - Volcano Version: v1.8.0
 - Spark Version: 3.4.1
 - Kubernetes version: v1.26.7
 - Cloud provider: GCP",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 11 12:35:20 UTC 2023,,,,,,,,,,0|z1m4bc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"11/Dec/23 12:35;lsbx96_;Just saw [https://github.com/volcano-sh/volcano/issues/3250]  - can someone please confirm that VolcanoFeatureStep doesn't support Spark in client mode? If that's the case, what would be the alternative for using Volcano with client mode deployment?;;;",,,,,
DiskBlockManager should check and be able to handle stale directories,SPARK-44632,13545788,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,yao,yao,8/2/23 4:14,12/11/23 0:19,7/17/24 20:45,,"3.4.1, 3.5.0",,Spark Core,,0,pull-request-available,"The subDir in the memory cache could be stale, for example, after a damaged disk repair or replacement. This dir could be accessed subsequently by others. Especially,  `filename` generated by `RDDBlockId` is unchanged between task reties, so it probably attempts to access the same subDir repeatedly. Therefore, it is necessary to check if the subDir exists. If it is stale and the hardware has been recovered without data and directories, we will recreate the subDir to prevent FileNotFoundException during writing.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 31 09:20:28 UTC 2023,,,,,,,,,,0|z1jj4o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"31/Aug/23 09:20;githubbot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42287;;;",3.5.0,,,,
Hide Jetty info ,SPARK-46239,13560433,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenyu-opensource,chenyu-opensource,chenyu-opensource,12/4/23 9:23,12/5/23 1:38,7/17/24 20:45,12/4/23 22:42,"3.0.3, 3.1.3, 3.2.4, 3.3.2, 3.4.1, 3.5.0","3.3.4, 3.4.3, 3.5.1, 4.0.0",Spark Core,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,04/Dec/23 09:23;chenyu-opensource;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13064936/screenshot-1.png,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 04 22:42:09 UTC 2023,,,,,,,,,,0|z1m0rc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"04/Dec/23 09:24;chenyu-opensource;It is unsafe to expose version information.

It will obtain remote WWW service information through HTTP.;;;, 04/Dec/23 22:42;dongjoon;Issue resolved by pull request 44158
[https://github.com/apache/spark/pull/44158];;;",3.1.3,3.2.4,3.3.2,3.4.1,"04/Dec/23 22:42;dongjoon;Issue resolved by pull request 44158
[https://github.com/apache/spark/pull/44158];;;"
Shuffle data lost on decommissioned executor caused by race condition between lastTaskRunningTime and lastShuffleMigrationTime,SPARK-46182,13559998,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,11/30/23 3:09,12/4/23 19:00,7/17/24 20:45,12/4/23 6:09,"3.4.1, 3.5.0","3.4.3, 3.5.1, 4.0.0",Spark Core,,0,pull-request-available,"We recently identified a very tricky race condition in decommissioned node, which could lead to shuffle data lost even data migration is enabled:

* At 04:30:51, RDD block refresh happened, and found no pending works
* Shortly after that (a few milliseconds), the shutdownThread in CoarseGrainedExecutorBackend found 1 running task, so lastTaskRunningTime updated to the current system nano time
* Shortly after that, Shuffle block refresh happened, and found no pending works
* Shortly after that, a task finished on the decommissioned executor, and generated new shuffle blocks
* One second later, the shutdownThread in CoarseGrainedExecutorBackend found no running task, lastTaskRunningTime would not be updated, and the executor didn’t exit because min(lastRDDMigrationTime, lastShuffleMigrationTime) <  lastTaskRunningTime
* After 30 seconds, at 04:31:21, RDD block refresh happened, and found no pending works, lastRDDMigrationTime updated to the current system nano time
* At this exact moment, all known blocks are migrated, and min(lastRDDMigrationTime, lastShuffleMigrationTime) > lastTaskRunningTime
* shutdownThread is triggered, and asked to stop the executor
* Shuffle block refresh thread was still sleeping, and got interrupted by the stop command, so it didn’t have the chance to discover the shuffle blocks generated by the previously finished task
* Eventually, the executor exited, and the output of the task was lost, Spark need to recompute that partition

The root cause for the race condition is that the Shuffle block refresh happened between lastTaskRunningTime was updated and task finished, in that case the shutdownThread could request to stop the executor before the BlockManagerDecommissioner discover the new shuffle blocks generated by the latest finished task.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 04 06:09:18 UTC 2023,,,,,,,,,,0|z1ly2o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"04/Dec/23 06:09;dongjoon;Issue resolved by pull request 44090
[https://github.com/apache/spark/pull/44090];;;",3.5.0,,,,
Expression encoding fails for Seq/Map of Option[Seq/Date/Timestamp/BigDecimal],SPARK-45896,13557588,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,11/11/23 18:47,12/4/23 16:27,7/17/24 20:45,11/12/23 22:36,"3.4.1, 3.5.0","3.4.2, 3.5.1, 4.0.0",SQL,,0,pull-request-available,"The following action fails on 3.4.1, 3.5.0, and master:
{noformat}
scala> val df = Seq(Seq(Some(Seq(0)))).toDF(""a"")
val df = Seq(Seq(Some(Seq(0)))).toDF(""a"")
org.apache.spark.SparkRuntimeException: [EXPRESSION_ENCODING_FAILED] Failed to encode a value of the expressions: mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -2), assertnotnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -2), IntegerType, IntegerType)), unwrapoption(ObjectType(interface scala.collection.immutable.Seq), validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), ArrayType(IntegerType,false), ObjectType(class scala.Option))), None), input[0, scala.collection.immutable.Seq, true], None) AS value#0 to a row. SQLSTATE: 42846
...
Caused by: java.lang.RuntimeException: scala.Some is not a valid external type for schema of array<int>
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.MapObjects_0$(Unknown Source)
...
{noformat}
However, it succeeds on 3.3.3:
{noformat}
scala> val df = Seq(Seq(Some(Seq(0)))).toDF(""a"")
df: org.apache.spark.sql.DataFrame = [a: array<array<int>>]

scala> df.collect
res0: Array[org.apache.spark.sql.Row] = Array([WrappedArray(WrappedArray(0))])
{noformat}
Map of Option[Seq] also fails on 3.4.1, 3.5.0, and master:
{noformat}
scala> val df = Seq(Map(0 -> Some(Seq(0)))).toDF(""a"")
val df = Seq(Map(0 -> Some(Seq(0)))).toDF(""a"")
org.apache.spark.SparkRuntimeException: [EXPRESSION_ENCODING_FAILED] Failed to encode a value of the expressions: externalmaptocatalyst(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), false, -1), assertnotnull(validateexternaltype(lambdavariable(ExternalMapToCatalyst_key, ObjectType(class java.lang.Object), false, -1), IntegerType, IntegerType)), lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -3), assertnotnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -3), IntegerType, IntegerType)), unwrapoption(ObjectType(interface scala.collection.immutable.Seq), validateexternaltype(lambdavariable(ExternalMapToCatalyst_value, ObjectType(class java.lang.Object), true, -2), ArrayType(IntegerType,false), ObjectType(class scala.Option))), None), input[0, scala.collection.immutable.Map, true]) AS value#0 to a row. SQLSTATE: 42846
...
Caused by: java.lang.RuntimeException: scala.Some is not a valid external type for schema of array<int>
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.MapObjects_0$(Unknown Source)
...
{noformat}
As with the first example, this succeeds on 3.3.3:
{noformat}
scala> val df = Seq(Map(0 -> Some(Seq(0)))).toDF(""a"")
df: org.apache.spark.sql.DataFrame = [a: map<int,array<int>>]

scala> df.collect
res0: Array[org.apache.spark.sql.Row] = Array([Map(0 -> WrappedArray(0))])
{noformat}
Other cases the fail on 3.4.1, 3.5.0, and master but work fine on 3.3.3:
- {{Seq[Option[Timestamp]]}}
- {{Map[Option[Timestamp]]}}
- {{Seq[Option[Date]]}}
- {{Map[Option[Date]]}}
- {{Seq[Option[BigDecimal]]}}
- {{Map[Option[BigDecimal]]}}

However, the following work fine on 3.3.3, 3.4.1, 3.5.0, and master:

- {{Seq[Option[Map]]}}
- {{Map[Option[Map]]}}
- {{Seq[Option[<primitive-type>]]}}
- {{Map[Option[<primitive-type>]]}}",,,,,,,,,,,,,,,,,,SPARK-45644,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 11 23:02:30 UTC 2023,,,,,,,,,,0|z1ljjc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,11/Nov/23 23:02;bersprockets;I think I have a handle on this and will make a PR shortly.;;;,3.5.0,,,,
"CONV produces incorrect result near Long.MIN_VALUE, fails to detect overflow",SPARK-44943,13548480,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jira.shegalov,jira.shegalov,8/24/23 7:49,12/4/23 0:19,7/17/24 20:45,,"3.4.1, 3.5.0, 4.0.0",,SQL,,0,pull-request-available,"Signed conversion does not detect overflow 
{code:java}
>>> spark.conf.set('spark.sql.ansi.enabled', True)
>>> sql(""SELECT conv('-9223372036854775809', 10, -10)"").show(truncate=False)
+-----------------------------------+
|conv(-9223372036854775809, 10, -10)|
+-----------------------------------+
|-9223372036854775807               |
+-----------------------------------+
{code}

Unsigned conversion produces -1 but does not throw in the ANSI mode
{code}
>>> sql(""SELECT conv('-9223372036854775809', 10, 10)"").show(truncate=False)
+----------------------------------+
|conv(-9223372036854775809, 10, 10)|
+----------------------------------+
|18446744073709551615              |
+----------------------------------+
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 09:20:15 UTC 2023,,,,,,,,,,0|z1jzdk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"24/Aug/23 09:19;githubbot;User 'gerashegalov' has created a pull request for this issue:
https://github.com/apache/spark/pull/42652;;;, 24/Aug/23 09:20;githubbot;User 'gerashegalov' has created a pull request for this issue:
https://github.com/apache/spark/pull/42652;;;",3.5.0,4.0.0,,,"24/Aug/23 09:20;githubbot;User 'gerashegalov' has created a pull request for this issue:
https://github.com/apache/spark/pull/42652;;;"
Various Pandas functions fail in interpreted mode,SPARK-46189,13560133,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,11/30/23 21:58,12/1/23 2:31,7/17/24 20:45,12/1/23 2:29,"3.4.1, 3.5.0","3.4.3, 3.5.1, 4.0.0","Pandas API on Spark, SQL",,0,pull-request-available,"Various Pandas functions ({{kurt}}, {{var}}, {{skew}}, {{cov}}, and {{stddev}}) fail with an unboxing-related exception when run in interpreted mode.

Here are some reproduction cases for pyspark interactive mode:
{noformat}
spark.sql(""set spark.sql.codegen.wholeStage=false"")
spark.sql(""set spark.sql.codegen.factoryMode=NO_CODEGEN"")

import numpy as np
import pandas as pd

import pyspark.pandas as ps

pser = pd.Series([1, 2, 3, 7, 9, 8], index=np.random.rand(6), name=""a"")
psser = ps.from_pandas(pser)

# each of the following actions gets an unboxing error
psser.kurt()
psser.var()
psser.skew()

# set up for covariance test
pdf = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)], columns=[""a"", ""b""])
psdf = ps.from_pandas(pdf)

# this gets an unboxing error
psdf.cov()

# set up for stddev resr
from pyspark.pandas.spark import functions as SF
from pyspark.sql.functions import col
from pyspark.sql import Row
df = spark.createDataFrame([Row(a=1), Row(a=2), Row(a=3), Row(a=7), Row(a=9), Row(a=8)])

# this gets an unboxing error
df.select(SF.stddev(col(""a""), 1)).collect()
{noformat}
Exception from the first case ({{psser.kurt()}}) is
{noformat}
java.lang.ClassCastException: class java.lang.Integer cannot be cast to class java.lang.Double (java.lang.Integer and java.lang.Double are in module java.base of loader 'bootstrap')
	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:112)
	at org.apache.spark.sql.catalyst.types.PhysicalDoubleType$$anonfun$2.compare(PhysicalDataType.scala:184)
	at scala.math.Ordering.lt(Ordering.scala:98)
	at scala.math.Ordering.lt$(Ordering.scala:98)
	at org.apache.spark.sql.catalyst.types.PhysicalDoubleType$$anonfun$2.lt(PhysicalDataType.scala:184)
	at org.apache.spark.sql.catalyst.expressions.LessThan.nullSafeEval(predicates.scala:1196)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 01 02:29:54 UTC 2023,,,,,,,,,,0|z1lywo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"01/Dec/23 02:29;podongfeng;Issue resolved by pull request 44099
[https://github.com/apache/spark/pull/44099];;;",3.5.0,,,,
ANSI Double quoted identifiers do not work in Python threads,SPARK-46190,13560134,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,mpayson,mpayson,11/30/23 22:30,11/30/23 22:30,7/17/24 20:45,,"3.4.0, 3.4.1, 3.5.0",,PySpark,,0,"ansi-sql, python, threads","Enabling and using `spark.sql.ansi.doubleQuotedIdentifiers` does not work correctly in Python threads

The following example shows how applying a filter, ""\""status\"" = 'Unchanged'"", leads to empty results when run in a thread. I believe this is because the ""status"" field is interpreted as a literal in the thread, but as an attribute outside of it.
{code:python}
from concurrent import futures
from pyspark import sql

spark = (
  sql.SparkSession.builder.master(""local[*]"")
  .config(""spark.sql.ansi.enabled"", ""true"")
  .config(""spark.sql.ansi.doubleQuotedIdentifiers"", ""true"")
  .getOrCreate()
)

def demonstrate_issue(spark):
  # Path to JSON file with contents:
  # [{""status"": ""Unchanged""}, {""status"": ""Changed""}]
  df = spark.read.json(""data/example.json"")
  df.filter(""\""status\"" = 'Unchanged'"").show()

# Shows 1 record, expected
demonstrate_issue(spark)

with futures.ThreadPoolExecutor(1) as executor:
  # Shows 0 records, unexpected
  executor.submit(demonstrate_issue, spark)
 {code}
 

Additional testing notes:
 * When parsing the expression with `sql.functions.expr` in Java via Py4J, the ""status"" field is interpreted as a literal value from the thread, not an attribute
 * Using double quotes with `spark.sql` does work in the thread
 * Using a dataframe created in memory does work in the thread
 * Tested in versions 3.4.0, 3.4.1, & 3.5.0 on Windows and Mac

 

The original PR that added this option is here: [https://github.com/apache/spark/pull/38022]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,30:57.0,,,,,,,,,,0|z1lyww:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,
can't return prediction that has different length than ml input,SPARK-46175,13559971,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,rbavery,rbavery,11/29/23 19:15,11/29/23 19:16,7/17/24 20:45,,3.4.1,,"MLlib, PySpark",,0,,"I'm using

 
from pyspark.ml.functions import predict_batch_udf
to construct a pandas udf that runs a computer vision model to predict classification labels for images. The model takes a 4D array as input and returns a 4D array as output (Batch, Channels, Height, Width)
 
However I'd like to run some additional processing in the pandas_udf to convert the 4D output array (floats) to text labels since this is a more useful output and we are trying to register pandas_udfs ahead of time for spark.sql users.
 
 
When I set the return type to a StringType though I get an error
 
```
23/11/29 02:43:04 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 16) (172.18.0.2 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last): File ""/opt/spark/python/pyspark/ml/functions.py"", line 809, in predict yield _validate_and_transform_prediction_result( File ""/opt/spark/python/lib/pyspark.zip/pyspark/ml/functions.py"", line 331, in _validate_and_transform_prediction_result raise ValueError(""Prediction results must have same length as input data."") ValueError: Prediction results must have same length as input data.
```
 
This seems like an unnecessary limitation, since it is common for ML models, especially in computer vision, to take input shapes different from output shapes.
 
I think the issue is here: [https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/functions.html]
 
Can this check that enforces same shape be removed?
 
 
to illustrate the problem, here are my StructTypes. The Raw one works but the Str one does not
 
```
from collections import namedtuple
from pyspark.sql.types import ArrayType, IntegerType, StringType,StructType, StructField, FloatType

Task = namedtuple('TaskSchema', ['inference_input', 'inference_result'])

SingleLabelClassificationRaw = Task(
inference_input=StructType([
StructField(""array"", ArrayType(IntegerType()), nullable=False),
StructField(""shape"", ArrayType(IntegerType()), nullable=False)
]),
inference_result=ArrayType(FloatType())
)

SingleLabelClassificationStr = Task(
inference_input=StructType([
StructField(""array"", ArrayType(IntegerType()), nullable=False),
StructField(""shape"", ArrayType(IntegerType()), nullable=False)
]),
inference_result=StringType()
)
```",I'm on spark 3.4,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,15:14.0,,,,,,,,,,0|z1lxwo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnAllocator miss clean targetNumExecutorsPerResourceProfileId after YarnSchedulerBackend call stop,SPARK-46006,13558671,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,11/20/23 9:53,11/28/23 3:06,7/17/24 20:45,11/22/23 8:52,"3.1.3, 3.2.4, 3.3.2, 3.4.1, 3.5.0","3.3.4, 3.4.2, 3.5.1, 4.0.0",YARN,,0,pull-request-available,"We meet a case that user call sc.stop() after run all custom code, but stuck in some place. 

Cause below situation
 # User call sc.stop()
 # sc.stop() stuck in some process, but SchedulerBackend.stop was called
 # Since tarn ApplicationMaster didn't finish， still call YarnAllocator.allocateResources()
 # Since driver endpoint stop new allocated executor failed to register
 # untll trigger Max number of executor failures

Caused by 

Before call CoarseGrainedSchedulerBackend.stop() will call YarnSchedulerBackend.requestTotalExecutor() to clean request info

!image-2023-11-20-17-56-56-507.png|width=898,height=297!

 

From the log we make sure that CoarseGrainedSchedulerBackend.stop()  was called

 

 

When YarnAllocator handle then empty resource request,  since resourceTotalExecutorsWithPreferedLocalities is empty, miss clean targetNumExecutorsPerResourceProfileId.

!image-2023-11-20-17-56-45-212.png|width=708,height=379!

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/23 09:56;angerszhuuu;image-2023-11-20-17-56-45-212.png;https://issues.apache.org/jira/secure/attachment/13064567/image-2023-11-20-17-56-45-212.png, 20/Nov/23 09:56;angerszhuuu;image-2023-11-20-17-56-56-507.png;https://issues.apache.org/jira/secure/attachment/13064568/image-2023-11-20-17-56-56-507.png",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 22 08:52:36 UTC 2023,,,,,,,,,,0|z1lpwo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"22/Nov/23 08:52;yao;Issue resolved by pull request 43906
[https://github.com/apache/spark/pull/43906];;;",3.2.4,3.3.2,3.4.1,3.5.0,
Add support for MSK IAM to the kafka connector,SPARK-44285,13542351,,New Feature,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,nihal.pot,nihal.pot,7/3/23 23:18,11/27/23 14:59,7/17/24 20:45,,3.4.1,,Structured Streaming,,0,pull-request-available,"Managed Streaming for Kafka (MSK) is a fully managed service offered by Amazon Web Services (AWS) that simplifies the deployment, management, and scaling of Apache Kafka clusters. Kafka is an open-source streaming platform widely used for building real-time data pipelines and streaming applications. MSK provides a managed environment for running Kafka clusters, reducing the operational overhead associated with self-managing Kafka infrastructure.

Currently when we connect to MSK using spark, users would have to install an external library and then try to connect with MSK IAM auth. To simplify this for users, we can add this as a dependency directly to Spark and then users can connect to it by simply specifying the class paths directly.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 23:19:27 UTC 2023,,,,,,,,,,0|z1ixzs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,03/Jul/23 23:19;nihal.pot;Working on it in [https://github.com/apache/spark/pull/41791] ;;;,,,,,
Restore documentation for DSv2 API,SPARK-45963,13558315,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,11/16/23 23:53,11/24/23 3:49,7/17/24 20:45,11/17/23 21:04,"3.4.1, 3.5.0, 4.0.0","3.4.2, 3.5.1, 4.0.0","Documentation, SQL",,0,pull-request-available,DSv2 documentation is mistakenly gone after https://github.com/apache/spark/pull/38392. It used to exist in 3.3.0: https://spark.apache.org/docs/3.3.0/api/scala/org/apache/spark/sql/connector/catalog/index.html,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 17 21:04:34 UTC 2023,,,,,,,,,,0|z1lo0w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"17/Nov/23 21:04;dongjoon;Issue resolved by pull request 43865
[https://github.com/apache/spark/pull/43865];;;",3.5.0,4.0.0,,,
CTE reference node does not inherit the flag `isStreaming` from CTE definition node,SPARK-46062,13559131,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,11/23/23 2:28,11/23/23 14:33,7/17/24 20:45,11/23/23 14:33,"3.3.2, 3.4.1, 3.5.0, 4.0.0","3.4.2, 3.5.1, 4.0.0",SQL,,0,pull-request-available,"Looks like this is a long standing bug.

We figured out that CTE reference node would never set the isStreaming flag to true, regardless of the value for flag in CTE definition. The node cannot determine the right value of isStreaming flag by itself (likewise it cannot determine about resolution by itself) but it has no parameter in constructor, hence always takes the default (no children, so batch one).

This may impact some rules which behaves differently depending on isStreaming flag. It would no longer be a problem once CTE reference is replaced with CTE definition at some point in ""optimization phase"", but all rules in analyzer and optimizer being triggered before the rule takes effect may be impacted.

We probably couldn't sync the flag in real time, but we should sync the flag when we mark CTE reference to be ""resolved"". The rule `ResolveWithCTE` will be a good place to do that.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 23 14:33:11 UTC 2023,,,,,,,,,,0|z1lsqg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Nov/23 02:28;kabhwan;Working on the PR. Will submit a PR sooner.;;;, 23/Nov/23 14:33;kabhwan;Issue resolved by pull request 43966
[https://github.com/apache/spark/pull/43966];;;",3.4.1,3.5.0,4.0.0,,"23/Nov/23 14:33;kabhwan;Issue resolved by pull request 43966
[https://github.com/apache/spark/pull/43966];;;"
EliminateEventTimeWatermark does not consider the fact that isStreaming flag can change for current child during resolution,SPARK-46064,13559147,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,11/23/23 5:05,11/23/23 11:26,7/17/24 20:45,11/23/23 11:26,"3.3.2, 3.4.1, 3.5.0, 4.0.0","3.4.2, 3.5.1, 4.0.0",Structured Streaming,,0,pull-request-available,"Looks like this is a long standing bug.

The object `EliminateEventTimeWatermark` is implemented as a rule, but it is not registered in analyzer/optimizer. Instead, it is called directly when withWatermark method is called, which means the rule is applied immediately against the child, regardless whether child is resolved or not.

It is not an issue for the usage of pure DataFrame API because streaming sources have the flag isStreaming set to true even it is yet resolved, but mix-up of SQL and DataFrame API would expose the issue; we may not know the exact value of isStreaming flag on unresolved node and it is subject to change upon resolution.

We should register EliminateEventTimeWatermark as a rule on analysis (or pre-optimization) instead, and do not apply the elimination if the child is not yet resolved.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 23 11:26:38 UTC 2023,,,,,,,,,,0|z1lsu0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Nov/23 11:26;kabhwan;Issue resolved by pull request 43971
[https://github.com/apache/spark/pull/43971];;;",3.4.1,3.5.0,4.0.0,,
EventLogFileReader should not read rolling logs if appStatus is missing,SPARK-46012,13558753,13557406,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,11/20/23 20:59,11/22/23 3:27,7/17/24 20:45,11/21/23 1:53,"3.0.0, 3.1.3, 3.2.4, 3.3.3, 3.4.1","3.3.4, 3.4.2, 3.5.1, 4.0.0",Spark Core,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 21 01:53:14 UTC 2023,,,,,,,,,,0|z1lqew:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"21/Nov/23 01:53;dongjoon;Issue resolved by pull request 43914
[https://github.com/apache/spark/pull/43914];;;",3.1.3,3.2.4,3.3.3,3.4.1,
Plugin API for PySpark and SparkR workers,SPARK-44767,13546881,,New Feature,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,rshkv,rshkv,8/10/23 21:41,11/22/23 0:19,7/17/24 20:42,,3.4.1,,Spark Core,,0,pull-request-available,"An API to customize Python and R workers allows for extensibility beyond what can be expressed via static configs and environment variables like, e.g., {{spark.pyspark.python}}.

A use case for this is overriding {{PATH}} when using {{spark.archives}} with, say, conda-pack (as documented [here|https://spark.apache.org/docs/3.1.1/api/python/user_guide/python_packaging.html#using-conda]). Some packages rely on binaries. And if we want to use those packages in Spark, we need to include their binaries in the {{PATH}}.

But we can't set the {{PATH}} via some config because 1) the environment with its binaries may be at a dynamic location (archives are unpacked on the driver [into a directory with random name|https://github.com/apache/spark/blob/5db87787d5cc1cefb51ec77e49bac7afaa46d300/core/src/main/scala/org/apache/spark/SparkFiles.scala#L33-L37]), and 2) we may not want to override the {{PATH}} that's pre-configured on the hosts.

Other use cases unlocked by this include overriding the executable dynamically (e.g., to select a version) or forking/redirecting the worker's output stream.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 21 10:48:02 UTC 2023,,,,,,,,,,0|z1jpvk:,9223372036854775807,,,,,,,,,,,,,4.0.0,,,,,,,,"10/Aug/23 22:16;rshkv;I put up a proposal implementation here: https://github.com/apache/spark/pull/42440;;;, 21/Nov/23 10:48;rshkv;[~gurwls223], curious what you think about this proposal. I know you're leaning towards dynamic environment selection for Spark Connect [(apache/spark#41215)|https://github.com/apache/spark/pull/41215] instead of relying on a single environment per Spark application or per host. 

At Palantir, we use conda-pack based environments with {{spark.archives}}. But that wasn't sufficient to make native library dependencies work. Internally, we implemented a {{ProcessBuilder}} plugin (using the [proposed API|https://github.com/apache/spark/pull/42440]). Among other things we use it to append the environment's {{bin/}} to the process' {{PATH}} variable or to discover Python module and non-Python binary locations outside the packaged environment.;;;",,,,,"21/Nov/23 10:48;rshkv;[~gurwls223], curious what you think about this proposal. I know you're leaning towards dynamic environment selection for Spark Connect [(apache/spark#41215)|https://github.com/apache/spark/pull/41215] instead of relying on a single environment per Spark application or per host. 

At Palantir, we use conda-pack based environments with {{spark.archives}}. But that wasn't sufficient to make native library dependencies work. Internally, we implemented a {{ProcessBuilder}} plugin (using the [proposed API|https://github.com/apache/spark/pull/42440]). Among other things we use it to append the environment's {{bin/}} to the process' {{PATH}} variable or to discover Python module and non-Python binary locations outside the packaged environment.;;;"
Fix ArrayIndexOutOfBoundsException in conv(),SPARK-44973,13548693,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,markj-db,jira.shegalov,jira.shegalov,8/25/23 21:05,11/21/23 22:16,7/17/24 20:45,11/21/23 19:39,"3.0.3, 3.3.3, 3.4.1, 3.5.0","3.3.4, 3.4.2, 3.5.1, 4.0.0",SQL,,0,pull-request-available,"
{code:scala}
scala> sql(s""SELECT CONV('${Long.MinValue}', 10, -2)"").show(false)
java.lang.ArrayIndexOutOfBoundsException: -1
  at org.apache.spark.sql.catalyst.util.NumberConverter$.convert(NumberConverter.scala:183)
  at org.apache.spark.sql.catalyst.expressions.Conv.nullSafeEval(mathExpressions.scala:463)
  at org.apache.spark.sql.catalyst.expressions.TernaryExpression.eval(Expression.scala:821)
  at org.apache.spark.sql.catalyst.expressions.ToPrettyString.eval(ToPrettyString.scala:57)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.org$apache$spark$sql$catalyst$optimizer$ConstantFolding$$constantFolding(expressions.scala:81)
  at org.apache.spark.sql.catalyst.optimizer.ConstantFolding$.$anonfun$constantFolding$4(expressions.scala:91)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 21 22:16:01 UTC 2023,,,,,,,,,,0|z1k0ow:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Nov/23 05:52;dongjoon;I added 3.3.3 to the `Affected Version`.
{code}
scala> spark.version
res1: String = 3.3.3

scala> sql(s""SELECT CONV('${Long.MinValue}', 10, -2)"").show(false)
java.lang.ArrayIndexOutOfBoundsException: -1
{code};;;, 19/Nov/23 16:15;markj-db;I believe it's affected back to 3.2.0: https://github.com/apache/spark/blob/v3.2.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberConverter.scala#L131;;;, 20/Nov/23 21:29;jira.shegalov;3.0.3 is the oldest version on my box and it exhibits the same bug:

 
{code:java}
scala> spark.version
res1: String = 3.0.3
scala> sql(s""SELECT CONV('${Long.MinValue}', 10, -2)"").show(false)
java.lang.ArrayIndexOutOfBoundsException: -1
  at org.apache.spark.sql.catalyst.util.NumberConverter$.convert(NumberConverter.scala:148)
  at org.apache.spark.sql.catalyst.expressions.Conv.nullSafeEval(mathExpressions.scala:338)
  at org.apache.spark.sql.catalyst.expressions.TernaryExpression.eval(Expression.scala:690)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:457)
{code}
 ;;;, 21/Nov/23 19:39;dongjoon;Issue resolved by pull request 43880
[https://github.com/apache/spark/pull/43880];;;, 21/Nov/23 22:16;markj-db;Hmm, possibly it goes back to Spark 2.1.0 due to this line:

[https://github.com/apache/spark/commit/95db8a44f3e2d79913cbe0d29297796b4c3b0d1b#diff-924be5a0a35024a5f63a1411b1a4c3000150356ab59f12eda84fada0659514a2R135]
{code:java}
val temp = new Array[Byte](64){code}
I was looking only for the earliest version with

[https://github.com/apache/spark/commit/c5b0cb2d945437a998c35917bbc9d653883244db#diff-924be5a0a35024a5f63a1411b1a4c3000150356ab59f12eda84fada0659514a2R131]
{code:java}
val temp = new Array[Byte](Math.max(n.length, 64)){code};;;",3.3.3,3.4.1,3.5.0,,19/Nov/23 16:15;markj-db;I believe it's affected back to 3.2.0: https://github.com/apache/spark/blob/v3.2.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/NumberConverter.scala#L131;;;
support grouping set operation in dataframe api,SPARK-45929,13557994,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,JacobZheng,JacobZheng,JacobZheng,11/15/23 2:58,11/21/23 1:41,7/17/24 20:45,11/21/23 1:41,3.4.1,4.0.0,SQL,,0,pull-request-available,"I am using spark dataframe api for complex calculations. When I need to use the grouping sets function, I can only convert the expression to sql via analyzedPlan and then splice these sql into a complex sql to execute. In some cases, this operation generates an extremely complex sql. executing this complex sql, antlr4 continues to consume a large amount of memory, similar to a memory leak scenario. If you can and rollup, cube function through the dataframe api to calculate these operations will be much simpler.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 21 01:41:35 UTC 2023,,,,,,,,,,0|z1lm1k:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"21/Nov/23 01:41;gurwls223;Issue resolved by pull request 43813
[https://github.com/apache/spark/pull/43813];;;",,,,,
Add logging for complete write events to file in EventLogFileWriter.closeWriter,SPARK-44699,13546298,,Task,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,shuyouZZ,shuyouZZ,8/7/23 6:52,11/17/23 0:18,7/17/24 20:45,,3.4.1,,Spark Core,,0,pull-request-available,"Sometimes we want to know when to finish logging the events to eventLog file, we need add a log to make it clearer.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 08 16:52:49 UTC 2023,,,,,,,,,,0|z1jma0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 16:52;hudson;User 'shuyouZZ' has created a pull request for this issue:
https://github.com/apache/spark/pull/42372;;;",,,,,
Support deserializing long fields into `Metadata` object,SPARK-44488,13544207,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,richardc-db,richardc-db,richardc-db,7/20/23 3:10,11/16/23 2:45,7/17/24 20:45,8/3/23 3:08,3.4.1,4.0.0,Spark Core,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 03 03:08:35 UTC 2023,,,,,,,,,,0|z1j9e0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Aug/23 03:08;gurwls223;Issue resolved by pull request 42083
[https://github.com/apache/spark/pull/42083];;;",,,,,
Refactor the optimizer plan validation to decouple validateSchemaOutput and validateExprIdUniqueness,SPARK-45892,13557543,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xiliang678,xiliang678,xiliang678,11/10/23 22:26,11/13/23 21:28,7/17/24 20:45,11/13/23 21:28,"3.4.0, 3.4.1",4.0.0,Optimizer,,0,pull-request-available,"Currently, the expressionIDUniqueness validation is closely coupled with output schema validation. 

https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala#L403C7-L411C8

Some refactoring can improve readability and reuse.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 13 21:28:36 UTC 2023,,,,,,,,,,0|z1lj9k:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"10/Nov/23 23:22;xiliang678;cc [~cloud_fan] here's the PR [https://github.com/apache/spark/pull/43761] PTAL, thanks!;;;, 13/Nov/23 21:28;cloud_fan;Issue resolved by pull request 43761
[https://github.com/apache/spark/pull/43761];;;",3.4.1,,,,"13/Nov/23 21:28;cloud_fan;Issue resolved by pull request 43761
[https://github.com/apache/spark/pull/43761];;;"
Number check for InputFileBlockSources is missing for V2 source (BatchScan) ?,SPARK-45879,13557463,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,firestarmanllc,firestarmanllc,11/10/23 8:57,11/13/23 15:16,7/17/24 20:45,,"3.2.3, 3.4.1, 3.5.0",,SQL,,0,,"When doing a join with the ""input_file_name()"" function, it will blow up with a

*AnalysisException* if using the v1 data source (FileSourceScan). That's ok.

 

But if we change to use the v2 data source (BatchScan), the expected exception is gone, and the join passes.

Is this number check for InputFileDataSources mssing for V2 data source ? or is it by design ?

 

Repro steps:
{code:java}
scala> spark.range(100).withColumn(""const1"", lit(""from_t1"")).write.parquet(""/data/tmp/t1"")
 
scala> spark.range(100).withColumn(""const2"", lit(""from_t2"")).write.parquet(""/data/tmp/t2"")
 
scala> spark.conf.set(""spark.sql.sources.useV1SourceList"", ""parquet"")
 
scala> spark.read.parquet(""/data/tmp/t1"").join(spark.read.parquet(""/data/tmp/t2""), ""id"", ""inner"").selectExpr(""*"", ""input_file_name()"").show(5, false)
org.apache.spark.sql.AnalysisException: 'input_file_name' does not support more than one sources.; line 1 pos 0;
Project id#376L, const1#377, const2#381, input_file_name() AS input_file_name()#389
+- Project id#376L, const1#377, const2#381
   +- Join Inner, (id#376L = id#380L)
      :- Relation id#376L,const1#377 parquet
      +- Relation id#380L,const2#381 parquet
 
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:52)
  at org.apache.spark.sql.execution.datasources.PreReadCheck$.org$apache$spark$sql$execution$datasources$PreReadCheck$$checkNumInputFileBlockSources(rules.scala:476)
  at org.apache.spark.sql.execution.datasources.PreReadCheck$.$anonfun$checkNumInputFileBlockSources$2(rules.scala:472)
  at org.apache.spark.sql.execution.datasources.PreReadCheck$.$anonfun$checkNumInputFileBlockSources$2$adapted(rules.scala:472)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)

scala> spark.conf.set(""spark.sql.sources.useV1SourceList"", """")
 
scala> spark.read.parquet(""/data/tmp/t1"").join(spark.read.parquet(""/data/tmp/t2""), ""id"", ""inner"").selectExpr(""*"", ""input_file_name()"").show(5, false)
+---+-------+-------+---------------------------------------------------------------------------------------+
|id |const1 |const2 |input_file_name()                                                                      |
+---+-------+-------+---------------------------------------------------------------------------------------+
|91 |from_t1|from_t2|file:///data/tmp/t1/part-00011-a52b9990-4463-447c-9cdf-7a84542de2f7-c000.snappy.parquet|
|92 |from_t1|from_t2|file:///data/tmp/t1/part-00011-a52b9990-4463-447c-9cdf-7a84542de2f7-c000.snappy.parquet|
|93 |from_t1|from_t2|file:///data/tmp/t1/part-00011-a52b9990-4463-447c-9cdf-7a84542de2f7-c000.snappy.parquet|
|94 |from_t1|from_t2|file:///data/tmp/t1/part-00011-a52b9990-4463-447c-9cdf-7a84542de2f7-c000.snappy.parquet|
|95 |from_t1|from_t2|file:///data/tmp/t1/part-00011-a52b9990-4463-447c-9cdf-7a84542de2f7-c000.snappy.parquet|
+---+-------+-------+---------------------------------------------------------------------------------------+
only showing top 5 rows{code}","I tried on Spark 323 and Spark 341, and both can reproduce this issue.",,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,57:20.0,,,,,,,,,,0|z1lirs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,
current_date() not supported in Streaming Query Observed metrics,SPARK-45655,13555397,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bhuwan.sahni,bhuwan.sahni,bhuwan.sahni,10/24/23 20:48,11/12/23 21:44,7/17/24 20:45,11/12/23 7:50,"3.4.1, 3.5.0",4.0.0,Structured Streaming,,0,pull-request-available,"Streaming queries do not support current_date() inside CollectMetrics. The primary reason is that current_date() (resolves to CurrentBatchTimestamp) is marked as non-deterministic. However, {{current_date}} and {{current_timestamp}} are both deterministic today, and {{current_batch_timestamp}} should be the same.

 

As an example, the query below fails due to observe call on the DataFrame.

 
{quote}val inputData = MemoryStream[Timestamp]

inputData.toDF()
      .filter(""value < current_date()"")
      .observe(""metrics"", count(expr(""value >= current_date()"")).alias(""dropped""))
      .writeStream
      .queryName(""ts_metrics_test"")
      .format(""memory"")
      .outputMode(""append"")
      .start()
{quote}
 ",,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 12 07:50:29 UTC 2023,,,,,,,,,,0|z1l614:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"24/Oct/23 20:48;bhuwan.sahni;I am working on a fix for this issue, and will submit a PR soon.;;;, 24/Oct/23 22:21;bhuwan.sahni;PR link https://github.com/apache/spark/pull/43517;;;, 12/Nov/23 07:50;kabhwan;Issue resolved by pull request 43517
[https://github.com/apache/spark/pull/43517];;;",3.5.0,,,,24/Oct/23 22:21;bhuwan.sahni;PR link https://github.com/apache/spark/pull/43517;;;
java.lang.NoClassDefFoundError: javax/servlet/Servlet incompatibilities upgrading springboot-dependencies 2.7 to 3.* (package javax.* -> jakarta.*),SPARK-45897,13557623,,Dependency upgrade,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,arnaud.nauwynck,arnaud.nauwynck,11/12/23 18:10,11/12/23 18:22,7/17/24 20:45,,"3.2.4, 3.3.3, 3.4.0, 3.4.1, 3.5.0",,Web UI,,0,,"updating springboot-dependencies 2.7 to 3.1 breaks spark with springboot compatibilities.


{noformat}
    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-dependencies</artifactId>
                <version>2.7.14</version> <!-- OK 2.7.* works fine, but  3.* ERROR --> 
                <type>pom</type>
                <scope>import</scope>
            </dependency>
        </dependencies>
    </dependencyManagement>
{noformat}

Internally, everything compile ok, but fail at runtime while creating the SparkSession, with SparkUI:


{noformat}
Caused by: java.lang.NoClassDefFoundError: javax/servlet/Servlet
	at org.apache.spark.ui.SparkUI$.create(SparkUI.scala:239) ~[spark-core_2.13-3.5.0.jar:3.5.0]
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:503) ~[spark-core_2.13-3.5.0.jar:3.5.0]
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2888) ~[spark-core_2.13-3.5.0.jar:3.5.0]
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099) ~[spark-sql_2.13-3.5.0.jar:3.5.0]
	at scala.Option.getOrElse(Option.scala:201) ~[scala-library-2.13.8.jar:na]
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093) ~[spark-sql_2.13-3.5.0.jar:3.5.0]
....

Caused by: java.lang.ClassNotFoundException: javax.servlet.Servlet
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641) ~[na:na]
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188) ~[na:na]
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521) ~[na:na]
	... 49 common frames omitted

{noformat}

Root cause: major change with springboot 3: 'javax.servlet.Servlet' class is NO more defined in package 'javax.servlet.', but migrated to 'jakarta.servlet.'

The compiled dependency from spark-core to javax-servlet-api is replaced by maven dependencyManagement


It ""might be"" possible to get rid of maven dependencyManagement or to override several artifact versions, but it is NOT PRACTICAL.

Here is a version that work, much more complex  with springboot3 than springboot2.

It requires few of forced versions, and exclusions in maven.

    
{noformat}
<properties>
        <spark.version>3.5.0</spark.version>
        <scala.version>2.13</scala.version>
    </properties>

    <dependencyManagement>
        <dependencies>
            <dependency>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-dependencies</artifactId>
                <version>3.1.2</version>
                <type>pom</type>
                <scope>import</scope>
            </dependency>
            <dependency>
                <groupId>org.glassfish.jersey.containers</groupId>
                <artifactId>jersey-container-servlet-core</artifactId>
                <version>2.41</version>
            </dependency>
            <dependency>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-api</artifactId>
                <version>2.0.9</version>
            </dependency>
        </dependencies>
    </dependencyManagement>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_${scala.version}</artifactId>
            <version>${spark.version}</version>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.logging.log4j</groupId>
                    <artifactId>log4j-slf4j2-impl</artifactId>
                </exclusion>
            </exclusions>
        </dependency>

        <dependency>
            <groupId>jakarta.servlet</groupId>
            <artifactId>jakarta.servlet-api</artifactId>
            <version>4.0.4</version>
        </dependency>

    </dependencies>
{noformat}


there are several examples showing errors and fixed for servlet, log4j, glassfish incompatibilities here as jira attachment, or directly from github: https://github.com/Arnaud-Nauwynck/test-snippets/tree/master/test-springboot-spark



A possible long term solution is to migrate to servlet in package ""jakarta.servlet."" instead of ""javax.servlet."" as springboot already did, so there would ""not (?)"" be incompatibilities between very old and much more recent jars.

Any roadmap for this?




",,,,,,,,,,,,,,,,,,,,,,,,,12/Nov/23 18:21;arnaud.nauwynck;test-springboot-spark.zip;https://issues.apache.org/jira/secure/attachment/13064353/test-springboot-spark.zip,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,10:54.0,,,,,,,,,,0|z1ljr4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.3.3,3.4.0,3.4.1,3.5.0,
"When a task failed and the inferred task for that task is still executing, the number of dynamically scheduled executors will be calculated incorrectly",SPARK-44179,13541302,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,liangyongyuan,liangyongyuan,6/25/23 11:14,11/11/23 0:17,7/17/24 20:45,,3.4.1,,Spark Core,,0,pull-request-available,"Assuming a stage has Task 1, with Task 1.0 and a speculative task Task 1.1 running concurrently, the dynamic scheduler calculates the number of executors as 2 (pendingTask: 0, pendingSpeculative: 0, running: 2).

At this point, Task 1.0 fails, and the dynamic scheduler recalculates the number of executors as 2 (pendingTask: 1, pendingSpeculative: 0, running: 1).

Due to the failure of Task 1.0, copyRunning(1) becomes 1. As a result, Task 1 is speculated again and a SparkListenerSpeculativeTaskSubmitted event is triggered. However, the dynamic scheduler's calculation for the number of executors becomes 3 (pendingTask: 1, pendingSpeculative: 1, running: 1), which is obviously not as expected.

Then, Task 1.2 starts, and it is marked as a speculative task. However, the dynamic scheduler still calculates the number of executors as 3 (pendingTask: 1, pendingSpeculative: 1, running: 1), which again is not as expected.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,14:15.0,,,,,,,,,,0|z1irjk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade ORC to 1.8.6,SPARK-45884,13557478,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,11/10/23 10:32,11/10/23 16:58,7/17/24 20:45,11/10/23 16:56,3.4.1,3.4.2,Build,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 10 16:56:15 UTC 2023,,,,,,,,,,0|z1liv4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"10/Nov/23 16:56;dongjoon;Issue resolved by pull request 43755
[https://github.com/apache/spark/pull/43755];;;",,,,,
Fix Spark History Server to sort `Duration` column properly,SPARK-45749,13556329,13557406,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,11/1/23 0:37,11/10/23 2:41,7/17/24 20:45,11/1/23 1:57,"3.2.0, 3.3.2, 3.4.1, 3.5.0, 4.0.0","3.3.4, 3.4.2, 3.5.1, 4.0.0","Spark Core, Web UI",,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,SPARK-34123,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 01 01:57:47 UTC 2023,,,,,,,,,,0|z1lbs0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"01/Nov/23 01:57;yao;Issue resolved by pull request 43613
[https://github.com/apache/spark/pull/43613];;;",3.3.2,3.4.1,3.5.0,4.0.0,
Fix getBaseURI error in Spark Worker LogPage UI buttons,SPARK-44857,13547788,13557406,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,8/17/23 21:42,11/10/23 2:41,7/17/24 20:45,8/18/23 1:07,"3.2.0, 3.2.4, 3.3.2, 3.4.1, 3.5.0","3.3.4, 3.4.2, 3.5.0, 4.0.0","Spark Core, Web UI",,0,,,,,,,,,,,,,,,,,,,,,,SPARK-35822,,,,,17/Aug/23 21:42;dongjoon;Screenshot 2023-08-17 at 2.38.45 PM.png;https://issues.apache.org/jira/secure/attachment/13062258/Screenshot+2023-08-17+at+2.38.45+PM.png,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 22 22:11:04 UTC 2023,,,,,,,,,,0|z1jv40:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Aug/23 01:07;dongjoon;Issue resolved by pull request 42546
[https://github.com/apache/spark/pull/42546];;;, 22/Aug/23 22:11;dongjoon;Thank you for fixing the `Fix Version`, [~yumwang].;;;",3.2.4,3.3.2,3.4.1,3.5.0,"22/Aug/23 22:11;dongjoon;Thank you for fixing the `Fix Version`, [~yumwang].;;;"
Fix WorkerPage to use the same pattern for `logPage` urls,SPARK-45187,13550960,13557406,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,9/17/23 8:45,11/10/23 2:41,7/17/24 20:45,9/17/23 17:34,"3.2.4, 3.3.2, 3.4.1, 3.5.0","3.3.4, 3.4.2, 3.5.1, 4.0.0",Spark Core,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 17 17:34:42 UTC 2023,,,,,,,,,,0|z1keog:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"17/Sep/23 17:34;dongjoon;Issue resolved by pull request 42959
[https://github.com/apache/spark/pull/42959];;;",3.3.2,3.4.1,3.5.0,,
ClassCastException with SerializedLambda in Spark Cluster Mode,SPARK-45860,13557374,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,punditavi@gmail.com,punditavi@gmail.com,11/9/23 19:13,11/9/23 19:13,7/17/24 20:45,,"3.2.1, 3.4.1",,"Spark Core, Spark Submit",,0,,"h3. Issue Description

Running a Spark application in cluster mode encounters a `{*}java.lang.ClassCastException{*}` related to `j{*}ava.lang.invoke.SerializedLambda{*}`. This issue seems to be specific to the Spark Cluster mode, and it doesn't occur when running the application locally without Spring Boot.

 
h3. Steps to Reproduce
 # Create a dummy dataset
{code:java}
Dataset<String> dummyData = spark.createDataset(Arrays.asList(""Abhi"", ""Andrii"", ""Rick"", ""Duc""), Encoders.STRING()); {code}

 # Call flatMap function to transform the data
{code:java}
Dataset<TestData> transformedData = dummyData.flatMap(new TestDataFlatMap(), Encoders.bean(TestData.class)); {code}

 # Call any action on the transformed dataset
{code:java}
transformedData.show(); {code}

 # Running this Spark application with spark submit command in cluster mode with Spring Boot results in the mentioned ClassCastException.

 
h3. *Complete Code:*

 
{code:java}
@SpringBootApplication(exclude = {org.springframework.boot.autoconfigure.gson.GsonAutoConfiguration.class})
public class SampleSparkJob{
    public static void main(String[] args) {
        SpringApplication.run(DataIngestionServiceApplication.class, args);

        SparkSession spark = SparkSession.builder()
                .appName(""SampleSparkJob"")
                .master(""local[*]"")
                .getOrCreate();
        Dataset<String> dummyData = spark.createDataset(Arrays.asList(""Abhi"", ""Andrii"", ""Rick"", ""Duc""), Encoders.STRING());
        Dataset<TestData> transformedData = dummyData.flatMap(new TestDataFlatMap(), Encoders.bean(TestData.class));
        transformedData.show();
        transformedData.write().mode(""append"").parquet(""outputpath"");
        spark.stop();
    }
}{code}
{code:java}
class TestDataFlatMap implements FlatMapFunction<String, TestData>, Serializable {
    @Override
    public Iterator<TestData> call(String name) {
        return Arrays.asList(new TestData(name)).iterator();
    }
}{code}
{code:java}
@Data
@AllArgsConstructor
public class TestData implements Serializable {
    private String name;
} {code}
 
h3. 
Stack trace:
{code:java}
WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (10.248.66.38 executor 0): java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.spark.rdd.MapPartitionsRDD.f of type scala.Function3 in instance of org.apache.spark.rdd.MapPartitionsRDD	at java.base/java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2076)	at java.base/java.io.ObjectStreamClass$FieldReflector.checkObjectFieldValueTypes(ObjectStreamClass.java:2039)	at java.base/java.io.ObjectStreamClass.checkObjFieldValueTypes(ObjectStreamClass.java:1293)	at java.base/java.io.ObjectInputStream.defaultCheckFieldValues(ObjectInputStream.java:2512)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2419)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)	at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:527)	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.base/java.lang.reflect.Method.invoke(Method.java:566)	at java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1046)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2357)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)	at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:527)	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.base/java.lang.reflect.Method.invoke(Method.java:566)	at java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1046)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2357)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)	at scala.collection.immutable.List$SerializationProxy.readObject(List.scala:527)	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)	at java.base/java.lang.reflect.Method.invoke(Method.java:566)	at java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1046)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2357)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2496)	at java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2390)	at java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2228)	at java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1687)	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:489)	at java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:447)	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:83)	at org.apache.spark.scheduler.Task.run(Task.scala:131)	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)	at java.base/java.lang.Thread.run(Thread.java:829) {code}
 

*Environment*
Java Version: 11
Spring Boot Version: 2.7.10
Spark Version: 3.2.1
h3. Additional Information:

The issue seems to be related to Spring Boot auto-configuration or the dependencies included with Spring Boot.","*Environment*
Java Version: 11
Spring Boot Version: 2.7.10
Spark Version: 3.2.1",,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,13:47.0,,,,,,,,,,0|z1li80:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,,,,
No way to exclude jars setting to classpath while doing spark-submit,SPARK-45115,13550212,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,sumanto_pal_07,sumanto_pal_07,9/11/23 12:19,11/8/23 3:12,7/17/24 20:45,,3.4.1,,Spark Submit,,0,,"The challenge is whenever you do spark-submit to start the application, the jars present in spark home directory gets added to classpath automatically and there is no way to exclude specific jars from there. For example, we dont want slf4j jars present in spark home directory to be setted in classpath as in codebase slf4j is already there. Thus it causes conflicts in jars. This forces user to change there codebase to support spark-submit or to manually remove the jars from spark-home directory. This i believe is not right practice as we deviating from using spark as it supposed to be and it causes unfixable behaviors at various instances with no clue. Example linkages errors are common with the jar conflicts. 

 

There is detailed stackoverflow question on this issue. 

refer : https://stackoverflow.com/questions/76476618/linkageerror-facing-while-doing-spark-submit

 ",,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,https://stackoverflow.com/questions/76476618/linkageerror-facing-while-doing-spark-submit,,,,,,,,,,9223372036854775807,,,,Wed Sep 13 07:15:36 UTC 2023,,,,,,,,,,0|z1ka2g:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Sep/23 06:14;sumanto_pal_07;[~yao] , is there a way I could prioritize this issue as it'sblocking us to move to production ?;;;, 12/Sep/23 09:49;yao;Build your spark app with slf4j provided scope?;;;, 12/Sep/23 13:54;sumanto_pal_07;Challenge is we are using Firm recommended FAT jar which has slf4j and logging jars which are kind of customised. And we cannot go away with it as its not allowed. So we were hoping if spark could provide any way to exclude jar from spar-submit. [~yao]  
I believe this Jira will help user to avoid conflicts and onboard spark to there systems more conviniently as version issue sare very common with spark. 

I can work on the code fix if we believe this fix is helpful.;;;, 13/Sep/23 04:25;yao;Spark provides a user classpath first feature that might fix your issue.

 

Anyway, feel free to open a PR if you think it is necessary. It's not necessary to wait for any positive permission from spark committers to get started;;;, 13/Sep/23 07:15;sumanto_pal_07;_""Spark provides a user classpath first feature that might fix your issue.""_

_~_ We tried this, did'nt helped.

_""Anyway, feel free to open a PR if you think it is necessary. It's not necessary to wait for any positive permission from spark committers to get started""_

~ Thanks, am new to the community, will share the pr soon.;;;",,,,,12/Sep/23 09:49;yao;Build your spark app with slf4j provided scope?;;;
Support encoding of scala.collection.immutable.ArraySeq,SPARK-45820,13557000,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,eejbyfeldt,eejbyfeldt,11/7/23 8:47,11/7/23 8:47,7/17/24 20:45,,"3.4.0, 3.4.1, 3.5.0, 4.0.0",,SQL,,0,,"Trying to use `scala.collection.immutable.ArraySeq` will currently derive the encoder as it a subtype of scala.collection.Seq but will then fail at runtime since the builder interface is different then for other Seq.


{code:java}
scala> spark.createDataset(Seq(scala.collection.immutable.ArraySeq(1,2,3))).collect()
23/11/07 09:44:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/11/07 09:44:39 INFO SharedState: Warehouse path is 'file:/home/eejbyfeldt/spark-warehouse'.
23/11/07 09:44:40 INFO CodeGenerator: Code generated in 188.491705 ms
23/11/07 09:44:40 INFO CodeGenerator: Code generated in 14.382264 ms
23/11/07 09:44:40 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 101: No applicable constructor/method found for zero actual parameters; candidates are: ""public scala.collection.mutable.Builder scala.collection.immutable.ArraySeq$.newBuilder(scala.reflect.ClassTag)"", ""public scala.collection.mutable.Builder scala.collection.immutable.ArraySeq$.newBuilder(java.lang.Object)"", ""public abstract scala.collection.mutable.Builder scala.collection.EvidenceIterableFactory.newBuilder(java.lang.Object)""
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 101: No applicable constructor/method found for zero actual parameters; candidates are: ""public scala.collection.mutable.Builder scala.collection.immutable.ArraySeq$.newBuilder(scala.reflect.ClassTag)"", ""public scala.collection.mutable.Builder scala.collection.immutable.ArraySeq$.newBuilder(java.lang.Object)"", ""public abstract scala.collection.mutable.Builder scala.collection.EvidenceIterableFactory.newBuilder(java.lang.Object)""
    at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:13014)
    at org.codehaus.janino.UnitCompiler.findMostSpecificIInvocable(UnitCompiler.java:9615)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9475)
    at org.codehaus.janino.UnitCompiler.findIMethod(UnitCompiler.java:9391)
    at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:5232)
    at org.codehaus.janino.UnitCompiler.access$9300(UnitCompiler.java:236)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4735)
    at org.codehaus.janino.UnitCompiler$16.visitMethodInvocation(UnitCompiler.java:4711)
    at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:5470)
    at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:4711)
    at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:5854)
    at org.codehaus.janino.UnitCompiler.access$3800(UnitCompiler.java:236)
    at org.codehaus.janino.UnitCompiler$7.visitRvalue(UnitCompiler.java:2766)
    at org.codehaus.janino.UnitCompiler$7.visitRvalue(UnitCompiler.java:2754)
    at org.codehaus.janino.Java$Rvalue.accept(Java.java:4498)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2754)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2741)
    at org.codehaus.janino.UnitCompiler.access$2700(UnitCompiler.java:236)
    at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1589)
    at org.codehaus.janino.UnitCompiler$6.visitLocalVariableDeclarationStatement(UnitCompiler.java:1575)
    at org.codehaus.janino.Java$LocalVariableDeclarationStatement.accept(Java.java:3842)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1646)
    at org.codehaus.janino.UnitCompiler.access$1700(UnitCompiler.java:236)
    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1579)
    at org.codehaus.janino.UnitCompiler$6.visitBlock(UnitCompiler.java:1575)
    at org.codehaus.janino.Java$Block.accept(Java.java:3115)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2649)
    at org.codehaus.janino.UnitCompiler.access$1900(UnitCompiler.java:236)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1581)
    at org.codehaus.janino.UnitCompiler$6.visitIfStatement(UnitCompiler.java:1575)
    at org.codehaus.janino.Java$IfStatement.accept(Java.java:3284)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:1575)
    at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1661)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:3658)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:3329)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1447)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:1420)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:829)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1026)
    at org.codehaus.janino.UnitCompiler.access$700(UnitCompiler.java:236)
    at org.codehaus.janino.UnitCompiler$3.visitMemberClassDeclaration(UnitCompiler.java:425)
    at org.codehaus.janino.UnitCompiler$3.visitMemberClassDeclaration(UnitCompiler.java:418)
    at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1533)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:418)
    at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:1397)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:864)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:442)
    at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:236)
    at org.codehaus.janino.UnitCompiler$3.visitPackageMemberClassDeclaration(UnitCompiler.java:422)
    at org.codehaus.janino.UnitCompiler$3.visitPackageMemberClassDeclaration(UnitCompiler.java:418)
    at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1688)
    at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:418)
    at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:392)
    at org.codehaus.janino.UnitCompiler.access$000(UnitCompiler.java:236)
    at org.codehaus.janino.UnitCompiler$2.visitCompilationUnit(UnitCompiler.java:363)
    at org.codehaus.janino.UnitCompiler$2.visitCompilationUnit(UnitCompiler.java:361)
    at org.codehaus.janino.Java$CompilationUnit.accept(Java.java:371)
    at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:361)
    at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:264)
    at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:294)
    at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:288)
    at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:267)
    at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:82)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.doCompile(CodeGenerator.scala:1497)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.$anonfun$cache$1(CodeGenerator.scala:1589)
    at org.apache.spark.util.NonFateSharingCache$$anon$1.load(NonFateSharingCache.scala:68)
    at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
    at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
    at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
    at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
    at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
    at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
    at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
    at org.apache.spark.util.NonFateSharingLoadingCache.$anonfun$get$2(NonFateSharingCache.scala:94)
    at org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)
    at org.apache.spark.util.NonFateSharingLoadingCache.get(NonFateSharingCache.scala:94)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1444)
    at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:205)
    at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:39)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1369)
    at org.apache.spark.sql.catalyst.expressions.SafeProjection$.createCodeGeneratedObject(Projection.scala:171)
    at org.apache.spark.sql.catalyst.expressions.SafeProjection$.createCodeGeneratedObject(Projection.scala:168)
    at org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:50)
    at org.apache.spark.sql.catalyst.expressions.SafeProjection$.create(Projection.scala:194)
    at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:180)
    at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:173)
    at scala.collection.ArrayOps$.map$extension(ArrayOps.scala:929)
    at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
    at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3585)
    at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
    at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
    at org.apache.spark.sql.Dataset.collect(Dataset.scala:3585)
    at $line14.$read$$iw.<init>(<console>:1)
    at $line14.$read.<init>(<console>:15)
    at $line14.$read$.<clinit>(<console>:1)
    at $line14.$eval$.$print$lzycompute(<synthetic>:6)
    at $line14.$eval$.$print(<synthetic>:5)
    at $line14.$eval.$print(<synthetic>)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:670)
    at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
    at scala.tools.nsc.interpreter.IMain.$anonfun$doInterpret$1(IMain.scala:506)
    at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
    at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
    at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:43)
    at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:505)
    at scala.tools.nsc.interpreter.IMain.$anonfun$doInterpret$3(IMain.scala:519)
    at scala.tools.nsc.interpreter.IMain.doInterpret(IMain.scala:519)
    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:503)
    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:501)
    at scala.tools.nsc.interpreter.shell.ILoop.loop$1(ILoop.scala:878)
    at scala.tools.nsc.interpreter.shell.ILoop.interpretStartingWith(ILoop.scala:906)
    at scala.tools.nsc.interpreter.shell.ILoop.command(ILoop.scala:433)
    at scala.tools.nsc.interpreter.shell.ILoop.processLine(ILoop.scala:440)
    at scala.tools.nsc.interpreter.shell.ILoop.loop(ILoop.scala:458)
    at scala.tools.nsc.interpreter.shell.ILoop.run(ILoop.scala:968)
    at org.apache.spark.repl.Main$.doMain(Main.scala:84)
    at org.apache.spark.repl.Main$.main(Main.scala:59)
    at org.apache.spark.repl.Main.main(Main.scala)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
23/11/07 09:44:40 INFO CodeGenerator: 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private int value_MapObject_lambda_variable_1;
/* 010 */   private boolean globalIsNull_0;
/* 011 */
/* 012 */   public SpecificSafeProjection(Object[] references) {
/* 013 */     this.references = references;
/* 014 */     mutableRow = (InternalRow) references[references.length - 1];
/* 015 */
/* 016 */
/* 017 */   }
/* 018 */
/* 019 */   public void initialize(int partitionIndex) {
/* 020 */
/* 021 */   }
/* 022 */
/* 023 */   public java.lang.Object apply(java.lang.Object _i) {
/* 024 */     InternalRow i = (InternalRow) _i;
/* 025 */     scala.collection.immutable.ArraySeq value_3 = MapObjects_0(i);
/* 026 */     if (globalIsNull_0) {
/* 027 */       mutableRow.setNullAt(0);
/* 028 */     } else {
/* 029 */
/* 030 */       mutableRow.update(0, value_3);
/* 031 */     }
/* 032 */
/* 033 */     return mutableRow;
/* 034 */   }
/* 035 */
/* 036 */
/* 037 */   private scala.collection.immutable.ArraySeq MapObjects_0(InternalRow i) {
/* 038 */     boolean isNull_1 = i.isNullAt(0);
/* 039 */     ArrayData value_1 = isNull_1 ?
/* 040 */     null : (i.getArray(0));
/* 041 */     scala.collection.immutable.ArraySeq value_0 = null;
/* 042 */
/* 043 */     if (!isNull_1) {
/* 044 */
/* 045 */       int dataLength_0 = value_1.numElements();
/* 046 */
/* 047 */       scala.collection.mutable.Builder collectionBuilder_0 = scala.collection.immutable.ArraySeq$.MODULE$.newBuilder();
/* 048 */       collectionBuilder_0.sizeHint(dataLength_0);
/* 049 */
/* 050 */
/* 051 */       int loopIndex_0 = 0;
/* 052 */
/* 053 */       while (loopIndex_0 < dataLength_0) {
/* 054 */         value_MapObject_lambda_variable_1 = (int) (value_1.getInt(loopIndex_0));
/* 055 */
/* 056 */
/* 057 */         if (false) {
/* 058 */           throw new NullPointerException(((java.lang.String) references[0] /* errMsg */));
/* 059 */         }
/* 060 */         if (false) {
/* 061 */           collectionBuilder_0.$plus$eq(null);
/* 062 */         } else {
/* 063 */           collectionBuilder_0.$plus$eq(value_MapObject_lambda_variable_1);
/* 064 */         }
/* 065 */
/* 066 */         loopIndex_0 += 1;
/* 067 */       }
/* 068 */
/* 069 */       value_0 = (scala.collection.immutable.ArraySeq) collectionBuilder_0.result();
/* 070 */     }
/* 071 */     globalIsNull_0 = isNull_1;
/* 072 */     return value_0;
/* 073 */   }
/* 074 */
/* 075 */ }23/11/07 09:44:40 WARN SafeProjection: Expr codegen error and falling back to interpreter mode
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 101: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 101: No applicable constructor/method found for zero actual parameters; candidates are: ""public scala.collection.mutable.Builder scala.collection.immutable.ArraySeq$.newBuilder(scala.reflect.ClassTag)"", ""public scala.collection.mutable.Builder scala.collection.immutable.ArraySeq$.newBuilder(java.lang.Object)"", ""public abstract scala.collection.mutable.Builder scala.collection.EvidenceIterableFactory.newBuilder(java.lang.Object)""
    at org.sparkproject.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
    at org.sparkproject.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
    at org.sparkproject.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
    at org.sparkproject.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
    at org.sparkproject.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
    at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
    at org.sparkproject.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
    at org.sparkproject.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
    at org.sparkproject.guava.cache.LocalCache.get(LocalCache.java:4000)
    at org.sparkproject.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
    at org.sparkproject.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
    at org.apache.spark.util.NonFateSharingLoadingCache.$anonfun$get$2(NonFateSharingCache.scala:94)
    at org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)
    at org.apache.spark.util.NonFateSharingLoadingCache.get(NonFateSharingCache.scala:94)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:1444)
    at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:205)
    at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:39)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:1369)
    at org.apache.spark.sql.catalyst.expressions.SafeProjection$.createCodeGeneratedObject(Projection.scala:171)
    at org.apache.spark.sql.catalyst.expressions.SafeProjection$.createCodeGeneratedObject(Projection.scala:168)
    at org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:50)
    at org.apache.spark.sql.catalyst.expressions.SafeProjection$.create(Projection.scala:194)
    at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:180)
    at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Deserializer.apply(ExpressionEncoder.scala:173)
    at scala.collection.ArrayOps$.map$extension(ArrayOps.scala:929)
    at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
    at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3585)
    at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
    at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
    at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
    at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
    at org.apache.spark.sql.Dataset.collect(Dataset.scala:3585)
    at $line14.$read$$iw.<init>(<console>:1)
    at $line14.$read.<init>(<console>:15)
    at $line14.$read$.<clinit>(<console>:1)
    at $line14.$eval$.$print$lzycompute(<synthetic>:6)
    at $line14.$eval$.$print(<synthetic>:5)
    at $line14.$eval.$print(<synthetic>)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:670)
    at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)
    at scala.tools.nsc.interpreter.IMain.$anonfun$doInterpret$1(IMain.scala:506)
    at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)
    at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)
    at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:43)
    at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:505)
    at scala.tools.nsc.interpreter.IMain.$anonfun$doInterpret$3(IMain.scala:519)
    at scala.tools.nsc.interpreter.IMain.doInterpret(IMain.scala:519)
    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:503)
    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:501)
    at scala.tools.nsc.interpreter.shell.ILoop.loop$1(ILoop.scala:878)
    at scala.tools.nsc.interpreter.shell.ILoop.interpretStartingWith(ILoop.scala:906)
    at scala.tools.nsc.interpreter.shell.ILoop.command(ILoop.scala:433)
    at scala.tools.nsc.interpreter.shell.ILoop.processLine(ILoop.scala:440)
    at scala.tools.nsc.interpreter.shell.ILoop.loop(ILoop.scala:458)
    at scala.tools.nsc.interpreter.shell.ILoop.run(ILoop.scala:968)
    at org.apache.spark.repl.Main$.doMain(Main.scala:84)
    at org.apache.spark.repl.Main$.main(Main.scala:59)
    at org.apache.spark.repl.Main.main(Main.scala)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:568)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1029)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 101: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 47, Column 101: No applicable constructor/method found for zero actual parameters; candidates are: ""public scala.collection.mutable.Builder scala.collection.immutable.ArraySeq$.newBuilder(scala.reflect.ClassTag)"", ""public scala.collection.mutable.Builder scala.collection.immutable.ArraySeq$.newBuilder(java.lang.Object)"", ""public abstract scala.collection.mutable.Builder scala.collection.EvidenceIterableFactory.newBuilder(java.lang.Object)""
    at org.apache.spark.sql.errors.QueryExecutionErrors$.compilerError(QueryExecutionErrors.scala:663)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.doCompile(CodeGenerator.scala:1509)
    at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.$anonfun$cache$1(CodeGenerator.scala:1589)
    at org.apache.spark.util.NonFateSharingCache$$anon$1.load(NonFateSharingCache.scala:68)
    at org.sparkproject.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
    at org.sparkproject.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
    ... 73 more
java.lang.ArrayStoreException: scala.collection.immutable.$colon$colon
  at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:74)
  at scala.collection.ArrayOps$.map$extension(ArrayOps.scala:929)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)
  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3585)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)
  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:3585)
  ... 42 elided
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,47:50.0,,,,,,,,,,0|z1lfww:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,4.0.0,,
"When using Spark to read the hive table, the number of file partitions cannot be set using Spark's configuration settings",SPARK-44483,13544108,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,hao.duan,hao.duan,7/19/23 11:58,11/5/23 0:19,7/17/24 20:45,,3.4.1,,SQL,,0,pull-request-available,"When using Spark to read the hive table, the number of file partitions cannot be set using Spark's configuration settings",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,58:17.0,,,,,,,,,,0|z1j8s0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
first operator should respect the nullability of child expression as well as ignoreNulls option,SPARK-44517,13544575,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,codingcat,codingcat,7/24/23 2:14,11/4/23 0:17,7/17/24 20:45,,"3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1",,SQL,,0,pull-request-available,"I found the following problem when using Spark recently:

 
{code:java}
// code placeholder
import spark.implicits._

val s = Seq((1.2, ""s"", 2.2)).toDF(""v1"", ""v2"", ""v3"")

val schema = StructType(Seq(StructField(""v1"", DoubleType, nullable = false),StructField(""v2"", StringType, nullable = true),StructField(""v3"", DoubleType, nullable = false)))

val df = spark.createDataFrame(s.rdd, schema)val inputDF = 

val inputDF = df.dropDuplicates(""v3"")

spark.sql(""CREATE TABLE local.db.table (\n v1 DOUBLE NOT NULL,\n v2 STRING, v3 DOUBLE NOT NULL)"")

inputDF.write.mode(""overwrite"").format(""iceberg"").save(""local.db.table"") {code}
 

 

when I use the above code to write to iceberg (i guess Delta Lake will have the same problem) , I got very confusing exception


{code:java}
Exception in thread ""main"" java.lang.IllegalArgumentException: Cannot write incompatible dataset to table with schema:

table 

{  1: v1: required double  2: v2: optional string  3: v3: required double}

Provided schema:

table {  1: v1: optional double  2: v2: optional string  3: v3: required double} {code}

basically it complains that we have v1 as the nullable column in our `inputDF` above which is not allowed since we created table with the v1 as not nullable. The confusion comes from that,  if we check the schema with printSchema() of inputDF, v1 is not nullable
{noformat}
root 
|-- v1: double (nullable = false) 
|-- v2: string (nullable = true) 
|-- v3: double (nullable = false){noformat}
Clearly, something changed the v1's nullability unexpectedly!

 

After some debugging I found that the key is that dropDuplicates(""v3""). In optimization phase, we have ReplaceDeduplicateWithAggregate to replace the Deduplicate with aggregate on v3 and run first() over all other columns. However, first() operator has hard coded nullable as always ""true"" which is the source of changed nullability of v1

 

this is a very confusing behavior of Spark, and probably no one really noticed as we do not care too much without the new table formats like delta lake and iceberg which can make nullability check correctly. Nowadays, we users adopt them more and more, this is surfaced up

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 05 16:51:43 UTC 2023,,,,,,,,,,0|z1jbns:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"05/Aug/23 16:51;hiveqa;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/42117;;;",3.2.1,3.2.2,3.2.3,3.2.4,
Extremely slow execution of sum of columns in Spark 3.4.1,SPARK-45745,13556321,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,jabot,jabot,10/31/23 21:59,11/1/23 7:53,7/17/24 20:45,,3.4.1,,PySpark,,0,,"We are in the process of upgrading some pySpark jobs from Spark 3.1.2 to Spark 3.4.1 and some code that was running fine is now basically never ending even for small dataframes.

We have simplified the problematic piece of code and the minimum pySpark example below shows the issue:
{code:java}
n_cols = 50
data = [{f""col{i}"": i for i in range(n_cols)} for _ in range(5)]
df_data = sql_context.createDataFrame(data)

df_data = df_data.withColumn(
    ""col_sum"", sum([F.col(f""col{i}"") for i in range(n_cols)])
)
df_data.show(10, False) {code}
Basically, this code with Spark 3.1.2 runs fine but with 3.4.1 the computation time seems to explode when the value of `n_cols` is bigger than about 25 columns. A colleague suggested that it could be related to the limit of 22 elements in a tuple in Scala 2.13 (https://www.scala-lang.org/api/current/scala/Tuple22.html), since the 25 columns are suspiciously close to this. Is there any known defect in the logical plan optimization in 3.4.1? Or is this kind of operations (sum of multiple columns) supposed to be implemented differently?",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 01 07:53:18 UTC 2023,,,,,,,,,,0|z1lbq8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"31/Oct/23 22:03;jabot;I originally posted a comment in StackOverflow asking for feedback on this ([https://stackoverflow.com/questions/77391731/extremely-slow-execution-in-spark-3-4-1-when-computing-the-sum-of-pyspark-datafr]) and a user there pointed me to a problem to a never ending UT reported here https://issues.apache.org/jira/browse/SPARK-43972 It is for the same Spark version, but I honestly don't know if this can be related.;;;, 31/Oct/23 22:18;bersprockets;Likely SPARK-45071 (which was also reported as SPARK-44912).;;;, 01/Nov/23 07:53;jabot;Thanks for the pointer [~bersprockets]! I'll try to check if 3.4.2 runs my example correctly.;;;",,,,,31/Oct/23 22:18;bersprockets;Likely SPARK-45071 (which was also reported as SPARK-44912).;;;
Reenable CatalogTests without Spark Connect,SPARK-45735,13556172,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,10/31/23 3:44,10/31/23 7:48,7/17/24 20:45,10/31/23 7:48,"3.4.1, 3.5.0, 4.0.0","3.4.2, 3.5.1, 4.0.0","PySpark, Tests",,0,pull-request-available,https://issues.apache.org/jira/browse/SPARK-41707 mistakenly make the original tests skipped.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 31 07:48:13 UTC 2023,,,,,,,,,,0|z1lat4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"31/Oct/23 07:48;gurwls223;Issue resolved by pull request 43595
[https://github.com/apache/spark/pull/43595];;;",3.5.0,4.0.0,,,
Add option to use Java tmp dir for RocksDB state store,SPARK-44639,13545896,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kimahriman,kimahriman,8/2/23 17:47,10/29/23 21:35,7/17/24 20:45,,3.4.1,,"SQL, Structured Streaming",,0,pull-request-available,"Currently local RocksDB state is stored in a local directory given by Utils.getLocalDir. On yarn this is a directory created inside the root application folder such as

{{/tmp/nm-local-dir/usercache/<user>/appcache/<app_id>/}}

The problem with this is that if an executor crashes for some reason (like OOM) and the shutdown hooks don't get run, this directory will stay around forever until the application finishes, which can cause jobs to slowly accumulate more and more temporary space until finally the node manager goes unhealthy.

Because this data will only ever be accessed by the executor that created this directory, it would make sense to store the data inside the container folder, which will always get cleaned up by the node manager when that yarn container gets cleaned up. Yarn sets the `java.io.tmpdir` to a path inside this directory, such as

{{/tmp/nm-local-dir/usercache/<user>/appcache/<app_id>/<container_id>/tmp/}}

I'm not sure the behavior for other resource managers, so this could be an opt-in config that can be specified.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,47:35.0,,,,,,,,,,0|z1jjso:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
change the external catalog thread safety way,SPARK-44472,13543895,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,igreenfi,igreenfi,7/18/23 7:38,10/29/23 15:13,7/17/24 20:45,,3.4.1,,Spark Core,,0,,"We test changing the sync of the external catalog to use thread-local instead of the synchronized methods.

in our tests, it improve the runtime of parallel actions by about 45% for certain workload ** (time reduced from ~15min to ~9min) ",,,,,,,,,,,,,,,,,,,,,,,,,18/Jul/23 08:56;igreenfi;add_hive_concurrent_connections.diff;https://issues.apache.org/jira/secure/attachment/13061392/add_hive_concurrent_connections.diff,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 29 15:11:27 UTC 2023,,,,,,,,,,0|z1j7go:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"29/Oct/23 15:11;igreenfi;Hi, Any comments about this?;;;",,,,,
SparkSubmit does not support --total-executor-cores when deploying on K8s,SPARK-45670,13555583,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,10/26/23 4:09,10/27/23 6:26,7/17/24 20:45,10/27/23 6:24,"3.3.3, 3.4.1, 3.5.0","3.3.4, 3.4.2, 3.5.1",Spark Submit,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 27 06:24:50 UTC 2023,,,,,,,,,,0|z1l76g:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"27/Oct/23 06:24;gurwls223;Issue resolved by pull request 43548
[https://github.com/apache/spark/pull/43548];;;",3.4.1,3.5.0,,,
Caching SQL UNION of different column data types does not work inside Dataset.union,SPARK-45657,13555412,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jzhuge,jzhuge,10/25/23 0:20,10/25/23 5:34,7/17/24 20:45,10/25/23 5:34,"3.3.2, 3.4.0, 3.4.1",3.5.0,SQL,,0,," 

Cache SQL UNION of 2 sides with different column data types
{code:java}
scala> spark.sql(""select 1 id union select 's2' id"").cache()  {code}
Dataset.union does not leverage the cache
{code:java}
scala> spark.sql(""select 1 id union select 's2' id"").union(spark.sql(""select 's3'"")).queryExecution.optimizedPlan
res15: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Union false, false
:- Aggregate [id#109], [id#109]
:  +- Union false, false
:     :- Project [1 AS id#109]
:     :  +- OneRowRelation
:     +- Project [s2 AS id#108]
:        +- OneRowRelation
+- Project [s3 AS s3#111]
   +- OneRowRelation {code}
SQL UNION of the cached SQL UNION does use the cache! Please note `InMemoryRelation` used.
{code:java}
scala> spark.sql(""(select 1 id union select 's2' id) union select 's3'"").queryExecution.optimizedPlan
res16: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Aggregate [id#117], [id#117]
+- Union false, false
   :- InMemoryRelation [id#117], StorageLevel(disk, memory, deserialized, 1 replicas)
   :     +- *(4) HashAggregate(keys=[id#100], functions=[], output=[id#100])
   :        +- Exchange hashpartitioning(id#100, 500), ENSURE_REQUIREMENTS, [plan_id=241]
   :           +- *(3) HashAggregate(keys=[id#100], functions=[], output=[id#100])
   :              +- Union
   :                 :- *(1) Project [1 AS id#100]
   :                 :  +- *(1) Scan OneRowRelation[]
   :                 +- *(2) Project [s2 AS id#99]
   :                    +- *(2) Scan OneRowRelation[]
   +- Project [s3 AS s3#116]
      +- OneRowRelation {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 25 05:34:44 UTC 2023,,,,,,,,,,0|z1l64g:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"25/Oct/23 00:35;jzhuge;Root cause:
 # SQL UNION of 2 sides with different data types produce a Project of Project on 1 side to cast the type. When this is cached, the Project of Project is preserved.
{noformat}
Distinct
+- Union false, false
   :- Project [cast(id#153 as string) AS id#155]
   :  +- Project [1 AS id#153]
   :     +- OneRowRelation
   +- Project [s2 AS id#154]
      +- OneRowRelation{noformat}

 # Dataset.union applies `CombineUnions` which applies to all unions in the tree. CombineUnions collapses the 2 Projects into 1, thus Dataset.union of the above plan with any plan will not be able to find a matching cached plan.
{code:java}
object CombineUnions extends Rule[LogicalPlan] {
...
  private def flattenUnion(union: Union, flattenDistinct: Boolean):
...
    case p1 @ Project(_, p2: Project)
      if canCollapseExpressions(p1.projectList, p2.projectList, alwaysInline = false) &&
        !p1.projectList.exists(SubqueryExpression.hasCorrelatedSubquery) &&
        !p2.projectList.exists(SubqueryExpression.hasCorrelatedSubquery) =>
      val newProjectList = buildCleanedProjectList(p1.projectList, p2.projectList)
      stack.pushAll(Seq(p2.copy(projectList = newProjectList))){code}

 ;;;, 25/Oct/23 00:46;jzhuge;Checking whether this is still an issue in main branch.;;;, 25/Oct/23 00:50;jzhuge;Interesting, there is warning in Dataset.union
{code:java}
def union(other: Dataset[T]): Dataset[T] = withSetOperator {
  // This breaks caching, but it's usually ok because it addresses a very specific use case:
  // using union to union many files or partitions.
  CombineUnions(Union(logicalPlan, other.logicalPlan))
} {code};;;, 25/Oct/23 04:32;jzhuge;It is fixed in main branch
{code:java}
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 4.0.0-SNAPSHOT
      /_/Using Scala version 2.13.12 (OpenJDK 64-Bit Server VM, Java 17.0.7)
Type in expressions to have them evaluated.
Type :help for more information.
23/10/24 21:30:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Spark context Web UI available at http://192.168.86.29:4040
Spark context available as 'sc' (master = local[*], app id = local-1698208231783).
Spark session available as 'spark'.scala> spark.sql(""select 1 id union select 's2' id"").cache()
val res0: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string]scala> spark.sql(""select 1 id union select 's2' id"").union(spark.sql(""select 's3'"")).queryExecution.optimizedPlan
val res1: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
Union false, false
:- InMemoryRelation [id#11], StorageLevel(disk, memory, deserialized, 1 replicas)
:     +- AdaptiveSparkPlan isFinalPlan=false
:        +- HashAggregate(keys=[id#2], functions=[], output=[id#2])
:           +- Exchange hashpartitioning(id#2, 200), ENSURE_REQUIREMENTS, [plan_id=30]
:              +- HashAggregate(keys=[id#2], functions=[], output=[id#2])
:                 +- Union
:                    :- Project [1 AS id#2]
:                    :  +- Scan OneRowRelation[]
:                    +- Project [s2 AS id#1]
:                       +- Scan OneRowRelation[]
+- Project [s3 AS s3#13]
   +- OneRowRelation {code};;;, 25/Oct/23 05:34;jzhuge;The issue is fixed in 3.5.0;;;",3.4.0,3.4.1,,,25/Oct/23 00:46;jzhuge;Checking whether this is still an issue in main branch.;;;
Spark SQL returning incorrect values for full outer join on keys with the same name.,SPARK-45583,13554492,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,hcampbell,hcampbell,10/18/23 3:38,10/20/23 15:03,7/17/24 20:45,10/20/23 15:03,3.4.1,3.5.0,SQL,,0,,"{{The following query gives the wrong results.}}

 

{{WITH people as (}}
{{  SELECT * FROM (VALUES }}
{{    (1, 'Peter'), }}
{{    (2, 'Homer'), }}
{{    (3, 'Ned'),}}
{{    (3, 'Jenny')}}
{{  ) AS Idiots(id, FirstName)}}
{{{}){}}}{{{}, location as ({}}}
{{  SELECT * FROM (VALUES}}
{{    (1, 'sample0'),}}
{{    (1, 'sample1'),}}
{{    (2, 'sample2')  }}
{{  ) as Locations(id, address)}}
{{{}){}}}{{{}SELECT{}}}
{{  *}}
{{FROM}}
{{  people}}
{{FULL OUTER JOIN}}
{{  location}}
{{ON}}
{{  people.id = location.id}}

{{We find the following table:}}
||id: integer||FirstName: string||id: integer||address: string||
|2|Homer|2|sample2|
|null|Ned|null|null|
|null|Jenny|null|null|
|1|Peter|1|sample0|
|1|Peter|1|sample1|

{{But clearly the first `id` column is wrong, the nulls should be 3.}}

If we rename the id column in (only) the person table to pid we get the correct results:
||pid: integer||FirstName: string||id: integer||address: string||
|2|Homer|2|sample2|
|3|Ned|null|null|
|3|Jenny|null|null|
|1|Peter|1|sample0|
|1|Peter|1|sample1|",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 19 23:28:53 UTC 2023,,,,,,,,,,0|z1l0g0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Oct/23 16:30;bersprockets;Strangely, I cannot reproduce. Is some setting required?
{noformat}
sql(""select version()"").show(false)
+----------------------------------------------+
|version()                                     |
+----------------------------------------------+
|3.5.0 ce5ddad990373636e94071e7cef2f31021add07b|
+----------------------------------------------+

scala> sql(""""""WITH people as (
  SELECT * FROM (VALUES 
    (1, 'Peter'), 
    (2, 'Homer'), 
    (3, 'Ned'),
    (3, 'Jenny')
  ) AS Idiots(id, FirstName)
), location as (
  SELECT * FROM (VALUES
    (1, 'sample0'),
    (1, 'sample1'),
    (2, 'sample2')  
  ) as Locations(id, address)
)SELECT
  *
FROM
  people
FULL OUTER JOIN
  location
ON
  people.id = location.id"""""").show(false)
     |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      |      | 
+---+---------+----+-------+
|id |FirstName|id  |address|
+---+---------+----+-------+
|1  |Peter    |1   |sample0|
|1  |Peter    |1   |sample1|
|2  |Homer    |2   |sample2|
|3  |Ned      |NULL|NULL   |
|3  |Jenny    |NULL|NULL   |
+---+---------+----+-------+

scala> 
{noformat};;;, 19/Oct/23 23:28;hcampbell;Ahh, apologies, it looks like I was running 3.4.1 when I found this issue.

Testing in 3.5 it does appear to be resolved.;;;",,,,,"19/Oct/23 23:28;hcampbell;Ahh, apologies, it looks like I was running 3.4.1 when I found this issue.

Testing in 3.5 it does appear to be resolved.;;;"
Porting k8s PVC reuse logic to spark standalone,SPARK-44526,13544667,,New Feature,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,haldefaiz,haldefaiz,7/24/23 14:06,10/18/23 20:40,7/17/24 20:45,,3.4.1,,"Shuffle, Spark Core",,0,,"Hi,

This ticket is meant to understand the work that would be involved in porting the k8s PVC reuse feature onto the spark standalone cluster manager which reuses the shuffle files present locally in the disk

We are a heavy user of spot instances and we suffer from spot terminations impacting our long running jobs

The logic in `KubernetesLocalDiskShuffleExecutorComponents` itself is not that much. However when I tried this on the `LocalDiskShuffleExecutorComponents` it was not a successful experiment which suggests there is more to recovering shuffle files

I'd like to understand what will be the work involved for this. We'll be more than happy to contribute",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,06:23.0,,,,,,,,,,0|z1jc88:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add ""--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED"" so Platform can access cleaner on Java 9+",SPARK-45508,13553769,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,10/12/23 0:59,10/13/23 17:44,7/17/24 20:45,10/13/23 5:30,"3.3.2, 3.4.1, 3.5.0","3.3.4, 3.4.2, 3.5.1, 4.0.0",Spark Core,,0,pull-request-available,We need to add `--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED` to our JVM options so that the code in `org.apache.spark.unsafe.Platform` can access the JDK internal cleaner classes.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 13 05:30:00 UTC 2023,,,,,,,,,,0|z1kvzc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"13/Oct/23 05:30;LuciferYang;Issue resolved by pull request 43344
[https://github.com/apache/spark/pull/43344];;;",3.4.1,3.5.0,,,
Support stage level task resource profile for k8s cluster when dynamic allocation disabled,SPARK-45495,13553604,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wbo4958,wbo4958,wbo4958,10/11/23 0:38,10/13/23 15:52,7/17/24 20:45,10/13/23 15:52,3.4.1,"3.5.1, 4.0.0",Spark Core,,0,pull-request-available,"[https://github.com/apache/spark/pull/37268] has introduced a new feature that supports stage-level schedule task resource profile for standalone cluster when dynamic allocation is disabled. It's really cool feature, especially for ML/DL cases, more details can be found in that PR.

 

The problem here is that the feature is only available for standalone and YARN cluster for now, but most users would also expect it can be used for other spark clusters like K8s.

 

So I filed this issue to track this task.",,,,,,,,,,,,,,,,SPARK-45250,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,38:46.0,,,,,,,,,,0|z1kuyo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Mapstatus location type changed from external shuffle service to executor after decommission migration,SPARK-45310,13551859,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,9/25/23 8:33,10/11/23 7:29,7/17/24 20:45,10/11/23 4:05,"3.0.3, 3.1.3, 3.2.4, 3.3.2, 3.4.1, 3.5.0",4.0.0,Spark Core,,0,pull-request-available,"When migrating shuffle blocks during decommission, the updated mapstatus location doesn't respect the external shuffle service location when external shuffle service is enabled.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 11 04:05:04 UTC 2023,,,,,,,,,,0|z1kk88:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"11/Oct/23 04:05;dongjoon;Issue resolved by pull request 43112
[https://github.com/apache/spark/pull/43112];;;",3.1.3,3.2.4,3.3.2,3.4.1,
Support stage level task resource profile for yarn cluster when dynamic allocation disabled,SPARK-45250,13551521,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wbo4958,wbo4958,wbo4958,9/21/23 11:50,10/11/23 0:38,7/17/24 20:45,10/3/23 4:01,3.4.1,"3.5.1, 4.0.0",Spark Core,,0,pull-request-available,"[https://github.com/apache/spark/pull/37268] has introduced a new feature that supports stage-level schedule task resource profile for standalone cluster when dynamic allocation is disabled. It's really cool feature, especially for ML/DL cases, more details can be found in that PR.

 

The problem here is that the feature is only available for standalone cluster for now, but most users would also expect it can be used for other spark clusters like yarn and k8s.

 

So I file this issue to track this task.",,,,,,,,,,,,,,,SPARK-45495,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 03 04:01:10 UTC 2023,,,,,,,,,,0|z1ki54:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"21/Sep/23 12:02;wbo4958;I made a PR to fix it [https://github.com/apache/spark/pull/43030.] Could someone help to review it?;;;, 21/Sep/23 12:07;wbo4958;Hi [~ivoson], May I ask why your previous PR [https://github.com/apache/spark/pull/37268]  supports task resource profile only for standalone cluster. what's your concern about not enabling this feature for yarn or k8s?;;;, 03/Oct/23 04:01;mridulm80;Issue resolved by pull request 43030
[https://github.com/apache/spark/pull/43030];;;",,,,,"21/Sep/23 12:07;wbo4958;Hi [~ivoson], May I ask why your previous PR [https://github.com/apache/spark/pull/37268]  supports task resource profile only for standalone cluster. what's your concern about not enabling this feature for yarn or k8s?;;;"
Add more tests for Scala foreachBatch and streaming listeners ,SPARK-44434,13543591,,Task,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,rangadi,rangadi,7/14/23 17:58,10/10/23 1:49,7/17/24 20:45,,3.4.1,,"Connect, Structured Streaming",,0,,Currently there are very few tests for Scala foreachBatch. Consider adding more tests and covering more test scenarios (multiple queries etc). ,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,SPARK-42938,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 10 01:49:34 UTC 2023,,,,,,,,,,0|z1j5l4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,10/Oct/23 01:49;bcarlsonprogram;I'd like to work on this.;;;,,,,,
DataSourceV2 cannot report KeyGroupedPartitioning with multiple keys per partition,SPARK-42716,13527620,,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,enricomi,enricomi,3/8/23 11:14,10/10/23 0:17,7/17/24 20:45,,"3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1",,SQL,,0,pull-request-available,"From Spark 3.0.0 until 3.2.3, a DataSourceV2 could report its partitioning as {{KeyGroupedPartitioning}} via {{SupportsReportPartitioning}}, even if multiple keys belong to a partition.

With SPARK-37377, only if all partitions implement {{HasPartitionKey}}, the partition information reported through {{SupportsReportPartitioning}} is considered by catalyst. But this limits the number of keys per partition to 1.

Spark should continue to support the more general situation of {{KeyGroupedPartitioning}} with multiple keys per partition, like {{HashPartitioning}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 11:23:06 UTC 2023,,,,,,,,,,0|z1gfi8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"08/Mar/23 11:22;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40334;;;, 08/Mar/23 11:23;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40334;;;",3.3.1,3.3.2,3.4.0,3.4.1,"08/Mar/23 11:23;apachespark;User 'EnricoMi' has created a pull request for this issue:
https://github.com/apache/spark/pull/40334;;;"
DS V2 supports push down V2 UDF that has magic method,SPARK-44913,13548254,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coneyliu,coneyliu,coneyliu,8/22/23 14:58,10/9/23 4:47,7/17/24 20:45,9/29/23 23:37,3.4.1,4.0.0,SQL,,0,pull-request-available,"Right now we only support pushing down the V2 UDF that has not a magic method. Because the V2 UDF will be analyzed into the `ApplyFunctionExpression` which could be translated and pushed down. However, a V2 UDF that has the magic method will be analyzed into `StaticInvoke` or `Invoke` that can not be translated into V2 expression and then can not be pushed down to the data source. The magic method is suggested. So this PR adds the support of pushing down the V2 UDF that has a magic method.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 29 23:37:03 UTC 2023,,,,,,,,,,0|z1jxzc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 03:29;snoot;User 'ConeyLiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/42612;;;, 29/Sep/23 23:37;csun;Issue resolved by pull request 42612
[https://github.com/apache/spark/pull/42612];;;",,,,,"29/Sep/23 23:37;csun;Issue resolved by pull request 42612
[https://github.com/apache/spark/pull/42612];;;"
Decimal precision exceeds max precision error when using unary minus on min Decimal values on Scala 2.13 Spark,SPARK-45438,13553173,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,navkumar,navkumar,10/6/23 17:46,10/6/23 17:48,7/17/24 20:45,,"3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.4.0, 3.4.1, 3.5.0",,SQL,,0,scala,"When submitting an application to Spark built with Scala 2.13, there are issues with Decimal overflow that show up when using unary minus (and also {{abs()}} which uses unary minus under the hood.

Here is an example PySpark reproduce use case:

{code}
from decimal import Decimal

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType,StructField, DecimalType

spark = SparkSession.builder \
      .master(""local[*]"") \
      .appName(""decimal_precision"") \
      .config(""spark.rapids.sql.explain"", ""ALL"") \
      .config(""spark.sql.ansi.enabled"", ""true"") \
      .config(""spark.sql.legacy.allowNegativeScaleOfDecimal"", 'true') \
      .getOrCreate()  

precision = 38
scale = 0
DECIMAL_MIN = Decimal('-' + ('9' * precision) + 'e' + str(-scale))

data = [[DECIMAL_MIN]]

schema = StructType([
    StructField(""a"", DecimalType(precision, scale), True)])
df = spark.createDataFrame(data=data, schema=schema)

df.selectExpr(""a"", ""-a"").show()
{code}

This particular example will run successfully on Spark built with Scala 2.12, but throw a java.math.ArithmeticException on Spark built with Scala 2.13. 

If you change the value of {{DECIMAL_MIN}} in the previous code to something just ahead of the original DECIMAL_MIN, you will not get an exception thrown, but instead you will get an incorrect answer (possibly due to overflow):

{code}
...
DECIMAL_MIN = Decimal('-8' + ('9' * (precision-1)) + 'e' + str(-scale))
...
{code} 

Output:
{code}
+--------------------+--------------------+
|                   a|               (- a)|
+--------------------+--------------------+
|-8999999999999999...|90000000000000000...|
+--------------------+--------------------+
{code}

It looks like the code in {{Decimal.scala}} uses {{scala.math.BigDecimal}}. See https://github.com/scala/bug/issues/11590 with updates on how Scala 2.13 handles BigDecimal. It looks like there is {{java.math.MathContext}} missing when performing these operations. ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,46:16.0,,,,,,,,,,0|z1ksb4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,"3.2.1, 3.4.1, 3.5.0",3.2.2,3.2.3,3.2.4,
Add extra per-rule validation for optimization rewrites.,SPARK-44219,13541636,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,outis,outis,outis,6/27/23 21:47,10/6/23 3:20,7/17/24 20:45,10/6/23 3:20,"3.4.0, 3.4.1",4.0.0,Optimizer,,0,pull-request-available,"Adds per-rule validation checks for the following:

1.  aggregate expressions in Aggregate plans are valid.
2. Grouping key types in Aggregate plans cannot by of type Map. 
3. No dangling references have been generated.

This is validation is by default enabled for all tests or selectively using the spark.sql.planChangeValidation=true flag.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 06 03:20:51 UTC 2023,,,,,,,,,,0|z1itlc:,9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,"23/Aug/23 09:00;githubbot;User 'YannisSismanis' has created a pull request for this issue:
https://github.com/apache/spark/pull/41763;;;, 06/Oct/23 03:20;cloud_fan;Issue resolved by pull request 41763
[https://github.com/apache/spark/pull/41763];;;",3.4.1,,,,"06/Oct/23 03:20;cloud_fan;Issue resolved by pull request 41763
[https://github.com/apache/spark/pull/41763];;;"
MLlib GBTClassifier has wrong impurity method 'variance' instead of 'gini' or 'entropy'. ,SPARK-44848,13547715,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,lisi,lisi,8/17/23 12:04,10/3/23 11:40,7/17/24 20:45,,3.4.1,,MLlib,,1,,"Impurity method 'variance' should only be used for regressors, *not* classifiers. For classifiers gini and entropy should be available as it is already the case for the RandomForestClassifier [https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.ml.classification.RandomForestClassifier.html] .

Because of this bug 'minInfoGain' hyperparameter cannot be tuned to combat overfitting. ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 03 11:40:48 UTC 2023,,,,,,,,,,0|z1juns:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Oct/23 11:40;oumarnour;Hello,

I have the same issue. I want to know if that issue is solved ?

Thanks;;;",,,,,
Deadlock caused by rdd replication level of 2,SPARK-45057,13549452,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,warrenzhu25,warrenzhu25,warrenzhu25,9/1/23 23:32,9/28/23 23:53,7/17/24 20:45,9/28/23 23:52,3.4.1,"3.3.4, 3.4.2, 3.5.1, 4.0.0",Spark Core,,0,pull-request-available," 
When 2 tasks try to compute same rdd with replication level of 2 and running on only 2 executors. Deadlock will happen.

Task only release lock after writing into local machine and replicate to remote executor.

 
||Time||Exe 1 (Task Thread T1)||Exe 1 (Shuffle Server Thread T2)||Exe 2 (Task Thread T3)||Exe 2 (Shuffle Server Thread T4)||
|T0|write lock of rdd| | | |
|T1| | |write lock of rdd| |
|T2|replicate -> UploadBlockSync (blocked by T4)| | | |
|T3| | | |Received UploadBlock request from T1 (blocked by T4)|
|T4| | |replicate -> UploadBlockSync (blocked by T2)| |
|T5| |Received UploadBlock request from T3 (blocked by T1)| | |
|T6|Deadlock|Deadlock|Deadlock|Deadlock|",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 28 23:52:57 UTC 2023,,,,,,,,,,0|z1k5dk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"27/Sep/23 06:37;Ngone51;In the case of ""Received UploadBlock request from T1 (blocked by T4)"", shouldn't it be blocked by T3?;;;, 28/Sep/23 23:52;mridulm80;Issue resolved by pull request 43067
[https://github.com/apache/spark/pull/43067];;;",,,,,"28/Sep/23 23:52;mridulm80;Issue resolved by pull request 43067
[https://github.com/apache/spark/pull/43067];;;"
Switch languages consistently across docs for all code snippets,SPARK-44820,13547434,13546596,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,panbingkun,allisonwang-db,allisonwang-db,8/15/23 18:57,9/25/23 2:34,7/17/24 20:45,8/25/23 0:49,"3.4.1, 3.5.0","3.5.0, 4.0.0",Documentation,,0,,"When a user chooses a different language for a code snippet, all code snippets on that page should switch to the chosen language. This was the behavior for, for example, Spark 2.0 doc: [https://spark.apache.org/docs/2.0.0/structured-streaming-programming-guide.html]

But it was broken for later docs, for example the Spark 3.4.1 doc: [https://spark.apache.org/docs/latest/quick-start.html]

We should fix this behavior change and possibly add test cases to prevent future regressions.",,"allisonwang-db commented on PR #474:
URL: https://github.com/apache/spark-website/pull/474#issuecomment-1730342331

   Hi @panbingkun this is an important bug fix and we should merge it! Shall we re-open this?


;21/Sep/23 21:44;githubbot;600, panbingkun commented on PR #474:
URL: https://github.com/apache/spark-website/pull/474#issuecomment-1730705788

   > Hi @panbingkun this is an important bug fix and we should merge it! Shall we re-open this?
   
   Actually, this feature has been fixed in `Spark` project, https://github.com/apache/spark/pull/42657
   The current fixes are: master, branch-3.5 (https://github.com/apache/spark/pull/42657), and branch-3.4 (https://github.com/apache/spark/pull/42989)


;22/Sep/23 02:07;githubbot;600, allisonwang-db commented on PR #474:
URL: https://github.com/apache/spark-website/pull/474#issuecomment-1730748815

   Yea we should apply the change in the `spark` repo to the actual released Spark website docs here. @panbingkun which option do you think is better?


;22/Sep/23 03:22;githubbot;600, panbingkun commented on PR #474:
URL: https://github.com/apache/spark-website/pull/474#issuecomment-1730757033

   > Yea we should apply the change in the `spark` repo to the actual released Spark website docs here. @panbingkun which option do you think is better?
   
   If we are not suitable for republishing historically published documents, we can only manually update them on Spark website. If possible, I can complete it.


;22/Sep/23 03:37;githubbot;600, allisonwang-db commented on PR #474:
URL: https://github.com/apache/spark-website/pull/474#issuecomment-1731741985

   @panbingkun yes let's update the spark website (this repo) to fix this UI issue for published docs.


;22/Sep/23 16:55;githubbot;600, panbingkun commented on PR #474:
URL: https://github.com/apache/spark-website/pull/474#issuecomment-1732813770

   > @panbingkun yes let's update the spark website (this repo) to fix this UI issue for published docs.
   
   Okay, let me to fix it.


;25/Sep/23 02:34;githubbot;600",,0,3600,,,0,3600,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 00:49:22 UTC 2023,,,,,,,,,,0|z1jsxc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"16/Aug/23 06:26;podongfeng;update:

this issue emerged since 3.1.1 (note that we don't have an official 3.1.0, since 3.1.0 was a mistake https://spark.apache.org/news/index.html)

3.0.3 works well: https://spark.apache.org/docs/3.0.3/structured-streaming-programming-guide.html

3.1.1 was broken: https://spark.apache.org/docs/3.1.1/structured-streaming-programming-guide.html

;;;, 24/Aug/23 02:28;panbingkun;Let me try to investigate it.;;;, 25/Aug/23 00:49;Gengliang.Wang;Resolved in https://github.com/apache/spark/pull/42657;;;",3.5.0,,,,24/Aug/23 02:28;panbingkun;Let me try to investigate it.;;;
Arrow DurationWriter fails when vector is at capacity,SPARK-45256,13551547,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sgoos-db,sgoos-db,sgoos-db,9/21/23 15:04,9/22/23 16:17,7/17/24 20:45,9/22/23 16:17,"3.4.0, 3.4.1, 3.4.2, 3.5.0, 3.5.1",4.0.0,SQL,,0,pull-request-available,The DurationWriter fails if more values are written than the initial capacity of the DurationVector (4032). Fix by using `setSafe` instead of `set` method. ,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 22 16:17:36 UTC 2023,,,,,,,,,,0|z1kiaw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"22/Sep/23 16:17;dongjoon;Issue resolved by pull request 43035
[https://github.com/apache/spark/pull/43035];;;",3.4.1,3.4.2,3.5.0,3.5.1,
Improve error handling in Connect foreachBatch worker.,SPARK-44463,13543838,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,rangadi,rangadi,7/17/23 19:23,9/20/23 3:45,7/17/24 20:45,9/20/23 3:44,3.4.1,4.0.0,"Connect, Structured Streaming",,0,,An error in user code inside foreachBatch worker is not propagated correctly to the user. We should. ,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,SPARK-42938,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 20 03:45:08 UTC 2023,,,,,,,,,,0|z1j740:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"20/Sep/23 03:44;gurwls223;Issue resolved by pull request 42986
[https://github.com/apache/spark/pull/42986];;;, 20/Sep/23 03:45;snoot;User 'bogao007' has created a pull request for this issue:
https://github.com/apache/spark/pull/42986;;;",,,,,"20/Sep/23 03:45;snoot;User 'bogao007' has created a pull request for this issue:
https://github.com/apache/spark/pull/42986;;;"
Switch languages consistently across docs for all code snippets (Spark 3.4 and below),SPARK-45210,13551170,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,9/19/23 2:56,9/19/23 5:52,7/17/24 20:45,9/19/23 5:52,"3.1.3, 3.2.4, 3.3.2, 3.4.1","3.3.4, 3.4.2",Documentation,,0,,Similar with SPARK-44820 but needs different change as they were refactored at https://github.com/apache/spark/commit/12b9b771c7ad75cb90c0a51cd2d0581dd3c719e2,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 19 05:52:49 UTC 2023,,,,,,,,,,0|z1kfz4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"19/Sep/23 05:52;gurwls223;Issue resolved by pull request 42989
[https://github.com/apache/spark/pull/42989];;;",3.2.4,3.3.2,3.4.1,,
The ArrayInsert function should make explicit casting when element type not equals derived component type,SPARK-45078,13549654,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,taoran,taoran,9/5/23 11:24,9/18/23 18:56,7/17/24 20:45,9/17/23 8:17,3.4.1,"3.4.2, 3.5.1, 4.0.0",SQL,,0,pull-request-available,"Generally speaking, array_insert has same insert semantic with  array_prepend/array_append. however, if we run sql use element cast like below, array_prepend/array_append can get right result. but array_insert failed.
{code:java}
spark-sql (default)> select array_prepend(array(1), cast(2 as tinyint));
[2,1]
Time taken: 0.123 seconds, Fetched 1 row(s) {code}
{code:java}
spark-sql (default)> select array_append(array(1), cast(2 as tinyint)); 
[1,2] 
Time taken: 0.206 seconds, Fetched 1 row(s)
{code}
{code:java}
spark-sql (default)> select array_insert(array(1), 2, cast(2 as tinyint));
[DATATYPE_MISMATCH.ARRAY_FUNCTION_DIFF_TYPES] Cannot resolve ""array_insert(array(1), 2, CAST(2 AS TINYINT))"" due to data type mismatch: Input to `array_insert` should have been ""ARRAY"" followed by a value with same element type, but it's [""ARRAY<INT>"", ""TINYINT""].; line 1 pos 7;
'Project [unresolvedalias(array_insert(array(1), 2, cast(2 as tinyint)), None)]
+- OneRowRelation {code}
The reported error is clear, however, we may should do explicit casting here. because multiset type such as array or map allow the operands of same type family  to coexist.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 18 18:56:22 UTC 2023,,,,,,,,,,0|z1k6mg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"17/Sep/23 08:17;maxgekk;Issue resolved by pull request 42951
[https://github.com/apache/spark/pull/42951];;;, 18/Sep/23 18:56;dongjoon;This is resolved via https://github.com/apache/spark/pull/42960;;;",,,,,18/Sep/23 18:56;dongjoon;This is resolved via https://github.com/apache/spark/pull/42960;;;
"Parquet reads fail with ""RuntimeException: Unable to create Parquet converter for data type ""timestamp_ntz"" due to incorrect schema inference",SPARK-45194,13550996,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ivan.sadikov,ivan.sadikov,9/18/23 4:59,9/18/23 5:03,7/17/24 20:45,,"3.4.1, 3.5.0, 4.0.0",,SQL,,0,,"I found that Parquet reads could fail due to incorrect schema inference with two conflicting types exist in files. This is caused by the fact that schema inference only considers one file by default which could contain different types than what in other file.

We have {{spark.sql.parquet.mergeSchema}} is set to `false` by default. This causes schema inference to pick a file (depending on the order the file system returns files) and infer schema based on that file. However, if you have conflicting types or a smaller/narrower type is selected, instead of failing during schema inference, an exception is thrown during the subsequent read.

In this case, we infer schema based on the file with TIMESTAMP_NTZ and fail to read the file that contains TIMESTAMP_LTZ:
{code:java}
[info]   Cause: java.lang.RuntimeException: Unable to create Parquet converter for data type ""timestamp_ntz"" whose Parquet type is int64 time(TIMESTAMP(MILLIS,true))
[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.convertErrorForTimestampNTZ(ParquetVectorUpdaterFactory.java:209)
[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.validateTimestampType(ParquetVectorUpdaterFactory.java:203)
[info]   at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:121)
[info]   at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175){code}
Note that if the file with TIMESTAMP_LTZ is selected, the read succeeds.

 

Here is the repro as a unit test that you can run in Spark master. Just add the test to ParquetIOSuite or some other test suite.
{code:java}
import org.apache.hadoop.conf._
import org.apache.hadoop.fs._
import org.apache.parquet.example.data.simple._
import org.apache.parquet.hadoop.example._
import org.apache.parquet.schema._

// Creates a Parquet file with two simple columns: integer and timestamp.
// Depending on isUTC flag, the timestamp is either NTZ or LTZ.
private def createParquetFile(path: String, isUTC: Boolean): Unit = {
  val schema = MessageTypeParser.parseMessageType(
    s""""""
    message schema {
      optional int32 a;
      optional int64 ts (TIMESTAMP(MILLIS, $isUTC));
    }
    """"""
  )
  val conf = new Configuration(false)
  conf.set(""parquet.example.schema"", schema.toString)
  val writer = ExampleParquetWriter.builder(new Path(path)).withConf(conf).build()
  for (i <- 0 until 2) {
    val group = new SimpleGroup(schema)
    group.add(""a"", 1)
    group.add(""ts"", System.currentTimeMillis)
    writer.write(group)
  }
  writer.close()
}

test(""repro"") {
  withTempPath { dir =>
    createParquetFile(dir + ""/file-1.parquet"", false) // NTZ
    createParquetFile(dir + ""/file-2.parquet"", true) // LTZ    

    val df = spark.read.parquet(dir.getAbsolutePath)
    df.show() // fails
  }
} {code}
If you run the repro as is, you will get: 
{code:java}
[info]   Cause: java.lang.RuntimeException: Unable to create Parquet converter for data type ""timestamp_ntz"" whose Parquet type is int64 time(TIMESTAMP(MILLIS,true)) {code}
If you swap the files (file names), the read succeeds.
{code:java}
+---+--------------------+
|  a|                  ts|
+---+--------------------+
|  1|2023-09-17 21:59:...|
|  1|2023-09-17 21:59:...|
|  1|2023-09-17 21:59:...|
|  1|2023-09-17 21:59:...|
+---+--------------------+ {code}
If you set spark.sql.parquet.mergeSchema to true, the schema inference fails with
{code:java}
[info]   org.apache.spark.SparkException: [CANNOT_MERGE_SCHEMAS] Failed merging schemas:
[info] Initial schema:
[info] ""STRUCT<a: INT, ts: TIMESTAMP_NTZ>""
[info] Schema that cannot be merged with the initial schema:
[info] ""STRUCT<a: INT, ts: TIMESTAMP>"". {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 18 05:00:22 UTC 2023,,,,,,,,,,0|z1kewg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,18/Sep/23 05:00;ivan.sadikov;cc [~gengliang] [~cloud_fan];;;,3.5.0,4.0.0,,,
AggregatingAccumulator with TypedImperativeAggregate throwing ClassCastException,SPARK-45176,13550779,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,hcampbell,hcampbell,9/15/23 3:48,9/15/23 3:50,7/17/24 20:45,,"3.4.0, 3.4.1",,SQL,,0,,"Probably related to SPARK-39044. But potentially also this comment in Executor.scala.
{quote}// TODO: do not serialize value twice
val directResult = new DirectTaskResult(valueByteBuffer, accumUpdates, metricPeaks)
{quote}
The class cast exception I'm seeing is
{quote}
java.lang.ClassCastException: class [B cannot be cast to class org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir
{quote}
But I've seen it with other aggregation buffers like QuantileSummaries as well.

It's my belief that withBufferSerialized() for the Aggregating Accumulator is being called twice, leading to on serializeAggregateBuffernPlace(buffer)
also being called twice for the an Imperative aggregate, the second time round, the buffer is already a byte array and the asInstanceOf[T] in getBufferObject is throwing.

This doesn't appear to happen on all runs, and it might be its only occurring when there's a transitive exception. I have a further suspicion that the cause might originate with
{quote}
SerializationDebugger.improveException
{quote}
which is traversing the task and forcing writeExternal, to be called.

Setting
|spark.serializer.extraDebugInfo|false|

Seems to make things a bit more reliable (I haven't seen the error while this setting is on), and points strongly in that direction.

Stack trace:
{quote}
Job aborted due to stage failure: Authorized committer (attemptNumber=0, stage=15, partition=10) failed; but task commit success, data duplication may happen. reason=ExceptionFailure(java.io.IOException,java.lang.ClassCastException: class [B cannot be cast to class org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir ([B is in module java.base of loader 'bootstrap'; org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir is in unnamed module of loader 'app'),[Ljava.lang.StackTraceElement;@7fe2f462,java.io.IOException: java.lang.ClassCastException: class [B cannot be cast to class org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir ([B is in module java.base of loader 'bootstrap'; org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir is in unnamed module of loader 'app')
at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1502)
at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:59)
at java.base/java.io.ObjectOutputStream.writeExternalData(Unknown Source)
at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(Unknown Source)
at java.base/java.io.ObjectOutputStream.writeObject0(Unknown Source)
at java.base/java.io.ObjectOutputStream.writeObject(Unknown Source)
at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46)
at org.apache.spark.serializer.SerializerHelper$.serializeToChunkedBuffer(SerializerHelper.scala:42)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:643)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassCastException: class [B cannot be cast to class org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir ([B is in module java.base of loader 'bootstrap'; org.apache.spark.sql.catalyst.expressions.aggregate.Reservoir is in unnamed module of loader 'app')
at org.apache.spark.sql.catalyst.expressions.aggregate.ReservoirSample.serialize(ReservoirSample.scala:33)
at org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate.serializeAggregateBufferInPlace(interfaces.scala:624)
at org.apache.spark.sql.execution.AggregatingAccumulator.withBufferSerialized(AggregatingAccumulator.scala:206)
at org.apache.spark.sql.execution.AggregatingAccumulator.withBufferSerialized(AggregatingAccumulator.scala:33)
at org.apache.spark.util.AccumulatorV2.writeReplace(AccumulatorV2.scala:186)
at jdk.internal.reflect.GeneratedMethodAccessor62.invoke(Unknown Source)
at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.base/java.lang.reflect.Method.invoke(Unknown Source)
at java.base/java.io.ObjectStreamClass.invokeWriteReplace(Unknown Source)
at java.base/java.io.ObjectOutputStream.writeObject0(Unknown Source)
at java.base/java.io.ObjectOutputStream.writeObject(Unknown Source)
at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$2(TaskResult.scala:62)
at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$2$adapted(TaskResult.scala:62)
at scala.collection.immutable.Vector.foreach(Vector.scala:1856)
at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:62)
at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1495)
... 11 more
 
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,48:59.0,,,,,,,,,,0|z1kdk8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,,,,
Data lost after union using spark.sql.parquet.enableNestedColumnVectorizedReader=true,SPARK-44805,13547273,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,jwozniak,jwozniak,8/14/23 14:46,9/13/23 15:32,7/17/24 20:45,9/8/23 20:11,"3.3.1, 3.4.1","3.3.4, 3.4.2, 3.5.1, 4.0.0",Spark Core,,0,"correctness, pull-request-available","When union-ing two DataFrames read from parquet containing nested structures (2 fields of array types where one is double and second is integer) data from the second field seems to be lost (zeros are set instead). 

This seems to be the case only if nested vectorised reader is used (spark.sql.parquet.enableNestedColumnVectorizedReader=true). 

The following Python code reproduces the problem: 
{code:java}
from pyspark.sql import SparkSession
from pyspark.sql.types import *

# PREPARING DATA
data1 = []
data2 = []

for i in range(2): 
    data1.append( (([1,2,3],[1,1,2]),i))
    data2.append( (([1.0,2.0,3.0],[1,1]),i+10))

schema1 = StructType([
        StructField('value', StructType([
             StructField('f1', ArrayType(IntegerType()), True),
             StructField('f2', ArrayType(IntegerType()), True)             
             ])),
         StructField('id', IntegerType(), True)
])

schema2 = StructType([
        StructField('value', StructType([
             StructField('f1', ArrayType(DoubleType()), True),
             StructField('f2', ArrayType(IntegerType()), True)             
             ])),
         StructField('id', IntegerType(), True)
])

spark = SparkSession.builder.getOrCreate()
data_dir = ""/user/<user>/""

df1 = spark.createDataFrame(data1, schema1)
df1.write.mode('overwrite').parquet(data_dir + ""data1"") 
df2 = spark.createDataFrame(data2, schema2)
df2.write.mode('overwrite').parquet(data_dir + ""data2"") 


# READING DATA
parquet1 = spark.read.parquet(data_dir + ""data1"")
parquet2 = spark.read.parquet(data_dir + ""data2"")


# UNION
out = parquet1.union(parquet2)


parquet1.select(""value.f2"").distinct().show()
out.select(""value.f2"").distinct().show()
print(parquet1.collect())
print(out.collect()) {code}
Output: 
{code:java}
+---------+
|       f2|
+---------+
|[1, 1, 2]|
+---------+

+---------+
|       f2|
+---------+
|[0, 0, 0]|
|   [1, 1]|
+---------+


[
Row(value=Row(f1=[1, 2, 3], f2=[1, 1, 2]), id=0), 
Row(value=Row(f1=[1, 2, 3], f2=[1, 1, 2]), id=1)
]

[
Row(value=Row(f1=[1.0, 2.0, 3.0], f2=[0, 0, 0]), id=0), 
Row(value=Row(f1=[1.0, 2.0, 3.0], f2=[0, 0, 0]), id=1), 
Row(value=Row(f1=[1.0, 2.0, 3.0], f2=[1, 1]), id=10), 
Row(value=Row(f1=[1.0, 2.0, 3.0], f2=[1, 1]), id=11)
] {code}
Please notice that values for the field f2 are lost after the union is done. This only happens when this data is read from parquet files. 

Could you please look into this? 

Best regards,

Jakub","pySpark, linux, hadoop, parquet. ",,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 08 20:11:31 UTC 2023,,,,,,,,,,0|z1jrxk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"15/Aug/23 00:14;bersprockets;[~sunchao] 

It seems to be some weird interaction between Parquet nested vectorization and the {{Cast}} expression:
{noformat}
drop table if exists t1;

create table t1 using parquet as
select * from values
(named_struct('f1', array(1, 2, 3), 'f2', array(1, 1, 2)))
as (value);

select value from t1;
{""f1"":[1,2,3],""f2"":[1,1,2]}         <== this is expected
Time taken: 0.126 seconds, Fetched 1 row(s)

select cast(value as struct<f1:array<double>,f2:array<int>>) AS value from t1;
{""f1"":[1.0,2.0,3.0],""f2"":[0,0,0]}   <== this is not expected
Time taken: 0.102 seconds, Fetched 1 row(s)

set spark.sql.parquet.enableNestedColumnVectorizedReader=false;

select cast(value as struct<f1:array<double>,f2:array<int>>) AS value from t1;
{""f1"":[1.0,2.0,3.0],""f2"":[1,1,2]}   <== now has expected value
Time taken: 0.244 seconds, Fetched 1 row(s)
{noformat}
The union operation adds this {{Cast}} expression because {{value}} has different datatypes between your two dataframes.;;;, 28/Aug/23 15:21;jwozniak;Hello,

Is it possible to know any ETA on this one?

Is this something that could potentially be fixed in the next version of Spark or rather not? 

Thanks,

Jakub;;;, 05/Sep/23 23:50;bersprockets;I looked at this yesterday and I think I have a handle on what's going on. I will make a PR in the coming days.;;;, 06/Sep/23 07:30;jwozniak;Would be great, thanks! ;;;, 07/Sep/23 15:47;bersprockets;PR here: https://github.com/apache/spark/pull/42850;;;, 08/Sep/23 03:23;snoot;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/42850;;;, 08/Sep/23 20:11;dongjoon;Issue resolved by pull request 42850
[https://github.com/apache/spark/pull/42850];;;",3.4.1,,,,"28/Aug/23 15:21;jwozniak;Hello,

Is it possible to know any ETA on this one?

Is this something that could potentially be fixed in the next version of Spark or rather not? 

Thanks,

Jakub;;;"
Implement missing otherCopyArgs for the MultiCommutativeOp expression,SPARK-45117,13550258,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scnakandala,scnakandala,scnakandala,9/11/23 17:28,9/12/23 15:53,7/17/24 20:45,9/12/23 15:53,3.4.1,"3.5.1, 4.0.0",SQL,,0,pull-request-available,Calling toJSON on a `MultiCommutativeOp` throws an assertion error as it does not implement the `otherCopyArgs` method.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 15:53:05 UTC 2023,,,,,,,,,,0|z1kacg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Sep/23 15:53;cloud_fan;Issue resolved by pull request 42873
[https://github.com/apache/spark/pull/42873];;;",,,,,
create hive table with invalid column should return error class,SPARK-44911,13548206,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Zing,Zing,Zing,8/22/23 12:33,9/12/23 8:55,7/17/24 20:45,9/12/23 8:55,3.4.1,4.0.0,SQL,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 08:55:22 UTC 2023,,,,,,,,,,0|z1jxow:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"06/Sep/23 13:29;Zing;https://github.com/apache/spark/pull/42609;;;, 12/Sep/23 08:55;maxgekk;Issue resolved by pull request 42609
[https://github.com/apache/spark/pull/42609];;;",,,,,"12/Sep/23 08:55;maxgekk;Issue resolved by pull request 42609
[https://github.com/apache/spark/pull/42609];;;"
Alter table with invalid default value will not report error,SPARK-45075,13549612,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,9/5/23 3:49,9/12/23 6:33,7/17/24 20:45,9/8/23 20:18,"3.4.1, 3.5.0","3.4.2, 3.5.1, 4.0.0",SQL,,0,pull-request-available,"create table t(i boolean, s bigint);
alter table t alter column s set default badvalue;
 
The code wouldn't report error on DataSource V2, not align with V1.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 12 06:29:08 UTC 2023,,,,,,,,,,0|z1k6d4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"05/Sep/23 14:18;ignitetcbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42810;;;, 08/Sep/23 20:18;dongjoon;Issue resolved by pull request 42810
[https://github.com/apache/spark/pull/42810];;;, 12/Sep/23 06:29;dongjoon;This landed at branch-3.4 via https://github.com/apache/spark/pull/42876;;;",3.5.0,,,,"08/Sep/23 20:18;dongjoon;Issue resolved by pull request 42810
[https://github.com/apache/spark/pull/42810];;;"
Multi-tenant history server,SPARK-45126,13550308,,Wish,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ramuramaiah,ramuramaiah,9/12/23 4:48,9/12/23 4:48,7/17/24 20:45,,3.4.1,,Spark Core,,0,,"Spark history server makes use of the configuration ""spark.history.fs.logDirectory"" to locate the log events. This works well for a single tenant. When it is used for a multi-tenant deployment, the log events of multiple tenants are stored in a single directory which does not provide a logical separation of events for each tenant.

The proposal/wish is to have a support for Multi-tenant history server, where-in the configuration ""spark.history.fs.logDirectory"" can be a base directory. The sub-directories can contain the log events for each tenant. The sub-directories can be named after each tenant, for e.g. ""tenant1"", ""tenant2"" etc.

When it is combined to work with Spark Driver/Executor which makes use of the property ""spark.eventLog.dir"", the value of this property can be appropriately set for each tenant.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,48:56.0,,,,,,,,,,0|z1kank:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 3.4 multi-column sum slows with many columns,SPARK-44912,13548238,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,brbickel,brbickel,8/22/23 14:16,9/11/23 19:43,7/17/24 20:45,9/11/23 19:43,"3.4.0, 3.4.1",,PySpark,,0,,"The code below is a minimal reproducible example of an issue I discovered with Pyspark 3.4.x. I want to sum the values of multiple columns and put the sum of those columns (per row) into a new column. This code works and returns in a reasonable amount of time in Pyspark 3.3.x, but is extremely slow in Pyspark 3.4.x when the number of columns grows. See below for execution timing summary as N varies.
{code:java}
import pyspark.sql.functions as F
import random
import string
from functools import reduce
from operator import add
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# generate a dataframe N columns by M rows with random 8 digit column 
# names and random integers in [-5,10]
N = 30
M = 100
columns = [''.join(random.choices(string.ascii_uppercase +
                                  string.digits, k=8))
           for _ in range(N)]
data = [tuple([random.randint(-5,10) for _ in range(N)])
        for _ in range(M)]

df = spark.sparkContext.parallelize(data).toDF(columns)
# 3 ways to add a sum column, all of them slow for high N in spark 3.4
df = df.withColumn(""col_sum1"", sum(df[col] for col in columns))
df = df.withColumn(""col_sum2"", reduce(add, [F.col(col) for col in columns]))
df = df.withColumn(""col_sum3"", F.expr(""+"".join(columns))) {code}
Timing results for Spark 3.3:
||N||Exe Time (s)||
|5|0.514|
|10|0.248|
|15|0.327|
|20|0.403|
|25|0.279|
|30|0.322|
|50|0.430|

Timing results for Spark 3.4:
||N||Exe Time (s)||
|5|0.379|
|10|0.318|
|15|0.405|
|20|1.32|
|25|28.8|
|30|448|
|50|>10000 (did not finish)|",,,,,,,,,,,,,,,,,SPARK-45071,,SPARK-45071,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 11 19:43:46 UTC 2023,,,,,,,,,,0|z1jxvs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"10/Sep/23 17:15;bersprockets;It looks like this was fixed with SPARK-45071. Your issue was reported earlier, but missed somehow.;;;, 11/Sep/23 19:43;brbickel;Verified build containing linked issue fix solved the problem.;;;",3.4.1,,,,11/Sep/23 19:43;brbickel;Verified build containing linked issue fix solved the problem.;;;
 percentile_cont gets internal error when user input fails runtime replacement's input type check,SPARK-45106,13550053,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,9/8/23 14:19,9/8/23 19:40,7/17/24 20:45,9/8/23 19:40,"3.3.2, 3.4.1, 3.5.0, 4.0.0",3.5.1,SQL,,0,pull-request-available,"This query throws an internal error rather than producing a useful error message:
{noformat}
select percentile_cont(b) WITHIN GROUP (ORDER BY a DESC) as x 
from (values (12, 0.25), (13, 0.25), (22, 0.25)) as (a, b);

[INTERNAL_ERROR] Cannot resolve the runtime replaceable expression ""percentile_cont(a, b)"". The replacement is unresolved: ""percentile(a, b, 1)"".
org.apache.spark.SparkException: [INTERNAL_ERROR] Cannot resolve the runtime replaceable expression ""percentile_cont(a, b)"". The replacement is unresolved: ""percentile(a, b, 1)"".
	at org.apache.spark.SparkException$.internalError(SparkException.scala:92)
	at org.apache.spark.SparkException$.internalError(SparkException.scala:96)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:313)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:277)
...
{noformat}
It should instead inform the user that the input expression must be foldable.

{{PercentileCont}} does not check the user's input. If the runtime replacement (an instance of {{Percentile}}) rejects the user's input, the runtime replacement ends up unresolved.
",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 08 19:40:13 UTC 2023,,,,,,,,,,0|z1k934:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,08/Sep/23 19:40;dongjoon;This is resolved via https://github.com/apache/spark/pull/42857;;;,3.4.1,3.5.0,4.0.0,,
reflect() fails with an internal error on NULL class and method,SPARK-45100,13549931,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,9/7/23 13:22,9/8/23 16:00,7/17/24 20:45,9/8/23 8:13,"3.3.2, 3.4.1, 3.5.0, 4.0.0","3.3.4, 3.4.2, 3.5.0, 4.0.0",SQL,,0,pull-request-available,"The example below demonstrates the issue:

{code:sql}
spark-sql (default)> select reflect('java.util.UUID', CAST(NULL AS STRING));
[INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace.
{code}
",,,,,,,,,,,,,,,,SPARK-45079,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 08 15:09:18 UTC 2023,,,,,,,,,,0|z1k8c0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"08/Sep/23 08:13;maxgekk;Issue resolved by pull request 42849
[https://github.com/apache/spark/pull/42849];;;, 08/Sep/23 15:09;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/42855;;;",3.4.1,3.5.0,4.0.0,,08/Sep/23 15:09;dongjoon;This is backported to branch-3.4 via https://github.com/apache/spark/pull/42855;;;
percentile_approx() fails with an internal error on NULL accuracy,SPARK-45079,13549658,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,9/5/23 11:46,9/7/23 13:22,7/17/24 20:45,9/6/23 7:33,"3.3.2, 3.4.1, 3.5.0, 4.0.0","3.3.4, 3.4.2, 3.5.0, 4.0.0",SQL,,0,,"The example below demonstrates the issue:

{code:sql}
spark-sql (default)> SELECT percentile_approx(col, array(0.5, 0.4, 0.1), NULL) FROM VALUES (0), (1), (2), (10) AS tab(col);
[INTERNAL_ERROR] The Spark SQL phase analysis failed with an internal error. You hit a bug in Spark or the Spark plugins you use. Please, report this bug to the corresponding communities or vendors, and provide the full stack trace.
{code}
",,,,,,,,,,,,,,,SPARK-45100,SPARK-45060,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 06 10:33:07 UTC 2023,,,,,,,,,,0|z1k6nc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"05/Sep/23 11:59;aparna.garg;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/42817;;;, 06/Sep/23 07:33;maxgekk;Issue resolved by pull request 42817
[https://github.com/apache/spark/pull/42817];;;, 06/Sep/23 10:33;aparna.garg;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/42835;;;",3.4.1,3.5.0,4.0.0,,"06/Sep/23 07:33;maxgekk;Issue resolved by pull request 42817
[https://github.com/apache/spark/pull/42817];;;"
Display hexadecimal for thread lock hash code,SPARK-45086,13549731,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,9/6/23 4:06,9/7/23 7:18,7/17/24 20:45,9/7/23 7:18,"3.4.1, 3.5.0, 4.0.0",4.0.0,Web UI,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 07 07:18:50 UTC 2023,,,,,,,,,,0|z1k73k:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"06/Sep/23 04:15;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42826;;;, 07/Sep/23 07:18;yao;Issue resolved by pull request 42826
[https://github.com/apache/spark/pull/42826];;;",3.5.0,4.0.0,,,"07/Sep/23 07:18;yao;Issue resolved by pull request 42826
[https://github.com/apache/spark/pull/42826];;;"
spark job copies jars repeatedly if fs.defaultFS and application jar are same url,SPARK-44845,13547678,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zheju_he,zheju_he,zheju_he,8/17/23 7:22,9/7/23 4:16,7/17/24 20:45,9/7/23 4:16,3.4.1,4.0.0,YARN,,0,,"In the org.apache.spark.deploy.yarn.Client#compareUri method, hdfs://hadoop81:8020 and hdfs://192.168.0.81:8020 are regarded as different file systems (hadoop81 corresponds to 192.168.0.81). The specific reason is that in the last pr, different URIs of user information are also regarded as different file systems. Uri.getauthority is used to determine the user information, but authority contains the host so the URI above must be different from authority. To determine whether the user authentication information is different, you only need to determine URI.getUserInfo.

 

the last pr and issue link:
https://issues.apache.org/jira/browse/SPARK-22587

https://github.com/apache/spark/pull/19885",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 07 04:16:30 UTC 2023,,,,,,,,,,0|z1jufk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"17/Aug/23 07:34;zheju_he;my pr https://github.com/apache/spark/pull/42529;;;, 21/Aug/23 03:56;snoot;User 'zekai-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/42529;;;, 21/Aug/23 03:57;snoot;User 'zekai-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/42529;;;, 07/Sep/23 04:16;mridulm80;Issue resolved by pull request 42529
[https://github.com/apache/spark/pull/42529];;;",,,,,"21/Aug/23 03:56;snoot;User 'zekai-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/42529;;;"
SQL Page does not capture failed queries in analyzer ,SPARK-44801,13547185,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,8/14/23 6:16,9/6/23 4:15,7/17/24 20:45,8/24/23 15:32,"3.2.4, 3.3.2, 3.4.1, 3.5.0",4.0.0,"SQL, Web UI",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 06 04:15:51 UTC 2023,,,,,,,,,,0|z1jre0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"15/Aug/23 03:13;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42481;;;, 24/Aug/23 15:32;Qin Yao;Issue resolved by pull request 42481
[https://github.com/apache/spark/pull/42481];;;, 06/Sep/23 04:15;snoot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42825;;;",3.3.2,3.4.1,3.5.0,,"24/Aug/23 15:32;Qin Yao;Issue resolved by pull request 42481
[https://github.com/apache/spark/pull/42481];;;"
Introduce simpe conf system for sql/api,SPARK-44284,13542350,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/3/23 22:41,9/5/23 14:42,7/17/24 20:45,7/6/23 12:23,3.4.1,3.5.0,Connect,,0,,"Create a simple conf system for classes in sql/api. This is needed for a number of classes that are moved from sql/catalyst to sql/api that require configuration access (e.g. timeZone, parsing behavior, ...).

The change will add a small common interface that allows you to read the needed configurations, this interface is implemented by SQLConf and SQLConf will be used when we are executing on the driver, and there will be an implementation using the default values for when we are in Connect mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 05 14:42:48 UTC 2023,,,,,,,,,,0|z1ixzk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"04/Jul/23 03:52;snoot;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/41838;;;, 04/Jul/23 03:53;snoot;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/41838;;;, 05/Sep/23 14:09;tgraves;Can we get a description on this? This seems like a fairly significant change for a one line without description here or in the pr.;;;, 05/Sep/23 14:42;hvanhovell;I added a description. IMO the change itself not too spectacular.;;;",,,,,"04/Jul/23 03:53;snoot;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/41838;;;"
PushFoldableIntoBranches in complex grouping expressions may cause bindReference error,SPARK-44846,13547697,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhuml,zhuml,zhuml,8/17/23 9:05,9/4/23 12:48,7/17/24 20:45,9/4/23 12:48,3.4.1,"3.4.2, 3.5.0, 4.0.0",SQL,,0,,"SQL:
{code:java}
select c*2 as d from
(select if(b > 1, 1, b) as c from
(select if(a < 0, 0 ,a) as b from t group by b) t1
group by c) t2 {code}
ERROR:
{code:java}
Couldn't find _groupingexpression#15 in [if ((_groupingexpression#15 > 1)) 1 else _groupingexpression#15#16]
java.lang.IllegalStateException: Couldn't find _groupingexpression#15 in [if ((_groupingexpression#15 > 1)) 1 else _groupingexpression#15#16]
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren(TreeNode.scala:1241)
    at org.apache.spark.sql.catalyst.trees.BinaryLike.mapChildren$(TreeNode.scala:1240)
    at org.apache.spark.sql.catalyst.expressions.BinaryExpression.mapChildren(Expression.scala:653)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.TernaryLike.mapChildren(TreeNode.scala:1272)
    at org.apache.spark.sql.catalyst.trees.TernaryLike.mapChildren$(TreeNode.scala:1271)
    at org.apache.spark.sql.catalyst.expressions.If.mapChildren(conditionalExpressions.scala:41)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1215)
    at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1214)
    at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:533)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:405)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)
    at scala.collection.immutable.List.map(List.scala:293)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:94)
    at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:360)
    at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:538)
    at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.doProduce(AggregateCodegenSupport.scala:69)
    at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.doProduce$(AggregateCodegenSupport.scala:65)
    at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:49)
    at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
    at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
    at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:49)
    at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:660)
    at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:723)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
    at org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:93)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$doExecute$1(AdaptiveSparkPlanExec.scala:386)
    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)
    at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:386)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)
    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
    at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)
    at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)
    at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:207)
    at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:206)
    at org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:3857)
    at org.apache.spark.sql.Dataset.rdd(Dataset.scala:3855)
    at org.apache.spark.sql.QueryTest$.$anonfun$getErrorMessageInCheckAnswer$1(QueryTest.scala:266)
    at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:209)
    at org.apache.spark.sql.QueryTest$.getErrorMessageInCheckAnswer(QueryTest.scala:266)
    at org.apache.spark.sql.QueryTest$.checkAnswer(QueryTest.scala:243)
    at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:151)
    at org.apache.spark.sql.DataFrameSuite.$anonfun$new$737(DataFrameSuite.scala:3676)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
    at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:95)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withTempView(SQLTestUtils.scala:276)
    at org.apache.spark.sql.test.SQLTestUtilsBase.withTempView$(SQLTestUtils.scala:274)
    at org.apache.spark.sql.DataFrameSuite.withTempView(DataFrameSuite.scala:60)
    at org.apache.spark.sql.DataFrameSuite.$anonfun$new$736(DataFrameSuite.scala:3667)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.scalatest.enablers.Timed$$anon$1.timeoutAfter(Timed.scala:127)
    at org.scalatest.concurrent.TimeLimits$.failAfterImpl(TimeLimits.scala:282)
    at org.scalatest.concurrent.TimeLimits.failAfter(TimeLimits.scala:231)
    at org.scalatest.concurrent.TimeLimits.failAfter$(TimeLimits.scala:230)
    at org.apache.spark.SparkFunSuite.failAfter(SparkFunSuite.scala:69)
    at org.apache.spark.SparkFunSuite.$anonfun$test$2(SparkFunSuite.scala:155)
    at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
    at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
    at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
    at org.scalatest.Transformer.apply(Transformer.scala:22)
    at org.scalatest.Transformer.apply(Transformer.scala:20)
    at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
    at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:227)
    at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
    at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
    at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
    at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:69)
    at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
    at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
    at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:69)
    at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
    at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
    at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
    at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
    at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
    at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1564)
    at org.scalatest.Suite.run(Suite.scala:1114)
    at org.scalatest.Suite.run$(Suite.scala:1096)
    at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1564)
    at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273)
    at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
    at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273)
    at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272)
    at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:69)
    at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
    at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
    at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
    at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:69)
    at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:47)
    at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1321)
    at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1315)
    at scala.collection.immutable.List.foreach(List.scala:431)
    at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1315)
    at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:992)
    at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:970)
    at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1481)
    at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:970)
    at org.scalatest.tools.Runner$.run(Runner.scala:798)
    at org.scalatest.tools.Runner.run(Runner.scala)
    at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2or3(ScalaTestRunner.java:38)
    at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:25)
 {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 04 12:48:17 UTC 2023,,,,,,,,,,0|z1jujs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 09:17;githubbot;User 'zml1206' has created a pull request for this issue:
https://github.com/apache/spark/pull/42531;;;, 31/Aug/23 16:52;ignitetcbot;User 'zml1206' has created a pull request for this issue:
https://github.com/apache/spark/pull/42633;;;, 04/Sep/23 12:48;yumwang;Issue resolved by pull request 42633
[https://github.com/apache/spark/pull/42633];;;",,,,,"31/Aug/23 16:52;ignitetcbot;User 'zml1206' has created a pull request for this issue:
https://github.com/apache/spark/pull/42633;;;"
HiveExternalCatalog.listPartitions should restore Spark SQL stats,SPARK-45054,13549437,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,9/1/23 18:14,9/2/23 3:25,7/17/24 20:45,9/2/23 3:22,"3.2.4, 3.3.2, 3.4.1","3.4.2, 3.5.0, 4.0.0",SQL,,0,,"If partitions are stored in HMS with Spark populated stats such as {{spark.sql.statistics.totalSize}}, currently {{HiveExternalCatalog.listPartitions}} doesn't call {{restorePartitionMetadata}} to restore those stats.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 02 03:22:50 UTC 2023,,,,,,,,,,0|z1k5a8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"02/Sep/23 03:22;csun;Issue resolved by pull request 42777
[https://github.com/apache/spark/pull/42777];;;",3.3.2,3.4.1,,,
CSV conversion performance severely degraded for null fields,SPARK-44990,13548875,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,atulpayapilly_amazon,atulpayapilly_amazon,8/28/23 18:40,9/1/23 16:39,7/17/24 20:45,8/30/23 17:55,"3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0, 3.4.1, 3.5.0, 3.5.1, 4.0.0","3.3.4, 3.4.2, 3.5.0",SQL,,0,," 
[https://github.com/apache/spark/pull/36110/files]
 introduced a SQLConf access in a critical section for every field processed in a record that is null.

This causes severe degradation of performance causing one workload that was completing in a couple of seconds to now take around 8 minutes.

This conf needs to be moved out of the critical path, there's no need for it to be in this location.

The version of Spark prior to this commit didn't exhibit the slowdown. I also generated a patch on an affected version with the suspected line removed and the problem went away.","Ran on Spark 3.3.1/EMR 6.10.0 with driver r5.xlarge and 4 x r5.16xlarge core nodes. The workload was:

spark.read.parquet(""<redacted HDFS location>"").repartition(100).write.format(""com.databricks.spark.csv"").option(""compression"",""gzip"").option(""header"", ""true"").option(""encoding"",""utf-8"").option(""charset"",""utf-8"").option(""escape"", """").option(""quote"", """").option(""quote"", ""\u0000"").option(""emptyValue"", """").option(""delimiter"", ""\t"").mode(""overwrite"").save(""<redacted HDFS location>"")

Input data contained 5 parquet data files 41MB each.

Most of the fields were null values.

Schema was very wide (1099 columns).",,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 01 16:39:22 UTC 2023,,,,,,,,,,0|z1k1tc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"30/Aug/23 17:55;dongjoon;Issue resolved by pull request 42738
[https://github.com/apache/spark/pull/42738];;;, 31/Aug/23 04:28;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42744;;;, 31/Aug/23 04:28;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42744;;;, 01/Sep/23 16:39;atulpayapilly_amazon;Thanks for fixing this. I see that a null test was added and removed. While I agree that an all null test is not very meaningful a mostly null test is still valid and would be good to avoid this regression again.;;;",3.3.1,3.3.2,3.3.3,3.3.4,"31/Aug/23 04:28;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42744;;;"
Reflect function behavior different from Hive,SPARK-44743,13546705,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,nownikhil,nownikhil,8/9/23 17:54,9/1/23 12:25,7/17/24 20:45,9/1/23 12:24,3.4.1,4.0.0,"PySpark, SQL",,0,,"Spark reflect function will fail if underlying method call throws exception. This causes the whole job to fail.

In Hive however the exception is caught and null is returned. Simple test to reproduce the behavior
{code:java}
select reflect('java.net.URLDecoder', 'decode', '%') {code}
The workaround would be to wrap this call in a try
[https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/CallMethodViaReflection.scala#L136]


We can support this by adding a new UDF `try_reflect` which mimics the Hive's behavior. Please share your thoughts on this.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 01 12:24:22 UTC 2023,,,,,,,,,,0|z1josg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"25/Aug/23 03:35;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42661;;;, 01/Sep/23 12:24;cloud_fan;Issue resolved by pull request 42661
[https://github.com/apache/spark/pull/42661];;;",,,,,"01/Sep/23 12:24;cloud_fan;Issue resolved by pull request 42661
[https://github.com/apache/spark/pull/42661];;;"
Make direct Arrow encoding work with SQL/API,SPARK-44450,13543703,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/16/23 23:21,8/30/23 1:30,7/17/24 20:45,8/29/23 17:39,3.4.1,3.5.0,Connect,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 30 01:30:15 UTC 2023,,,,,,,,,,0|z1j6a0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"29/Aug/23 21:37;h-vetinari;[~hvanhovell], what was the outcome here? I cannot find a reference for SPARK-44450 on github, so I'm wondering if this feature has been abandoned, and if so, why?;;;, 29/Aug/23 22:22;hvanhovell;[~h-vetinari] this was mostly about making sure we moved all needed classes to sql/api module, and that was done about a month ago in SPARK-44532 (https://github.com/apache/spark/pull/42156).

I assume you are mostly interested in the actual encoders. Those can be found here: https://github.com/apache/spark/tree/master/connector/connect/common/src/main/scala/org/apache/spark/sql/connect/client/arrow;;;, 29/Aug/23 22:38;h-vetinari;Thanks for the quick response! Are there any docs for using this? I've checked the [rc3 docs|https://dist.apache.org/repos/dist/dev/spark/v3.5.0-rc3-docs/_site/api/scala/org/apache/spark/sql/] (not easily searchable, so only in API for now), and there's no section/page for {{apache.spark.sql.connect.client}}.;;;, 30/Aug/23 01:30;hvanhovell;It is not really part of the public API, and we don't write user facing documentation for that. The use case is main that we can encode  directly between user objects and arrow batches. We needed this to get rid of the Catalyst dependency. You can check how it is integrated with the Spark Connect Scala Client, look at SparkResult for deserialization, and look at SparkSession.createDataset for serialization. Alternatively you can look at the ArrowEncoder suite.;;;",,,,,"29/Aug/23 22:22;hvanhovell;[~h-vetinari] this was mostly about making sure we moved all needed classes to sql/api module, and that was done about a month ago in SPARK-44532 (https://github.com/apache/spark/pull/42156).

I assume you are mostly interested in the actual encoders. Those can be found here: https://github.com/apache/spark/tree/master/connector/connect/common/src/main/scala/org/apache/spark/sql/connect/client/arrow;;;"
Prepare RowEncoder for the move to sql/api,SPARK-44344,13542880,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/8/23 12:55,8/29/23 17:39,7/17/24 20:45,8/29/23 17:39,3.4.1,3.5.0,"Connect, SQL",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,55:28.0,,,,,,,,,,0|z1j17s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Unescape and consist error summary across UI pages,SPARK-44960,13548608,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,8/25/23 6:37,8/28/23 5:52,7/17/24 20:45,8/28/23 5:52,"3.3.2, 3.4.1, 3.5.0, 4.0.0",4.0.0,Web UI,,0,,"We escape html4 for error summary for some pages., it's not necessary",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 28 05:52:21 UTC 2023,,,,,,,,,,0|z1k060:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"28/Aug/23 05:52;yao;Issue resolved by pull request 42674
[https://github.com/apache/spark/pull/42674];;;",3.4.1,3.5.0,4.0.0,,
Deterministic ApplyFunctionExpression should be foldable,SPARK-44930,13548349,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coneyliu,coneyliu,coneyliu,8/23/23 8:58,8/25/23 7:02,7/17/24 20:45,8/25/23 7:02,3.4.1,3.5.0,SQL,,0,,"Currently, ApplyFunctionExpression is unfoldable because inherits the default value from Expression.  However, it should be foldable for a deterministic ApplyFunctionExpression. This could help optimize the usage for V2 UDF applying to constant expressions.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 07:02:10 UTC 2023,,,,,,,,,,0|z1jykg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 09:04;githubbot;User 'ConeyLiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/42629;;;, 25/Aug/23 07:02;cloud_fan;Issue resolved by pull request 42629
[https://github.com/apache/spark/pull/42629];;;",,,,,"25/Aug/23 07:02;cloud_fan;Issue resolved by pull request 42629
[https://github.com/apache/spark/pull/42629];;;"
BlockManagerDecommissioner throws exceptions when migrating RDD cached blocks to fallback storage,SPARK-44547,13544848,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ukby1234,ukby1234,ukby1234,7/25/23 18:53,8/25/23 5:20,7/17/24 20:45,8/25/23 5:20,3.4.1,"3.3.4, 3.4.2, 3.5.0, 4.0.0",Spark Core,,0,,"Looks like the RDD cache doesn't support fallback storage and we should stop the migration if the only viable peer is the fallback storage. 

  [^spark-error.log] 23/07/25 05:12:58 WARN BlockManager: Failed to replicate rdd_18_25 to BlockManagerId(fallback, remote, 7337, None), failure #0
java.io.IOException: Failed to connect to remote:7337
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:288)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:218)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:230)
	at org.apache.spark.network.netty.NettyBlockTransferService.uploadBlock(NettyBlockTransferService.scala:168)
	at org.apache.spark.network.BlockTransferService.uploadBlockSync(BlockTransferService.scala:121)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$replicate(BlockManager.scala:1784)
	at org.apache.spark.storage.BlockManager.$anonfun$replicateBlock$2(BlockManager.scala:1721)
	at org.apache.spark.storage.BlockManager.$anonfun$replicateBlock$2$adapted(BlockManager.scala:1707)
	at scala.Option.forall(Option.scala:390)
	at org.apache.spark.storage.BlockManager.replicateBlock(BlockManager.scala:1707)
	at org.apache.spark.storage.BlockManagerDecommissioner.migrateBlock(BlockManagerDecommissioner.scala:356)
	at org.apache.spark.storage.BlockManagerDecommissioner.$anonfun$decommissionRddCacheBlocks$3(BlockManagerDecommissioner.scala:340)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.storage.BlockManagerDecommissioner.decommissionRddCacheBlocks(BlockManagerDecommissioner.scala:339)
	at org.apache.spark.storage.BlockManagerDecommissioner$$anon$1.run(BlockManagerDecommissioner.scala:214)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.net.UnknownHostException: remote
	at java.base/java.net.InetAddress$CachedAddresses.get(Unknown Source)
	at java.base/java.net.InetAddress.getAllByName0(Unknown Source)
	at java.base/java.net.InetAddress.getAllByName(Unknown Source)
	at java.base/java.net.InetAddress.getAllByName(Unknown Source)
	at java.base/java.net.InetAddress.getByName(Unknown Source)
	at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:156)
	at io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:153)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:153)
	at io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:41)
	at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:61)
	at io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:53)
	at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:55)
	at io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:31)
	at io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:106)
	at io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:206)
	at io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:46)
	at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:180)
	at io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:166)
	at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
	at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552)
	at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
	at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
	at io.netty.util.concurrent.DefaultPromise.setSuccess0(DefaultPromise.java:605)
	at io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)
	at io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:84)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:990)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:516)
	at io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:429)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:486)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)",,,,,,,,,,,,,,,,,,,,,,,,,25/Jul/23 18:54;ukby1234;spark-error.log;https://issues.apache.org/jira/secure/attachment/13061620/spark-error.log,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 25 05:20:22 UTC 2023,,,,,,,,,,0|z1jdcg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"27/Jul/23 20:59;ignitetcbot;User 'ukby1234' has created a pull request for this issue:
https://github.com/apache/spark/pull/42155;;;, 25/Aug/23 05:20;dongjoon;Issue resolved by pull request 42155
[https://github.com/apache/spark/pull/42155];;;",,,,,"25/Aug/23 05:20;dongjoon;Issue resolved by pull request 42155
[https://github.com/apache/spark/pull/42155];;;"
Add more tests for Python foreachBatch and StreamingQueryListener,SPARK-44435,13543592,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,rangadi,rangadi,7/14/23 17:59,8/24/23 17:35,7/17/24 20:45,8/24/23 10:19,3.4.1,4.0.0,"Connect, Structured Streaming",,0,,"Currently there are few few tests included for ForeachBatch and StreamingQueryListener.

Add more tests covering more scenarios (multiple queries, different error conditions, process termination, etc).",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,SPARK-42938,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 10:19:05 UTC 2023,,,,,,,,,,0|z1j5lc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"24/Aug/23 10:19;gurwls223;Issue resolved by pull request 42521
[https://github.com/apache/spark/pull/42521];;;",,,,,
Fix `RELEASE` file to have the correct information in Docker images,SPARK-44935,13548438,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,8/23/23 21:23,8/23/23 23:01,7/17/24 20:45,8/23/23 23:01,"2.4.8, 3.0.3, 3.1.3, 3.2.4, 3.3.2, 3.4.1, 3.5.0","3.3.4, 3.4.2, 3.5.0, 4.0.0",Kubernetes,,0,,"{code}
$ docker run -it --rm apache/spark:latest ls -al /opt/spark/RELEASE
-rw-r--r-- 1 spark spark 0 Jun 25 03:13 /opt/spark/RELEASE

$ docker run -it --rm apache/spark:v3.1.3 ls -al /opt/spark/RELEASE | tail -n1
-rw-r--r-- 1 root root 0 Feb 21  2022 /opt/spark/RELEASE
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 23:01:39 UTC 2023,,,,,,,,,,0|z1jz48:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 23:01;dongjoon;Issue resolved by pull request 42636
[https://github.com/apache/spark/pull/42636];;;",3.0.3,3.1.3,3.2.4,3.3.2,
Continuous Structured Streaming not reporting streaming metrics,SPARK-44932,13548418,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,bryanqiang,bryanqiang,8/23/23 16:22,8/23/23 16:25,7/17/24 20:45,,3.4.1,,Structured Streaming,,0,,"Hello, we've been running spark continuous structured streaming on standalone cluster and happy with the performance however we noticed streaming metrics like input rate and process rate are not updated by the `ProgressReporter` in `ContinuousExecution` because the [`finishTrigger`|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala#L293] function is never invoked in `ContinuousExecution`. I'm wondering why and how may get metrics like in micro-batch structured streaming.

 

!https://preview.redd.it/mzh4oc0cbojb1.png?width=1901&format=png&auto=webp&s=8d649ae515e6adb7d6ce853802e0a4134c9fa277!!https://preview.redd.it/8ou3uyuofojb1.png?width=1523&format=png&auto=webp&s=ac6bf7fa05cb90b09cb10b9ac815f64b5e97175e!",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,22:52.0,,,,,,,,,,0|z1jyzs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Support correlated references under window functions,SPARK-44549,13544879,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gubichev,gubichev,gubichev,7/26/23 0:27,8/23/23 14:50,7/17/24 20:45,8/23/23 14:50,3.4.1,4.0.0,SQL,,0,,We should support subqueries with correlated references under a window function operator,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 14:50:10 UTC 2023,,,,,,,,,,0|z1jdjc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 14:50;cloud_fan;Issue resolved by pull request 42383
[https://github.com/apache/spark/pull/42383];;;",,,,,
K8s default service token file should not be materialized into token,SPARK-44925,13548314,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,8/23/23 3:08,8/23/23 5:52,7/17/24 20:45,8/23/23 5:52,"2.4.8, 3.0.3, 3.1.3, 3.2.4, 3.3.2, 3.4.1, 3.5.0","3.3.4, 3.4.2, 3.5.0, 4.0.0",Kubernetes,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 05:52:24 UTC 2023,,,,,,,,,,0|z1jyco:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Aug/23 03:20;snoot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/42624;;;, 23/Aug/23 03:20;snoot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/42624;;;, 23/Aug/23 05:52;dongjoon;Issue resolved by pull request 42624
[https://github.com/apache/spark/pull/42624];;;",3.0.3,3.1.3,3.2.4,3.3.2,"23/Aug/23 03:20;snoot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/42624;;;"
NullPointerException on stateful expression evaluation,SPARK-44905,13548142,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,8/22/23 5:46,8/23/23 5:49,7/17/24 20:45,8/23/23 5:49,"3.4.1, 3.5.0, 4.0.0",3.5.0,SQL,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 23 05:49:16 UTC 2023,,,,,,,,,,0|z1jxao:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"22/Aug/23 09:11;githubbot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/42601;;;, 23/Aug/23 05:49;yao;Issue resolved by https://github.com/apache/spark/pull/42601;;;",3.5.0,4.0.0,,,23/Aug/23 05:49;yao;Issue resolved by https://github.com/apache/spark/pull/42601;;;
Set io.connectionTimeout/connectionCreationTimeout to zero or negative will cause executor incessantes cons/destructions,SPARK-44241,13541900,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,6/29/23 8:03,8/23/23 0:41,7/17/24 20:45,6/30/23 10:34,"3.3.2, 3.4.1, 3.5.0","3.3.3, 3.4.2, 3.5.0",Spark Core,,0,,"{code:java}
2023-06-28 14:57:23 CST Bootstrap WARN - Failed to set channel option 'CONNECT_TIMEOUT_MILLIS' with value '-1000' for channel '[id: 0xf4b54a73]'
java.lang.IllegalArgumentException: connectTimeoutMillis : -1000 (expected: >= 0)
	at io.netty.util.internal.ObjectUtil.checkPositiveOrZero(ObjectUtil.java:144) ~[netty-common-4.1.74.Final.jar:4.1.74.Final] {code}",,,,,,,,,,,,,,,,,,,,,SPARK-44920,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 10:34:59 UTC 2023,,,,,,,,,,0|z1iv80:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 09:23;githubbot;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/41785;;;, 30/Jun/23 10:34;Qin Yao;Issue resolved by pull request 41785
[https://github.com/apache/spark/pull/41785];;;",3.4.1,3.5.0,,,"30/Jun/23 10:34;Qin Yao;Issue resolved by pull request 41785
[https://github.com/apache/spark/pull/41785];;;"
NullPointerException is thrown when column with ROWID type contains NULL values,SPARK-44885,13547998,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tnieradzik,tnieradzik,tnieradzik,8/20/23 19:47,8/22/23 10:49,7/17/24 20:45,8/22/23 10:48,3.4.1,"3.5.0, 4.0.0","Spark Core, SQL",,0,,"A row ID may be NULL in an Oracle table. When this is the case, the following exception is thrown:

{{[info] Cause: java.lang.NullPointerException:}}
{{{}[info] at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$12(JdbcUtils.scala:452){}}}{{{}[info] at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$12$adapted(JdbcUtils.scala:451){}}}
{{{}[info] at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:361){}}}{{{}[info] at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:343){}}}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Scala,Tue Aug 22 10:48:30 UTC 2023,,,,,,,,,,0|z1jweo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"20/Aug/23 19:54;tnieradzik;PR submitted: https://github.com/apache/spark/pull/42576;;;, 22/Aug/23 10:48;gurwls223;Issue resolved by pull request 42576
[https://github.com/apache/spark/pull/42576];;;",,,,,"22/Aug/23 10:48;gurwls223;Issue resolved by pull request 42576
[https://github.com/apache/spark/pull/42576];;;"
Remove unnecessary curly braces at the end of the thread locks info,SPARK-44880,13547952,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,8/19/23 16:52,8/20/23 1:41,7/17/24 20:45,8/20/23 1:41,"3.3.2, 3.4.1, 3.5.0, 4.0.0","3.5.1, 4.0.0",Web UI,,0,,Remove unnecessary curly braces at the end of the thread locks info,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 20 01:41:08 UTC 2023,,,,,,,,,,0|z1jw4g:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"20/Aug/23 01:41;yumwang;Issue resolved by pull request 42571
[https://github.com/apache/spark/pull/42571];;;",3.4.1,3.5.0,4.0.0,,
Alter nested view fails because of HMS client,SPARK-44873,13547889,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kyle.rong,kyle.rong,kyle.rong,8/18/23 16:30,8/19/23 1:34,7/17/24 20:45,8/18/23 18:07,"3.1.0, 3.1.1, 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0, 3.4.1, 3.4.2, 3.5.0, 3.5.1","3.5.0, 4.0.0",SQL,,0,,"Currently, 
{code:java}
CREATE OR REPLACE VIEW t AS SELECT "" +
        ""struct(id AS `$col2`, struct(id AS `$col`) AS s1) AS s2 FROM RANGE(5)
ALTER VIEW t SET TBLPROPERTIES ('x' = 'y'){code}
would fail when calling HMS's updateTable, because HMS does not support nested struct in view. We can fix this by passing HMS an empty schema, since we store the actual view schema in the table's properties already. This fix is similar to https://github.com/apache/spark/pull/37364",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 01:34:09 UTC 2023,,,,,,,,,,0|z1jvqg:,9223372036854775807,,,,,Gengliang.Wang,,,,,,,,,,,,,,,,"18/Aug/23 18:07;Gengliang.Wang;Issue resolved by pull request 42532
[https://github.com/apache/spark/pull/42532];;;, 19/Aug/23 01:34;paulk;User 'kylerong-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/42566;;;","3.1.1, 3.3.1, 3.3.2, 3.3.3, 3.3.4, 3.4.0, 3.4.1, 3.4.2, 3.5.0, 3.5.1",3.1.2,3.1.3,3.2.0,"19/Aug/23 01:34;paulk;User 'kylerong-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/42566;;;"
Implement termination of Python process for foreachBatch & streaming listener,SPARK-44433,13543588,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WweiL,rangadi,rangadi,7/14/23 17:55,8/18/23 17:41,7/17/24 20:45,8/5/23 2:43,3.4.1,3.5.0,"Connect, Structured Streaming",,0,,"In the first implementation of Python support for foreachBatch, the python process termination is not handled correctly. 

 

See the long TODO in [https://github.com/apache/spark/blob/master/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/planner/StreamingForeachBatchHelper.scala] 

about an outline of the feature to terminate the runners by registering StreamingQueryListners. ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,SPARK-42938,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 18 17:41:29 UTC 2023,,,,,,,,,,0|z1j5kg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"05/Aug/23 02:43;ueshin;Issue resolved by pull request 42283
https://github.com/apache/spark/pull/42283;;;, 16/Aug/23 18:19;awsthni;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/42460;;;, 16/Aug/23 18:34;awsthni;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/42283;;;, 18/Aug/23 17:41;ignitetcbot;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/42555;;;",,,,,"16/Aug/23 18:19;awsthni;User 'rangadi' has created a pull request for this issue:
https://github.com/apache/spark/pull/42460;;;"
Do not combine multiple Generate operators in the same WholeStageCodeGen node because it can  easily cause OOM failures if arrays are relatively large,SPARK-44759,13546803,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tafranky@gmail.com,tafranky@gmail.com,8/10/23 9:23,8/13/23 6:03,7/17/24 20:45,,"3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1",,"Deploy, Optimizer, Spark Core",,0,,"This is an issue since the WSCG  implementation of the generate node. 

Because WSCG compute rows in batches , the combination of WSCG and the explode operation consume a lot of the dedicated executor memory. This is even more true when the WSCG node contains multiple explode nodes. This is the case when flattening a nested array.

The generate node used to flatten array generally  produces an amount of output rows that is significantly higher than the input rows.

the number of output rows generated is even drastically higher when flattening a nested array .

When we combine more that 1 generate node in the same WholeStageCodeGen  node, we run  a high risk of running out of memory for multiple reasons. 

1- As you can see from snapshots added in the comments ,  the rows created in the nested loop are saved in a writer buffer.  In this case because the rows were big , the job failed with an Out Of Memory Exception error .

2_ The generated WholeStageCodeGen result in a nested loop that for each row  , will explode the parent array and then explode the inner array.  The rows are accumulated in the writer buffer without accounting for the row size.

Please view the attached Spark Gui and Spark Dag 

In my case the wholestagecodegen includes 2 explode nodes. 

Because the array elements are large , we end up with an Out Of Memory error. 

 

I recommend that we do not merge  multiple explode nodes in the same whole stage code gen node . Doing so leads to potential memory issues.

In our case , the job execution failed with an  OOM error because the the WSCG executed  into a nested for loop . 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 16:27;tafranky@gmail.com;image-2023-08-10-09-27-24-124.png;https://issues.apache.org/jira/secure/attachment/13062036/image-2023-08-10-09-27-24-124.png, 10/Aug/23 16:29;tafranky@gmail.com;image-2023-08-10-09-29-24-804.png;https://issues.apache.org/jira/secure/attachment/13062037/image-2023-08-10-09-29-24-804.png, 10/Aug/23 16:32;tafranky@gmail.com;image-2023-08-10-09-32-46-163.png;https://issues.apache.org/jira/secure/attachment/13062038/image-2023-08-10-09-32-46-163.png, 10/Aug/23 16:33;tafranky@gmail.com;image-2023-08-10-09-33-47-788.png;https://issues.apache.org/jira/secure/attachment/13062039/image-2023-08-10-09-33-47-788.png, 10/Aug/23 09:25;tafranky@gmail.com;wholestagecodegen_wc1_debug_wholecodegen_passed;https://issues.apache.org/jira/secure/attachment/13062030/wholestagecodegen_wc1_debug_wholecodegen_passed",,5,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Thu Aug 10 16:34:45 UTC 2023,,,,,,,,,,0|z1jpe8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"10/Aug/23 16:27;tafranky@gmail.com;WSCG  generated code that calls  generate_doConsume_0

!image-2023-08-10-09-27-24-124.png!;;;, 10/Aug/23 16:29;tafranky@gmail.com;WSCG  generated code for first Generate node 

!image-2023-08-10-09-29-24-804.png!;;;, 10/Aug/23 16:33;tafranky@gmail.com;WSCG  generated code for second Generate node 

!image-2023-08-10-09-32-46-163.png!;;;, 10/Aug/23 16:34;tafranky@gmail.com;Spark Dag for the use case . The failure is from the execution of WholeStageCodeGen(2)

!image-2023-08-10-09-33-47-788.png!;;;","3.0.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1",3.0.2,3.0.3,3.1.0,"10/Aug/23 16:29;tafranky@gmail.com;WSCG  generated code for first Generate node 

!image-2023-08-10-09-29-24-804.png!;;;"
Spark job submission failed because Xmx string is available on one parameter provided into spark.driver.extraJavaOptions,SPARK-44242,13541901,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nfraison.datadog,nfraison.datadog,nfraison.datadog,6/29/23 8:16,8/12/23 0:09,7/17/24 20:45,8/12/23 0:09,"3.3.2, 3.4.1",4.0.0,Spark Submit,,0,,"The spark-submit command failed if Xmx string is found on any parameters provided to spark.driver.extraJavaOptions.

For ex. running this spark-submit command line
{code:java}
./bin/spark-submit --class org.apache.spark.examples.SparkPi --conf ""spark.driver.extraJavaOptions=-Dtest=Xmx""  examples/jars/spark-examples_2.12-3.4.1.jar 100{code}
failed due to
{code:java}
Error: Not allowed to specify max heap(Xmx) memory settings through java options (was -Dtest=Xmx). Use the corresponding --driver-memory or spark.driver.memory configuration instead.{code}
The check performed in [https://github.com/apache/spark/blob/master/launcher/src/main/java/org/apache/spark/launcher/SparkSubmitCommandBuilder.java#L314] seems to broad",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 12 00:09:27 UTC 2023,,,,,,,,,,0|z1iv88:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Aug/23 00:09;mridulm80;Issue resolved by pull request 41806
[https://github.com/apache/spark/pull/41806];;;",3.4.1,,,,
SaveMode.ErrorIfExists does not work with kafka-sql,SPARK-44774,13546956,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dolfinus,dolfinus,8/11/23 9:46,8/11/23 9:48,7/17/24 20:45,,3.4.1,,SQL,,0,,"I' trying to write batch dataframe to Kafka topic with {{mode=""error""}}, but when topic exists it does not raise exception. Instead it appends data to a topic.

Steps to reproduce:

1. Start Kafka:

docker-compose.yml
{code:yaml}
version: '3.9'

services:
  zookeeper:
    image: bitnami/zookeeper:3.8
    environment:
      ALLOW_ANONYMOUS_LOGIN: 'yes'

  kafka:
    image: bitnami/kafka:latest
    restart: unless-stopped
    ports:
    - 9093:9093
    environment:
      ALLOW_PLAINTEXT_LISTENER: 'yes'
      KAFKA_ENABLE_KRAFT: 'no'
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_INTER_BROKER_LISTENER_NAME: INTERNAL_PLAINTEXT_ANONYMOUS
      KAFKA_CFG_LISTENERS: INTERNAL_PLAINTEXT_ANONYMOUS://:9092,EXTERNAL_PLAINTEXT_ANONYMOUS://:9093
      KAFKA_CFG_ADVERTISED_LISTENERS: INTERNAL_PLAINTEXT_ANONYMOUS://kafka:9092,EXTERNAL_PLAINTEXT_ANONYMOUS://localhost:9093
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL_PLAINTEXT_ANONYMOUS:PLAINTEXT,EXTERNAL_PLAINTEXT_ANONYMOUS:PLAINTEXT
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: 'true'
    depends_on:
    - zookeeper
{code}

{code:bash}
docker-compose up -d
{code}

2. Start Spark session:

{code:bash}
pip install pyspark[sql]==3.4.1
{code}


{code:python}
from pyspark.sql import SparkSession

spark = SparkSession.builder.config(""spark.jars.packages"", ""org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1"").getOrCreate()
{code}

3. Create DataFrame and write it to Kafka. First write using {{mode=""append""}} to create topic, then with {{mode=""error""}} to raise because topic already exist:
{code}
df = spark.createDataFrame([{""value"": ""string""}])
df.write.format(""kafka"").option(""kafka.bootstrap.servers"", ""localhost:9093"").option(""topic"", ""new_topic"").mode(""append"").save()

# no exception is raised
df.write.format(""kafka"").option(""kafka.bootstrap.servers"", ""localhost:9093"").option(""topic"", ""new_topic"").mode(""error"").save()
{code}

4. Check topic content - 2 rows are added to topic instead of one:
{code:python}
spark.read.format(""kafka"").option(""kafka.bootstrap.servers"", ""localhost:9093"").option(""subscribe"", ""new_topic"").load().show(10, False)
{code}
{code}
+----+-------------------+---------+---------+------+-----------------------+-------------+
|key |value              |topic    |partition|offset|timestamp              |timestampType|
+----+-------------------+---------+---------+------+-----------------------+-------------+
|null|[73 74 72 69 6E 67]|new_topic|0        |0     |2023-08-11 09:39:35.813|0            |
|null|[73 74 72 69 6E 67]|new_topic|0        |1     |2023-08-11 09:39:36.122|0            |
+----+-------------------+---------+---------+------+-----------------------+-------------+
{code}

It looks like mode is checked by KafkaSourceProvider, but is not used at all:
https://github.com/apache/spark/blob/v3.4.1/connector/kafka-0-10-sql/src/main/scala/org/apache/spark/sql/kafka010/KafkaSourceProvider.scala#L172-L178

So data is always appended to topic.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,46:54.0,,,,,,,,,,0|z1jqc8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve WSCG handling of row buffer by accounting for executor memory  .  Exploding nested arrays can easily lead to out of memory errors. ,SPARK-44768,13546887,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tafranky@gmail.com,tafranky@gmail.com,8/10/23 22:36,8/11/23 3:36,7/17/24 20:45,,"3.3.2, 3.4.0, 3.4.1",,Optimizer,,0,,"The code sample below is to showcase the wholestagecodegen generated when exploding nested arrays.  The data sample in the dataframe is quite small so it will not trigger the Out of Memory error . 

However if the array is larger and the row size is large , you will definitely end up with an OOM error .  

 

consider a scenario where you flatten  a nested array 

// e.g you can use the following steps to create the dataframe 

//create a partClass case class
case class partClass (PARTNAME: String , PartNumber: String , PartPrice : Double )

//create a nested array array class
case  class array_array_class (
 col_int: Int,
 arr_arr_string : Seq[Seq[String]],
 arr_arr_bigint : Seq[Seq[Long]],
 col_string     : String,
 parts_s        : Seq[Seq[partClass]]
 
)

//create a dataframe
var df_array_array = sc.parallelize(
 Seq(
 (1,Seq(Seq(""a"",""b"" ,""c"" ,""d"") ,Seq(""aa"",""bb"" ,""cc"",""dd"")) , Seq(Seq(1000,20000), Seq(30000,-10000)),""ItemPart1"",
  Seq(Seq(partClass(""PNAME1"",""P1"",20.75),partClass(""PNAME1_1"",""P1_1"",30.75)))
 ) ,
 
 (2,Seq(Seq(""ab"",""bc"" ,""cd"" ,""de"") ,Seq(""aab"",""bbc"" ,""ccd"",""dde""),Seq(""aaaaaabbbbb"")) , Seq(Seq(-1000,-20000,-1,-2), Seq(0,30000,-10000)),""ItemPart2"",
  Seq(Seq(partClass(""PNAME2"",""P2"",170.75),partClass(""PNAME2_1"",""P2_1"",33.75),partClass(""PNAME2_2"",""P2_2"",73.75)))
 )
  
 )

).toDF(""c1"" ,""c2"" ,""c3"" ,""c4"" ,""c5"")

//explode a nested array 

var  result   =  df_array_array.select( col(""c1""), explode(col(""c2""))).select('c1 , explode('col))

result.explain

 

The physical for this operator is seen below.

-------------------------------------
Physical plan 

== Physical Plan ==
*(1) Generate explode(col#27), [c1#17|#17], false, [col#30|#30]
+- *(1) Filter ((size(col#27, true) > 0) AND isnotnull(col#27))
   +- *(1) Generate explode(c2#18), [c1#17|#17], false, [col#27|#27]
      +- *(1) Project [_1#6 AS c1#17, _2#7 AS c2#18|#6 AS c1#17, _2#7 AS c2#18]
         +- *(1) Filter ((size(_2#7, true) > 0) AND isnotnull(_2#7))
            +- *(1) SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple5, true]))._1 AS _1#6, mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -2), staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -2), StringType, ObjectType(class java.lang.String)), true, false, true), validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -1), ArrayType(StringType,true), ObjectType(interface scala.collection.Seq)), None), knownnotnull(assertnotnull(input[0, scala.Tuple5, true]))._2, None) AS _2#7, mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -3), mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -4), assertnotnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -4), IntegerType, IntegerType)), validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -3), ArrayType(IntegerType,false), ObjectType(interface scala.collection.Seq)), None), knownnotnull(assertnotnull(input[0, scala.Tuple5, true]))._3, None) AS _3#8, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(assertnotnull(input[0, scala.Tuple5, true]))._4, true, false, true) AS _4#9, mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -5), mapobjects(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -6), if (isnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -6), StructField(PARTNAME,StringType,true), StructField(PartNumber,StringType,true), StructField(PartPrice,DoubleType,false), ObjectType(class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$partClass)))) null else named_struct(PARTNAME, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -6), StructField(PARTNAME,StringType,true), StructField(PartNumber,StringType,true), StructField(PartPrice,DoubleType,false), ObjectType(class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$partClass))).PARTNAME, true, false, true), PartNumber, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -6), StructField(PARTNAME,StringType,true), StructField(PartNumber,StringType,true), StructField(PartPrice,DoubleType,false), ObjectType(class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$partClass))).PartNumber, true, false, true), PartPrice, knownnotnull(validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -6), StructField(PARTNAME,StringType,true), StructField(PartNumber,StringType,true), StructField(PartPrice,DoubleType,false), ObjectType(class $line14.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$partClass))).PartPrice), validateexternaltype(lambdavariable(MapObject, ObjectType(class java.lang.Object), true, -5), ArrayType(StructType(StructField(PARTNAME,StringType,true),StructField(PartNumber,StringType,true),StructField(PartPrice,DoubleType,false)),true), ObjectType(interface scala.collection.Seq)), None), knownnotnull(assertnotnull(input[0, scala.Tuple5, true]))._5, None) AS _5#10]
               +- Scan[obj#5|#5]

 

 

Because the explode function can create multiple rows from a single row  , we should account for the memory available when adding rows to the buffer .  

 

This is even more important when we are exploding nested arrays . ",,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/23 03:32;tafranky@gmail.com;image-2023-08-10-20-32-55-684.png;https://issues.apache.org/jira/secure/attachment/13062044/image-2023-08-10-20-32-55-684.png, 11/Aug/23 00:21;tafranky@gmail.com;spark-jira_wscg_code.txt;https://issues.apache.org/jira/secure/attachment/13062043/spark-jira_wscg_code.txt",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 11 03:33:02 UTC 2023,,,,,,,,,,0|z1jpww:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,11/Aug/23 03:33;tafranky@gmail.com;!image-2023-08-10-20-32-55-684.png!;;;,3.4.0,3.4.1,,,
"""pyspark.pandas.resample"" is incorrect when DST is overlapped and setting ""spark.sql.timestampType"" to TIMESTAMP_NTZ does not help",SPARK-44717,13546451,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,attilapiros,attilapiros,8/8/23 4:01,8/9/23 2:04,7/17/24 20:45,8/9/23 2:04,"3.4.0, 3.4.1, 4.0.0","3.5.0, 4.0.0",Pandas API on Spark,,0,,"Use one of the existing test:
- ""11H"" case of test_dataframe_resample (pyspark.pandas.tests.test_resample.ResampleTests) 
- ""1001H"" case of test_series_resample (pyspark.pandas.tests.test_resample.ResampleTests) 

After setting the TZ for example to New York (like by using the following python code in a ""setUpClass"":  
{noformat}
os.environ[""TZ""] = 'America/New_York'
{noformat})

You will get the error for the latter mentioned test:

{noformat}
======================================================================
FAIL [4.219s]: test_series_resample (pyspark.pandas.tests.test_resample.ResampleTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/__w/spark/spark/python/pyspark/pandas/tests/test_resample.py"", line 276, in test_series_resample
    self._test_resample(self.pdf3.A, self.psdf3.A, [""1001H""], ""right"", ""right"", ""sum"")
  File ""/__w/spark/spark/python/pyspark/pandas/tests/test_resample.py"", line 259, in _test_resample
    self.assert_eq(
  File ""/__w/spark/spark/python/pyspark/testing/pandasutils.py"", line 457, in assert_eq
    _assert_pandas_almost_equal(lobj, robj)
  File ""/__w/spark/spark/python/pyspark/testing/pandasutils.py"", line 228, in _assert_pandas_almost_equal
    raise PySparkAssertionError(
pyspark.errors.exceptions.base.PySparkAssertionError: [DIFFERENT_PANDAS_SERIES] Series are not almost equal:
Left:
Freq: 1001H
float64
Right:
float64
{noformat}

The problem is the in the pyspark resample there will be more resampled rows in the result. The DST change will cause those extra lines as the computed __tmp_resample_bin_col__ be something like:

{noformat}
| __index_level_0__  | __tmp_resample_bin_col__ | A
.....
|2011-03-08 00:00:00|2011-03-26 11:00:00     |0.3980551570183919  |
|2011-03-09 00:00:00|2011-03-26 11:00:00     |0.6511376673995046  |
|2011-03-10 00:00:00|2011-03-26 11:00:00     |0.6141085426890365  |
|2011-03-11 00:00:00|2011-03-26 11:00:00     |0.11557638066163867 |
|2011-03-12 00:00:00|2011-03-26 11:00:00     |0.4517788243490799  |
|2011-03-13 00:00:00|2011-03-26 11:00:00     |0.8637060550157284  |
|2011-03-14 00:00:00|2011-03-26 10:00:00     |0.8169499149450166  |
|2011-03-15 00:00:00|2011-03-26 10:00:00     |0.4585916249356583  |
|2011-03-16 00:00:00|2011-03-26 10:00:00     |0.8362472880832088  |
|2011-03-17 00:00:00|2011-03-26 10:00:00     |0.026716901748386812|
|2011-03-18 00:00:00|2011-03-26 10:00:00     |0.9086816462089563  |
{noformat}

You can see the extra lines around when the DST kicked in on 2011-03-13 in New York.

Even setting the conf ""spark.sql.timestampType"" to""TIMESTAMP_NTZ"" does not help.

You can see my tests here:
https://github.com/attilapiros/spark/pull/5

Pandas timestamps are TZ less:
`
{noformat}
import pandas as pd
a = pd.Timestamp(year=2011, month=3, day=13, hour=1)
b = pd.Timedelta(hours=1)

>> a 
Timestamp('2011-03-13 01:00:00')
>>> a+b
Timestamp('2011-03-13 02:00:00')
>>> a+b+b
Timestamp('2011-03-13 03:00:00')
{noformat}

But pyspark TimestampType uses TZ and DST:

{noformat}
>>> sql(""select  TIMESTAMP '2011-03-13 01:00:00'"").show()
+-------------------------------+
|TIMESTAMP '2011-03-13 01:00:00'|
+-------------------------------+
|            2011-03-13 01:00:00|
+-------------------------------+

>>> sql(""select  TIMESTAMP '2011-03-13 01:00:00' + make_interval(0,0,0,0,1,0,0)"").show()
+--------------------------------------------------------------------+
|TIMESTAMP '2011-03-13 01:00:00' + make_interval(0, 0, 0, 0, 1, 0, 0)|
+--------------------------------------------------------------------+
|                                                 2011-03-13 03:00:00|
+--------------------------------------------------------------------+
{noformat}

The current resample code uses the above interval based calculation.
",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 09 02:04:12 UTC 2023,,,,,,,,,,0|z1jn7s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 04:05;attilapiros;cc [~itholic] [~gurwls223]
;;;, 08/Aug/23 04:14;gurwls223;cc [~podongfeng];;;, 08/Aug/23 05:26;gurwls223;I think we should at least make this respects {{spark.sql.timestampType}} when it;s set to {{TIMESTAMP_NTZ}}. Took a quick look, and I think there's some problem in calculation logic in https://github.com/apache/spark/blob/master/python/pyspark/pandas/resample.py#L277-L310 especially date_trunc returns always {{TimestampType}}. We might need a dedicated internal expression cc [~podongfeng];;;, 08/Aug/23 07:51;gurwls223;[~attilapiros] which time zone are you in? Would you mind trying this one below:

{code}
sql(""select  TIMESTAMP_NTZ '2011-03-13 01:00:00' + make_interval(0,0,0,0,1,0,0)"").show()
{code};;;, 08/Aug/23 08:54;attilapiros;The TIMESTAMP_NTZ would work for sure.

Here is the test:

{noformat}
$ export TZ=""America/New_York""
$ ./bin/pyspark
....
>>> sql(""select  TIMESTAMP_NTZ '2011-03-13 01:00:00' + make_interval(0,0,0,0,1,0,0)"").show()

+------------------------------------------------------------------------+
|TIMESTAMP_NTZ '2011-03-13 01:00:00' + make_interval(0, 0, 0, 0, 1, 0, 0)|
+------------------------------------------------------------------------+
|                                                     2011-03-13 02:00:00|
+------------------------------------------------------------------------+

>>> sql(""select  TIMESTAMP '2011-03-13 01:00:00' + make_interval(0,0,0,0,1,0,0)"").show()
+--------------------------------------------------------------------+
|TIMESTAMP '2011-03-13 01:00:00' + make_interval(0, 0, 0, 0, 1, 0, 0)|
+--------------------------------------------------------------------+
|                                                 2011-03-13 03:00:00|
+--------------------------------------------------------------------+
{noformat}

;;;, 08/Aug/23 11:17;gurwls223;Made a quick fix: https://github.com/apache/spark/pull/42392. I believe there are other corner cases like this a lot .. but the PR fixes this one alone for now.;;;, 09/Aug/23 02:04;gurwls223;Issue resolved by pull request 42392
[https://github.com/apache/spark/pull/42392];;;",3.4.1,4.0.0,,,08/Aug/23 04:14;gurwls223;cc [~podongfeng];;;
"Even `spark.sql.codegen.factoryMode` is NO_CODEGEN, the WholeStageCodegen also will be generated.",SPARK-44236,13541871,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,6/29/23 2:28,8/8/23 9:46,7/17/24 20:45,8/8/23 9:46,3.4.1,3.5.0,SQL,,0,,"The `spark.sql.codegen.factoryMode` is NO_CODEGEN, but Spark always generate WholeStageCodegen plan when set `spark.sql.codegen.wholeStage` to `true`.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 08 09:46:39 UTC 2023,,,,,,,,,,0|z1iv1k:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 09:46;cloud_fan;Issue resolved by pull request 41779
[https://github.com/apache/spark/pull/41779];;;",,,,,
Incorrect limit handling and config parsing in Arrow collect,SPARK-44657,13545991,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,8/3/23 12:46,8/8/23 8:32,7/17/24 20:45,8/8/23 8:32,"3.4.0, 3.4.1, 3.4.2, 3.5.0","3.4.2, 3.5.0, 4.0.0",Connect,,0,,"In the arrow writer [code|https://github.com/apache/spark/blob/6161bf44f40f8146ea4c115c788fd4eaeb128769/sql/core/src/main/scala/org/apache/spark/sql/execution/arrow/ArrowConverters.scala#L154-L163] , the conditions don’t seem to hold what the documentation says regd ""{_}maxBatchSize and maxRecordsPerBatch, respect whatever smaller""{_} since it seems to actually respect the conf which is ""larger"" (i.e less restrictive) due to _||_ operator.

 

Further, when the `{_}CONNECT_GRPC_ARROW_MAX_BATCH_SIZE{_}` conf is read, the value is not converted to bytes from Mib ([example|https://github.com/apache/spark/blob/3e5203c64c06cc8a8560dfa0fb6f52e74589b583/connector/connect/server/src/main/scala/org/apache/spark/sql/connect/execution/SparkConnectPlanExecution.scala#L103]).",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 08 08:32:04 UTC 2023,,,,,,,,,,0|z1jkds:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 08:32;gurwls223;Issue resolved by pull request 42321
[https://github.com/apache/spark/pull/42321];;;",3.4.1,3.4.2,3.5.0,,
Log eventLog rewrite duration when compact old event log files,SPARK-44703,13546336,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shuyouZZ,shuyouZZ,shuyouZZ,8/7/23 10:55,8/8/23 3:18,7/17/24 20:45,8/8/23 3:18,3.4.1,4.0.0,Spark Core,,0,,"When enable {{spark.eventLog.rolling.enabled}} and the number of eventLog files exceeds the value of {{{}spark.history.fs.eventLog.rolling.maxFilesToRetain{}}},
HistoryServer will compact the old event log files into one compact file.

Currently there is no log the rewrite duration in {{rewrite}} method, this metric is useful for understand the compact duration, so we need add logs in the method.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 08 03:18:09 UTC 2023,,,,,,,,,,0|z1jmig:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 03:18;kabhwan;Issue resolved by pull request 42378
[https://github.com/apache/spark/pull/42378];;;",,,,,
Support deal with microsecond for `from_json` function,SPARK-44696,13546289,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,1365976815@qq.com,1365976815@qq.com,8/7/23 3:47,8/7/23 3:47,7/17/24 20:45,,3.4.1,,SQL,,0,,"Currently, `from_json` function will treat all input timestamp as second precise.
However, if the input is millisecond or microsecond, this function will return wrong result and throw no exception, which will lead to misunderstanding.

Example:
{code:java}
spark-sql> SELECT from_json('{""a"":1, ""b"":1691241070}', 'a INT, b TIMESTAMP');
{""a"":1,""b"":2023-08-05 21:11:10}

spark-sql> SELECT from_json('{""a"":1, ""b"":1691241070000}', 'a INT, b TIMESTAMP');
{""a"":1,""b"":+55563-04-19 18:06:40}

spark-sql> SELECT from_json('{""a"":1, ""b"":1691241070000000}', 'a INT, b TIMESTAMP');
{""a"":1,""b"":-183707-06-27 20:24:24.251328}{code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,47:44.0,,,,,,,,,,0|z1jm80:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
DataSourceStrategy#selectFilters returns predicates in non-deterministic order,SPARK-41636,13514992,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,jwserencsa,jwserencsa,12/21/22 0:04,8/7/23 0:08,7/17/24 20:45,8/7/23 0:08,"3.1.0, 3.4.0, 3.4.1","3.5.0, 4.0.0",SQL,,0,,"Method org.apache.spark.sql.execution.datasources.DataSourceStrategy#selectFilters, which is used to determine ""pushdown-able"" filters, does not preserve the order of the input {{Seq[Expression]}} nor does it return the same order across the same plans (modulo ExprId differences). This is resulting in CodeGenerator cache misses even when the exact same LogicalPlan is executed. 

The aforementioned method does not attempt to maintain the order of the input predicates, though it happens to do so when there are less than 5 pushdown-able {{Expression}} in the input (due to some ""small maps"" logic in {{{}scala.collection.TraversableOnce#toMap{}}}). 

Returning in the same order as the input will reduce churn on the CodeGenerator cache under prolonged workloads that execute queries that are very similar. ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 07 00:08:10 UTC 2023,,,,,,,,,,0|z1e9zs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Aug/23 03:28;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42265;;;, 03/Aug/23 03:29;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42265;;;, 07/Aug/23 00:08;gurwls223;Issue resolved by pull request 42265
[https://github.com/apache/spark/pull/42265];;;",3.4.0,3.4.1,,,"03/Aug/23 03:29;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/42265;;;"
Support partition operation on dataframe in Spark Connect Go Client,SPARK-44368,13543098,13534768,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bobyangbo,bobyangbo,bobyangbo,7/11/23 3:32,8/3/23 0:13,7/17/24 20:45,8/3/23 0:13,3.4.1,4.0.0,Connect Contrib,,0,,Support partition operation on dataframe in Spark Connect Go Client,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 03 00:13:19 UTC 2023,,,,,,,,,,0|z1j2js:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Aug/23 00:13;podongfeng;Issue resolved by pull request 13
[https://github.com/apache/spark-connect-go/pull/13];;;",,,,,
Revert SPARK-43043 Improve the performance of MapOutputTracker.updateMapOutput,SPARK-44630,13545781,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,8/2/23 2:41,8/2/23 6:16,7/17/24 20:45,8/2/23 6:16,3.4.1,3.4.2,Spark Core,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 02 06:16:03 UTC 2023,,,,,,,,,,0|z1jj34:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"02/Aug/23 06:16;gurwls223;Issue resolved by pull request 42285
[https://github.com/apache/spark/pull/42285];;;",,,,,
Add Encoders.scala to Spark Connect Scala Client,SPARK-44613,13545573,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/31/23 18:40,8/1/23 18:54,7/17/24 20:45,8/1/23 18:54,3.4.1,3.5.0,Connect,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 11:38:15 UTC 2023,,,,,,,,,,0|z1jhsw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"01/Aug/23 11:38;ignitetcbot;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/42264;;;",,,,,
UDF should support function taking value classes,SPARK-44311,13542539,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,7/5/23 12:46,8/1/23 14:52,7/17/24 20:45,8/1/23 14:50,3.4.1,3.5.0,SQL,,0,,"Running the following code in a spark 
```
final case class ValueClass(a: Int) extends AnyVal
final case class Wrapper(v: ValueClass)

val f = udf((a: ValueClass) => a.a > 0)

spark.createDataset(Seq(Wrapper(ValueClass(1)))).filter(f(col(""v""))).show()
```

fails with
```
java.lang.ClassCastException: class org.apache.spark.sql.types.IntegerType$ cannot be cast to class org.apache.spark.sql.types.StructType (org.apache.spark.sql.types.IntegerType$ and org.apache.spark.sql.types.StructType are in unnamed module of loader 'app')
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$42$$anonfun$applyOrElse$218.$anonfun$applyOrElse$220(Analyzer.scala:3241)
  at scala.Option.map(Option.scala:242)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$42$$anonfun$applyOrElse$218.$anonfun$applyOrElse$219(Analyzer.scala:3239)
  at scala.collection.immutable.List.map(List.scala:246)
  at scala.collection.immutable.List.map(List.scala:79)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$42$$anonfun$applyOrElse$218.applyOrElse(Analyzer.scala:3237)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$42$$anonfun$applyOrElse$218.applyOrElse(Analyzer.scala:3234)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:566)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:566)
```",,,,,,,,,,,,,,,,,,,,,,,SPARK-44621,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,46:17.0,,,,,,,,,,0|z1iz4g:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove exclusion for scala-xml for Spark Connect Scala Client,SPARK-44611,13545561,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/31/23 15:35,8/1/23 1:02,7/17/24 20:45,8/1/23 1:02,3.4.1,3.5.0,Connect,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,35:28.0,,,,,,,,,,0|z1jhq8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Terminate streaming queries when a session times out in Spark Connect,SPARK-44432,13543587,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rangadi,rangadi,rangadi,7/14/23 17:51,7/30/23 1:42,7/17/24 20:45,7/30/23 1:42,3.4.1,"3.5.0, 4.0.0","Connect, Structured Streaming",,0,,"Spark Connect server keeps a session id at connect server alive as long as a streaming query is alive. This may not be desirable. If the client disconnects long enough and times out in server, we should let the session mapping to be removed and terminate the streaming queries. 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,SPARK-42938,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 30 01:42:15 UTC 2023,,,,,,,,,,0|z1j5k8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"30/Jul/23 01:42;gurwls223;Issue resolved by pull request 42193
[https://github.com/apache/spark/pull/42193];;;",,,,,
Remove ToJsonUtil,SPARK-44538,13544730,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/25/23 1:20,7/29/23 1:32,7/17/24 20:45,7/29/23 1:32,3.4.1,3.5.0,"Connect, SQL",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 27 06:06:44 UTC 2023,,,,,,,,,,0|z1jcm8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"27/Jul/23 06:06;awsthni;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/42164;;;",,,,,
Spark Connect DataFrame does not allow to add custom instance attributes and check for it,SPARK-44528,13544702,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,grundprinzip-db,grundprinzip-db,grundprinzip-db,7/24/23 20:32,7/26/23 23:54,7/17/24 20:45,7/26/23 23:54,3.4.1,"3.5.0, 4.0.0",Connect,,0,,"```
df = spark.range(10)
df._test = 10

assert(hasattr(df, ""_test""))
assert(!hasattr(df, ""_test_no""))
```

Treats `df._test` like a column",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 26 23:54:23 UTC 2023,,,,,,,,,,0|z1jcg0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Jul/23 23:54;gurwls223;Issue resolved by pull request 42132
[https://github.com/apache/spark/pull/42132];;;",,,,,
Move encoder inference to sql/api,SPARK-44531,13544710,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/24/23 22:01,7/26/23 11:16,7/17/24 20:45,7/26/23 11:16,3.4.1,3.5.0,"Connect, SQL",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,01:36.0,,,,,,,,,,0|z1jchs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Move Streaming API to sql/api,SPARK-44535,13544726,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/24/23 23:47,7/26/23 3:58,7/17/24 20:45,7/26/23 3:58,3.4.1,3.5.0,"Connect, SQL",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 26 03:58:48 UTC 2023,,,,,,,,,,0|z1jclc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Jul/23 03:58;snoot;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/42140;;;",,,,,
Move ArrowUtil to sql/api,SPARK-44532,13544711,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/24/23 22:14,7/26/23 3:51,7/17/24 20:45,7/26/23 3:51,3.4.1,3.5.0,"Connect, SQL",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,14:36.0,,,,,,,,,,0|z1jci0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Move SparkBuildInfo to common/util,SPARK-44530,13544709,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/24/23 21:43,7/26/23 0:52,7/17/24 20:45,7/26/23 0:52,3.4.1,3.5.0,Spark Core,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,43:06.0,,,,,,,,,,0|z1jchk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup .spark-staging directories when yarn application fails,SPARK-44543,13544805,,New Feature,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,dipayandev,dipayandev,7/25/23 12:21,7/25/23 12:35,7/17/24 20:45,,3.4.1,,"Spark Core, Spark Shell",,0,,"Spark creates the staging directories like .hive-staging, .spark-staging etc which get created when you run an dynamic insert overwrite to a partitioned table. Spark spends maximum time in renaming the partitioned files, and because GCS renaming are too slow, there are frequent scenarios where YARN fails due to network error etc.

Such directories will remain forever in Google Cloud Storage, in case the yarn application manager gets killed.
 
Over time this pileup and incurs a lot of cloud storage cost.
 
Can we update our File committer to clean up the temporary directories in case the job commit fails.

PS : This request is specifically for GCS.
Image for reference 
 

!image-2023-07-25-17-52-55-006.png|width=458,height=177!",,,,,,,,,,,,,,,,,,,,,,,,,25/Jul/23 12:22;dipayandev;image-2023-07-25-17-52-55-006.png;https://issues.apache.org/jira/secure/attachment/13061612/image-2023-07-25-17-52-55-006.png,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 12:31:00 UTC 2023,,,,,,,,,,0|z1jd2w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,25/Jul/23 12:31;dipayandev;Happy to contribute to this development of this feature. ;;;,,,,,
Exclude configs starting with SPARK_DRIVER_PREFIX and SPARK_EXECUTOR_PREFIX from modifiedConfigs,SPARK-44466,13543869,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,7/18/23 2:38,7/25/23 2:46,7/17/24 20:45,7/25/23 2:46,3.4.1,"3.5.0, 4.0.0",SQL,,0,,"Should not include this value: 
!screenshot-1.png! ",,,,,,,,,,,,,,,,,,,,,,,,,18/Jul/23 02:38;yumwang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13061380/screenshot-1.png,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 25 02:46:31 UTC 2023,,,,,,,,,,0|z1j7aw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Jul/23 02:48;yumwang;https://github.com/apache/spark/pull/42049;;;, 25/Jul/23 02:46;yumwang;Issue resolved by pull request 42049
[https://github.com/apache/spark/pull/42049];;;",,,,,"25/Jul/23 02:46;yumwang;Issue resolved by pull request 42049
[https://github.com/apache/spark/pull/42049];;;"
Add upcasting to Arrow deserializers,SPARK-44449,13543702,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/16/23 23:17,7/25/23 0:46,7/17/24 20:45,7/25/23 0:46,3.4.1,3.5.0,Connect,,0,,,,,,,,,,,,,,,SPARK-44396,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,17:34.0,,,,,,,,,,0|z1j69s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
parse_url treats key as regular expression,SPARK-44500,13544306,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,revans2,revans2,7/20/23 14:36,7/24/23 23:41,7/17/24 20:45,,"3.2.0, 3.3.0, 3.4.0, 3.4.1",,SQL,,0,,"To be clear I am not 100% sure that this is a bug. It might be a feature, but I don't see anywhere that it is used as a feature. If it is a feature it really should be documented, because there are pitfalls. If it is a bug it should be fixed because it is really confusing and it is simple to shoot yourself in the foot.

```scala
> val urls = Seq(""http://foo/bar?abc=BAD&a.c=GOOD"", ""http://foo/bar?a.c=GOOD&abc=BAD"").toDF
> urls.selectExpr(""parse_url(value, 'QUERY', 'a.c')"").show(false)

+----------------------------+
|parse_url(value, QUERY, a.c)|
+----------------------------+
|BAD                         |
|GOOD                        |
+----------------------------+

> urls.selectExpr(""parse_url(value, 'QUERY', 'a[c')"").show(false)
java.util.regex.PatternSyntaxException: Unclosed character class near index 15
(&|^)a[c=([^&]*)
               ^
  at java.util.regex.Pattern.error(Pattern.java:1969)
  at java.util.regex.Pattern.clazz(Pattern.java:2562)
  at java.util.regex.Pattern.sequence(Pattern.java:2077)
  at java.util.regex.Pattern.expr(Pattern.java:2010)
  at java.util.regex.Pattern.compile(Pattern.java:1702)
  at java.util.regex.Pattern.<init>(Pattern.java:1352)
  at java.util.regex.Pattern.compile(Pattern.java:1028)

```

The simple fix is to quote the key when making the pattern.

```scala
  private def getPattern(key: UTF8String): Pattern = {
    Pattern.compile(REGEXPREFIX + Pattern.quote(key.toString) + REGEXSUBFIX)
  }
```",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 23:41:37 UTC 2023,,,,,,,,,,0|z1ja00:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,24/Jul/23 23:41;planga82;[~jan.chou.wu@gmail.com] What do you think?;;;,3.3.0,3.4.0,3.4.1,,
SHOW CREATE TABLE does not quote identifiers with special characters,SPARK-44455,13543721,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,runyao,runyao,runyao,7/17/23 6:32,7/24/23 22:20,7/17/24 20:45,7/24/23 22:20,"3.4.0, 3.4.1",3.5.0,SQL,,0,,"Create a table with special characters:

```

CREATE CATALOG `a_catalog-with+special^chars`; CREATE SCHEMA `a_catalog-with+special^chars`.`a_schema-with+special^chars`; CREATE TABLE `a_catalog-with+special^chars`.`a_schema-with+special^chars`.`table1` ( id int, feat1 varchar(255), CONSTRAINT pk PRIMARY KEY (id,feat1) );

```

Then run SHOW CREATE TABLE:

```

SHOW CREATE TABLE `a_catalog-with+special^chars`.`a_schema-with+special^chars`.`table1`;

```

The response is:

```

createtab_stmt ""CREATE TABLE a_catalog-with+special^chars.a_schema-with+special^chars.table1 ( id INT NOT NULL, feat1 VARCHAR(255) NOT NULL, CONSTRAINT pk PRIMARY KEY (id, feat1)) USING delta TBLPROPERTIES ( 'delta.minReaderVersion' = '1', 'delta.minWriterVersion' = '2') ""

```

As you can see, the table name in the response is not properly escaped with backticks. As a result, if a user copies and pastes this create table command to recreate the table, it will fail:

{{[INVALID_IDENTIFIER] The identifier a_catalog-with is invalid. Please, consider quoting it with back-quotes as `a_catalog-with`.(line 1, pos 22)}}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 22:20:34 UTC 2023,,,,,,,,,,0|z1j6e0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"17/Jul/23 06:39;runyao;Fix in https://github.com/apache/spark/pull/42034;;;, 24/Jul/23 22:20;Gengliang.Wang;Issue resolved by pull request 42034
[https://github.com/apache/spark/pull/42034];;;",3.4.1,,,,"24/Jul/23 22:20;Gengliang.Wang;Issue resolved by pull request 42034
[https://github.com/apache/spark/pull/42034];;;"
optimize generateTreeString code path,SPARK-44485,13544186,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liuzq12,liuzq12,liuzq12,7/19/23 21:25,7/22/23 7:44,7/17/24 20:45,7/22/23 7:44,3.4.1,"3.5.0, 4.0.0",Spark Core,,0,,"In `TreeNode.generateTreeString`, we observed inefficiency in scala collection operations and virtual function call.

This inefficiency become significant in large plan (we hit a example of more than 1000 nodes). So {*}it’s worth optimizing the super hot code path{*}. By rewriting into native Java code(not so sweet as scala syntax sugar though), we should be able to get rid of most of the overhead.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 22 07:44:01 UTC 2023,,,,,,,,,,0|z1j99c:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"22/Jul/23 07:44;gurwls223;Issue resolved by pull request 42095
[https://github.com/apache/spark/pull/42095];;;",,,,,
KubernetesSuite report NPE when not set spark.kubernetes.test.unpackSparkDir,SPARK-44487,13544200,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,7/20/23 1:56,7/21/23 1:48,7/17/24 20:45,7/21/23 1:48,3.4.1,4.0.0,"Kubernetes, Tests",,0,,"KubernetesSuite report NPE when not set spark.kubernetes.test.unpackSparkDir

 

Exception encountered when invoking run on a nested suite.
java.lang.NullPointerException
    at sun.nio.fs.UnixPath.normalizeAndCheck(UnixPath.java:77)
    at sun.nio.fs.UnixPath.<init>(UnixPath.java:71)
    at sun.nio.fs.UnixFileSystem.getPath(UnixFileSystem.java:281)
    at java.nio.file.Paths.get(Paths.java:84)
    at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.$anonfun$beforeAll$4(KubernetesSuite.scala:164)
    at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.$anonfun$beforeAll$4$adapted(KubernetesSuite.scala:163)
    at scala.collection.LinearSeqOptimized.find(LinearSeqOptimized.scala:115)
    at scala.collection.LinearSeqOptimized.find$(LinearSeqOptimized.scala:112)
    at scala.collection.immutable.List.find(List.scala:91)
    at org.apache.spark.deploy.k8s.integrationtest.KubernetesSuite.beforeAll(KubernetesSuite.scala:163)",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 21 01:48:03 UTC 2023,,,,,,,,,,0|z1j9cg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"21/Jul/23 01:48;gurwls223;Issue resolved by pull request 42081
[https://github.com/apache/spark/pull/42081];;;",,,,,
FileSourceScanExec OutputPartitioning for non bucketed scan,SPARK-44499,13544302,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,tushar.mahale,tushar.mahale,7/20/23 13:42,7/20/23 13:42,7/17/24 20:45,,3.4.1,,SQL,,0,,"FileSourceScanExec.outputPartitioning currently is calculated for bucketed scan only and for non-bucketed scan, we return UnknownPartitioning(0). This may result into unnecessary empty tasks creation, based on the SQLConf defaultParallelism setting even though the actual file may have very low number of partitions.

We need to also calculate and set the number of output partitions correctly for non-bucketed scan.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,42:21.0,,,,,,,,,,0|z1j9z4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Add direct Arrow deserialization,SPARK-44396,13543336,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/12/23 19:23,7/19/23 13:28,7/17/24 20:45,7/19/23 13:28,3.4.1,3.5.0,"Connect, SQL",,0,,,,,,,,,,,,,,,,SPARK-44449,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,SPARK-42554,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 18 09:10:18 UTC 2023,,,,,,,,,,0|z1j40o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Jul/23 09:10;githubbot;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/42011;;;",,,,,
Handle char/varchar in Dataset.to to keep consistent with others,SPARK-44409,13543408,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,7/13/23 9:45,7/17/23 5:35,7/17/24 20:45,7/17/23 5:35,"3.4.1, 3.5.0",3.5.0,SQL,,0,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33641,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 17 05:35:43 UTC 2023,,,,,,,,,,0|z1j4gg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"17/Jul/23 05:35;Qin Yao;Issue resolved by pull request 41992
[https://github.com/apache/spark/pull/41992];;;",3.5.0,,,,
DistributionAndOrderingUtils should apply ResolveTimeZone,SPARK-44180,13541305,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,6/25/23 12:31,7/14/23 1:26,7/17/24 20:45,7/14/23 1:26,3.4.1,"3.4.2, 3.5.0",SQL,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 14 01:26:11 UTC 2023,,,,,,,,,,0|z1irk8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Jun/23 03:39;snoot;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/41725;;;, 14/Jul/23 01:26;cloud_fan;Issue resolved by pull request 41725
[https://github.com/apache/spark/pull/41725];;;",,,,,"14/Jul/23 01:26;cloud_fan;Issue resolved by pull request 41725
[https://github.com/apache/spark/pull/41725];;;"
Scala foreachBatch API in Streaming Spark Connect,SPARK-44398,13543339,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rangadi,rangadi,rangadi,7/12/23 20:26,7/13/23 17:48,7/17/24 20:45,7/13/23 17:48,3.4.1,3.5.0,Connect,,0,,Implement foreachBatch API in Scala Spark Connect,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,SPARK-42938,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 13 17:48:19 UTC 2023,,,,,,,,,,0|z1j41c:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"13/Jul/23 17:48;XinrongM;Issue resolved by pull request 41969
[https://github.com/apache/spark/pull/41969];;;",,,,,
Remove toAttributes from StructType,SPARK-44353,13543003,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/10/23 12:01,7/12/23 6:27,7/17/24 20:45,7/12/23 6:27,3.4.1,3.5.0,"Connect, SQL",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,01:44.0,,,,,,,,,,0|z1j1z4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Status of execution w/ error and w/o jobs shall be FAILED not COMPLETED,SPARK-44334,13542788,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,7/7/23 9:37,7/12/23 5:35,7/17/24 20:45,7/12/23 5:35,"3.3.2, 3.4.1, 3.5.0",3.5.0,"SQL, Web UI",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 05:35:21 UTC 2023,,,,,,,,,,0|z1j0nc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Jul/23 05:35;Qin Yao;Issue resolved by pull request 41891
[https://github.com/apache/spark/pull/41891];;;",3.4.1,3.5.0,,,
Migrate Buf remote generation alpha to remote plugins,SPARK-44370,13543106,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,7/11/23 6:17,7/12/23 4:45,7/17/24 20:45,7/12/23 4:45,3.4.1,3.5.0,Connect,,0,,Buf unsupported remote generation alpha at now. Please refer [https://buf.build/docs/migration-guides/migrate-remote-generation-alpha/] . We should migrate Buf remote generation alpha to remote plugins by follow the guide.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 04:45:11 UTC 2023,,,,,,,,,,0|z1j2lk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"12/Jul/23 04:26;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41933;;;, 12/Jul/23 04:45;gurwls223;Issue resolved by pull request 41933
[https://github.com/apache/spark/pull/41933];;;",,,,,"12/Jul/23 04:45;gurwls223;Issue resolved by pull request 41933
[https://github.com/apache/spark/pull/41933];;;"
Broadcast Joins taking up too much memory,SPARK-44379,13543178,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,shardulm,shardulm,7/11/23 18:02,7/11/23 18:05,7/17/24 20:45,,3.4.1,,SQL,,0,,"Context: After migrating to Spark 3 with AQE, we saw a significant increase in driver and executor memory usage in our jobs which contains star joins. By analyzing heapdump, we saw that majority of the memory was being taken up by {{UnsafeHashedRelation}} used for broadcast joins; in this case there were 92 broadcast joins in the query.

!screenshot-1.png|width=851,height=70!

This took up over 6GB of total memory, even though every table being broadcasted was around ~1MB and hence should only have been ~100MB total. I found that this is because {{BytesToBytesMap}} used within {{UnsafeHashedRelation}} allocates memory in [""pageSize"" increments|https://github.com/apache/spark/blob/37aa62f629e652ed70505620473530cd9611018e/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java#L117] which in our case was 64MB. Based on the [default page size calculation|https://github.com/apache/spark/blob/37aa62f629e652ed70505620473530cd9611018e/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala#L251], this should be the case for any container with > 1 GB of memory (assuming executor.cores = 1) which is far too common. Thus in our case, most of the memory requested by {{BytesToBytesMap}} was un-utilized with just trailing 0s.

!screenshot-2.png|width=389,height=101!

I think this is a major inefficiency for broadcast joins (especially star joins). I think there are a few ways to tackle the problem.
1) Reduce {{spark.buffer.pageSize}} globally to a lower value. This does reduce the memory consumption of broadcast joins, but I am not sure what it implies for the rest of Spark machinery
2) Add a ""finalize"" operation to {{BytesToBytesMap}} which is called after all values are added to the map and allocates a new page only for the required bytes. 
3) Enhance the serialization of {{BytesToBytesMap}} to record the number of keys and values, and use those during deserialization to only request the required memory.
4) Use a lower page size for certain {{BytesToBytesMap}} based on the estimated data size of broadcast joins.

I believe Option 3 would be simple enough to implement and I have a POC PR which I will post soon, but I am interested in knowing other people's thoughts here. ",,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/23 18:03;shardulm;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13061250/screenshot-1.png, 11/Jul/23 18:03;shardulm;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/13061251/screenshot-2.png",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 11 18:05:47 UTC 2023,,,,,,,,,,0|z1j31k:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,11/Jul/23 18:05;shardulm;cc: [~cloud_fan] [~joshrosen] [~mridul] Would be interested in knowing your thoughts here.;;;,,,,,
Allow ChannelBuilder extensions -- Scala,SPARK-44263,13542094,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cdkrot,cdkrot,cdkrot,6/30/23 12:31,7/11/23 8:34,7/17/24 20:45,7/11/23 8:34,3.4.1,3.5.0,Connect,,0,,"Follow up to https://issues.apache.org/jira/browse/SPARK-43332

Provide similar extension capabilities in Scala",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 11 08:34:11 UTC 2023,,,,,,,,,,0|z1iweo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"04/Jul/23 09:04;githubbot;User 'cdkrot' has created a pull request for this issue:
https://github.com/apache/spark/pull/41807;;;, 07/Jul/23 11:20;ggintegration;User 'cdkrot' has created a pull request for this issue:
https://github.com/apache/spark/pull/41880;;;, 11/Jul/23 08:34;gurwls223;Issue resolved by pull request 41880
[https://github.com/apache/spark/pull/41880];;;",,,,,"07/Jul/23 11:20;ggintegration;User 'cdkrot' has created a pull request for this issue:
https://github.com/apache/spark/pull/41880;;;"
Potential for incorrect results or NPE when full outer USING join has null key value,SPARK-44251,13541996,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,6/29/23 19:53,7/11/23 3:22,7/17/24 20:45,7/11/23 3:21,"3.3.2, 3.4.1, 3.5.0","3.3.3, 3.4.2, 3.5.0",SQL,,0,correctness,"The following query produces incorrect results:
{noformat}
create or replace temp view v1 as values (1, 2), (null, 7) as (c1, c2);
create or replace temp view v2 as values (2, 3) as (c1, c2);

select explode(array(c1)) as x
from v1
full outer join v2
using (c1);

-1   <== should be null
1
2
{noformat}
The following query fails with a {{NullPointerException}}:
{noformat}
create or replace temp view v1 as values ('1', 2), (null, 7) as (c1, c2);
create or replace temp view v2 as values ('2', 3) as (c1, c2);

select explode(array(c1)) as x
from v1
full outer join v2
using (c1);

23/06/25 17:06:39 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID 11)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.generate_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.smj_consumeFullOuterJoinRow_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.wholestagecodegen_findNextJoinRows_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
...
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 11 03:21:22 UTC 2023,,,,,,,,,,0|z1ivsw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 19:57;bersprockets;This is similar to, but not quite the same as SPARK-43718, and the fix will be similar too.

I will make a PR shortly.
 ;;;, 30/Jun/23 17:17;bersprockets;PR can be found here: https://github.com/apache/spark/pull/41809;;;, 11/Jul/23 03:21;yumwang;Issue resolved by pull request 41809
[https://github.com/apache/spark/pull/41809];;;",3.4.1,3.5.0,,,30/Jun/23 17:17;bersprockets;PR can be found here: https://github.com/apache/spark/pull/41809;;;
Move sameType back to DataType,SPARK-44352,13542998,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/10/23 11:22,7/10/23 18:46,7/17/24 20:45,7/10/23 18:46,3.4.1,3.5.0,"Connect, SQL",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 10 12:01:30 UTC 2023,,,,,,,,,,0|z1j1y0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"10/Jul/23 12:01;awsthni;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/41921;;;",,,,,
Separate encoder inference from expression encoder generation in ScalaReflection,SPARK-44343,13542876,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/8/23 12:17,7/10/23 18:45,7/17/24 20:45,7/10/23 18:45,3.4.1,3.5.0,"Connect, SQL",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,17:56.0,,,,,,,,,,0|z1j16w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Any fields set to Any.getDefaultInstance cause exceptions.,SPARK-44337,13542859,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rangadi,rangadi,rangadi,7/8/23 1:01,7/10/23 6:41,7/17/24 20:45,7/10/23 6:41,3.4.1,3.5.0,Protobuf,,0,,"Protobuf functions added support for converting `Any` fields to json strings. 

It uses Protobuf's built in `JsonFormat` to covert to JSON.

JsonFormat fails to handled the fields when they are set to `Any.getDefaultInstance()` in the original message. This fails only while using descriptor set, but does not fail while using Java classes. Since using descriptor files is the common case, this can be blocker. ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,SPARK-40653,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 10 06:41:10 UTC 2023,,,,,,,,,,0|z1j134:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"10/Jul/23 06:41;gurwls223;Issue resolved by pull request 41897
[https://github.com/apache/spark/pull/41897];;;",,,,,
Move EnhancedLogicalPlan out of ParserUtils,SPARK-44333,13542769,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/7/23 7:35,7/7/23 17:37,7/17/24 20:45,7/7/23 17:37,3.4.1,3.5.0,SQL,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,35:18.0,,,,,,,,,,0|z1j0j4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Make parser use SqlApiConf,SPARK-44322,13542694,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/6/23 12:28,7/7/23 17:36,7/17/24 20:45,7/7/23 17:36,3.4.1,3.5.0,Connect,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,28:35.0,,,,,,,,,,0|z1j02o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
[PYTHON][CONNECT] Use SPARK_CONNECT_USER_AGENT environment variable for the user agent,SPARK-44312,13542590,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dillitz,dillitz,dillitz,7/5/23 20:53,7/7/23 2:58,7/17/24 20:45,7/7/23 2:58,3.4.1,3.5.0,Connect,,0,,Allow us to prepend a Spark Connect user agent with an environment variable: *SPARK_CONNECT_USER_AGENT*,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 07 02:58:13 UTC 2023,,,,,,,,,,0|z1izfk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"07/Jul/23 02:58;gurwls223;Issue resolved by pull request 41866
[https://github.com/apache/spark/pull/41866];;;",,,,,
Move Origin to api,SPARK-44283,13542347,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/3/23 22:07,7/6/23 12:26,7/17/24 20:45,7/6/23 12:26,3.4.1,3.5.0,Connect,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,07:34.0,,,,,,,,,,0|z1ixyw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Add tests to ensure error-classes.json and docs are in sync,SPARK-44268,13542203,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fanjia,fanjia,fanjia,7/1/23 15:24,7/6/23 3:45,7/17/24 20:45,7/2/23 15:51,3.4.1,3.5.0,Spark Core,,0,,"We should add tests to ensure error-classes.json and docs are in sync, docs and error-classes.json are always up to date before the PR is committed.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 03:45:20 UTC 2023,,,,,,,,,,0|z1ix2w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"02/Jul/23 15:51;maxgekk;Issue resolved by pull request 41813
[https://github.com/apache/spark/pull/41813];;;, 03/Jul/23 10:22;ignitetcbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41813;;;, 06/Jul/23 03:45;snoot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41865;;;",,,,,"03/Jul/23 10:22;ignitetcbot;User 'Hisoka-X' has created a pull request for this issue:
https://github.com/apache/spark/pull/41813;;;"
Generated column expression validation fails if there is a char/varchar column anywhere in the schema,SPARK-44313,13542601,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allison-portis,allison-portis,allison-portis,7/5/23 22:37,7/6/23 2:27,7/17/24 20:45,7/6/23 2:27,"3.4.0, 3.4.1","3.4.2, 3.5.0",SQL,,0,,"When validating generated column expressions, this call https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/util/GeneratedColumn.scala#L123 to checkAnalysis fails when there are char or varchar columns anywhere in the schema.

 

For example, this query will fail
{code:java}
CREATE TABLE default.example (
    name VARCHAR(64),
    tstamp TIMESTAMP,
    tstamp_date DATE GENERATED ALWAYS AS (CAST(tstamp as DATE))
){code}
 ",,,,,,,,,,,,,,,,,,,,,,,SPARK-41290,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 02:27:18 UTC 2023,,,,,,,,,,0|z1izi0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"06/Jul/23 02:27;Qin Yao;Issue resolved by pull request 41868
[https://github.com/apache/spark/pull/41868];;;",3.4.1,,,,
Split of DataType parsing for Connect,SPARK-44282,13542346,,New Feature,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,7/3/23 21:58,7/5/23 20:23,7/17/24 20:45,7/5/23 20:23,3.4.1,3.5.0,Connect,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,58:19.0,,,,,,,,,,0|z1ixyo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
[CONNECT][SCALA] range query returns incorrect schema,SPARK-44291,13542365,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nija,nija,nija,7/4/23 5:58,7/5/23 19:13,7/17/24 20:45,7/5/23 19:13,3.4.1,3.5.0,Connect,,0,,"The following code on Spark Connect produces the following output

Code:

 
{code:java}
val df = spark.range(3)

df.show()
df.printSchema(){code}
 

Output:
{code:java}
+---+
| id|
+---+
|  0|
|  1|
|  2|
+---+

root
 |-- value: long (nullable = true) {code}
The mismatch is that one shows the column as ""id"" while the other shows this as ""value"".",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,58:14.0,,,,,,,,,,0|z1iy2w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
CacheManager refreshes the fileIndex unnecessarily,SPARK-44199,13541463,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vihangk1,vihangk1,vihangk1,6/26/23 18:30,7/3/23 23:08,7/17/24 20:45,7/3/23 23:08,3.4.1,3.5.0,Spark Core,,0,,"The CacheManager on this line [https://github.com/apache/spark/blob/680ca2e56f2c8fc759743ad6755f6e3b1a19c629/sql/core/src/main/scala/org/apache/spark/sql/execution/CacheManager.scala#L372] uses a prefix based matching to decide which file index needs to be refreshed. However, that can be incorrect if the users have paths which are not subdirectories but share prefixes. For example, in the function below:

 
{code:java}
  private def refreshFileIndexIfNecessary(
      fileIndex: FileIndex,
      fs: FileSystem,
      qualifiedPath: Path): Boolean = {
    val prefixToInvalidate = qualifiedPath.toString
    val needToRefresh = fileIndex.rootPaths
      .map(_.makeQualified(fs.getUri, fs.getWorkingDirectory).toString)
      .exists(_.startsWith(prefixToInvalidate))
    if (needToRefresh) fileIndex.refresh()
    needToRefresh
  } {code}
{{If the prefixToInvalidate is s3://bucket/mypath/table_dir and the file index has one of the root paths as s3://bucket/mypath/table_dir_2/part=1, then the needToRefresh will be true and the file index gets refreshed unnecessarily. This is not just wasted CPU cycles but can cause query failures as well, if there are access restrictions to the path being refreshed.}}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 16:52:55 UTC 2023,,,,,,,,,,0|z1isjc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 16:52;ignitetcbot;User 'vihangk1' has created a pull request for this issue:
https://github.com/apache/spark/pull/41749;;;",,,,,
Inconsistent path qualifying between catalog and data operations,SPARK-44185,13541343,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yao,yao,yao,6/26/23 3:49,7/3/23 9:27,7/17/24 20:45,7/3/23 9:27,"3.2.4, 3.3.2, 3.4.1, 3.5.0",3.5.0,SQL,,0,,"For example
 * CREATE TABLE statement with relative LOCATION will infer schema from files from the directory relative to the current working directory and store the directory relative to the warehouse path. 
 * CTAS statement with relative LOCATION cannot assert empty root path as it checks the wrong path it will finally use.
 * DataframeWriter does not qualify the path before checking",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 09:27:20 UTC 2023,,,,,,,,,,0|z1irso:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,03/Jul/23 09:27;yumwang;Issue resolved by pull request https://github.com/apache/spark/pull/41733;;;,3.3.2,3.4.1,3.5.0,,
Potential memory leak when temp views created from DF created by structured streaming,SPARK-44253,13542000,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,attilapiros,attilapiros,6/29/23 20:22,6/29/23 20:41,7/17/24 20:45,,"2.4.8, 3.0.3, 3.1.3, 3.2.4, 3.3.2, 3.4.0, 3.4.1",,Structured Streaming,,0,,"If the user registers a temporary view from a dataframe created by Structured Streaming and tries to drop the temporary view via his original SparkSession then memory will be leaking.

The reason is Structured streaming has its own SparkSession (as a clone of the original SparkSession, for details see https://issues.apache.org/jira/browse/SPARK-26586 and [https://github.com/apache/spark/blob/branch-3.4/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/StreamExecution.scala#L193-L194]) the created temporary view belongs the cloned SparkSession and the dropping of the temporary view must be done via the cloned SparkSession.

Example for the {*}memory leak{*}:
{noformat}
streamingDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>
  val view = s“tempView_$batchId” 
  batchDF.createOrReplaceTempView(view)
  ...
  spark.catalog.dropTempView(view)
}
{noformat}
*Workaround* (the _dropTempView_ must be called on SparkSession accessed from dataframe created by streaming):
{noformat}
streamingDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>
  val view = s“tempView_$batchId” 
  batchDF.createOrReplaceTempView(view)
  ...
  batchDF.sparkSession.catalog.dropTempView(view)
 }
{noformat}
h4. Example heap dump

The SparkSession with the leak:

!1.png|width=807,height=120!

The two SparkSession instances where the first one was is the original SparkSession created by the user and the second is the clone:
!2.png|width=813,height=157!",,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/23 20:22;attilapiros;1.png;https://issues.apache.org/jira/secure/attachment/13060978/1.png, 29/Jun/23 20:23;attilapiros;2.png;https://issues.apache.org/jira/secure/attachment/13060979/2.png",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,22:46.0,,,,,,,,,,0|z1ivts:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.0.3,3.1.3,3.2.4,3.3.2,
Exception when reading parquet file with TIME fields,SPARK-44165,13541242,,New Feature,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,ramon_garcia_fer,ramon_garcia_fer,6/24/23 11:41,6/29/23 16:52,7/17/24 20:45,,"3.4.0, 3.4.1",,SQL,,0,,"When one reads a parquet file containing TIME fields (either with INT32 or INT64 storage) and exception is thrown. From spark shell

 

{{> val df = spark.read.parquet(""timeonly.parquet"")}}

{color:#de350b}23/06/24 13:24:54 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 1]{color}
{color:#de350b}org.apache.spark.sql.AnalysisException: Illegal Parquet type: INT32 (TIME(MILLIS,true)).{color}
{color:#de350b}    at org.apache.spark.sql.errors.QueryCompilationErrors$.illegalParquetTypeError(QueryCompilationErrors.scala:1762){color}
{color:#de350b}    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.illegalType$1(ParquetSchemaConverter.scala:206){color}
{color:#de350b}    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertPrimitiveField$2(ParquetSchemaConverter.scala:252){color}
{color:#de350b}    at scala.Option.getOrElse(Option.scala:189){color}
{color:#de350b}    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertPrimitiveField(ParquetSchemaConverter.scala:224){color}
{color:#de350b}    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.convertField(ParquetSchemaConverter.scala:187){color}
{color:#de350b}    at org.apache.spark.sql.execution.datasources.parquet.ParquetToSparkSchemaConverter.$anonfun$convertInternal$3(ParquetSchemaConverter.scala:147){color}","Spark 3.4.0 downloaded from apache.spark.org

Also reproduced with latest build.",,,,,,,,,,,,,,,,,,,,,,,,24/Jun/23 11:41;ramon_garcia_fer;timeonly.parquet;https://issues.apache.org/jira/secure/attachment/13060833/timeonly.parquet,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 16:52:46 UTC 2023,,,,,,,,,,0|z1ir68:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Jun/23 17:06;ramon_garcia_fer;Added [pull request 41717|https://github.com/apache/spark/pull/41717] to support TIME columns.;;;, 29/Jun/23 16:52;ignitetcbot;User 'ramon-garcia' has created a pull request for this issue:
https://github.com/apache/spark/pull/41717;;;",3.4.1,,,,"29/Jun/23 16:52;ignitetcbot;User 'ramon-garcia' has created a pull request for this issue:
https://github.com/apache/spark/pull/41717;;;"
Add Apache Spark 3.4.1 Dockerfiles,SPARK-44168,13541268,13482511,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,6/25/23 1:29,6/29/23 8:05,7/17/24 20:45,6/25/23 7:07,3.4.1,3.4.1,Spark Docker,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,29:06.0,,,,,,,,,,0|z1irc0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS missing the child info on UI when groupSQLSubExecutionEnabled is enabled,SPARK-44213,13541605,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,yumwang,yumwang,6/27/23 16:36,6/27/23 16:39,7/17/24 20:45,,"3.4.0, 3.4.1",,SQL,,0,,"{code:sql}
create table tbl using parquet as select t1.id from range(10) as t1 join range(100) as t2 on t1.id = t2.id;
{code}
Enabled:
 !enabled.png! 
Disabled:
 !screenshot-1.png! ",,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/23 16:36;yumwang;enabled.png;https://issues.apache.org/jira/secure/attachment/13060897/enabled.png, 27/Jun/23 16:36;yumwang;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13060898/screenshot-1.png",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 16:39:25 UTC 2023,,,,,,,,,,0|z1iteg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"27/Jun/23 16:37;yumwang;cc [~linhongliu-db];;;, 27/Jun/23 16:39;yumwang;Related issue ticket: SPARK-41752.;;;",3.4.1,,,,27/Jun/23 16:39;yumwang;Related issue ticket: SPARK-41752.;;;
Remove a wrong doc about ARROW_PRE_0_15_IPC_FORMAT,SPARK-44184,13541334,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,6/25/23 22:21,6/26/23 1:53,7/17/24 20:45,6/26/23 1:53,"3.0.3, 3.1.3, 3.2.4, 3.3.2, 3.4.1, 3.5.0","3.3.3, 3.4.2, 3.5.0","Documentation, PySpark",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 01:53:57 UTC 2023,,,,,,,,,,0|z1irqo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Jun/23 01:53;dongjoon;Issue resolved by pull request 41730
[https://github.com/apache/spark/pull/41730];;;",3.1.3,3.2.4,3.3.2,3.4.1,
Update the incorrect sql example of insert table documentation,SPARK-44072,13540295,,Documentation,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yoha.zy,yoha.zy,yoha.zy,6/16/23 2:15,6/16/23 5:26,7/17/24 20:45,6/16/23 3:08,"3.3.3, 3.4.1, 3.5.0",3.5.0,Documentation,,0,,"Latest docs of insert table has an incorrect sql example about 'Insert Using a Typed Date Literal for a Partition Column Value'.

It should be
{code:java}
INSERT OVERWRITE students PARTITION (birthday = date'2019-01-02') VALUES('Jason Wang', '908 Bird St, Saratoga'); {code}
Doc link: https://spark.apache.org/docs/latest/sql-ref-syntax-dml-insert-table.html#insert-using-a-typed-date-literal-for-a-partition-column-value-1",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 03:51:38 UTC 2023,,,,,,,,,,0|z1ildc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"16/Jun/23 03:08;Qin Yao;Issue resolved by pull request 41619
[https://github.com/apache/spark/pull/41619];;;, 16/Jun/23 03:51;snoot;User 'Yohahaha' has created a pull request for this issue:
https://github.com/apache/spark/pull/41619;;;",3.4.1,3.5.0,,,"16/Jun/23 03:51;snoot;User 'Yohahaha' has created a pull request for this issue:
https://github.com/apache/spark/pull/41619;;;"
Update ORC to 1.8.4,SPARK-44053,13540006,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Guiyankuang,Guiyankuang,Guiyankuang,6/14/23 8:25,6/14/23 17:56,7/17/24 20:45,6/14/23 16:37,"3.4.1, 3.5.0","3.4.1, 3.5.0",Build,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 14 17:56:09 UTC 2023,,,,,,,,,,0|z1ijl4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"14/Jun/23 08:43;Guiyankuang;Our plan is to
spark 3.4.1 upgrade to ORC 1.8.4
spark 3.5.0 upgrade to ORC 1.9.0
So I set the affected version to 3.4.1

[~yumwang]  :);;;, 14/Jun/23 16:37;dongjoon;Issue resolved by pull request 41593
[https://github.com/apache/spark/pull/41593];;;, 14/Jun/23 17:56;dongjoon;Apache ORC 1.9.0 PR will arrive soon in this month.;;;",3.5.0,,,,"14/Jun/23 16:37;dongjoon;Issue resolved by pull request 41593
[https://github.com/apache/spark/pull/41593];;;"
SerializerHelper.deserializeFromChunkedBuffer leaks deserialization streams,SPARK-43378,13534986,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,5/4/23 12:59,6/6/23 7:04,7/17/24 20:45,5/5/23 0:34,"3.4.0, 3.4.1, 3.5.0","3.4.1, 3.5.0",Spark Core,,0,,The method SerializerHelper.deserializeFromChunkedBuffer leaks serializations stream. This can lead to huge performance regressions when using kryo serializer as the spark application can become bottlenecked on the driver creating expensive kryo objects that are then leaked as part of the deserialization stream,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 00:34:57 UTC 2023,,,,,,,,,,0|z1hots:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,05/May/23 00:34;srowen;Resolved by https://github.com/apache/spark/pull/41049;;;,3.4.1,3.5.0,,,
Document for unbase64 behavior change,SPARK-43751,13537344,,Documentation,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,chengpan,5/23/23 13:54,5/26/23 3:34,7/17/24 20:45,5/26/23 3:34,"3.3.3, 3.4.1, 3.5.0","3.3.3, 3.4.1, 3.5.0","Documentation, SQL",,0,,Document behavior change caused by https://issues.apache.org/jira/browse/SPARK-37820,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 03:34:55 UTC 2023,,,,,,,,,,0|z1i36w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/May/23 03:34;Qin Yao;Issue resolved by pull request 41280
[https://github.com/apache/spark/pull/41280];;;",3.4.1,3.5.0,,,
Avoid allocation of unwritten ColumnVector in VectorizedReader,SPARK-43264,13533864,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,majdyz,majdyz,4/24/23 14:40,5/23/23 16:50,7/17/24 20:45,,"3.4.1, 3.5.0",,"Spark Core, SQL",,0,,Spark Vectorized Reader allocates the array for every fields for each value count even the array is ended up empty. This causes a high memory consumption when reading a table with large struct+array or many columns with sparse value. One way to fix this is by lazily allocating the column vector and only allocates the array only when it is needed (array is written).,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 16:50:46 UTC 2023,,,,,,,,,,0|z1hhxk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/May/23 16:50;ggintegration;User 'majdyz' has created a pull request for this issue:
https://github.com/apache/spark/pull/40929;;;",3.5.0,,,,
Support the minimum number of range shuffle partitions,SPARK-43593,13536876,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,wankun,wankun,5/19/23 14:28,5/19/23 14:28,7/17/24 20:45,,3.4.1,,SQL,,0,,"If there are few distinct values in the RangePartitioner, there will be very few partitions that could be very large. We can append a random expression to increase the number of partitions.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,28:27.0,,,,,,,,,,0|z1i0aw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve the error messages for INVALID_CONNECT_URL,SPARK-43375,13534918,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,5/4/23 6:51,5/5/23 2:00,7/17/24 20:45,5/5/23 2:00,"3.4.1, 3.5.0",3.5.0,Connect,,0,,Make the INVALID_CONNECT_URL error messages more user-friendly.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 02:00:39 UTC 2023,,,,,,,,,,0|z1hoeo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"05/May/23 02:00;podongfeng;Issue resolved by pull request 41044
[https://github.com/apache/spark/pull/41044];;;",3.5.0,,,,
Casting between Timestamp and TimestampNTZ requires timezone,SPARK-43336,13534643,13382384,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,5/2/23 4:43,5/2/23 20:10,7/17/24 20:45,5/2/23 20:10,"3.4.1, 3.5.0","3.4.1, 3.5.0",SQL,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 02 20:10:10 UTC 2023,,,,,,,,,,0|z1hmps:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"02/May/23 20:10;Gengliang.Wang;Issue resolved by pull request 41010
[https://github.com/apache/spark/pull/41010];;;",3.5.0,,,,
Ignore generated Java files in checkstyle,SPARK-43141,13532652,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,4/14/23 11:36,4/17/23 2:57,7/17/24 20:45,4/16/23 10:47,3.4.1,"3.4.1, 3.5.0",Build,,0,,Files such as {{.../spark/core/target/scala-2.12/src_managed/main/org/apache/spark/status/protobuf/StoreTypes.java}} are checked in checkstyle. We shouldn't check them in the linter.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 16 10:47:41 UTC 2023,,,,,,,,,,0|z1haig:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"16/Apr/23 10:47;gurwls223;Issue resolved by pull request 40792
[https://github.com/apache/spark/pull/40792];;;",,,,,
Redact debug string in UI,SPARK-43089,13532111,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,gurwls223,4/11/23 1:40,4/11/23 3:23,7/17/24 20:45,4/11/23 3:22,3.4.1,3.5.0,"Connect, PySpark",,0,,https://github.com/apache/spark/pull/40603 exposes all data without redaction. We should redact it.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 11 03:23:34 UTC 2023,,,,,,,,,,0|z1h76o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"11/Apr/23 03:22;gurwls223;Issue resolved by pull request 40733
[https://github.com/apache/spark/pull/40733];;;, 11/Apr/23 03:23;snoot;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/40733;;;",,,,,"11/Apr/23 03:23;snoot;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/40733;;;"
"Support SELECT DEFAULT with ORDER BY, LIMIT, OFFSET for INSERT source relation",SPARK-43071,13531913,13430784,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dtenedor,dtenedor,dtenedor,4/7/23 22:16,4/10/23 23:09,7/17/24 20:45,4/10/23 23:09,3.4.1,3.4.1,SQL,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 10 23:09:46 UTC 2023,,,,,,,,,,0|z1h5yo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"10/Apr/23 23:09;Gengliang.Wang;Issue resolved by pull request 40710
[https://github.com/apache/spark/pull/40710];;;",,,,,
TableOutputResolver must use correct column paths in error messages for arrays and maps,SPARK-42997,13531021,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,aokolnychyi,aokolnychyi,3/31/23 19:04,4/3/23 22:21,7/17/24 20:45,4/3/23 22:21,"3.3.0, 3.3.1, 3.3.2, 3.3.3, 3.4.0, 3.4.1, 3.5.0",3.5.0,SQL,,0,,TableOutputResolver must use correct column paths in error messages for arrays and maps.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 22:21:27 UTC 2023,,,,,,,,,,0|z1h0h4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Apr/23 22:21;dongjoon;Issue resolved by pull request 40630
[https://github.com/apache/spark/pull/40630];;;",3.3.1,3.3.2,3.3.3,3.4.0,
Support TimestampNTZ in Cached Batch,SPARK-42796,13528517,13382384,Sub-task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,3/14/23 22:40,3/15/23 23:23,7/17/24 20:45,3/15/23 23:23,3.4.1,3.4.1,SQL,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 22:47:48 UTC 2023,,,,,,,,,,0|z1gl1k:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"14/Mar/23 22:47;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40426;;;, 14/Mar/23 22:47;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40426;;;",,,,,"14/Mar/23 22:47;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40426;;;"
Infer filters for Join produced by IN and EXISTS clause (RewritePredicateSubquery rule),SPARK-42660,13526947,,Improvement,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,,,kapilks_ms,kapilks_ms,3/3/23 7:05,3/3/23 7:36,7/17/24 20:45,,3.4.1,,SQL,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 07:36:16 UTC 2023,,,,,,,,,,0|z1gbcw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Mar/23 07:36;apachespark;User 'mskapilks' has created a pull request for this issue:
https://github.com/apache/spark/pull/40266;;;",,,,,
Track state store provider load time and log warning if it exceeds a threshold,SPARK-42567,13526119,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,anishshri-db,anishshri-db,anishshri-db,2/24/23 20:04,2/25/23 9:41,7/17/24 20:45,2/25/23 9:41,3.4.1,3.5.0,Structured Streaming,,0,,"Track state store provider load time and log warning if it exceeds a threshold

 

In some cases, we see that the filesystem initialization might take time for the first time that we create the provider and initialize it. This change will log the time taken if it exceeds a certain threshold",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 25 09:41:06 UTC 2023,,,,,,,,,,0|z1g69s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"24/Feb/23 20:05;anishshri-db;I will send the fix out soon

 

cc - [~kabhwan] 

 ;;;, 24/Feb/23 21:28;apachespark;User 'anishshri-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/40163;;;, 24/Feb/23 21:29;anishshri-db;Sent the PR here: https://github.com/apache/spark/pull/40163;;;, 25/Feb/23 09:41;kabhwan;Issue resolved by pull request 40163
[https://github.com/apache/spark/pull/40163];;;",,,,,"24/Feb/23 21:28;apachespark;User 'anishshri-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/40163;;;"
Refactor SparkConnect Service to extracted error handling functions to trait,SPARK-48674,13583336,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,asethia,asethia,6/20/24 14:25,6/20/24 17:04,7/17/24 20:45,,"3.4.0, 3.4.1, 3.4.2, 3.4.3",,Connect,,0,,"Since SparkConnect gRPC server can have multiple services (addService function on NettyServerBuilder) and these functions can be reused across services, specially when we would like to extend sparkconnect with various services.

We can extract error handling functions from SparkConnectService to a trait, that will increase code reusability. By doing this we can reuse these functions across multiple service implementations. Since we can add multiple Bindable service handlers to SparkConnect gRPC server it will be easy to use such common functions to handle errors and exceptions.",,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 20 17:04:54 UTC 2024,,,,,,,,,,0|z1px20:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,20/Jun/24 17:04;asethia;I think we can do cherry picking from 3.5 (ErrorUtils).;;;,3.4.1,3.4.2,3.4.3,,
Executor decommission causes stage failure,SPARK-44478,13544013,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,dhuett,dhuett,7/18/23 18:05,4/24/24 13:31,7/17/24 20:45,,"3.4.0, 3.4.1",,Scheduler,,0,,"During spark execution, save fails due to executor decommissioning. Issue not present in 3.3.0

Sample error:

 
{code:java}
An error occurred while calling o8948.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Authorized committer (attemptNumber=0, stage=170, partition=233) failed; but task commit success, data duplication may happen. reason=ExecutorLostFailure(1,false,Some(Executor decommission: Executor 1 is decommissioned.))
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageFailed$1(DAGScheduler.scala:1199)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageFailed$1$adapted(DAGScheduler.scala:1199)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleStageFailed(DAGScheduler.scala:1199)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2981)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
        at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
        at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
        at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
        at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
        at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
        at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
        at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
        at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
        at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
        at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
        at jdk.internal.reflect.GeneratedMethodAccessor497.invoke(Unknown Source)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.base/java.lang.reflect.Method.invoke(Unknown Source)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Unknown Source)
{code}
 

 

This occurred while running our production k8s spark jobs (spark 3.3.0) in a duplicate test environment, with the only change being the image used was spark 3.4.0 and 3.4.1, and the only changes in jar versions were the requisite dependencies. 

Current workaround is to retry the job, but this can cause substantial slowdowns if it occurs during a longer job.  Possibly related to https://issues.apache.org/jira/browse/SPARK-44389 ?",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 24 13:31:53 UTC 2024,,,,,,,,,,0|z1j86w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"27/Feb/24 18:49;dongjoon;Do you still see this issue with Apache Spark 3.4.2 or 3.5.1, [~dhuett]?;;;, 10/Apr/24 11:48;juhai;I am seeing this with Spark 3.5.0 for Spark k8s workloads. We use spot instances for executors but recently seen failures of full jobs when a large number of spot instances are lost at once. The error and stack trace is identical. We have a pending upgrade to 3.5.1 due to another issue with 3.5.0, I will report back soon. The incidence of this problem totally depends on how well we get to keep spot instances so isn't easy to reproduce.;;;, 24/Apr/24 13:31;juhai;Just a short update. This is clearly related to evictions/lost nodes/etc. I've now retried with spark 3.5.1 and 3.4.3 with a cluster configuration that caused excessive pod evictions (by Karpenter). I had a handful of different tasks that all consistently failed during the execution with this error. But, not all tasks were equal; while having a pipeline of tasks, the map tasks mostly succeeded but those doing shuffles (join, sort, groupby) were the ones that mostly failed. I suppose decommissioning has an impact to resolving lost shuffle data so issue may lie somewhere there.

Our SRE team then fixed the eviction issue and suddenly all tasks with the above spark versions as well as 3.5.0 work normally. Conclusion: No evictions => no failures. I don't know what the mechanism of triggering this is but it is related to pods decommissioned by k8s. I am going to follow with this ticket in case I see any more issues.;;;",3.4.1,,,,"10/Apr/24 11:48;juhai;I am seeing this with Spark 3.5.0 for Spark k8s workloads. We use spot instances for executors but recently seen failures of full jobs when a large number of spot instances are lost at once. The error and stack trace is identical. We have a pending upgrade to 3.5.1 due to another issue with 3.5.0, I will report back soon. The incidence of this problem totally depends on how well we get to keep spot instances so isn't easy to reproduce.;;;"
Official Spark Docker Container images are available from DockerHub in 2 locations,SPARK-47837,13575654,,Documentation,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,mkandes,mkandes,4/12/24 14:57,4/12/24 14:57,7/17/24 20:45,,"3.4.1, 3.5.0, 3.5.1",,Documentation,,0,"containers, docker, Docker, dockerhub, DockerHub, documentation, Documentation, documentation-update","The downloads section of the project's website [1] provides a link [2] to an official set of Docker containers hosted on DockerHub [3]. However, most of these containers are quite outdated now and it appears as if there is now a new location where the lateset offical containers are maintained and distributed on DockerHub [4]. It would be nice to fix/update the documentation to guide users to the new / recommendation location of the official containers maintained or endorsed by the project and/or community.

[1] [https://spark.apache.org/downloads.html]

[2] [https://hub.docker.com/r/apache/spark-py/tags]

[3] [https://hub.docker.com/r/apache/spark]

[4] [https://hub.docker.com/_/spark]",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,57:33.0,,,,,,,,,,0|z1olrs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.5.0,3.5.1,,,
Upgrade optionator to ^0.9.3,SPARK-44279,13542334,,Dependency upgrade,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bjornjorgensen,bjornjorgensen,bjornjorgensen,7/3/23 18:23,4/5/24 15:23,7/17/24 20:45,7/13/23 18:34,"3.4.1, 3.5.0",3.5.0,Build,,0,pull-request-available,[Regular Expression Denial of Service (ReDoS) - CVE-2023-26115|https://github.com/jonschlinkert/word-wrap/issues/32],,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 13 18:34:18 UTC 2023,,,,,,,,,,0|z1ixw0:,9223372036854775807,,,,,,,,,,,,,3.5.0,,,,,,,,"11/Jul/23 17:53;srowen;Is this a library that's used in spark? I couldn't find it;;;, 11/Jul/23 18:52;bjornjorgensen;[~srowen] https://github.com/apache/spark/blob/37aa62f629e652ed70505620473530cd9611018e/dev/package-lock.json#L2226 

[word-wrap vulnerable to Regular Expression Denial of Service|https://github.com/jonschlinkert/word-wrap/issues/40]
;;;, 11/Jul/23 18:57;srowen;This is a dumb question, but what is that file? packages that what part of Spark uses? I have never seen it;;;, 11/Jul/23 19:17;bjornjorgensen;have a look at https://github.com/apache/spark/pull/35628 and https://github.com/apache/spark/pull/39143;;;, 13/Jul/23 18:34;sarutak;Issue resolved in https://github.com/apache/spark/pull/41955;;;",3.5.0,,,,"11/Jul/23 18:52;bjornjorgensen;[~srowen] https://github.com/apache/spark/blob/37aa62f629e652ed70505620473530cd9611018e/dev/package-lock.json#L2226 

[word-wrap vulnerable to Regular Expression Denial of Service|https://github.com/jonschlinkert/word-wrap/issues/40]
;;;"
count_distinct ignores null values,SPARK-47397,13571910,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,martinitus,martinitus,3/14/24 15:12,4/2/24 9:01,7/17/24 20:45,,3.4.1,,"Documentation, Spark Core",,0,,"The documentation states, that in group by and count statements, null values will not be ignored / form their own groups.

!image-2024-03-14-16-13-03-107.png|width=491,height=373!
However, the behavior of count_distinct does not account for nulls. 
Either the documentation or the implementation is wrong here...

!image-2024-03-14-16-12-35-267.png!",,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/24 15:12;martinitus;image-2024-03-14-16-12-35-267.png;https://issues.apache.org/jira/secure/attachment/13067450/image-2024-03-14-16-12-35-267.png, 14/Mar/24 15:13;martinitus;image-2024-03-14-16-13-03-107.png;https://issues.apache.org/jira/secure/attachment/13067451/image-2024-03-14-16-13-03-107.png, 02/Apr/24 08:32;martinitus;image-2024-04-02-10-32-44-461.png;https://issues.apache.org/jira/secure/attachment/13067796/image-2024-04-02-10-32-44-461.png",,3,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 02 08:52:23 UTC 2024,,,,,,,,,,0|z1nz2o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"28/Mar/24 13:55;pspengler;This behavior is in line with the SQL standard. COUNT(column_name) is different from COUNT(*) (or COUNT(1))  in that in does not count null values. See the different output of
{code:java}
SELECT value, count(value) from test GROUP BY value
{code}
and
{code:java}
SELECT value, count(*) from test GROUP BY value
{code}
A nested distinct clause in COUNT does not change it's behavior - nulls will not be counted.;;;, 02/Apr/24 08:52;martinitus;Hi Phillip, 

thanks for your reply!

I was told that this is inline with the SQL standard by a colleague :) I am not necessarily suggesting to change the behavior.

However, looking at the respective documentation again, the first part

[https://spark.apache.org/docs/latest/sql-ref-null-semantics.html#built-in-aggregate]

explains what you wrote above (except of your extra side note that 'distinct' does not change that). 

The second relevant part of the docs then lures the unaware user (like me) more into a wrong direction. 
[https://spark.apache.org/docs/latest/sql-ref-null-semantics.html#aggregate-operator-group-by-distinct-]

After reading both those parts, I would 100% expect the null to be counted.
Maybe this exact example should be added to the docs? I can prepare a PR for that - If i can figure out where the docs are..;;;",,,,,"02/Apr/24 08:52;martinitus;Hi Phillip, 

thanks for your reply!

I was told that this is inline with the SQL standard by a colleague :) I am not necessarily suggesting to change the behavior.

However, looking at the respective documentation again, the first part

[https://spark.apache.org/docs/latest/sql-ref-null-semantics.html#built-in-aggregate]

explains what you wrote above (except of your extra side note that 'distinct' does not change that). 

The second relevant part of the docs then lures the unaware user (like me) more into a wrong direction. 
[https://spark.apache.org/docs/latest/sql-ref-null-semantics.html#aggregate-operator-group-by-distinct-]

After reading both those parts, I would 100% expect the null to be counted.
Maybe this exact example should be added to the docs? I can prepare a PR for that - If i can figure out where the docs are..;;;"
Streaming Statistics link redirect causing 302 error,SPARK-47434,13572173,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hcampbell,hcampbell,hcampbell,3/17/24 22:16,3/18/24 14:40,7/17/24 20:45,3/18/24 14:38,"3.4.1, 3.5.1","3.4.3, 3.5.2, 4.0.0",Web UI,,0,pull-request-available,"When using a reverse proxy, links to streaming statistics page are missing a trailing slash, which causes a redirect (302) to an incorrect path.

Essentially the same issue as https://issues.apache.org/jira/browse/SPARK-24553 but for a different link.

.../StreamingQuery/statistics?id=abcd -> .../StreamingQuery/statistics/?id=abcd

Linked PR [https://github.com/apache/spark/pull/45527/files]",,,,,,,,,,,,,,,,,,,,,,,SPARK-24553,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 18 14:38:56 UTC 2024,,,,,,,,,,0|z1o0ps:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Mar/24 14:38;dongjoon;Issue resolved by pull request 45527
[https://github.com/apache/spark/pull/45527];;;",3.5.1,,,,
Spark Connect can not be started because of missing user home dir in Docker container,SPARK-45557,13554279,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,zrlpar,zrlpar,10/16/23 17:21,2/20/24 18:46,7/17/24 20:45,,"3.4.0, 3.4.1, 3.5.0",,Spark Docker,,0,,"I was trying to start Spark Connect within a container using the Spark Docker container images and ran into an issue where Ivy could not pull the Spark Connect JAR since the user home /home/spark does not exist.

Steps to reproduce:

1. Start the Spark container with `/bin/bash` as the command:
{code:java}
docker run -it --rm apache/spark:3.5.0 /bin/bash {code}
2. Try to start Spark Connect within the container:

 
{code:java}
/opt/spark/sbin/start-connect-server.sh --packages org.apache.spark:spark-connect_2.12:3.5.0 {code}
which lead to this output:

 

 
{code:java}
starting org.apache.spark.sql.connect.service.SparkConnectServer, logging to /opt/spark/logs/spark--org.apache.spark.sql.connect.service.SparkConnectServer-1-d8470a71dbd7.out
failed to launch: nice -n 0 bash /opt/spark/bin/spark-submit --class org.apache.spark.sql.connect.service.SparkConnectServer --name Spark Connect server --packages org.apache.spark:spark-connect_2.12:3.5.0
  	at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1535)
  	at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)
  	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)
  	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)
  	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
  	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
  	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
  	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
  	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
  	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
full log in /opt/spark/logs/spark--org.apache.spark.sql.connect.service.SparkConnectServer-1-d8470a71dbd7.out {code}
where then the full log file looks like this:
{code:java}
Spark Command: /opt/java/openjdk/bin/java -cp /opt/spark/conf:/opt/spark/jars/* -Xmx1g -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.connect.service.SparkConnectServer --name Spark Connect server --packages org.apache.spark:spark-connect_2.12:3.5.0 spark-internal
========================================
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /home/spark/.ivy2/cache
The jars for the packages stored in: /home/spark/.ivy2/jars
org.apache.spark#spark-connect_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-f8a04936-e8af-4f37-bdb0-e4026a8a3be5;1.0
	confs: [default]
Exception in thread ""main"" java.io.FileNotFoundException: /home/spark/.ivy2/cache/resolved-org.apache.spark-spark-submit-parent-f8a04936-e8af-4f37-bdb0-e4026a8a3be5-1.0.xml (No such file or directory)
	at java.base/java.io.FileOutputStream.open0(Native Method)
	at java.base/java.io.FileOutputStream.open(Unknown Source)
	at java.base/java.io.FileOutputStream.<init>(Unknown Source)
	at java.base/java.io.FileOutputStream.<init>(Unknown Source)
	at org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorWriter.write(XmlModuleDescriptorWriter.java:71)
	at org.apache.ivy.plugins.parser.xml.XmlModuleDescriptorWriter.write(XmlModuleDescriptorWriter.java:63)
	at org.apache.ivy.core.module.descriptor.DefaultModuleDescriptor.toIvyFile(DefaultModuleDescriptor.java:553)
	at org.apache.ivy.core.cache.DefaultResolutionCacheManager.saveResolvedModuleDescriptor(DefaultResolutionCacheManager.java:184)
	at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:259)
	at org.apache.ivy.Ivy.resolve(Ivy.java:522)
	at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1535)
	at org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:334)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) {code}
 

The issue is that the user home /home/spark directory does not exist.
{code:java}
$ ls -l /home
total 0 
$
{code}
It seems there is an easy fix: simply switching from useradd to adduser in the Dockerfile should get the user home directory created.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 20 18:46:36 UTC 2024,,,,,,,,,,0|z1kz4o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,20/Feb/24 18:46;albertatcelerdata;Related https://issues.apache.org/jira/browse/SPARK-47105;;;,3.4.1,3.5.0,,,
K8s will not allocate more execs if there are any pending execs until next snapshot,SPARK-42261,13522285,,Bug,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,holden,holden,1/31/23 18:51,2/3/24 0:18,7/17/24 20:39,,"3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1, 3.5.0",,Kubernetes,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 03:42:30 UTC 2023,,,,,,,,,,0|z1fiog:,9223372036854775807,,,,,,,,,,,,,4.0.0,,,,,,,,"31/Jan/23 19:00;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/39825;;;, 01/Feb/23 03:42;dongjoon;I'm -1 because this is a feature to prevent pending resource (pod and dependent resources like PVCs) explosions which results EKS control plane congestion and a waste of money.;;;",3.3.1,3.3.2,3.4.0,3.4.1,01/Feb/23 03:42;dongjoon;I'm -1 because this is a feature to prevent pending resource (pod and dependent resources like PVCs) explosions which results EKS control plane congestion and a waste of money.;;;
Reuse `OrcTail` when enable vectorizedReader,SPARK-44556,13544948,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,7/26/23 10:43,1/4/24 7:12,7/17/24 20:45,1/3/24 22:29,3.4.1,4.0.0,SQL,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 03 22:29:13 UTC 2024,,,,,,,,,,0|z1jdyg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"01/Dec/23 09:20;gag.teorver;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/42168;;;, 03/Jan/24 22:29;dongjoon;Issue resolved by pull request 42168
[https://github.com/apache/spark/pull/42168];;;",,,,,"03/Jan/24 22:29;dongjoon;Issue resolved by pull request 42168
[https://github.com/apache/spark/pull/42168];;;"
Add left-inclusive Param to Bucketizer,SPARK-45152,13550515,,New Feature,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,taosiyuan163@163.com,taosiyuan163@163.com,9/13/23 11:42,12/27/23 0:18,7/17/24 20:45,,3.4.1,,ML,,0,pull-request-available,This parameter is used to control the range of values for the bucket.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,42:40.0,,,,,,,,,,0|z1kbxk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
session_window doesn't identify sessions with provided gap when used as a window function,SPARK-46450,13562259,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,jipumarino,jipumarino,12/18/23 22:18,12/20/23 19:12,7/17/24 20:45,,"3.4.1, 3.5.0",,Spark Core,,1,,"{{PARTITION BY session_window}} doesn't produce the expected results. Here's an example:
{code:sql}
SELECT 
  id,
  ts,
  collect_list(id) OVER (PARTITION BY session_window(ts, '1 hour')) as window_ids

FROM VALUES
  (1, ""2023-12-11 01:10""),
  (2, ""2023-12-11 01:15""),
  (3, ""2023-12-11 01:40""),
  (4, ""2023-12-11 02:05""),
  (5, ""2023-12-11 03:15""),
  (6, ""2023-12-11 03:20""),
  (7, ""2023-12-11 04:10""),
  (8, ""2023-12-11 05:05"")
  AS tab(id, ts)
{code}
Actual result:
{code:java}
+---+----------------+----------+
|id |ts              |window_ids|
+---+----------------+----------+
|1  |2023-12-11 01:10|[1]       |
|2  |2023-12-11 01:15|[2]       |
|3  |2023-12-11 01:40|[3]       |
|4  |2023-12-11 02:05|[4]       |
|5  |2023-12-11 03:15|[5]       |
|6  |2023-12-11 03:20|[6]       |
|7  |2023-12-11 04:10|[7]       |
|8  |2023-12-11 05:05|[8]       |
+---+----------------+----------+
{code}
Expected result, assigning rows to two sessions with 1-hour gap:
{code:java}
+---+----------------+------------+
|id |ts              |window_ids  |
+---+----------------+------------+
|1  |2023-12-11 01:10|[1, 2, 3, 4]|
|2  |2023-12-11 01:15|[1, 2, 3, 4]|
|3  |2023-12-11 01:40|[1, 2, 3, 4]|
|4  |2023-12-11 02:05|[1, 2, 3, 4]|
|5  |2023-12-11 03:15|[5, 6, 7, 8]|
|6  |2023-12-11 03:20|[5, 6, 7, 8]|
|7  |2023-12-11 04:10|[5, 6, 7, 8]|
|8  |2023-12-11 05:05|[5, 6, 7, 8]|
+---+----------------+------------+
{code}
I compared its behavior with the results as a grouping function and with how {{window()}} behaves in both cases, which seems to confirm that the result is inconsistent. Here are the other examples:

*{{group by window()}}*
{code:sql}
SELECT 
  collect_list(id) AS ids,
  collect_list(ts) AS tss,
  window

FROM VALUES
  (1, ""2023-12-11 01:10""),
  (2, ""2023-12-11 01:15""),
  (3, ""2023-12-11 01:40""),
  (4, ""2023-12-11 02:05""),
  (5, ""2023-12-11 03:15""),
  (6, ""2023-12-11 03:20""),
  (7, ""2023-12-11 04:10""),
  (8, ""2023-12-11 05:05"")
  AS tab(id, ts)

GROUP by window(ts, '1 hour')
{code}
Correctly assigns rows to 1-hour windows:
{code:java}
+---------+------------------------------------------------------+------------------------------------------+
|ids      |tss                                                   |window                                    |
+---------+------------------------------------------------------+------------------------------------------+
|[1, 2, 3]|[2023-12-11 01:10, 2023-12-11 01:15, 2023-12-11 01:40]|{2023-12-11 01:00:00, 2023-12-11 02:00:00}|
|[4]      |[2023-12-11 02:05]                                    |{2023-12-11 02:00:00, 2023-12-11 03:00:00}|
|[5, 6]   |[2023-12-11 03:15, 2023-12-11 03:20]                  |{2023-12-11 03:00:00, 2023-12-11 04:00:00}|
|[7]      |[2023-12-11 04:10]                                    |{2023-12-11 04:00:00, 2023-12-11 05:00:00}|
|[8]      |[2023-12-11 05:05]                                    |{2023-12-11 05:00:00, 2023-12-11 06:00:00}|
+---------+------------------------------------------------------+------------------------------------------+
{code}
 

*{{group by session_window()}}*
{code:sql}
SELECT 
  collect_list(id) AS ids,
  collect_list(ts) AS tss,
  session_window

FROM VALUES
  (1, ""2023-12-11 01:10""),
  (2, ""2023-12-11 01:15""),
  (3, ""2023-12-11 01:40""),
  (4, ""2023-12-11 02:05""),
  (5, ""2023-12-11 03:15""),
  (6, ""2023-12-11 03:20""),
  (7, ""2023-12-11 04:10""),
  (8, ""2023-12-11 05:05"")
  AS tab(id, ts)

GROUP by session_window(ts, '1 hour')
{code}
Correctly assigns rows to two sessions with 1-hour gap:
{code:java}
+------------+------------------------------------------------------------------------+------------------------------------------+
|ids         |tss                                                                     |session_window                            |
+------------+------------------------------------------------------------------------+------------------------------------------+
|[1, 2, 3, 4]|[2023-12-11 01:10, 2023-12-11 01:15, 2023-12-11 01:40, 2023-12-11 02:05]|{2023-12-11 01:10:00, 2023-12-11 03:05:00}|
|[5, 6, 7, 8]|[2023-12-11 03:15, 2023-12-11 03:20, 2023-12-11 04:10, 2023-12-11 05:05]|{2023-12-11 03:15:00, 2023-12-11 06:05:00}|
+------------+------------------------------------------------------------------------+------------------------------------------+
{code}
 

*{{partition by window()}}*
{code:sql}
SELECT 
  id,
  ts,
  collect_list(id) OVER (PARTITION BY window(ts, '1 hour')) as window_ids

FROM VALUES
  (1, ""2023-12-11 01:10""),
  (2, ""2023-12-11 01:15""),
  (3, ""2023-12-11 01:40""),
  (4, ""2023-12-11 02:05""),
  (5, ""2023-12-11 03:15""),
  (6, ""2023-12-11 03:20""),
  (7, ""2023-12-11 04:10""),
  (8, ""2023-12-11 05:05"")
  AS tab(id, ts)
{code}
Correctly assigns rows to 1-hour windows:
{code:java}
+---+----------------+----------+
|id |ts              |window_ids|
+---+----------------+----------+
|1  |2023-12-11 01:10|[1, 2, 3] |
|2  |2023-12-11 01:15|[1, 2, 3] |
|3  |2023-12-11 01:40|[1, 2, 3] |
|4  |2023-12-11 02:05|[4]       |
|5  |2023-12-11 03:15|[5, 6]    |
|6  |2023-12-11 03:20|[5, 6]    |
|7  |2023-12-11 04:10|[7]       |
|8  |2023-12-11 05:05|[8]       |
+---+----------------+----------+
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 20 19:12:22 UTC 2023,,,,,,,,,,0|z1mc0w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"18/Dec/23 23:37;kabhwan;It's a missing one and maybe we will have to document - session window is only working properly with batch/streaming aggregation. (Because it requires custom logic to merge sessions.) If you use it as normal function and not ingesting the value to aggregation, merging sessions is never triggered.;;;, 20/Dec/23 19:12;jipumarino;[~kabhwan] thanks for the explanation; I learned a bit more about how Spark internals work.;;;",3.5.0,,,,20/Dec/23 19:12;jipumarino;[~kabhwan] thanks for the explanation; I learned a bit more about how Spark internals work.;;;
ArrowConverters.createEmptyArrowBatch may cause memory leak,SPARK-45814,13556967,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,xieshuaihu,xieshuaihu,xieshuaihu,11/7/23 1:43,12/20/23 10:03,7/17/24 20:45,11/10/23 4:33,"3.4.1, 3.5.0","3.4.2, 3.5.1, 4.0.0","Connect, SQL",,0,pull-request-available,"ArrowConverters.createEmptyArrowBatch don't call hasNext, if TaskContext.get is None, then memory leak happens",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 10 04:33:46 UTC 2023,,,,,,,,,,0|z1lfpk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"09/Nov/23 08:00;LuciferYang;Resolved by https://github.com/apache/spark/pull/43691;;;, 10/Nov/23 04:33;LuciferYang;Issue resolved by pull request 43728
[https://github.com/apache/spark/pull/43728];;;",3.5.0,,,,"10/Nov/23 04:33;LuciferYang;Issue resolved by pull request 43728
[https://github.com/apache/spark/pull/43728];;;"
No need to retry parsing event log path again when FileNotFoundException occurs,SPARK-44998,13548905,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,wforget,wforget,8/29/23 2:46,12/10/23 0:20,7/17/24 20:45,,3.4.1,,Spark Core,,0,pull-request-available,"I found a lot of retry parsing inprogress event log records in history server log. The application is already done while parsing, so we don't need to retry parsing it again when FileNotFoundException occurs.

 

!image-2023-08-29-10-47-08-027.png!

!image-2023-08-29-10-47-43-567.png!",,,,,,,,,,,,,,,,,,,,,,,,,"29/Aug/23 02:47;wforget;image-2023-08-29-10-47-08-027.png;https://issues.apache.org/jira/secure/attachment/13062535/image-2023-08-29-10-47-08-027.png, 29/Aug/23 02:47;wforget;image-2023-08-29-10-47-43-567.png;https://issues.apache.org/jira/secure/attachment/13062536/image-2023-08-29-10-47-43-567.png",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,46:16.0,,,,,,,,,,0|z1k200:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
"If Hadoop is not installed and configured, can the Spark cluster read and write OBS in standalone mode?",SPARK-46314,13561075,,IT Help,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,xueice,xueice,12/8/23 1:40,12/8/23 1:49,7/17/24 20:45,,3.4.1,,"Connect, Input/Output, PySpark",,0,,"If Hadoop is not deployed, PySpark APIs read data from OBS buckets and convert the data to RDD. How can I achieve it?

The following code reports an error: No FileSystem for scheme ""obs"",Can Spark read and write OBS without Hadoop installation and configuration?

And I'm not familiar with pyspark. Is the code wrong?
{code:java}
// code placeholder
from pyspark import SparkConf
from pyspark.sql import SparkSession

conf = SparkConf()
conf.set(""spark.app.name"", ""read and write OBS"")
conf.set(""spark.security.credentials.hbase.enabled"", ""true"")
conf.set(""spark.hadoop.fs.obs.access.key"", ak)
conf.set(""spark.hadoop.fs.obs.secret.key"", sk)
conf.set(""spark.hadoop.fs.obs.endpoint"", ""http://xxx"")
spark = SparkSession.builder.config(conf=conf).getOrCreate()

df = spark.read.json('obs://bucket_name/xxx.json')
df.coalesce(2).write.json(""obs://bucket_name/"", ""overwrite"") {code}","Python3.8

pyspark 3.4.1

operating system:Ubuntu 20.04",,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,40:06.0,,,,,,,,,,0|z1m4q0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Align streaming statistics link format with other page links,SPARK-44864,13547826,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,xleesf,xleesf,8/18/23 6:04,12/1/23 0:21,7/17/24 20:45,,3.4.1,,Structured Streaming,,0,pull-request-available,"Align the streaming query statistics link format with other link like Stages Page `/stages/stage/?id=xxx` , SQL Page `SQL/execution/?id`

`",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 21 03:21:52 UTC 2023,,,,,,,,,,0|z1jvcg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"21/Aug/23 03:21;snoot;User 'leesf' has created a pull request for this issue:
https://github.com/apache/spark/pull/42553;;;",,,,,
Support memory limit configurable,SPARK-44758,13546802,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,zhuml,zhuml,8/10/23 9:20,11/21/23 0:19,7/17/24 20:45,,3.4.1,,Kubernetes,,0,pull-request-available,"Currently the memory request and limit are set by summing the values of spark.\{driver,executor}.memory and spark.\{driver,executor}.memoryOverhead. Supporting memory limits configurable can bring some benefits. For example, use unfixed memory to use page cache, reduce disk IO of shuffle read to improve performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,20:42.0,,,,,,,,,,0|z1jpe0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Cleanup shuffle files from host node after migration due to graceful decommissioning,SPARK-44704,13546358,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,deependra,deependra,8/7/23 12:24,11/18/23 0:18,7/17/24 20:45,,3.4.1,,Block Manager,,0,pull-request-available,"Although these files will be deleted at the end of the application by the external shuffle service, doing this early can free up resources and can help in long running applications running out of disk space.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 07 12:24:59 UTC 2023,,,,,,,,,,0|z1jmnc:,9223372036854775807,,,,,,,,,,,,,3.4.2,,,,,,,,07/Aug/23 12:24;deependra;I will create for this soon.;;;,,,,,
Fix RST files link substitutions error,SPARK-45935,13558057,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,11/15/23 10:38,11/16/23 9:02,7/17/24 20:45,11/16/23 9:02,"3.3.3, 3.4.1, 3.5.0, 4.0.0","3.3.4, 3.4.2, 3.5.1, 4.0.0","Documentation, PySpark",,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 16 09:02:06 UTC 2023,,,,,,,,,,0|z1lmfk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"16/Nov/23 09:02;gurwls223;Issue resolved by pull request 43815
[https://github.com/apache/spark/pull/43815];;;",3.4.1,3.5.0,4.0.0,,
The obtainDelegationTokens method of HiveDelegationTokenProvider should return nextRenewalDate instead of None,SPARK-44203,13541494,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,jiaoqb,jiaoqb,6/27/23 2:57,11/14/23 0:18,7/17/24 20:45,,3.4.1,,SQL,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,57:56.0,,,,,,,,,,0|z1isq8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
ObjectSerializerPruning fails to align null types in custom serializer 'If' expressions.,SPARK-45766,13556456,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,piotrszul,piotrszul,11/1/23 23:58,11/1/23 23:59,7/17/24 20:41,,"3.3.3, 3.4.1, 3.5.0",,SQL,,0,,"We have a custom encoder for union like objects. 

The our custom serializer uses an expression like:

{{If(IsNull(If(.....)), Literal(null), NamedStruct(....)))}}

Using this encoder in a SQL expression that applies the 

`org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning`  rule  results in the exception below.

It's because the expression it transformed by `PushFoldableIntoBranches' rule prior to `ObjectSerializerPruning`, which changes the expression to:

{{If(If(.....), Literal(null), NamedStruct(....)))}}

which no longer matches the expressions for which null type alignment is performed.

See the attached scala repl code for the demonstration of this issue.

 

The exception:

 

java.lang.IllegalArgumentException: requirement failed: All input types must be the same except nullable, containsNull, valueContainsNull flags. The expression is: if (if (assertnotnull(input[0, UnionType, true]).hasValue) isnull(assertnotnull(input[0, UnionType, true]).value) else true) null else named_struct(given, staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, knownnotnull(knownnotnull(assertnotnull(input[0, UnionType, true])).value).given, true, false, true)). The input types found are

StructType(StructField(given,StringType,true),StructField(family,StringType,true))

StructType(StructField(given,StringType,true)).

  at scala.Predef$.require(Predef.scala:281)

  at org.apache.spark.sql.catalyst.expressions.ComplexTypeMergingExpression.dataTypeCheck(Expression.scala:1304)

  at org.apache.spark.sql.catalyst.expressions.ComplexTypeMergingExpression.dataTypeCheck$(Expression.scala:1297)

  at org.apache.spark.sql.catalyst.expressions.If.dataTypeCheck(conditionalExpressions.scala:41)

  at org.apache.spark.sql.catalyst.expressions.ComplexTypeMergingExpression.org$apache$spark$sql$catalyst$expressions$ComplexTypeMergingExpression$$internalDataType(Expression.scala:1309)

  at org.apache.spark.sql.catalyst.expressions.ComplexTypeMergingExpression.org$apache$spark$sql$catalyst$expressions$ComplexTypeMergingExpression$$internalDataType$(Expression.scala:1308)

  at org.apache.spark.sql.catalyst.expressions.If.org$apache$spark$sql$catalyst$expressions$ComplexTypeMergingExpression$$internalDataType$lzycompute(conditionalExpressions.scala:41)

  at org.apache.spark.sql.catalyst.expressions.If.org$apache$spark$sql$catalyst$expressions$ComplexTypeMergingExpression$$internalDataType(conditionalExpressions.scala:41)

  at org.apache.spark.sql.catalyst.expressions.ComplexTypeMergingExpression.dataType(Expression.scala:1313)

  at org.apache.spark.sql.catalyst.expressions.ComplexTypeMergingExpression.dataType$(Expression.scala:1313)

  at org.apache.spark.sql.catalyst.expressions.If.dataType(conditionalExpressions.scala:41)

  at org.apache.spark.sql.catalyst.expressions.Alias.dataType(namedExpressions.scala:166)

  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$.pruneSerializer(objects.scala:209)

  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$$anonfun$apply$8.$anonfun$applyOrElse$3(objects.scala:230)

  at scala.collection.immutable.List.map(List.scala:293)

  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$$anonfun$apply$8.applyOrElse(objects.scala:229)

  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$$anonfun$apply$8.applyOrElse(objects.scala:217)

  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)

  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)

  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)

  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)

  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)

  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)

  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)

  at org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:427)

  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$.apply(objects.scala:217)

  at org.apache.spark.sql.catalyst.optimizer.ObjectSerializerPruning$.apply(objects.scala:125)

  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)

 ",,,,,,,,,,,,,,,,,,,,,,,,,01/Nov/23 23:58;piotrszul;prunning_bug.scala;https://issues.apache.org/jira/secure/attachment/13064092/prunning_bug.scala,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,58:10.0,,,,,,,,,,0|z1lck8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,3.5.0,,,
Custom metrics should be updated after commit too,SPARK-45759,13556387,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,ali-ince,ali-ince,11/1/23 12:57,11/1/23 13:01,7/17/24 20:45,,3.4.1,,SQL,,2,,"We have a DataWriter component, which processes records in configurable batches, which are accumulated in {{write(T record)}} implementation and sent to the persistent store when the configured batch size is reached. Within this approach, last batch is handled during {{commit()}} call, as there is no other mechanism of knowing if there are more records or not.

We are now adding support for custom metrics, by implementing the {{supportedCustomMetrics()}} and {{currentMetricsValues()}} in the {{Write}} and {{DataWriter}} implementations. The problem we see is, since {{CustomMetrics.updateMetrics}} is only called [during|https://github.com/apache/spark/blob/af8907a0873f5ca192b150f28a0c112107594722/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala#L443-L443] and [just after|https://github.com/apache/spark/blob/af8907a0873f5ca192b150f28a0c112107594722/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala#L451-L451] record processing, we do not observe the complete metrics since the last batch that is handled during {{commit()}} call is not collected/updated.

We propose to also to add {{CustomMetrics.updateMetrics}} call after {{commit()}} is processed successfully, ideally just before {{run}} function exits (maybe just above [https://github.com/apache/spark/blob/af8907a0873f5ca192b150f28a0c112107594722/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala#L473-L473]).",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,57:26.0,,,,,,,,,,0|z1lc4w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala-specific improvements in Dataset[T] API ,SPARK-45170,13550709,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,salamahin,salamahin,9/14/23 15:13,10/28/23 16:00,7/17/24 20:45,,3.4.1,,Spark Core,,0,SPIP,"*Q1.* What are you trying to do? 

The main idea is to use the power of scala's macrosses to give developers more convenient and typesafe API to use in join conditions. 

 

*Q2.* What problem is this proposal NOT designed to solve?

R/Java/Python/DataFrame API is out of scope. The solution is not affecting plan generation too. 

 

*Q3.* How is it done today, and what are the limits of current practice?

Currently the join condition is specified via strings, which might lead to silly mistakes (typos, incompatible column types etc) and sometimes hard to read (in case when several joins are made and the final type is tuple of tuple of tuples...)

 

*Q4.* What is new in your approach and why do you think it will be successful?

Scala macroses can be used to extract the column name directly from lambda (extractor). As a side effect its possible to check the column type and prohibit to build inconsistent join expression (like boolean-timestamp comparison)

 

*Q5.* Who cares? If you are successful, what difference will it make?

Mainly scala developers who prefers typesafe code - they would have a more clean and nice API that will make the codebase a bit clearer, especially in case when several chained joins is used

 

*Q6.* What are the risks?

The overusage of macrosses may slow down the compilation speed. In additional macrosses are hard to maintain

 

*Q7.* How long will it take?

Currently the approach is already implemented as a separate [lib|https://github.com/Salamahin/joinwiz] that makes a bit more than just gives alternative API (for example abstracts Dataset[T] to F[T] which allows to run some spark-specific code without spark session for testing purposes)

Adaptation of it won't be a hard job, matter of several weeks

 

*Q8.* What are the mid-term and final “exams” to check for success?

API convenience is very hard to estimate as its more or less a question of taste

 

*Appendix A*

You may find the examples of such 'cleaner' API [here|https://github.com/Salamahin/joinwiz/blob/master/joinwiz_core/src/test/scala/joinwiz/ComputationEngineTest.scala]

Note that backward and forward compatibility is achieved by introducing a brand-new API without modifying an old one

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Scala,Sat Oct 28 16:00:02 UTC 2023,,,,,,,,,,0|z1kd4o:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"28/Oct/23 15:37;psun520;What is the difference between this and the [https://typelevel.org/frameless/FeatureOverview.html] ?;;;, 28/Oct/23 16:00;salamahin;Good question!

The proposed solution have a better integration with IDEs (symbols can't be suggested via intellisense or similar), which might be helpful when doing a chain of a complex joins. In addition, joinwiz allows speed up a unit-testing of spark-specific code because in some cases one doesn't need to initiate a spark session;;;",,,,,"28/Oct/23 16:00;salamahin;Good question!

The proposed solution have a better integration with IDEs (symbols can't be suggested via intellisense or similar), which might be helpful when doing a chain of a complex joins. In addition, joinwiz allows speed up a unit-testing of spark-specific code because in some cases one doesn't need to initiate a spark session;;;"
Cover BufferReleasingInputStream.available under tryOrFetchFailedException,SPARK-45678,13555735,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,10/26/23 21:00,10/28/23 4:15,7/17/24 20:45,10/28/23 2:21,"3.4.1, 3.5.0, 4.0.0","3.4.2, 3.5.1, 4.0.0",Spark Core,,0,pull-request-available,"We have encountered shuffle data corruption issue:

```
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:112)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:504)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:543)
	at org.xerial.snappy.SnappyInputStream.hasNextChunk(SnappyInputStream.java:450)
	at org.xerial.snappy.SnappyInputStream.available(SnappyInputStream.java:497)
	at org.apache.spark.storage.BufferReleasingInputStream.available(ShuffleBlockFetcherIterator.scala:1356)
 ```

Spark shuffle has capacity to detect corruption for a few stream op like `read` and `skip`, such `IOException` in the stack trace will be rethrown as `FetchFailedException` that will re-try the failed shuffle task. But in the stack trace it is `available` that is not covered by the mechanism. So no-retry has been happened and the Spark application just failed.

As the `available` op will also involve data decompression, we should be able to check it like `read` and `skip` do.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 28 02:21:18 UTC 2023,,,,,,,,,,0|z1l840:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"28/Oct/23 02:21;csun;Issue resolved by pull request 43543
[https://github.com/apache/spark/pull/43543];;;",3.5.0,4.0.0,,,
Improve error message for ALTER TABLE ALTER COLUMN on partition columns in non-delta tables,SPARK-44837,13547634,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,m.zhang,m.zhang,m.zhang,8/16/23 22:18,10/20/23 5:56,7/17/24 20:45,10/20/23 5:56,"3.0.3, 3.1.3, 3.2.4, 3.3.2, 3.4.1, 4.0.0",4.0.0,SQL,,1,pull-request-available," 
{code:java}
-- hive table
sql(""create table some_table (x int, y int, z int) using parquet PARTITIONED BY (x, y) "" +
""location '/Users/someone/runtime/tmp-data/some_table'"")
sql(""alter table some_table alter column x comment 'some-comment'"").collect()
Can't find column `x` given table data columns [`z`].{code}
Improve error message to indicate to users that the command is not supported on partition columns in non-delta tables.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 20 05:56:04 UTC 2023,,,,,,,,,,0|z1ju5s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"20/Oct/23 05:56;maxgekk;Issue resolved by pull request 42524
[https://github.com/apache/spark/pull/42524];;;",3.1.3,3.2.4,3.3.2,3.4.1,
cleanSource problem on FileStreamSource for Windows env,SPARK-45519,13553867,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,yemregurses,yemregurses,10/12/23 12:52,10/12/23 12:52,7/17/24 20:45,,3.4.1,,Structured Streaming,,0,,"We are using Spark with Scala in Windows environment. While streaming using Spark, I give the *{{cleanSource}}* option as ""archive"" and the *{{sourceArchiveDir}}* option as ""archived"" as in the code below.
{code:java}
spark.readStream
  .option(""cleanSource"", ""archive"")
  .option(""sourceArchiveDir"", ""archived""){code}
When I tried this in a Linux environment, I realized that the problem was with the paths. Because when I set archive mode to ""delete"", it works on both Linux and Windows. But for the archive mode, it does not work on Windows. 

The problem is related to appending paths in Windows. There is a method

 
{code:java}
override protected def cleanTask(entry: FileEntry): Unit{code}
in the FileStreamSource.scala file in the org.apache.spark.sql.execution.streaming package. On line 569, the !fileSystem.rename(curPath, newPath) code supposed to move source file to archive folder. However, when I debugged, I noticed that the curPath and newPath values were as follows in windows:

 
{code:java}
curPath: file:/C:/dev/be/data-integration-suite/test-data/streaming-folder/patients/patients-success.csv{code}
{code:java}
newPath: file:/C:/dev/be/data-integration-suite/archived/C:/dev/be/data-integration-suite/test-data/streaming-folder/patients/patients-success.csv{code}
It seems that absolute path of csv file were appended when creating newPath because there are two *C:/dev/be/data-integration-suite* in the newPath. This is the reason probably spark archiving does not work. Instead, newPath should be: file:/C:/dev/be/data-integration-suite/archived/test-data/streaming-folder/patients/patients-success.csv",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,52:16.0,,,,,,,,,,0|z1kwl4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect error message for RoundBase,SPARK-45473,13553442,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,10/9/23 22:16,10/11/23 2:51,7/17/24 20:45,10/11/23 2:50,"3.4.1, 3.5.0","3.4.2, 3.5.1, 4.0.0",SQL,,0,pull-request-available,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 11 02:50:31 UTC 2023,,,,,,,,,,0|z1ktyo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"11/Oct/23 02:50;dongjoon;Issue resolved by pull request 43316
[https://github.com/apache/spark/pull/43316];;;",3.5.0,,,,
download krb5.conf from remote storage in spark-submit on k8s,SPARK-45175,13550778,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,dcoliversun,dcoliversun,9/15/23 3:17,9/22/23 16:57,7/17/24 20:45,,3.4.1,,Kubernetes,,0,pull-request-available,"krb5.conf currently only supports the local file format. Tenants would like to save this file on their own servers and download it during the spark-submit phase for better implementation of multi-tenant scenarios. The proposed solution is to use the *downloadFile*  function[1], similar to the configuration of *spark.kubernetes.driver/executor.podTemplateFile*

 

[1]https://github.com/apache/spark/blob/822f58f0d26b7d760469151a65eaf9ee863a07a1/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/PodTemplateConfigMapStep.scala#L82C24-L82C24",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 22 08:59:30 UTC 2023,,,,,,,,,,0|z1kdk0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"22/Sep/23 08:59;dcoliversun;In multi-tenant scenarios, I find Apache Spark provide *{{spark.kubernetes.kerberos.krb5.configMapName}}* to mount ConfigMap containing the {{*krb5.conf*}} file, we could manage these files by creating multiple configMaps for multi-tenants.;;;",,,,,
Non-nullable schema is not effective in DF from JSON,SPARK-45254,13551536,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,michele_rastelli,michele_rastelli,9/21/23 13:41,9/21/23 13:41,7/17/24 20:45,,"3.3.3, 3.4.1",,Spark Core,,0,,"In Spark 3.3 and 3.4, when creating a DF with schema with non-nullable fields, the created DF ends up having schema with nullable fields.

 

 
{code:java}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{BooleanType, StructField, StructType}
object Foo extends App {
  val spark: SparkSession = SparkSession.builder()
    .appName(""foo"")
    .master(""local[*]"")
    .config(""spark.driver.host"", ""127.0.0.1"")
    .getOrCreate()
  val schema = StructType(Array(StructField(""a"", BooleanType, nullable = false)))
  import spark.implicits._
  val df = spark.read.schema(schema).json(Seq(
    """"""{""a"":null}"""""",
    """"""{""a"":true}"""""",
    """"""{""a"":false}"""""",
  ).toDS)
  df.collect()
    .map(_.toString())
    .foreach(println(_))
  schema.printTreeString()
  df.schema.printTreeString()
}
 
{code}
 

 

Produces:

 
{code:java}
[null]
[true]
[false]
root
 |-- a: boolean (nullable = false)
root
 |-- a: boolean (nullable = true)
{code}
 

 

 

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,41:58.0,,,,,,,,,,0|z1ki8g:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,3.4.1,,,,
SortMergeExec with Outer using join forgets sort information,SPARK-45099,13549919,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,hcampbell,hcampbell,9/7/23 11:18,9/7/23 11:22,7/17/24 20:45,,3.4.1,,SQL,,0,,"When performing a 'using' join with a sort hint in a full outer, the ResolveNaturalAndUsingJoin will kick in and build a new join with Equality conditions and a Projection like this:
{quote}val joinedCols = joinPairs.map \{ case (l, r) => Alias(Coalesce(Seq(l, r)), l.name)() }
{quote}
There's nothing wrong with this per se, but, SortMergeJoinExec has it's output ordering for a full outer join as empty, even though these join pairs in their final coalesced form actually are ordered.

This means that code like this:
{quote}frames.reduceLeft(case (l, r) => l.join(r.hint(""merge""), usingColumns = Seq(""a"", ""b""), joinType = ""outer""))
{quote}
Given a non empty list of frames, will not 'stream' without a shuffle step, as each join forgets its sort order.

Ideally this whole operation wouldn't require any shuffles if all the frames are grouped and sorted by the keys.

(Forgive the parens instead of brackets the code snippet please, Jira was inferring macros)",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,18:25.0,,,,,,,,,,0|z1k89c:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Cache results of simple udfs on executors if same arguments are passed.,SPARK-44979,13548750,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,dineshdharme,dineshdharme,8/27/23 13:02,8/29/23 14:01,7/17/24 20:45,,3.4.1,,Spark Core,,0,,"Consider two dataframes :

{{keyword_given = [
[""green pstr"",],
[""greenpstr"",],
[""wlmrt"", ],
[""walmart"",],
[""walmart super"",]
]}}

{{variations = [
(""type green pstr"", ""ABC"", 100),
(""type green pstr"",""PQR"",200),
(""type green pstr"", ""NZSD"", 2999),
(""wlmrt payment"",""walmart"",200),
(""wlmrt solutions"", ""walmart"", 200),
(""nppssdwlmrt"", ""walmart"", 2000)
]}}

{{Imagine I have a task to do fuzzy substring matching between keyword and variation[0] using in built levenstein function. It is possible to optimize this futher in the code itself where we extract out the uniques and then do fuzzy matching on the uniques and join back with the original tables. }}

{{But it could be possible as an optimization to cache the results of the already computed udfs till now and do a lookup on each executor separately.}}

Just a thought. Not sure if it makes any sense. This behaviour could be behind a config.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,02:30.0,,,,,,,,,,0|z1k11k:,9223372036854775807,,,,,_deepakgoyal,,,,,,,,,,,,,,,,,,,,,
Show task partition id in Task table,SPARK-44497,13544264,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,7/20/23 9:15,8/27/23 5:29,7/17/24 20:45,8/27/23 5:29,3.4.1,4.0.0,Web UI,,0,,"In SPARK-37831, the partition id is added in taskinfo, and the task partition id cannot be directly seen in the ui.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 27 05:29:21 UTC 2023,,,,,,,,,,0|z1j9qo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"20/Jul/23 09:21;githubbot;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/42093;;;, 27/Aug/23 05:29;Qin Yao;Issue resolved by pull request 42093
[https://github.com/apache/spark/pull/42093];;;",,,,,"27/Aug/23 05:29;Qin Yao;Issue resolved by pull request 42093
[https://github.com/apache/spark/pull/42093];;;"
PushdownPredicatesAndPruneColumnsForCTEDef creates invalid plan when called over CTE with duplicate attributes,SPARK-44934,13548434,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,wenyuen-db,wenyuen-db,wenyuen-db,8/23/23 20:47,8/24/23 17:57,7/17/24 20:45,8/24/23 15:06,"3.3.3, 3.4.1","3.5.0, 4.0.0",Optimizer,,0,,"When running the query
{code:java}
with cte as (
 select c1, c1, c2, c3 from t where random() > 0
)
select cte.c1, cte2.c1, cte.c2, cte2.c3 from
 (select c1, c2 from cte) cte
 inner join
 (select c1, c3 from cte) cte2
 on cte.c1 = cte2.c1 {code}
 
The query fails with the error
{code:java}
org.apache.spark.scheduler.DAGScheduler: Failed to update accumulator 9523 (Unknown class) for task 1
org.apache.spark.SparkException: attempted to access non-existent accumulator 9523{code}
Further investigation shows that the rule PushdownPredicatesAndPruneColumnsForCTEDef creates an invalid plan when the output of a CTE contains duplicate expression IDs.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 24 15:06:56 UTC 2023,,,,,,,,,,0|z1jz3c:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"24/Aug/23 15:06;ptoth;Issue resolved by pull request 42635
[https://github.com/apache/spark/pull/42635];;;",3.4.1,,,,
Fix incorrect property name `asyncProgressCheckpointingInterval` in structured streaming doc,SPARK-44859,13547792,,Documentation,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,8/17/23 22:21,8/17/23 22:52,7/17/24 20:45,8/17/23 22:52,3.4.1,"3.4.2, 3.5.0, 4.0.0",Structured Streaming,,0,,"We found that one structured streaming property `asyncProgressCheckpointingInterval` for asynchronous progress tracking is not correct when comparing with codebase.

",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 17 22:52:52 UTC 2023,,,,,,,,,,0|z1jv4w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"17/Aug/23 22:52;dongjoon;Issue resolved by pull request 42544
[https://github.com/apache/spark/pull/42544];;;",,,,,
Log when the K8s Exec Pods Allocator Stalls,SPARK-42260,13522284,,Improvement,In Progress,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,holden,holden,holden,1/31/23 18:34,8/9/23 19:15,7/17/24 20:45,,"3.4.0, 3.4.1",,Kubernetes,,0,,"Sometimes if the K8s APIs are being slow the ExecutorPods allocator can stall and it would be good for us to log this (and how long we've stalled for) so folks can tell more clearly why Spark is unable to reach the desired target number of executors.

 

This is _somewhat_ related to SPARK-36664 which logs the time spent waiting for executor allocation but goes a step further for K8s and logs when we've stalled because we have too many pending pods.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 10:05:09 UTC 2023,,,,,,,,,,0|z1fio8:,9223372036854775807,,,,,,,,,,,,,4.0.0,,,,,,,,"31/Jan/23 19:01;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/39825;;;, 27/Jun/23 10:05;yumwang;Remove the target version since 3.4.1 is released.;;;",3.4.1,,,,27/Jun/23 10:05;yumwang;Remove the target version since 3.4.1 is released.;;;
ShutdownHookManager get wrong hadoop user group information,SPARK-44581,13545256,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ylllllllll,ylllllllll,ylllllllll,7/28/23 9:18,8/9/23 5:58,7/17/24 20:45,8/9/23 5:58,"3.2.1, 3.3.2, 3.4.1","3.3.4, 3.4.2, 3.5.0","Deploy, YARN",,1,," I use spark 3.2.1 to run a job on yarn in cluster mode. 

when the job is finished, there is an exception that:
{code:java}
2023-07-28 10:57:16,324 ERROR yarn.ApplicationMaster: Failed to cleanup staging dir hdfs://dmp/user/ubd_dmp_test/.sparkStaging/application_1689318995305_0290 org.apache.hadoop.security.AccessControlException: Permission denied: user=yarn, access=WRITE, inode=""/user/ubd_dmp_test/.sparkStaging"":ubd_dmp_test:ubd_dmp_test:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:349) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943) at org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:105) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3266) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1128) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:725) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121) at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88) at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1656) at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:991) at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:988) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:998) at org.apache.spark.deploy.yarn.ApplicationMaster.cleanupStagingDir(ApplicationMaster.scala:686) at org.apache.spark.deploy.yarn.ApplicationMaster.$anonfun$run$3(ApplicationMaster.scala:268) at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214) at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019) at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at scala.util.Try$.apply(Try.scala:213) at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188) at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=yarn, access=WRITE, inode=""/user/ubd_dmp_test/.sparkStaging"":ubd_dmp_test:ubd_dmp_test:drwxr-xr-x at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:506) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:349) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermissionWithContext(FSPermissionChecker.java:370) at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:240) at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1943) at org.apache.hadoop.hdfs.server.namenode.FSDirDeleteOp.delete(FSDirDeleteOp.java:105) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.delete(FSNamesystem.java:3266) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.delete(NameNodeRpcServer.java:1128) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.delete(ClientNamenodeProtocolServerSideTranslatorPB.java:725) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976) at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1573) at org.apache.hadoop.ipc.Client.call(Client.java:1519) at org.apache.hadoop.ipc.Client.call(Client.java:1416) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129) at com.sun.proxy.$Proxy15.delete(Unknown Source) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:655) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) at com.sun.proxy.$Proxy16.delete(Unknown Source) at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1654) ... 20 more
 
  {code}
I used user ubd_dmp_test to run the job, but the program used user yarn to delete the staging file, this never happens before when I use spark2.4.

 

So I print some log about the current user when it tries to delete the staging file, turns out to be user yarn. Then I print the log about the current user when it execute the run method of ApplicationMaster object, turns out to be ubd_dmp_test.

 

I'm really confused about how this happened.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 09 05:58:39 UTC 2023,,,,,,,,,,0|z1jfuo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"02/Aug/23 10:22;ylllllllll;I found that the ShutDownHook Manager will start a new Thread when the JVM exists, so the UserGroupInformation will not be inherited from the SparkContext, then this hook will create a new ugi with user ""yarn"", which caused the exception.;;;, 03/Aug/23 09:16;githubbot;User 'liangyu-1' has created a pull request for this issue:
https://github.com/apache/spark/pull/42295;;;, 09/Aug/23 05:58;yao;Issue resolved by  [https://github.com/apache/spark/pull/42295];;;",3.3.2,3.4.1,,,"03/Aug/23 09:16;githubbot;User 'liangyu-1' has created a pull request for this issue:
https://github.com/apache/spark/pull/42295;;;"
INSET hash hset set to None when plan exported into JSON,SPARK-44724,13546575,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,minterlandi,minterlandi,8/8/23 18:45,8/8/23 18:45,7/17/24 20:45,,3.4.1,,PySpark,,0,,"I am exporting optimized plans using `_jdf.queryExecution().optimizedPlan().toJSON()`. I noticed that when the plan contains a `INSET` operator the `hset` attribute is None (instead of containing the set elements).

 

When printing directly `_jdf.queryExecution().optimizedPlan()` the `INSET` operator has all the elements so I guess that the problem is with the `toJSON` method.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,45:16.0,,,,,,,,,,0|z1jnzk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Logging level isn't passed to RocksDB state store provider correctly,SPARK-44683,13546200,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,siying,siying,siying,8/4/23 23:57,8/8/23 2:16,7/17/24 20:45,8/8/23 2:12,3.4.1,"3.5.1, 4.0.0",Structured Streaming,,0,,"We pass log4j's log level to RocksDB so that RocksDB debug log can go to log4j. However, we pass in log level after we create the logger. However, the way it is set isn't effective. This has two impacts: (1) setting DEBUG level don't make RocksDB generate DEBUG level logs; (2) setting WARN or ERROR level does prevent INFO level logging, but RocksDB still makes JNI calls to Scala, which is an unnecessary overhead.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 08 02:12:49 UTC 2023,,,,,,,,,,0|z1jlo8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 02:12;kabhwan;Issue resolved by pull request 42354
[https://github.com/apache/spark/pull/42354];;;",,,,,
`spark.executor.defaultJavaOptions` Check illegal java options,SPARK-44650,13545942,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,8/3/23 4:41,8/6/23 13:25,7/17/24 20:45,8/6/23 13:24,3.4.1,4.0.0,Spark Core,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 06 13:24:57 UTC 2023,,,,,,,,,,0|z1jk2w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"06/Aug/23 13:24;srowen;Issue resolved by pull request 42313
[https://github.com/apache/spark/pull/42313];;;",,,,,
Fix the `test_to_excel` tests for python3.7,SPARK-44670,13546088,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,madhukar1118,madhukar1118,madhukar1118,8/4/23 4:12,8/6/23 1:24,7/17/24 20:45,8/6/23 1:24,3.4.1,3.4.2,Pandas API on Spark,,0,,"With python3.7 and openpyxl installed got error:

======================================================================

ERROR: test_to_excel (pyspark.pandas.tests.test_dataframe_conversion.DataFrameConversionTest)

Traceback (most recent call last):

  File ""/workspace/apache-spark/python/pyspark/pandas/tests/test_dataframe_conversion.py"", line 102, in test_to_excel

    dataframes = self.get_excel_dfs(pandas_on_spark_location, pandas_location)

  File ""/workspace/apache-spark/python/pyspark/pandas/tests/test_dataframe_conversion.py"", line 89, in get_excel_dfs

    ""got"": pd.read_excel(pandas_on_spark_location, index_col=0),

  File ""/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py"", line 296, in wrapper

    return func(*args, **kwargs)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 304, in read_excel

    io = ExcelFile(io, engine=engine)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 867, in __init__

    self._reader = self._engines[engine](self._io)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py"", line 21, in __init__

    import_optional_dependency(""xlrd"", extra=err_msg)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/compat/_optional.py"", line 110, in import_optional_dependency

    raise ImportError(msg) from None

ImportError: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.

----------------------------------------------------------------------

 

 

 

But with xlrd 2.0.1 installed getting error

======================================================================

ERROR: test_to_excel (pyspark.pandas.tests.test_dataframe_conversion.DataFrameConversionTest)

----------------------------------------------------------------------

Traceback (most recent call last):

  File ""/workspace/apache-spark/python/pyspark/pandas/tests/test_dataframe_conversion.py"", line 102, in test_to_excel

    dataframes = self.get_excel_dfs(pandas_on_spark_location, pandas_location)

  File ""/workspace/apache-spark/python/pyspark/pandas/tests/test_dataframe_conversion.py"", line 89, in get_excel_dfs

    ""got"": pd.read_excel(pandas_on_spark_location, index_col=0),

  File ""/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py"", line 296, in wrapper

    return func(*args, **kwargs)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 304, in read_excel

    io = ExcelFile(io, engine=engine)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 867, in __init__

    self._reader = self._engines[engine](self._io)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py"", line 22, in __init__

    super().__init__(filepath_or_buffer)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_base.py"", line 353, in __init__

    self.book = self.load_workbook(filepath_or_buffer)

  File ""/opt/conda/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py"", line 37, in load_workbook

    return open_workbook(filepath_or_buffer)

  File ""/opt/conda/lib/python3.7/site-packages/xlrd/__init__.py"", line 170, in open_workbook

    raise XLRDError(FILE_FORMAT_DESCRIPTIONS[file_format]+'; not supported')

xlrd.biffh.XLRDError: Excel xlsx file; not supported

----------------------------------------------------------------------

 ",,,,,,,,,,,,,,,,SPARK-40353,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 06 01:24:20 UTC 2023,,,,,,,,,,0|z1jkzc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"04/Aug/23 05:38;madhukar1118;Raised a PR for using openpyxl instead of xlrd - [https://github.com/apache/spark/pull/42339] ;;;, 06/Aug/23 01:24;gurwls223;Issue resolved by pull request 42339
[https://github.com/apache/spark/pull/42339];;;",,,,,"06/Aug/23 01:24;gurwls223;Issue resolved by pull request 42339
[https://github.com/apache/spark/pull/42339];;;"
getMapOutputLocation should not throw NPE,SPARK-44661,13546051,,Test,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,8/3/23 19:41,8/3/23 21:57,7/17/24 20:45,8/3/23 21:52,"3.4.1, 3.5.0","3.4.2, 3.5.0, 4.0.0","Spark Core, Tests",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 03 21:52:01 UTC 2023,,,,,,,,,,0|z1jkr4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/Aug/23 21:52;dongjoon;Issue resolved by pull request 42326
[https://github.com/apache/spark/pull/42326];;;",3.5.0,,,,
Remove TaskPagedTable in StagePage,SPARK-44490,13544211,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,7/20/23 4:21,8/1/23 6:42,7/17/24 20:45,8/1/23 6:42,3.4.1,4.0.0,Web UI,,0,,"In [SPARK-21809|https://issues.apache.org/jira/browse/SPARK-21809], we introduced stagespage-template.html to show the running status of Stage. TaskPagedTable is no longer effective, but there are still many PRs updating related codes.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 06:42:02 UTC 2023,,,,,,,,,,0|z1j9ew:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,01/Aug/23 06:42;sarutak;Issue resolved in https://github.com/apache/spark/pull/42085;;;,,,,,
`spark.*.io.connectionCreationTimeout` parameter documentation,SPARK-44583,13545273,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,7/28/23 10:23,8/1/23 2:55,7/17/24 20:45,8/1/23 2:55,3.4.1,"3.5.0, 4.0.0",Documentation,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 01 02:55:34 UTC 2023,,,,,,,,,,0|z1jfyg:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"01/Aug/23 02:55;Qin Yao;Issue resolved by pull request 42205
[https://github.com/apache/spark/pull/42205];;;",,,,,
Fix warning condition in MLLib RankingMetrics ndcgAk,SPARK-44585,13545300,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gvuillier,gvuillier,gvuillier,7/28/23 13:23,7/28/23 22:30,7/17/24 20:45,7/28/23 22:30,3.4.1,"3.4.2, 3.5.0, 4.0.0",MLlib,,0,,"The implementation of nDCG evaluation in MLLib with relevance score (added in 3.4.0, see https://issues.apache.org/jira/browse/SPARK-39446 and [pull request|https://github.com/apache/spark/pull/36843]) implements the following warning when the input data isn't correct: ""# of ground truth set and # of relevance value set should be equal, check input data""

 

The logic for raising warnings is faulty at the moment: it raises a warning when the following conditions are both true:
 # {{rel}} is empty
 # {{lab.size}} and {{rel.size}} are not equal.

 

With the current logic, RankingMetrics will:
 * raise incorrect warning when a user is using it in the ""binary"" mode (i.e. no relevance values in the input)
 * not raise warning (that could be necessary) when the user is using it in the ""non-binary"" model (i.e. with relevance values in the input)

 

The logic should be to raise a warning should be:
 # {{rel}} is *not empty*
 # {{lab.size}} and {{rel.size}} are not equal.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 28 22:30:38 UTC 2023,,,,,,,,,,0|z1jg48:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"28/Jul/23 22:30;srowen;Issue resolved by pull request 42207
[https://github.com/apache/spark/pull/42207];;;",,,,,
Export Pyspark's Spark Connect Log Level,SPARK-44558,13545002,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cdkrot,cdkrot,cdkrot,7/26/23 20:50,7/28/23 2:34,7/17/24 20:45,7/28/23 2:34,3.4.1,"3.5.0, 4.0.0",PySpark,,0,,Export spark connect log level as API function,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 28 02:34:35 UTC 2023,,,,,,,,,,0|z1jeag:,9223372036854775807,,,,,,,,,,,,,3.5.0,,,,,,,,"28/Jul/23 02:34;gurwls223;Issue resolved by pull request 42175
[https://github.com/apache/spark/pull/42175];;;",,,,,
HiveShim getTablesByType support fallback,SPARK-44454,13543716,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dzcxzl,dzcxzl,dzcxzl,7/17/23 4:54,7/27/23 10:23,7/17/24 20:45,7/27/23 10:23,3.4.1,4.0.0,SQL,,0,,"When we use a high version of Hive Client to communicate with a low version of Hive meta store, we may encounter Invalid method name: 'get_tables_by_type'.

 
{code:java}
23/07/17 12:45:24,391 [main] DEBUG SparkSqlParser: Parsing command: show views
23/07/17 12:45:24,489 [main] ERROR log: Got exception: org.apache.thrift.TApplicationException Invalid method name: 'get_tables_by_type'
org.apache.thrift.TApplicationException: Invalid method name: 'get_tables_by_type'
    at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:79)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_tables_by_type(ThriftHiveMetastore.java:1433)
    at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_tables_by_type(ThriftHiveMetastore.java:1418)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTables(HiveMetaStoreClient.java:1411)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
    at com.sun.proxy.$Proxy23.getTables(Unknown Source)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2344)
    at com.sun.proxy.$Proxy23.getTables(Unknown Source)
    at org.apache.hadoop.hive.ql.metadata.Hive.getTablesByType(Hive.java:1427)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.spark.sql.hive.client.Shim_v2_3.getTablesByType(HiveShim.scala:1408)
    at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$listTablesByType$1(HiveClientImpl.scala:789)
    at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)
    at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)
    at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)
    at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)
    at org.apache.spark.sql.hive.client.HiveClientImpl.listTablesByType(HiveClientImpl.scala:785)
    at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$listViews$1(HiveExternalCatalog.scala:895)
    at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:108)
    at org.apache.spark.sql.hive.HiveExternalCatalog.listViews(HiveExternalCatalog.scala:893)
    at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.listViews(ExternalCatalogWithListener.scala:158)
    at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listViews(SessionCatalog.scala:1040)
    at org.apache.spark.sql.execution.command.ShowViewsCommand.$anonfun$run$5(views.scala:407)
    at scala.Option.getOrElse(Option.scala:189)
    at org.apache.spark.sql.execution.command.ShowViewsCommand.run(views.scala:407) {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 27 10:23:48 UTC 2023,,,,,,,,,,0|z1j6cw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"25/Jul/23 03:46;ci-cassandra.apache.org;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/42033;;;, 27/Jul/23 10:23;yumwang;Issue resolved by pull request 42033
[https://github.com/apache/spark/pull/42033];;;",,,,,"27/Jul/23 10:23;yumwang;Issue resolved by pull request 42033
[https://github.com/apache/spark/pull/42033];;;"
"Utils.getOrCreateLocalRootDirs will never take effect after the first call fails, even if the exception is recovered",SPARK-44469,13543877,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,toujours33,toujours33,7/18/23 3:22,7/18/23 3:24,7/17/24 20:45,,3.4.1,,Spark Core,,0,,"{code:java}
private[spark] def getOrCreateLocalRootDirs(conf: SparkConf): Array[String] = {
  if (localRootDirs == null || localRootDirs.isEmpty) {
    this.synchronized {
      if (localRootDirs == null) {
        localRootDirs = getOrCreateLocalRootDirsImpl(conf)
      }
    }
  }
  localRootDirs
}{code}
localRootDirs is only initialized once in the Executor/Driver life cycle. If it fails due to a FileSystem exception (such as a full disk) during the first initialization, localRootDirs will be assigned a value of None instead of null.

Even if the FileSystem exception recovered, the localRootDirs won't re-apply from FileSystem, causing the task on the Executor to continue to fail (tasks relay on fetchFiles locally)",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 18 03:24:00 UTC 2023,,,,,,,,,,0|z1j7co:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,18/Jul/23 03:24;toujours33;I'll work on it.;;;,,,,,
Add Python inbuilt functions to DataFrame for ease of use for Python developers,SPARK-44336,13542842,,New Feature,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,soffer,soffer,7/7/23 19:50,7/7/23 19:50,7/17/24 20:45,,3.4.1,,PySpark,,0,,"Python developers are used to common inbuilt functions when developing but PySpark doesn't support any of the most used inbuilt functionality for DataFrames. PySpark already has this functionality for columns but not for the DataFrame itself. Adding this support for DataFrames would simplify some parts of development. For example:


{code:java}
if df == df1:       # DataFrame Equality 
if df != df2:       # DataFrame Inequality

df_large = df * 100 # Quickly make a larger dataframe through union of copies
                    # Very useful for performance testing

df_sub = df1 - df2  # Simple DataFrame subtraction
                    # Equivalent to df1.subtract(df2)

df4 = df + df1      # Equivalent to df.union(df1)

len(df)             # Equivalent to df.count()

for row in df:      # Equivalent to `for row in df.collect():`
    some_work(row)

if ""company_name"" in df: # Check if item is in the DataFrame

{code}
 

There is an ongoing DataFrame equality function effort in PR: 41833, I've also built my own.


These are suggestions, any other functions to be added or removed from this list can be discussed.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,50:25.0,,,,,,,,,,0|z1j0zc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Avro to version 1.11.2,SPARK-44277,13542303,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,iemejia,iemejia,iemejia,7/3/23 12:57,7/5/23 7:11,7/17/24 20:45,7/5/23 7:11,3.4.1,3.5.0,Build,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 07:11:44 UTC 2023,,,,,,,,,,0|z1ixp4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"05/Jul/23 07:11;gurwls223;Issue resolved by pull request 41830
[https://github.com/apache/spark/pull/41830];;;",,,,,
pyspark.sql.dataframe doctests can behave differently,SPARK-44245,13541919,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cdkrot,cdkrot,cdkrot,6/29/23 9:57,7/4/23 23:49,7/17/24 20:45,7/4/23 23:49,3.4.1,3.5.0,PySpark,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 04 23:49:10 UTC 2023,,,,,,,,,,0|z1ivbs:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"04/Jul/23 23:49;gurwls223;Issue resolved by pull request 41787
[https://github.com/apache/spark/pull/41787];;;",,,,,
Remove useless code `resetAllPartitions` in ActiveJob,SPARK-44188,13541353,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,fanjia,fanjia,fanjia,6/26/23 7:14,6/26/23 17:06,7/17/24 20:45,6/26/23 17:06,3.4.1,3.5.0,Spark Core,,0,,In class ActiveJob have useless method `resetAllPartitions`. We should remove it.,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 17:06:22 UTC 2023,,,,,,,,,,0|z1iruw:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"26/Jun/23 17:06;dongjoon;Issue resolved by pull request 41737
[https://github.com/apache/spark/pull/41737];;;",,,,,
Enable dynamicPartitionOverwrite in SaveAsHiveFile for insert overwrite,SPARK-44166,13541243,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,pralabhkumar,pralabhkumar,6/24/23 11:53,6/24/23 12:08,7/17/24 20:45,,3.4.1,,Spark Core,,0,,"Currently in InsertIntoHiveTable.scala , there is no way to pass dynamicPartitionOverwrite to true , when calling  saveAsHiveFile . When dynamicPartitioOverwrite is true , spark will use  built-in FileCommitProtocol instead of Hadoop FileOutputCommitter , which is more performant. 

 

Here is the solution . 

When inserting overwrite into Hive table

 

Current code 

 
{code:java}
val writtenParts = saveAsHiveFile(
  sparkSession = sparkSession,
  plan = child,
  hadoopConf = hadoopConf,
  fileFormat = fileFormat,
  outputLocation = tmpLocation.toString,
  partitionAttributes = partitionColumns,
  bucketSpec = bucketSpec,
  options = options)
       {code}
 

 

Proposed code.  

enableDynamicPartitionOverwrite 
{code:java}
val USE_FILECOMMITPROTOCOL_DYNAMIC_PARTITION_OVERWRITE =
    buildConf(""spark.sql.hive.filecommit.dynamicPartitionOverwrite""){code}
 
{code:java}
 val enableDynamicPartitionOverwrite =
      SQLConf.get.getConf(HiveUtils.USE_FILECOMMITPROTOCOL_DYNAMIC_PARTITION_OVERWRITE)
    logWarning(s""enableDynamicPartitionOverwrite: $enableDynamicPartitionOverwrite""){code}
 

 

Now if enableDynamicPartitionOverwrite is true and numDynamicPartitions > 0 and overwrite is true , pass dynamicPartitionOverwrite true. 

 
{code:java}
val writtenParts = saveAsHiveFile( sparkSession = sparkSession, plan = child, hadoopConf = hadoopConf, fileFormat = fileFormat, outputLocation = tmpLocation.toString, partitionAttributes = partitionColumns, bucketSpec = bucketSpec, options = options, dynamicPartitionOverwrite =
        enableDynamicPartitionOverwrite && numDynamicPartitions > 0 && overwrite)       {code}
 

 

In saveAs File 
{code:java}
val committer = FileCommitProtocol.instantiate(
      sparkSession.sessionState.conf.fileCommitProtocolClass,
      jobId = java.util.UUID.randomUUID().toString,
      outputPath = outputLocation,
      dynamicPartitionOverwrite = dynamicPartitionOverwrite) {code}
This will internal call  with dynamicPartitionOverwrite value true. 

 
{code:java}
class SQLHadoopMapReduceCommitProtocol(
    jobId: String,
    path: String,
    dynamicPartitionOverwrite: Boolean = false)
  extends HadoopMapReduceCommitProtocol(jobId, path, dynamicPartitionOverwrite) {code}
 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,53:16.0,,,,,,,,,,0|z1ir6g:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unused `spark.kubernetes.executor.lostCheck.maxAttempts`,SPARK-44158,13541205,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,6/23/23 18:08,6/23/23 20:49,7/17/24 20:45,6/23/23 20:49,"2.4.8, 3.0.3, 3.1.3, 3.2.4, 3.3.3, 3.4.1","3.3.3, 3.4.2, 3.5.0",Kubernetes,,0,,,,,,,,,,,,,,,,,,,,,,SPARK-24248,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 23 20:49:57 UTC 2023,,,,,,,,,,0|z1iqy8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"23/Jun/23 20:49;dongjoon;Issue resolved by pull request 41713
[https://github.com/apache/spark/pull/41713];;;",3.0.3,3.1.3,3.2.4,3.3.3,
Outdated JARs in PySpark package,SPARK-44157,13541187,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,,,adriangonz,adriangonz,6/23/23 15:03,6/23/23 15:03,7/17/24 20:45,,3.4.1,,"Build, PySpark",,0,pyspark,"The JARs which ship embedded within PySpark's package in PyPi don't seem aligned with the deps specified in Spark's own `pom.xml`.

For example, in Spark's `pom.xml`, `protobuf-java` is set to `3.21.12`:

[https://github.com/apache/spark/blob/6b1ff22dde1ead51cbf370be6e48a802daae58b6/pom.xml#L127]

However, if we look at the JARs embedded within PySpark tarball, the version of `protobuf-java` is `2.5.0` (i.e. `..../site-packages/pyspark/jars/protobuf-java-2.5.0.jar`). Same seems to apply to all other dependencies.

This introduces a set of CVEs which are fixed on upstream Spark, but are still present in PySpark (e.g. `CVE-2022-3509`, `CVE-2021-22569`, ` CVE-2015-5237` and a few others). As well as potentially introduce a source of conflict whenever there's a breaking change on these deps.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,03:32.0,,,,,,,,,,0|z1iqu8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
Update YuniKorn docs with v1.3,SPARK-44038,13539822,,Documentation,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,6/13/23 10:05,6/13/23 10:37,7/17/24 20:45,6/13/23 10:37,"3.4.1, 3.5.0","3.4.1, 3.5.0","Documentation, Kubernetes",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 10:37:34 UTC 2023,,,,,,,,,,0|z1iig8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"13/Jun/23 10:37;dongjoon;Issue resolved by pull request 41571
[https://github.com/apache/spark/pull/41571];;;",3.5.0,,,,
Upgrade buf to v1.18.0,SPARK-43401,13535317,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,5/8/23 2:18,5/9/23 1:06,7/17/24 20:45,5/9/23 1:06,3.4.1,3.5.0,"Build, Connect",,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 09 01:06:34 UTC 2023,,,,,,,,,,0|z1hqvc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"09/May/23 01:06;dongjoon;Issue resolved by pull request 41087
[https://github.com/apache/spark/pull/41087];;;",,,,,
Upgrade scalafmt from 3.7.2 to 3.7.3,SPARK-43350,13534766,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,5/3/23 1:34,5/3/23 15:46,7/17/24 20:45,5/3/23 15:46,3.4.1,3.5.0,Build,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 03 15:46:39 UTC 2023,,,,,,,,,,0|z1hnh4:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"03/May/23 15:46;dongjoon;Issue resolved by pull request 41029
[https://github.com/apache/spark/pull/41029];;;",,,,,
Generalize handling of metadata attributes in FileSourceStrategy,SPARK-42918,13529963,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,johanl-db,johanl-db,johanl-db,3/24/23 11:38,3/31/23 12:53,7/17/24 20:45,3/31/23 12:51,3.4.1,3.5.0,Optimizer,,0,,"A first step towards allowing file format implementations to inject custom metadata fields into plans is to make the handling of metadata attributes in `FileSourceStrategy` more generic.

Today in `FileSourceStrategy` , the lists of constant and generated metadata fields are created manually, checking for known generated fields on one hand and considering the remaining fields as constant metadata fields. We need instead to introduce a way of declaring metadata fields as generated or constant directly in `FileFormat` and propagate that information to `FileSourceStrategy`.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 31 12:51:56 UTC 2023,,,,,,,,,,0|z1gtyo:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"31/Mar/23 12:51;cloud_fan;Issue resolved by pull request 40545
[https://github.com/apache/spark/pull/40545];;;",,,,,
Remove duplicated rule CombineFilters in Optimizer,SPARK-42850,13529031,,Task,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,3/17/23 23:02,3/20/23 0:49,7/17/24 20:45,3/20/23 0:49,3.4.1,3.5.0,SQL,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 00:49:02 UTC 2023,,,,,,,,,,0|z1go7s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"17/Mar/23 23:24;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40471;;;, 20/Mar/23 00:49;gurwls223;Issue resolved by pull request 40471
[https://github.com/apache/spark/pull/40471];;;",,,,,"20/Mar/23 00:49;gurwls223;Issue resolved by pull request 40471
[https://github.com/apache/spark/pull/40471];;;"
Implement textFile for DataFrameReader,SPARK-42757,13528068,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,panbingkun,panbingkun,panbingkun,3/11/23 11:13,3/15/23 7:43,7/17/24 20:45,3/14/23 23:41,3.4.1,3.4.1,Connect,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,SPARK-42554,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 23:41:46 UTC 2023,,,,,,,,,,0|z1gi9s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"11/Mar/23 11:16;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40377;;;, 11/Mar/23 11:17;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40377;;;, 14/Mar/23 23:41;gurwls223;Issue resolved by pull request 40377
[https://github.com/apache/spark/pull/40377];;;",,,,,"11/Mar/23 11:17;apachespark;User 'panbingkun' has created a pull request for this issue:
https://github.com/apache/spark/pull/40377;;;"
Expose amount of shuffle data available on the node,SPARK-44209,13541552,,New Feature,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,,,deependra,deependra,6/27/23 11:41,11/27/23 0:19,7/17/24 20:45,,3.4.1,,Shuffle,,0,pull-request-available,"[ShuffleMetrics|https://github.com/apache/spark/blob/43f7a86a05ad8c7ec7060607e43d9ca4d0fe4166/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockHandler.java#L318] doesn't have metrics like 
""totalShuffleDataBytes"" and ""numAppsWithShuffleData"", these metrics are per node published by External Shuffle Service.
 
Adding these metrics would help in - 
1. Deciding if we can decommission the node if no shuffle data present
2. Better live monitoring of customer's workload to see if there is skewed shuffle data present on the node",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Wed Jul 19 09:23:02 UTC 2023,,,,,,,,,,0|z1it2o:,9223372036854775807,,,,,abmodi,,,,,,,,,,,,,,,,"27/Jun/23 11:49;deependra;I will create a pull request for this soon;;;, 19/Jul/23 09:22;githubbot;User 'Deependra-Patel' has created a pull request for this issue:
https://github.com/apache/spark/pull/42071;;;, 19/Jul/23 09:23;githubbot;User 'Deependra-Patel' has created a pull request for this issue:
https://github.com/apache/spark/pull/42071;;;",,,,,"19/Jul/23 09:22;githubbot;User 'Deependra-Patel' has created a pull request for this issue:
https://github.com/apache/spark/pull/42071;;;"
Add Support for TPCH Micro Benchmark,SPARK-44301,13542442,,Improvement,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,,,Suraj Naik,Suraj Naik,7/4/23 16:54,10/31/23 0:17,7/17/24 20:45,,3.4.1,,SQL,,0,pull-request-available,"I am proposing this Jira to add support for benchmark for TPCH. I see that there is currently support for TPCDS, but couldn't find one for TPCH. I will add support for it.",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 08 04:07:36 UTC 2023,,,,,,,,,,0|z1iyk0:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"04/Jul/23 16:57;Suraj Naik;I have raised the PR here - https://github.com/apache/spark/pull/41856;;;, 08/Jul/23 04:07;snoot;User 'oss-maker' has created a pull request for this issue:
https://github.com/apache/spark/pull/41856;;;, 08/Jul/23 04:07;snoot;User 'oss-maker' has created a pull request for this issue:
https://github.com/apache/spark/pull/41856;;;",,,,,"08/Jul/23 04:07;snoot;User 'oss-maker' has created a pull request for this issue:
https://github.com/apache/spark/pull/41856;;;"
problem using broadcast join with parquet/iceberg tables,SPARK-45198,13551034,,Bug,Open,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,,,tonyuwarov,tonyuwarov,9/18/23 8:38,9/18/23 8:45,7/17/24 20:45,,3.4.1,,Build,,0,,"We have 2 Parquet tables: load_test_full_warehouse.gen_document_type and load_test_full_warehouse.generation_document_part.
Trying to make a left join of these two tables onto each other gives a strange result. In the case where on the left side of the join we use a large table load_test_full_warehouse.generation_document_part, the optimizer uses a broadcast join.
However, in the case when on the left in the join we use a small reference table, the optimizer chooses to execute the query using the merge sort. Although it would seem that the small table on the left in a left join should initiate a broadcast join.
  An attempt to use hints and collect statistics did not yield results. The following queries were used:

spark.sql(f""""""create table iceberg_warehouse.t1 using iceberg 
              as SELECT /*+ BROADCAST(doc_tp) */
                doc.DOCUMENT_DATE
                , doc_tp.NAME as DOCUMENT_TYPE
                , COUNT(*) as DOC_QTY
              FROM load_test_full_warehouse.generation_document_part doc
              LEFT JOIN load_test_full_warehouse.gen_document_type doc_tp ON doc.DOCUMENT_TYPE_ID_INT = doc_tp.DOCUMENT_TYPE_ID_INT
              GROUP BY doc.DOCUMENT_DATE, doc_tp.NAME"""""")

== Physical Plan ==
AtomicCreateTableAsSelect (25)
+- AdaptiveSparkPlan (24)
   +- == Final Plan ==
      * HashAggregate (15)
      +- AQEShuffleRead (14)
         +- ShuffleQueryStage (13), Statistics(sizeInBytes=16.7 MiB, rowCount=3.12E+5)
            +- Exchange (12)
               +- * HashAggregate (11)
                  +- * Project (10)
                     +- * BroadcastHashJoin LeftOuter BuildRight (9)
                        :- * Project (3)
                        :  +- * ColumnarToRow (2)
                        :     +- Scan parquet spark_catalog.load_test_full_warehouse.generation_document_part (1)
                        +- BroadcastQueryStage (8), Statistics(sizeInBytes=1031.8 KiB, rowCount=1.00E+3)
                           +- BroadcastExchange (7)
                              +- * Filter (6)
                                 +- * ColumnarToRow (5)
                                    +- Scan parquet spark_catalog.load_test_full_warehouse.gen_document_type (4)
   +- == Initial Plan ==
      HashAggregate (23)
      +- Exchange (22)
         +- HashAggregate (21)
            +- Project (20)
               +- BroadcastHashJoin LeftOuter BuildRight (19)
                  :- Project (16)
                  :  +- Scan parquet spark_catalog.load_test_full_warehouse.generation_document_part (1)
                  +- BroadcastExchange (18)
                     +- Filter (17)
                        +- Scan parquet spark_catalog.load_test_full_warehouse.gen_document_type (4)

 

spark.sql(f""""""create table iceberg_warehouse.t2 using iceberg 
              as SELECT /*+ BROADCAST(doc_tp) */
                doc.DOCUMENT_DATE
                , doc_tp.NAME as DOCUMENT_TYPE
                , COUNT(*) as DOC_QTY
              FROM load_test_full_warehouse.gen_document_type doc_tp
              LEFT JOIN load_test_full_warehouse.generation_document_part doc ON doc.DOCUMENT_TYPE_ID_INT = doc_tp.DOCUMENT_TYPE_ID_INT
              GROUP BY doc.DOCUMENT_DATE, doc_tp.NAME"""""")

== Physical Plan ==
AtomicCreateTableAsSelect (34)
+- AdaptiveSparkPlan (33)
   +- == Final Plan ==
      * HashAggregate (21)
      +- AQEShuffleRead (20)
         +- ShuffleQueryStage (19), Statistics(sizeInBytes=1695.3 KiB, rowCount=3.10E+4)
            +- Exchange (18)
               +- * HashAggregate (17)
                  +- * Project (16)
                     +- * SortMergeJoin LeftOuter (15)
                        :- * Sort (6)
                        :  +- AQEShuffleRead (5)
                        :     +- ShuffleQueryStage (4), Statistics(sizeInBytes=46.9 KiB, rowCount=1.00E+3)
                        :        +- Exchange (3)
                        :           +- * ColumnarToRow (2)
                        :              +- Scan parquet spark_catalog.load_test_full_warehouse.gen_document_type (1)
                        +- * Sort (14)
                           +- AQEShuffleRead (13)
                              +- ShuffleQueryStage (12), Statistics(sizeInBytes=234.7 GiB, rowCount=1.05E+10)
                                 +- Exchange (11)
                                    +- * Project (10)
                                       +- * Filter (9)
                                          +- * ColumnarToRow (8)
                                             +- Scan parquet spark_catalog.load_test_full_warehouse.generation_document_part (7)
   +- == Initial Plan ==
      HashAggregate (32)
      +- Exchange (31)
         +- HashAggregate (30)
            +- Project (29)
               +- SortMergeJoin LeftOuter (28)
                  :- Sort (23)
                  :  +- Exchange (22)
                  :     +- Scan parquet spark_catalog.load_test_full_warehouse.gen_document_type (1)
                  +- Sort (27)
                     +- Exchange (26)
                        +- Project (25)
                           +- Filter (24)
                              +- Scan parquet spark_catalog.load_test_full_warehouse.generation_document_part (7)",,,2203200,2203200,,0%,2203200,2203200,,,,,,,,,,,,,,,,,"18/Sep/23 08:45;tonyuwarov;T1-Details-for-Query.png;https://issues.apache.org/jira/secure/attachment/13062984/T1-Details-for-Query.png, 18/Sep/23 08:45;tonyuwarov;T2-Details-for-Query.png;https://issues.apache.org/jira/secure/attachment/13062983/T2-Details-for-Query.png",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,38:06.0,,,,,,,,,,0|z1kf4w:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,
The default value of ‘spark.executor.logs.rolling.strategy’ on the official website is incorrect,SPARK-45160,13550609,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,chenyu-opensource,chenyu-opensource,chenyu-opensource,9/14/23 2:27,9/18/23 2:35,7/17/24 20:45,9/18/23 2:35,3.4.1,4.0.0,Documentation,,0,pull-request-available,"Empty string and (none) are different. the default value of '

spark.executor.logs.rolling.strategy' is """"",,,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/23 02:34;chenyu-opensource;execute different logic.png;https://issues.apache.org/jira/secure/attachment/13062895/execute+different+logic.png, 14/Sep/23 02:33;chenyu-opensource;the default value.png;https://issues.apache.org/jira/secure/attachment/13062894/the+default+value.png, 14/Sep/23 02:31;chenyu-opensource;the value on the official website.png;https://issues.apache.org/jira/secure/attachment/13062893/the+value+on+the+official+website.png",,3,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 18 02:35:33 UTC 2023,,,,,,,,,,0|z1kcig:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"14/Sep/23 06:22;chenyu-opensource;I had sumbit a pr

https://github.com/apache/spark/pull/42919;;;, 18/Sep/23 02:35;srowen;Resolved by github.com/apache/spark/pull/42919;;;",,,,,18/Sep/23 02:35;srowen;Resolved by github.com/apache/spark/pull/42919;;;
The default value of ‘spark. submit. deployMode’ on the official website is incorrect,SPARK-45146,13550452,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,chenyu-opensource,chenyu-opensource,chenyu-opensource,9/13/23 6:57,9/16/23 13:18,7/17/24 20:45,9/13/23 13:49,3.4.1,4.0.0,Documentation,,0,pull-request-available,"The deploy mode of Spark driver program has default value,but the value on the official website is incorrect.",,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/23 06:58;chenyu-opensource;the default value.png;https://issues.apache.org/jira/secure/attachment/13062864/the+default+value.png, 13/Sep/23 06:58;chenyu-opensource;the value on the official website.png;https://issues.apache.org/jira/secure/attachment/13062863/the+value+on+the+official+website.png",,2,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 13 13:49:10 UTC 2023,,,,,,,,,,0|z1kbjk:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"13/Sep/23 07:18;chenyu-opensource;I had submit a pr to resolve this question.

https://github.com/apache/spark/pull/42902;;;, 13/Sep/23 13:49;srowen;Issue resolved by pull request 42902
[https://github.com/apache/spark/pull/42902];;;",,,,,"13/Sep/23 13:49;srowen;Issue resolved by pull request 42902
[https://github.com/apache/spark/pull/42902];;;"
Miswritten remarks in pom file,SPARK-44890,13548042,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,chenyu-opensource,chenyu-opensource,chenyu-opensource,8/21/23 8:22,9/4/23 14:13,7/17/24 20:45,9/4/23 14:13,3.4.1,4.0.0,Build,,0,,"Spelling issues in pom files affect understanding which uses 'dont update'.

It needs to maintain the same writing style as other places",,,,,,,,,,,,,,,,,,,,,,,,,21/Aug/23 08:23;chenyu-opensource;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13062311/screenshot-1.png,,1,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 04 14:13:41 UTC 2023,,,,,,,,,,0|z1jwog:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"21/Aug/23 08:30;chenyu-opensource;I had submit a patch 

[https://github.com/apache/spark/pull/42598|https://github.com/apache/spark/pull/42583]

 ;;;, 04/Sep/23 03:43;snoot;User 'chenyu-opensource' has created a pull request for this issue:
https://github.com/apache/spark/pull/42598;;;, 04/Sep/23 14:12;srowen;This is too trivial for a JIRA;;;, 04/Sep/23 14:13;srowen;Resolved by https://github.com/apache/spark/pull/42598;;;",,,,,"04/Sep/23 03:43;snoot;User 'chenyu-opensource' has created a pull request for this issue:
https://github.com/apache/spark/pull/42598;;;"
Improve python version mismatch logging,SPARK-45053,13549435,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,WweiL,WweiL,WweiL,9/1/23 17:37,9/4/23 0:51,7/17/24 20:45,9/4/23 0:51,3.4.1,4.0.0,PySpark,,0,,"Currently the syntax of the python version mismatching is a little bit confusing, it uses (3,9) to represent python version 3.9. Just a minor update to make it more straightforward ",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 04 00:51:07 UTC 2023,,,,,,,,,,0|z1k59s:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"02/Sep/23 03:54;srowen;[~WweiL] Please fill out this JIRA. It is also not ""Major"";;;, 02/Sep/23 03:54;snoot;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/42776;;;, 02/Sep/23 03:54;snoot;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/42776;;;, 04/Sep/23 00:51;gurwls223;Issue resolved by pull request 42776
[https://github.com/apache/spark/pull/42776];;;",,,,,"02/Sep/23 03:54;snoot;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/42776;;;"
Adjust Pull Request Template to incorporate the ASF Generative Tooling Guidance recommendations,SPARK-44782,13547033,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,zero323,zero323,zero323,8/12/23 10:18,8/19/23 2:14,7/17/24 20:45,8/19/23 2:14,"3.3.2, 3.4.1",4.0.0,Project Infra,,0,,"Recently releases [ASF Generative Tooling Guidance|https://www.apache.org/legal/generative-tooling.html] recommends keeping track of the generative AI tools used to author patches

??When providing contributions authored using generative AI tooling, a recommended practice is for contributors to indicate the tooling used to create the contribution. This should be included as a token in the source control commit message, for example including the phrase “Generated-by: ”. This allows for future release tooling to be considered that pulls this content into a machine parsable Tooling-Provenance file.??

We should adjust PR template accordingly.
",,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 19 02:14:38 UTC 2023,,,,,,,,,,0|z1jqtc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"15/Aug/23 06:14;smilegator;+1 We should update the PR template. ;;;, 15/Aug/23 08:24;zero323;Created a pull request for this issue:
https://github.com/apache/spark/pull/42469;;;, 15/Aug/23 09:20;githubbot;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/42469;;;, 19/Aug/23 02:14;srowen;Issue resolved by pull request 42469
[https://github.com/apache/spark/pull/42469];;;",3.4.1,,,,"15/Aug/23 08:24;zero323;Created a pull request for this issue:
https://github.com/apache/spark/pull/42469;;;"
Document spark.network.timeoutInterval,SPARK-44725,13546588,,Documentation,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dongjoon,dongjoon,dongjoon,8/8/23 20:49,8/9/23 4:46,7/17/24 20:45,8/8/23 23:04,"3.3.2, 3.4.1, 3.5.0","3.3.4, 3.4.2, 3.5.1, 4.0.0",Documentation,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 09 04:46:47 UTC 2023,,,,,,,,,,0|z1jo2g:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"08/Aug/23 23:04;dongjoon;Issue resolved by pull request 42402
[https://github.com/apache/spark/pull/42402];;;, 09/Aug/23 04:45;snoot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/42402;;;, 09/Aug/23 04:46;snoot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/42402;;;",3.4.1,3.5.0,,,"09/Aug/23 04:45;snoot;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/42402;;;"
Upgrade snappy-java to 1.1.10.3,SPARK-44513,13544550,,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,panbingkun,panbingkun,panbingkun,7/23/23 12:56,7/31/23 16:08,7/17/24 20:45,7/24/23 3:04,3.4.1,"3.4.2, 3.5.0",Build,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 24 03:04:12 UTC 2023,,,,,,,,,,0|z1jbi8:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"24/Jul/23 03:04;Qin Yao;Issue resolved by pull request 42113
[https://github.com/apache/spark/pull/42113];;;",,,,,
eagerly load SparkExitCode class in SparkUncaughtExceptionHandler,SPARK-44542,13544762,,Improvement,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,advancedxy,advancedxy,advancedxy,7/25/23 8:44,7/31/23 3:13,7/17/24 20:45,7/31/23 3:13,"3.1.3, 3.3.2, 3.4.1",3.5.0,Spark Core,,0,,"There are two background for this improvement proposal:

1. When running spark on yarn, the disk might be corrupted during application running. The corrupted disk might contain the spark jars(cache archive from spark.yarn.archive). In that case , the executor JVM cannot load any spark related classes any more.

2. Spark leverages the OutputCommitCoordinator to avoid data race between speculate tasks so that no tasks could commit the same partition in the same time. In other words, once a task's commit request is allowed, other commit requests would be denied until the committing task is failed.

 

We encountered a corner case combined the above two cases, which makes the spark hangs.  A short timeline could be described as below:
 # task 5372(tid: 21662) starts running in 21:55
 # the disk contains the spark archive for that task/executor is corrupted, thus making the archive inaccessible from executor's JVM perspective, it happened around 22:00
 # the task continues running, at 22:05, it requests commit from coordinator and performs the commit. 
 # however due the corrupted disk, some exception raised in the executor JVM.
 # The SparkUncaughtExceptionHandler kicks in, however as the jar/disk is corrupted, the handler itself throws an exception, and the halt process throws an exception too.
 # The executor is hanging there, no more tasks are running. However the authorized commit request is still valid in the driver side
 # Speculate tasks start to click in, due to no commit permission, all speculate tasks are killed/denied.
 # The job is hanging until our SRE killed the container from outside.

Some screenshot are provided below.

!image-2023-07-25-16-46-03-989.png!

!image-2023-07-25-16-46-28-158.png!

!image-2023-07-25-16-46-42-522.png!

For this specific case: I'd like to the propose to eagerly load SparkExitCode class in the 
SparkUncaughtExceptionHandler, so that the halt process could be executed rather than throws an exception as SparkExitCode is not loadable during the previous scenario.",,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/23 08:46;advancedxy;image-2023-07-25-16-46-03-989.png;https://issues.apache.org/jira/secure/attachment/13061599/image-2023-07-25-16-46-03-989.png, 25/Jul/23 08:46;advancedxy;image-2023-07-25-16-46-28-158.png;https://issues.apache.org/jira/secure/attachment/13061600/image-2023-07-25-16-46-28-158.png, 25/Jul/23 08:46;advancedxy;image-2023-07-25-16-46-42-522.png;https://issues.apache.org/jira/secure/attachment/13061601/image-2023-07-25-16-46-42-522.png",,3,,,,,,,,,,,,,,,,,,,,,,FALSE,,,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 31 03:13:08 UTC 2023,,,,,,,,,,0|z1jctc:,9223372036854775807,,,,,,,,,,,,,,,,,,,,,"31/Jul/23 03:13;srowen;Issue resolved by pull request 42195
[https://github.com/apache/spark/pull/42195];;;",3.3.2,3.4.1,,,