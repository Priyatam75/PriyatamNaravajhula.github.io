Summary: Optimize hive patition filter when the comparision dataType not match
Issue key: SPARK-45387
Issue id: 13552522
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: tianyima
Creator: tianyima
Created: 30/Sep/23 09:47
Updated: 16/Apr/24 07:44
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.3.0, 3.4.0, 3.5.0, 3.5.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Suppose we have a partitioned table `table_pt` with partition colum `dt` which is StringType and the table metadata is managed by Hive Metastore, if we filter partition by dt = '123', this filter can be pushed down to data source directly, but if the filter condition is number, e.g. dt = 123, Spark will not known which partition should be pushed down. Thus in the process of physical plan optimization, Spark will pull all of that table's partition meta data to client side, to decide which partition filter should be push down to the data source. This is poor of performance if the table has thousands of partitions and increasing the risk of hive metastore oom.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 13/Oct/23 15:40;tianyima;PruneFileSourcePartitions.diff;https://issues.apache.org/jira/secure/attachment/13063546/PruneFileSourcePartitions.diff
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 05 09:57:20 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1kobk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Feb/24 03:43;doki;I can't reproduce it in spark 3.5.0.

I try to create a partitioned csv table on hdfs like follow:
{code:java}
create external table noaa (column0 string, column1 int, column2 string, column3 int, column4 string, column5 string, column6 string, column7 string) PARTITIONED BY (year string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' LOCATION '/tmp/noaa'; 

alter table noaa add partition (year = '2019') LOCATION '/tmp/noaa/year=2019';

alter table noaa add partition (year = '2020') LOCATION '/tmp/noaa/year=2020';{code}
and the spark plan is 
{code:java}
scala> spark.sql("select * from noaa where year=2019 limit 10").explain(true)
== Parsed Logical Plan ==
'GlobalLimit 10
+- 'LocalLimit 10
   +- 'Project [*]
      +- 'Filter ('year = 2019)
         +- 'UnresolvedRelation [noaa], [], false== Analyzed Logical Plan ==
column0: string, column1: string, column2: string, column3: string, column4: string, column5: string, column6: string, column7: string, year: string
GlobalLimit 10
+- LocalLimit 10
   +- Project [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62, year#63]
      +- Filter (cast(year#63 as int) = 2019)
         +- SubqueryAlias spark_catalog.default.noaa
            +- HiveTableRelation [`spark_catalog`.`default`.`noaa`, org.apache.hadoop.hive.serde2.OpenCSVSerde, Data Cols: [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62], Partition Cols: [year#63]]== Optimized Logical Plan ==
GlobalLimit 10
+- LocalLimit 10
   +- Filter (isnotnull(year#63) AND (cast(year#63 as int) = 2019))
      +- HiveTableRelation [`spark_catalog`.`default`.`noaa`, org.apache.hadoop.hive.serde2.OpenCSVSerde, Data Cols: [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62], Partition Cols: [year#63], Pruned Partitions: [(year=2019)]]== Physical Plan ==
CollectLimit 10
+- Scan hive spark_catalog.default.noaa [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62, year#63], HiveTableRelation [`spark_catalog`.`default`.`noaa`, org.apache.hadoop.hive.serde2.OpenCSVSerde, Data Cols: [column0#55, column1#56, column2#57, column3#58, column4#59, column5#60, column6#61, column7#62], Partition Cols: [year#63], Pruned Partitions: [(year=2019)]], [isnotnull(year#63), (cast(year#63 as int) = 2019)]{code}
The filter has been pushed down.;;;, 05/Mar/24 09:57;tianyima;[~doki] the output execution plan is the final result, but the problem lies in the optimize process.

In your example, the partition key is stringType, but was cast to int to filter partitions. The driver will get all the partition to do this filter. If you have a hive table with thousands of partitions, this process will very slow and costly.;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.12.0, EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, EMR-7.0.0, EMR-7.1.0, Unknown

Summary: Null Pointer Exception when Loading ML Pipeline Model with Custom Transformer
Issue key: SPARK-37913
Issue id: 13422809
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: ally1221
Creator: ally1221
Created: 14/Jan/22 21:59
Updated: 15/Jan/24 00:35
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: MLPipelineModels, MLPipelines
Description: I am trying to create and persist a ML pipeline model using a custom Spark transformer that I created based on the [Unary Transformer example|https://github.com/apache/spark/blob/v3.1.2/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala] provided by Spark. I am able to save and load the transformer. When I include the custom transformer as a stage in a pipeline model, I can save the model, but am unable to load it. Here is the stack trace of the exception:

 
{code:java}
01-14-2022 03:49:52 PM ERROR Instrumentation: java.lang.NullPointerException at java.base/java.lang.reflect.Method.invoke(Method.java:559) at org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstanceReader(ReadWrite.scala:631) at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:276) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238) at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198) at scala.collection.TraversableLike.map(TraversableLike.scala:238) at scala.collection.TraversableLike.map$(TraversableLike.scala:231) at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198) at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274) at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191) at scala.util.Try$.apply(Try.scala:213) at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191) at org.apache.spark.ml.Pipeline$SharedReadWrite$.load(Pipeline.scala:268) at org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$7(Pipeline.scala:356) at org.apache.spark.ml.MLEvents.withLoadInstanceEvent(events.scala:160) at org.apache.spark.ml.MLEvents.withLoadInstanceEvent$(events.scala:155) at org.apache.spark.ml.util.Instrumentation.withLoadInstanceEvent(Instrumentation.scala:42) at org.apache.spark.ml.PipelineModel$PipelineModelReader.$anonfun$load$6(Pipeline.scala:355) at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191) at scala.util.Try$.apply(Try.scala:213) at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191) at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:355) at org.apache.spark.ml.PipelineModel$PipelineModelReader.load(Pipeline.scala:349) at org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:355) at org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:355) at org.apache.spark.ml.PipelineModel$.load(Pipeline.scala:337) at com.dtech.scala.pipeline.PipelineProcess.process(PipelineProcess.scala:122) at com.dtech.scala.pipeline.PipelineProcess$.main(PipelineProcess.scala:448) at com.dtech.scala.pipeline.PipelineProcess.main(PipelineProcess.scala) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:566) at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:65) at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala){code}
 

*Source Code*

[Unary Transformer|https://gist.github.com/ally1221/ff10ec50f7ef98fb6cd365172195bfd5]

[Persist Unary Transformer & Pipeline Model|https://gist.github.com/ally1221/42473cdc818a8cf795ac78d65d48ee14]
Environment: Spark 3.1.2, Scala 2.12, Java 11
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): java, scala
Custom field (Last public comment date): Tue Dec 12 11:43:30 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ylz4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Feb/22 10:36;podongfeng;does the `MyTransformer` in the example works?;;;, 12/Dec/23 11:43;apeng;[~ally1221] I can reproduce this issue.
h2. Solution:

A simple approach to resolve this issue is to extract _UnaryTransformerExample.MyTransformer_ from {_}UnaryTransformerExample{_}, transforming it into a *top-level class* instead of a *nested class.*
h2. Code to reproduce the issue:

Adding these 2 blocks in _org.apache.spark.examples.ml.UnaryTransformerExample_
{code:java}
val pipeline = new Pipeline().setStages(Array(myTransformer))
val pipelinePath = "~/tmp/ts/1";
pipeline.write.overwrite().save(pipelinePath)
val pipeline2 = Pipeline.load(pipelinePath){code}
{code:java}
val model = pipeline.fit(data)
val pipelineModelPath = "~/tmp/ts/2"; model.write.overwrite().save(pipelineModelPath)
val model2 = PipelineModel.load(pipelineModelPath){code}
there will be the exception:
{code:java}
23/12/12 19:20:40 ERROR Instrumentation: java.lang.NoSuchMethodException: org.apache.spark.examples.ml.UnaryTransformerExample$MyTransformer.read() at java.base/java.lang.Class.newNoSuchMethodException(Class.java:660) at java.base/java.lang.Class.throwExceptionOrReturnNull(Class.java:1458) at java.base/java.lang.Class.getMethodHelper(Class.java:1544) at java.base/java.lang.Class.getMethod(Class.java:1450) at org.apache.spark.ml.util.DefaultParamsReader$.loadParamsInstanceReader(ReadWrite.scala:631) at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$4(Pipeline.scala:276) at scala.collection.ArrayOps$.map$extension(ArrayOps.scala:934) at org.apache.spark.ml.Pipeline$SharedReadWrite$.$anonfun$load$3(Pipeline.scala:274){code}
h2. Root cause:

_Pipeline.PipelineWriter_ and _PipelineModel.PipelineModelWriter_ will persist the *class name* of each stage when saving _Pipeline_ or {_}PipelineModel{_}. In this example, the class name is {*}_org.apache.spark.examples.ml.UnaryTransformerExample$MyTransformer_{*}. The class name will be used by _PipelineReader_ and _PipelineModelReader_ to construct the Class object which represents the class, then invoke the _*read()*_ method.

The problem here is for the *nested class* {_}UnaryTransformerExample.MyTransformer{_}, the Scala compiler will generate 2 different Java classes, the _*read()*_ method which is part of the Scala companion object will be a member of Java class  {_}*org.apache.spark.examples.ml.UnaryTransformerExample$MyTransformer$*{_}, it is not a member of {_}*org.apache.spark.examples.ml.UnaryTransformerExample$MyTransformer*{_}. So there is the exception "{_}java.lang.NoSuchMethodException: org.apache.spark.examples.ml.UnaryTransformerExample$MyTransformer.read(){_}" when loading the object with  _PipelineReader_ and {_}PipelineModelReader{_}.
 
Note there is no such issue for a {*}non-nested class{*}, since the Scala compiler will generate only one Java class.
h2. Java classes generated by Scala compiler:
{code:java}
public final class UnaryTransformerExample {
  ...
  public static class MyTransformer extends UnaryTransformer<Object, Object, MyTransformer> implements DefaultParamsWritable {
    ...
    public MLWriter write() {
      return DefaultParamsWritable.write$(this);
    }
    public void save(String path) throws IOException {
      MLWritable.save$((MLWritable)this, path);
    }
    ...
  }
  public static class MyTransformer$ implements DefaultParamsReadable<MyTransformer>, Serializable {
    public MLReader<UnaryTransformerExample.MyTransformer> read() {
      return DefaultParamsReadable.read$(this);
    }
    public Object load(String path) {
      return MLReadable.load$((MLReadable)this, path);
    }
    ...
  }
}
{code}
 ;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: The filter operator gets wrong results in char type
Issue key: SPARK-37051
Issue id: 13407195
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: frankli
Creator: frankli
Created: 19/Oct/21 03:16
Updated: 02/Nov/21 03:43
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: When I try the following sample SQL on  the TPCDS data, the filter operator returns an empty row set (shown in web ui).

_select * from item where i_category = 'Music' limit 100;_

The table is in ORC format, and i_category is char(50) type. 

Data is inserted by hive, and queried by Spark.

I guest that the char(50) type will remains redundant blanks after the actual word.

It will affect the boolean value of  "x.equals(Y)", and results in wrong results.

Luckily, the varchar type is OK. 

 

This bug can be reproduced by a few steps.

>>> desc t2_orc;
 +------------+-----------++------------+
|col_name|data_type|comment|

+------------+-----------++------------+
|a|string      |NULL|
|b|char(50)  |NULL|
|c|int            |NULL|

+------------+-----------++----------–+

>>> select * from t2_orc where a='a';
 +-----+---++------+
|a|b|c|

+-----+---++------+
|a|b|1|
|a|b|2|
|a|b|3|
|a|b|4|
|a|b|5|

+-----+---++----–+

>>> select * from t2_orc where b='b';
 +-----+---++------+
|a|b|c|

+-----+---++------+
 +-----+---++------+

 

By the way, Spark's tests should add more cases on the char type.

 

== Physical Plan ==
 CollectLimit (3)
 +- Filter (2)
 +- Scan orc tpcds_bin_partitioned_orc_2.item (1)

(1) Scan orc tpcds_bin_partitioned_orc_2.item
 Output [22]: [i_item_sk#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21|#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21]
 Batched: false
 Location: InMemoryFileIndex [hdfs://tpcds_bin_partitioned_orc_2.db/item]
 PushedFilters: [IsNotNull(i_category), +EqualTo(i_category,+Music         )]++++
 ReadSchema: struct<i_item_sk:bigint,i_item_id:string,i_rec_start_date:date,i_rec_end_date:date,i_item_desc:string,i_current_price:decimal(7,2),i_wholesale_cost:decimal(7,2),i_brand_id:int,i_brand:string,i_class_id:int,i_class:string,i_category_id:int,i_category:string,i_manufact_id:int,i_manufact:string,i_size:string,i_formulation:string,i_color:string,i_units:string,i_container:string,i_manager_id:int,i_product_name:string>

(2) Filter
 Input [22]: [i_item_sk#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21|#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21]
 Condition : (isnotnull(i_category#12) AND +(i_category#12 = Music         ))+

(3) CollectLimit
 Input [22]: [i_item_sk#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21|#0L, i_item_id#1, i_rec_start_date#2, i_rec_end_date#3, i_item_desc#4, i_current_price#5, i_wholesale_cost#6, i_brand_id#7, i_brand#8, i_class_id#9, i_class#10, i_category_id#11, i_category#12, i_manufact_id#13, i_manufact#14, i_size#15, i_formulation#16, i_color#17, i_units#18, i_container#19, i_manager_id#20, i_product_name#21]
 Arguments: 100

 
Environment: Spark 3.1.2

Scala 2.12 / Java 1.8
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Nov 02 03:43:44 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vyco:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Oct/21 03:32;frankli;[~dongjoon] Can I trouble you to take a look. Thanks a lot.;;;, 19/Oct/21 09:58;frankli;It seems to be affected by the right padding.

[SPARK-34192][SQL] [https://github.com/apache/spark/commit/d1177b52304217f4cb86506fd1887ec98879ed16]

[~yaoqiang];;;, 26/Oct/21 03:05;wangzhun;We have the same problem, tcpds-q7 does not work. The cd_education_status field of the customer_demographics table in parquet format is char(20).
{code:java}
select count(*) from tpcds.customer_demographics where cd_education_status ='College'{code}
spark3.1.2  returns 0 rows
spark2.3.4/hive/presto returns 27w rows;;;, 26/Oct/21 08:37;dongjoon;Does Parquet work in those scenario, [~frankli] and [~wangzhun]?;;;, 26/Oct/21 08:48;frankli;This scenario also occur on Parquet. [~dongjoon]

Spark3.1 will do padding for both writer and reader side.

So, Spark 3.1 cannot read Hive data without padding, while Spark 2.4 works well.;;;, 26/Oct/21 09:10;dongjoon;Got it. If that happens on Parquet, we had better drop `ORC` from the JIRA title. I removed it first.
> This scenario also occur on Parquet.;;;, 02/Nov/21 03:37;LuciferYang;Can you test
{code:java}
select * from t2_orc where b='b0000000000000000000000000000000000000000000000000';
{code}
1 B and 49 zeros

 

 ;;;, 02/Nov/21 03:43;frankli;I know this SQL can work, but this behavior is different from MYSQL and PostgreSQL.;;;
Affects Version/s.1: 3.2.1
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: YARN shuffle server restart crashes all dynamic allocation jobs that have deallocated an executor
Issue key: SPARK-36446
Issue id: 13393920
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Critical
Resolution: 
Assignee: 
Reporter: adamkennedy77
Creator: adamkennedy77
Created: 06/Aug/21 16:21
Updated: 31/Aug/21 22:48
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.1.2
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: When dynamic allocation is enabled, executors that deallocate rely on the shuffle server to hold blocks and supply them to remaining executors.

When YARN Shuffle Server restarts (either intentionally or due to a crash), it loses block information and relies on being able to contact Executors (the locations of which it durably stores) to refetch the list of blocks.

This mutual dependency on the other to hold block information fails fatally under some common scenarios.

For example, if a Spark application is running under dynamic allocation, some amount of executors will almost always shut down.

If, after this has occurred, any shuffle server crashes, or is restarted (either directly when running as a standalone service, or as part of a YARN node manager restart) then there is no way to restore block data and it is permanently lost.

Worse, when Executors try to fetch blocks from the shuffle server, the shuffle server cannot location the exeutor, decides it doesn't exist, treats it as a fatal exception, and causes the application to terminate and crash.

Thus, in a real world scenario that we observe on a 1000+ node multi-tenant cluster  where dynamic allocation is on by default, a rolling restart of the YARN node managers will cause ALL jobs that have deallocated any executor and have shuffles or transferred blocks to the shuffle server in order to shut down, to crash.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): SPARK-27677, SPARK-21097, SPARK-25888
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Aug 31 22:48:08 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tohc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Aug/21 16:25;adamkennedy77;Note: While I haven't investigated any other shuffle servers than the YARN shuffle server, this may impact other shuffle server implementations and they should be checked for the same problem.;;;, 06/Aug/21 17:05;adamkennedy77;The problem was particularly amplified by the Executor deallocation improvements in  SPARK-27677 and SPARK-21097 that came about as a result of SPARK-25888. It is now much more likely that Executors will deallocate, even in the face of persist() or cache().

 ;;;, 06/Aug/21 17:15;dongjoon;cc [~tgraves] and [~mridul];;;, 06/Aug/21 18:00;tgraves;Is this with the yarn nodemangar recovery enabled?  ie yarn stores the necessarily information in a database which on restart it loads back up, if that is not configured it will not remember blocks.  ;;;, 11/Aug/21 16:25;tgraves;[~adamkennedy77] ^;;;, 31/Aug/21 22:48;adamkennedy77;[~tgraves] Yes, we are running with recovery enabled (in the case where shuffle server connections are secure).

But the same problem occurs when the shuffle server is run as an independent process outside of YARN (insecurely) if they crash and restart.;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: The Statistics size of PARQUET table is not estimated correctly
Issue key: SPARK-35833
Issue id: 13384706
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: honglun
Creator: honglun
Created: 20/Jun/21 12:34
Updated: 15/May/24 08:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: {code:java}
// Table 'test_txt' and 'test_parquet' have the same data.
scala> val sql="select * from tmp_db.test_txt"
sql: String = select * from tmp_db.test_txt
scala> spark.sql(sql).queryExecution.optimizedPlan.stats.sizeInBytes
res5: BigInt = 92990
scala> val sql = "select * from tmp_db.test_parquet"
sql: String = select * from tmp_db.test_parquet
scala> spark.sql(sql).queryExecution.optimizedPlan.stats.sizeInBytes
res6: BigInt = 37556
{code}
PARQUET file is compressed by default, this could lead to choose the wrong type of JOIN, like BROADCASTJOIN. Driver may be OOM in this case, because the actual size may be much greater than the AUTO_BROADCASTJOIN_THRESHOLD.  Can we improve this? 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-24914
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-06-20 12:34:03.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s3pk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Add LauncherProtocol to K8s resource manager client
Issue key: SPARK-36832
Issue id: 13402936
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: timothy65535
Creator: timothy65535
Created: 23/Sep/21 12:16
Updated: 21/Feb/24 06:42
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Submit
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Refer to: org.apache.spark.deploy.yarn.Client
{code:java}
private val launcherBackend = new LauncherBackend() {
  override protected def conf: SparkConf = sparkConf  override def onStopRequest(): Unit = {
    if (isClusterMode && appId != null) {
      yarnClient.killApplication(appId)
    } else {
      setState(SparkAppHandle.State.KILLED)
      stop()
    }
  }
}
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 23 12:26:58 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v83s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Sep/21 12:26;timothy65535;Feel free to take this Jira :);;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Reuse CachedDNSToSwitchMapping for yarn  container requests
Issue key: SPARK-36964
Issue id: 13405742
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gaoyajun02
Creator: gaoyajun02
Created: 09/Oct/21 07:03
Updated: 15/Feb/24 10:07
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Spark Core, YARN
Due Date: 
Votes: 1
Labels: pull-request-available
Description: Similar to SPARK-13704​, In some cases, YarnAllocator add container requests with locality preference can be expensive, it may call the topology script for rack awareness.

When submit a very large job in a very large Yarn cluster, the topology script may take signifiant time to run. And this blocks receiving YarnSchedulerBackend's RequestExecutors rpc calls, This request comes from spark dynamic executor allocation thread, which may blocks the ExecutorAllocationListener, and then result in executorManagement queue backlog.

 

Some logs:
{code:java}
21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76) at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:839) at org.apache.spark.ExecutorAllocationManager.addExecutors(ExecutorAllocationManager.scala:411) at org.apache.spark.ExecutorAllocationManager.updateAndSyncNumExecutorsTarget(ExecutorAllocationManager.scala:361) at org.apache.spark.ExecutorAllocationManager.org$apache$spark$ExecutorAllocationManager$$schedule(ExecutorAllocationManager.scala:316) at org.apache.spark.ExecutorAllocationManager$$anon$1.run(ExecutorAllocationManager.scala:227) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263) at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:294) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) ... 12 more21/09/29 12:04:35 WARN spark-dynamic-executor-allocation ExecutorAllocationManager: Unable to reach the cluster manager to request 1922 total executors!

21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.21/09/29 12:04:35 INFO spark-dynamic-executor-allocation ExecutorAllocationManager: Error reaching cluster manager.org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [120 seconds]. This timeout is controlled by spark.rpc.askTimeout at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62) at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58) at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76) at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:839) at org.apache.spark.ExecutorAllocationManager.addExecutors(ExecutorAllocationManager.scala:411) at org.apache.spark.ExecutorAllocationManager.updateAndSyncNumExecutorsTarget(ExecutorAllocationManager.scala:361) at org.apache.spark.ExecutorAllocationManager.org$apache$spark$ExecutorAllocationManager$$schedule(ExecutorAllocationManager.scala:316) at org.apache.spark.ExecutorAllocationManager$$anon$1.run(ExecutorAllocationManager.scala:227) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.util.concurrent.TimeoutException: Futures timed out after [120 seconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263) at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:294) at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75) ... 12 more21/09/29 12:04:35 WARN spark-dynamic-executor-allocation ExecutorAllocationManager: Unable to reach the cluster manager to request 1922 total executors!


21/09/29 12:02:49 ERROR dag-scheduler-event-loop AsyncEventQueue: Dropping event from queue executorManagement. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
21/09/29 12:02:49 WARN dag-scheduler-event-loop AsyncEventQueue: Dropped 1 events from executorManagement since the application started.
21/09/29 12:02:55 INFO spark-listener-group-eventLog AsyncEventQueue: Process of event SparkListenerExecutorAdded(1632888172920,543,org.apache.spark.scheduler.cluster.ExecutorData@8cfab8f5,None) by listener EventLoggingListener took 3.037686034s.
21/09/29 12:03:03 INFO spark-listener-group-eventLog AsyncEventQueue: Process of event SparkListenerBlockManagerAdded(1632888181779,BlockManagerId(1359, --, 57233, None),2704696934,Some(2704696934),Some(0)) by listener EventLoggingListener took 1.462598355s.
21/09/29 12:03:49 WARN dispatcher-BlockManagerMaster AsyncEventQueue: Dropped 74388 events from executorManagement since Wed Sep 29 12:02:49 CST 2021.
21/09/29 12:04:35 INFO spark-listener-group-executorManagement AsyncEventQueue: Process of event SparkListenerStageSubmitted(org.apache.spark.scheduler.StageInfo@52f810ad,{...}) by listener ExecutorAllocationListener took 116.526408932s.
21/09/29 12:04:49 WARN heartbeat-receiver-event-loop-thread AsyncEventQueue: Dropped 18892 events from executorManagement since Wed Sep 29 12:03:49 CST 2021.
21/09/29 12:05:49 WARN dispatcher-BlockManagerMaster AsyncEventQueue: Dropped 19397 events from executorManagement since Wed Sep 29 12:04:49 CST 2021.
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): YARN-9394
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Nov 10 21:38:07 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vpew:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Oct/21 07:43;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/34231;;;, 09/Oct/21 07:43;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/34231;;;, 10/Nov/22 21:38;geoff_langenderfer;environment:

{code:bash}

Release label:emr-6.8.0
Hadoop distribution:Amazon 3.2.1
Applications:Spark 3.3.0

{code}

here's another stacktrace:

 
{code:bash}

22/11/10 19:05:20 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
22/11/10 19:15:12 ERROR AsyncEventQueue: Dropping event from queue executorManagement. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
22/11/10 19:15:20 ERROR AsyncEventQueue: Dropping event from queue eventLog. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
22/11/10 19:16:20 ERROR TransportChannelHandler: Connection to /10.0.0.107:47700 has been quiet for 120000 ms while there are outstanding requests. Assuming connection is dead; please adjust spark.rpc.io.connectionTimeout if this is wrong.
22/11/10 19:16:20 ERROR TransportResponseHandler: Still have 1 requests outstanding when connection from /10.0.0.107:47700 is closed
22/11/10 19:16:20 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(Profile: id = 0, executor resources: cores -> name: cores, amount: 32, script: , vendor: ,memory -> name: memory, amount: 218880, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0 -> 6721),Map(0 -> 212218),Map(0 -> Map(* -> 212218)),Set()) to AM was unsuccessful
org.apache.spark.rpc.RpcTimeoutException: Cannot receive any reply from /10.0.0.107:47700 in 120 seconds. This timeout is controlled by spark.rpc.askTimeout
    at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38) ~[scala-library-2.12.15.jar:?]
    at scala.util.Failure.recover(Try.scala:234) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Future.$anonfun$recover$1(Future.scala:395) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
    at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.complete(Promise.scala:53) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.complete$(Promise.scala:52) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82) ~[scala-library-2.12.15.jar:?]
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.tryFailure(Promise.scala:112) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.Promise.tryFailure$(Promise.scala:112) ~[scala-library-2.12.15.jar:?]
    at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:187) ~[scala-library-2.12.15.jar:?]
    at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:214) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:264) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) ~[?:?]
    at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: java.util.concurrent.TimeoutException: Cannot receive any reply from /10.0.0.107:47700 in 120 seconds
    at org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1.run(NettyRpcEnv.scala:265) ~[spark-core_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    ... 6 more
22/11/10 19:16:20 ERROR TransportClient: Failed to send RPC RPC 6522570887499332644 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
io.netty.channel.StacklessClosedChannelException: null
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
22/11/10 19:16:20 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(Profile: id = 0, executor resources: cores -> name: cores, amount: 32, script: , vendor: ,memory -> name: memory, amount: 218880, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0 -> 6634),Map(0 -> 212218),Map(0 -> Map(* -> 212218)),Set()) to AM was unsuccessful
java.io.IOException: Failed to send RPC RPC 6522570887499332644 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
    at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: io.netty.channel.StacklessClosedChannelException
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
22/11/10 19:16:20 ERROR TransportClient: Failed to send RPC RPC 4915998445035541588 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
io.netty.channel.StacklessClosedChannelException: null
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
22/11/10 19:16:20 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(Profile: id = 0, executor resources: cores -> name: cores, amount: 32, script: , vendor: ,memory -> name: memory, amount: 218880, script: , vendor: ,offHeap -> name: offHeap, amount: 0, script: , vendor: , task resources: cpus -> name: cpus, amount: 1.0 -> 6568),Map(0 -> 212218),Map(0 -> Map(* -> 212218)),Set()) to AM was unsuccessful
java.io.IOException: Failed to send RPC RPC 4915998445035541588 to /10.0.0.107:47700: io.netty.channel.StacklessClosedChannelException
    at org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:392) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:369) ~[spark-network-common_2.12-3.3.0-amzn-0.jar:3.3.0-amzn-0]
    at io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:552) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:503) ~[netty-transport-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) ~[netty-common-4.1.74.Final.jar:4.1.74.Final]
    at java.lang.Thread.run(Thread.java:829) ~[?:?]
Caused by: io.netty.channel.StacklessClosedChannelException

{code}
;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: External scheduler cannot be instantiated
Issue key: SPARK-46128
Issue id: 13559629
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: prakashgurung123
Creator: prakashgurung123
Created: 28/Nov/23 01:03
Updated: 28/Nov/23 01:21
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.5.0
Fix Version/s: 
Component/s: Kubernetes, Spark Core, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: Spark submit driver fails to resolve "kubernetes.default.svc" when trying to create executors on newly added worker nodes, however there are no issue with the existing worker nodes.

Spark versions tried:
 * 3.5.0
 * 3.1.2

Kubernetes cluster on premises using kubeadm
 * Kubernetes version: v1.28.2
 * OS: Ubuntu 22.04.1 (Jammy)
 * Container Runtime: 1.6.24

Complete error :
{noformat}
+ CMD=("$SPARK_HOME/bin/spark-submit" --conf "spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS" --deploy-mode client "$@")
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=192.168.95.23 --deploy-mode client --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.examples.SparkPi local:///opt/spark/examples/jars/spark-examples_2.12-3.1.2.jar
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
23/11/28 01:20:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
23/11/28 01:20:03 INFO SparkContext: Running Spark version 3.1.2
23/11/28 01:20:03 INFO ResourceUtils: ==============================================================
23/11/28 01:20:03 INFO ResourceUtils: No custom resources configured for spark.driver.
23/11/28 01:20:03 INFO ResourceUtils: ==============================================================
23/11/28 01:20:03 INFO SparkContext: Submitted application: Spark Pi
23/11/28 01:20:03 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/11/28 01:20:03 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
23/11/28 01:20:03 INFO ResourceProfileManager: Added ResourceProfile id: 0
23/11/28 01:20:03 INFO SecurityManager: Changing view acls to: 185,root
23/11/28 01:20:03 INFO SecurityManager: Changing modify acls to: 185,root
23/11/28 01:20:03 INFO SecurityManager: Changing view acls groups to:
23/11/28 01:20:03 INFO SecurityManager: Changing modify acls groups to:
23/11/28 01:20:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(185, root); groups with view permissions: Set(); users  with modify permissions: Set(185, root); groups with modify permissions: Set()
23/11/28 01:20:04 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
23/11/28 01:20:04 INFO SparkEnv: Registering MapOutputTracker
23/11/28 01:20:04 INFO SparkEnv: Registering BlockManagerMaster
23/11/28 01:20:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
23/11/28 01:20:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
23/11/28 01:20:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/11/28 01:20:04 INFO DiskBlockManager: Created local directory at /var/data/spark-f0634fda-1366-4da1-8ac2-262e4bf9952b/blockmgr-7ac2193b-f7ad-4bc2-bdfa-386d2d3f4bf6
23/11/28 01:20:04 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
23/11/28 01:20:04 INFO SparkEnv: Registering OutputCommitCoordinator
23/11/28 01:20:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
23/11/28 01:20:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://spark-pi-96895e8c1382ff30-driver-svc.default.svc:4040
23/11/28 01:20:04 INFO SparkContext: Added JAR local:///opt/spark/examples/jars/spark-examples_2.12-3.1.2.jar at file:/opt/spark/examples/jars/spark-examples_2.12-3.1.2.jar with timestamp 1701134403914
23/11/28 01:20:04 WARN SparkContext: The jar local:///opt/spark/examples/jars/spark-examples_2.12-3.1.2.jar has been added already. Overwriting of added jars is not supported in the current version.
23/11/28 01:20:04 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
23/11/28 01:20:24 ERROR SparkContext: Error initializing SparkContext.
org.apache.spark.SparkException: External scheduler cannot be instantiated
    at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2961)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:557)
    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
    at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
    at scala.Option.getOrElse(Option.scala:189)
    at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
    at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30)
    at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.base/java.lang.reflect.Method.invoke(Unknown Source)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Operation: [get]  for kind: [Pod]  with name: [spark-pi-96895e8c1382ff30-driver]  in namespace: [default]  failed.
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:64)
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:72)
    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:225)
    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.get(BaseOperation.java:186)
    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.get(BaseOperation.java:84)
    at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$driverPod$1(ExecutorPodsAllocator.scala:75)
    at scala.Option.map(Option.scala:230)
    at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.<init>(ExecutorPodsAllocator.scala:74)
    at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager.createSchedulerBackend(KubernetesClusterManager.scala:123)
    at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2955)
    ... 19 more
Caused by: java.net.UnknownHostException: kubernetes.default.svc: Temporary failure in name resolution
    at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
    at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(Unknown Source)
    at java.base/java.net.InetAddress.getAddressesFromNameService(Unknown Source)
    at java.base/java.net.InetAddress$NameServiceAddresses.get(Unknown Source)
    at java.base/java.net.InetAddress.getAllByName0(Unknown Source)
    at java.base/java.net.InetAddress.getAllByName(Unknown Source)
    at java.base/java.net.InetAddress.getAllByName(Unknown Source)
    at okhttp3.Dns$1.lookup(Dns.java:40)
    at okhttp3.internal.connection.RouteSelector.resetNextInetSocketAddress(RouteSelector.java:185)
    at okhttp3.internal.connection.RouteSelector.nextProxy(RouteSelector.java:149)
    at okhttp3.internal.connection.RouteSelector.next(RouteSelector.java:84)
    at okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:215)
    at okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135)
    at okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114)
    at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:127)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at io.fabric8.kubernetes.client.utils.BackwardsCompatibilityInterceptor.intercept(BackwardsCompatibilityInterceptor.java:135)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at io.fabric8.kubernetes.client.utils.OIDCTokenRefreshInterceptor.intercept(OIDCTokenRefreshInterceptor.java:41)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at io.fabric8.kubernetes.client.utils.ImpersonatorInterceptor.intercept(ImpersonatorInterceptor.java:68)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at io.fabric8.kubernetes.client.utils.HttpClientUtils.lambda$createHttpClient$3(HttpClientUtils.java:151)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:257)
    at okhttp3.RealCall.execute(RealCall.java:93)
    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:490)
    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:451)
    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:416)
    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:397)
    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleGet(BaseOperation.java:933)
    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:220)
    ... 26 more
23/11/28 01:20:24 INFO SparkUI: Stopped Spark web UI at http://spark-pi-96895e8c1382ff30-driver-svc.default.svc:4040
23/11/28 01:20:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/11/28 01:20:24 INFO MemoryStore: MemoryStore cleared
23/11/28 01:20:24 INFO BlockManager: BlockManager stopped
23/11/28 01:20:24 INFO BlockManagerMaster: BlockManagerMaster stopped
23/11/28 01:20:24 WARN MetricsSystem: Stopping a MetricsSystem that is not running
23/11/28 01:20:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
23/11/28 01:20:24 INFO SparkContext: Successfully stopped SparkContext
Exception in thread "main" org.apache.spark.SparkException: External scheduler cannot be instantiated
    at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2961)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:557)
    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
    at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
    at scala.Option.getOrElse(Option.scala:189)
    at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
    at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30)
    at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.base/java.lang.reflect.Method.invoke(Unknown Source)
    at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Operation: [get]  for kind: [Pod]  with name: [spark-pi-96895e8c1382ff30-driver]  in namespace: [default]  failed.
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:64)
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:72)
    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:225)
    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.get(BaseOperation.java:186)
    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.get(BaseOperation.java:84)
    at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$driverPod$1(ExecutorPodsAllocator.scala:75)
    at scala.Option.map(Option.scala:230)
    at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.<init>(ExecutorPodsAllocator.scala:74)
    at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager.createSchedulerBackend(KubernetesClusterManager.scala:123)
    at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2955)
    ... 19 more
Caused by: java.net.UnknownHostException: kubernetes.default.svc: Temporary failure in name resolution
    at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
    at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(Unknown Source)
    at java.base/java.net.InetAddress.getAddressesFromNameService(Unknown Source)
    at java.base/java.net.InetAddress$NameServiceAddresses.get(Unknown Source)
    at java.base/java.net.InetAddress.getAllByName0(Unknown Source)
    at java.base/java.net.InetAddress.getAllByName(Unknown Source)
    at java.base/java.net.InetAddress.getAllByName(Unknown Source)
    at okhttp3.Dns$1.lookup(Dns.java:40)
    at okhttp3.internal.connection.RouteSelector.resetNextInetSocketAddress(RouteSelector.java:185)
    at okhttp3.internal.connection.RouteSelector.nextProxy(RouteSelector.java:149)
    at okhttp3.internal.connection.RouteSelector.next(RouteSelector.java:84)
    at okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:215)
    at okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135)
    at okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114)
    at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:127)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at io.fabric8.kubernetes.client.utils.BackwardsCompatibilityInterceptor.intercept(BackwardsCompatibilityInterceptor.java:135)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at io.fabric8.kubernetes.client.utils.OIDCTokenRefreshInterceptor.intercept(OIDCTokenRefreshInterceptor.java:41)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at io.fabric8.kubernetes.client.utils.ImpersonatorInterceptor.intercept(ImpersonatorInterceptor.java:68)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at io.fabric8.kubernetes.client.utils.HttpClientUtils.lambda$createHttpClient$3(HttpClientUtils.java:151)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:257)
    at okhttp3.RealCall.execute(RealCall.java:93)
    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:490)
    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:451)
    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:416)
    at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleGet(OperationSupport.java:397)
    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleGet(BaseOperation.java:933)
    at io.fabric8.kubernetes.client.dsl.base.BaseOperation.getMandatory(BaseOperation.java:220)
    ... 26 more
23/11/28 01:20:24 INFO ShutdownHookManager: Shutdown hook called
23/11/28 01:20:24 INFO ShutdownHookManager: Deleting directory /var/data/spark-f0634fda-1366-4da1-8ac2-262e4bf9952b/spark-0190347a-61ed-45b3-bddc-d0a92db7bcc8
23/11/28 01:20:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-7ca3d253-94b6-442c-b557-f4270c3d12ce{noformat}
 

Similar issue: https://issues.apache.org/jira/browse/SPARK-29640
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-11-28 01:03:55.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1lvso:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.5.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-7.0.0, EMR-7.1.0

Summary: Support to extract partial filters of datasource v2 table and push them down
Issue key: SPARK-44419
Issue id: 13543501
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 14/Jul/23 05:57
Updated: 03/Nov/23 00:17
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description:  

Run the following sql, and the date predicate in the where clause is not pushed down and it would cause a full table scan.

 
{code:java}
SELECT
id,
data,
date
FROM
testcat.db.table
where
(date = 20221110 and udfStrLen(data) = 8)
or
(date = 20221111 and udfStrLen(data) = 8)  {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 14 09:32:06 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1j514:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Jul/23 09:32;hudson;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/42000;;;
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.12.0, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: optimize adaptive skew join for ExistenceJoin
Issue key: SPARK-44426
Issue id: 13543538
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 14/Jul/23 10:24
Updated: 03/Nov/23 00:17
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: For this query,  InSubQuery would be cast to ExistenceJoin and now ExistenceJoin does not support automatic data skew for the left table.
{code:java}
SELECT * FROM skewData1
where
(key1 in (select key2 from skewData2)
or value1 in (select value2 from skewData2){code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-07-14 10:24:16.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1j59c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.12.0, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: SPIP: Improving the compile time performance, by improving  a couple of rules, from 24 hrs to under 8 minutes
Issue key: SPARK-36786
Issue id: 13401750
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ashahid7
Creator: ashahid7
Created: 17/Sep/21 00:32
Updated: 02/Nov/23 02:32
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.1, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: SPIP
Description: h2. Q1. What are you trying to do? Articulate your objectives using absolutely no jargon.

The aim is to improve the compile time performance of query which in WorkDay's use case takes > 24 hrs ( & eventually fails) , to  < 8 min.

To explain the problem, I will provide the context.

The query plan in our production system, is huge, with nested *case when* expressions ( level of nesting could be >  8) , where each *case when* can have branches sometimes > 1000.

The plan could look like
{quote}Project1
    |
   Filter 1
    |

Project2
    |
 Filter2
    |
 Project3
    |
 Filter3

  |

Join
{quote}
Now the optimizer has a Batch of Rules , intended to run at max 100 times.

*Also note that the, the batch will continue to run till one of the condition is satisfied*

*i.e  either numIter == 100 || inputPlan == outputPlan (idempotency is achieved)*

One of the early  Rule is   *PushDownPredicateRule.*

**Followed by **CollapseProject**.

 

The first issue is *PushDownPredicate* rule.

It picks  one filter at a time & pushes it at lowest level ( I understand that in 3.1 it pushes through join, while in 2.4 it stops at Join) , but either case it picks 1 filter at time starting from top, in each iteration.

*The above comment is no longer true in 3.1 release as it now combines filters. so it does push now all the encountered filters in a single pass. But it still materializes the filter on each push by realiasing.*



So if there are say  50 projects interspersed with Filters , the idempotency is guaranteedly not going to get achieved till around 49 iterations. Moreover, CollapseProject will also be modifying tree on each iteration as a filter will get removed within Project.

Moreover, on each movement of filter through project tree, the filter is re-aliased using transformUp rule.  transformUp is very expensive compared to transformDown. As the filter keeps getting pushed down , its size increases.

To optimize this rule , 2 things are needed
 # Instead of pushing one filter at a time,  collect all the filters as we traverse the tree in that iteration itself.
 # Do not re-alias the filters on each push. Collect the sequence of projects it has passed through, and  when the filters have reached their resting place, do the re-alias by processing the projects collected in down to up manner.

This will result in achieving idempotency in a couple of iterations. 

*How reducing the number of iterations help in performance*

There are many rules like *NullPropagation, OptimizeIn, SimplifyConditionals ( ... there are around 6 more such rules)*  which traverse the tree using transformUp, and they run unnecessarily in each iteration , even when the expressions in an operator have not changed since the previous runs.

*I have a different proposal which I will share later, as to how to avoid the above rules from running unnecessarily, if it can be guaranteed that the expression is not going to mutate in the operator.* 

The cause of our huge compilation time has been identified as the above.
  
h2. Q2. What problem is this proposal NOT designed to solve?

It is not going to change any runtime profile.
h2. Q3. How is it done today, and what are the limits of current practice?

Like mentioned above , currently PushDownPredicate pushes one filter at a time  & at each Project , it materialized the re-aliased filter.  This results in large number of iterations to achieve idempotency as well as immediate materialization of Filter after each Project pass,, results in unnecessary tree traversals of filter expression that too using transformUp. and the expression tree of filter is bound to keep increasing as it is pushed down.
h2. Q4. What is new in your approach and why do you think it will be successful?

In the new approach we push all the filters down in a single pass. And do not materialize filters as it pass through Project. Instead keep collecting projects in sequential order and materialize the final filter once its final position is achieved ( above a join , in case of 2.1 , or above the base relation etc).

This approach when coupled with the logic of identifying those Project operator whose expressions will not mutate ( which I will share later) , so that rules like 

NullPropagation,
 OptimizeIn.,
 LikeSimplification.,
 BooleanSimplification.,
 SimplifyConditionals.,
 RemoveDispensableExpressions.,
 SimplifyBinaryComparison.,
 SimplifyCaseConversionExpressions.,
 SimplifyExtractValueOps
  

are applied only in first pass on the expressions of that Project operator,  the compilation time of offending queries have been reduced to under 8 mins from 24 hrs or more.

 
h2. Q5. Who cares? If you are successful, what difference will it make?

For My company WorkDay, it will solve the currently failing plans due to OOM & compilation time running into 24 hrs or so. I have a PR for this locally, will publish it in some  time.

 
h2. Q6. What are the risks?

The risk in the change of PushDownPredicate is very low. 
 For the next proposal of identifying Project operator whose expressions will be immutable such that the above set of rules run only once has some relatively complex logic but with extra tests coverage it should be safe.
h2. Q7. How long will it take?

The basic changes are already in place. tests will take time. around 10 -15 days.
h2. Q8. What are the mid-term and final “exams” to check for success?

All tests should pass.
 The perf benefit should justify the changes.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Nov 02 02:32:23 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v0sg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): adoucet@sqli.com
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Nov/23 02:01;abhinavofficial;[~ashahid7] [~adoucet@sqli.com] where are we on this one?;;;, 02/Nov/23 02:32;ashahid7;I had put this on back burner as my changes were on 3.2, so I have to do a merge . on latest. Though whatever optimizations I did on 3.2 are still applicable as the drawback still exist. But chnages are going to be a a little extensive.
If there is interest on it I can pick up , after some days as right now occupied with another spip which proposes chnages for improving perf of broadcast hash joins on non partition column joins.
 ;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Fixed matching check for CharType/VarcharType
Issue key: SPARK-44414
Issue id: 13543489
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 14/Jul/23 02:22
Updated: 28/Oct/23 00:16
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: pull-request-available
Description: Running the following code throws an exception
{code:java}
val analyzer = getAnalyzer
// check varchar type
val json1 = "{\"__CHAR_VARCHAR_TYPE_STRING\":\"varchar(80)\"}"
val metadata1 = new MetadataBuilder().withMetadata(Metadata.fromJson(json1)).build()

val query1 = TestRelation(StructType(Seq(
StructField("x", StringType, metadata = metadata1),
StructField("y", StringType, metadata = metadata1))).toAttributes)

val table1 = TestRelation(StructType(Seq(
StructField("x", StringType, metadata = metadata1),
StructField("y", StringType, metadata = metadata1))).toAttributes)

val parsedPlanByName1 = byName(table1, query1)
analyzer.executeAndCheck(parsedPlanByName1, new QueryPlanningTracker()) {code}
 

Exception details are as follows
{code:java}
org.apache.spark.sql.AnalysisException: unresolved operator 'AppendData TestRelation [x#8, y#9], true;
'AppendData TestRelation [x#8, y#9], true
+- TestRelation [x#6, y#7]    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:52)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:51)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:156)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$47(CheckAnalysis.scala:704)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$47$adapted(CheckAnalysis.scala:702)
    at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:186)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:702)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:92)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:156)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:177)
    at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
    at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:174)
    at org.apache.spark.sql.catalyst.analysis.DataSourceV2AnalysisBaseSuite.$anonfun$new$36(DataSourceV2AnalysisSuite.scala:691) {code}
 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2023-07-14 02:22:16.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1j4yg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.12.0, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Spark job hangs due to task launch thread failed to create
Issue key: SPARK-45570
Issue id: 13554373
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: lifulong
Creator: lifulong
Created: 17/Oct/23 08:33
Updated: 18/Oct/23 10:22
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.5.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: spark job hangs while web ui show there is one task in running stage keep running for multi hours, while other tasks finished in a few minutes 

executor will never report task launch failed info to driver

 

Below is spark task execute thread launch log:

23/10/17 04:45:42 ERROR Inbox: An error happened while processing message in the inbox for Executor
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:717)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:957)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1378)
        at org.apache.spark.executor.Executor.launchTask(Executor.scala:270)
        at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1.applyOrElse(CoarseGrainedExecutorBackend.scala:173)
        at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
        at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
        at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
        at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
        at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Environment: spark.speculation is use default value false

spark version 3.1.2
 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 18/Oct/23 10:18;lifulong;image-2023-10-18-18-18-36-132.png;https://issues.apache.org/jira/secure/attachment/13063635/image-2023-10-18-18-18-36-132.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 18 10:22:53 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1kzpk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Oct/23 10:22;lifulong;!image-2023-10-18-18-18-36-132.png!
catch thread create exception from line "threadPool.execute(tr)", and do
execBackend.statusUpdate(taskDescription.taskId, TaskState.FAILED, EMPTY_BYTE_BUFFER)
after get exception can fix this problem in theory
is this solution ok?;;;
Affects Version/s.1: 3.5.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-7.0.0, EMR-7.1.0

Summary: MetadataFetchFailedException due to decommission block migrations
Issue key: SPARK-38101
Issue id: 13426562
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: eejbyfeldt
Creator: eejbyfeldt
Created: 04/Feb/22 07:19
Updated: 04/Sep/23 13:16
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.1.3, 3.2.1, 3.2.2, 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 2
Labels: 
Description: As noted in SPARK-34939 there is race when using broadcast for map output status. Explanation from SPARK-34939

> After map statuses are broadcasted and the executors obtain serialized broadcasted map statuses. If any fetch failure happens after, Spark scheduler invalidates cached map statuses and destroy broadcasted value of the map statuses. Then any executor trying to deserialize serialized broadcasted map statuses and access broadcasted value, IOException will be thrown. Currently we don't catch it in MapOutputTrackerWorker and above exception will fail the application.

But if running with `spark.decommission.enabled=true` and `spark.storage.decommission.shuffleBlocks.enabled=true` there is another way to hit this race, when a node is decommissioning and the shuffle blocks are migrated. After a block has been migrated an update will be sent to the driver for each block and the map output caches will be invalidated.

Here are a driver when we hit the race condition running with spark 3.2.0:
{code:java}
2022-01-28 03:20:12,409 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 5.5 MiB, free 11.0 GiB)
2022-01-28 03:20:12,410 INFO spark.ShuffleStatus: Updating map output for 192108 to BlockManagerId(760, ip-10-231-63-204.ec2.internal, 34707, None)
2022-01-28 03:20:12,410 INFO spark.ShuffleStatus: Updating map output for 179529 to BlockManagerId(743, ip-10-231-34-160.ec2.internal, 44225, None)
2022-01-28 03:20:12,414 INFO spark.ShuffleStatus: Updating map output for 187194 to BlockManagerId(761, ip-10-231-43-219.ec2.internal, 39943, None)
2022-01-28 03:20:12,415 INFO spark.ShuffleStatus: Updating map output for 190303 to BlockManagerId(270, ip-10-231-33-206.ec2.internal, 38965, None)
2022-01-28 03:20:12,416 INFO spark.ShuffleStatus: Updating map output for 192220 to BlockManagerId(270, ip-10-231-33-206.ec2.internal, 38965, None)
2022-01-28 03:20:12,416 INFO spark.ShuffleStatus: Updating map output for 182306 to BlockManagerId(688, ip-10-231-43-41.ec2.internal, 35967, None)
2022-01-28 03:20:12,417 INFO spark.ShuffleStatus: Updating map output for 190387 to BlockManagerId(772, ip-10-231-55-173.ec2.internal, 35523, None)
2022-01-28 03:20:12,417 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.0 MiB, free 10.9 GiB)
2022-01-28 03:20:12,417 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on ip-10-231-63-1.ec2.internal:34761 (size: 4.0 MiB, free: 11.0 GiB)
2022-01-28 03:20:12,418 INFO memory.MemoryStore: Block broadcast_27_piece1 stored as bytes in memory (estimated size 1520.4 KiB, free 10.9 GiB)
2022-01-28 03:20:12,418 INFO storage.BlockManagerInfo: Added broadcast_27_piece1 in memory on ip-10-231-63-1.ec2.internal:34761 (size: 1520.4 KiB, free: 11.0 GiB)
2022-01-28 03:20:12,418 INFO spark.MapOutputTracker: Broadcast outputstatuses size = 416, actual size = 5747443
2022-01-28 03:20:12,419 INFO spark.ShuffleStatus: Updating map output for 153389 to BlockManagerId(154, ip-10-231-42-104.ec2.internal, 44717, None)
2022-01-28 03:20:12,419 INFO broadcast.TorrentBroadcast: Destroying Broadcast(27) (from updateMapOutput at BlockManagerMasterEndpoint.scala:594)
2022-01-28 03:20:12,427 INFO storage.BlockManagerInfo: Added rdd_65_20310 on disk on ip-10-231-32-25.ec2.internal:40657 (size: 77.6 MiB)
2022-01-28 03:20:12,427 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on ip-10-231-63-1.ec2.internal:34761 in memory (size: 4.0 MiB, free: 11.0 GiB)
{code}
While the Broadcast is being constructed we have updates coming in and the broadcast is destroyed almost immediately. On this particular job we ended up hitting the race condition a lot of times and it caused ~18 task failures and stage retries within 20 seconds causing us to hit our stage retry limit and the job to fail.

As far I understand this was the expected behavior for handling this case after SPARK-34939. But it seems like when combined with decommissioning hitting the race is a bit too common.

We have observed this behavior running 3.2.0 and 3.2.1, but I think other current versions are all so affected.

The executor will usually fail with errors like
{code:java}
org.apache.spark.shuffle.MetadataFetchFailedException: Unable to deserialize broadcasted map statuses for shuffle xx: java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_171_piece0 of broadcast_171{{}} {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-34939
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Mar 10 06:37:08 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z90w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Feb/22 18:52;dongjoon;Thank you for filing a JIRA, [~eejbyfeldt].;;;, 04/Feb/22 19:21;viirya;Thanks for reporting this issue, [~eejbyfeldt].

;;;, 09/Mar/22 18:23;medb;Is there a workaround for this issue?;;;, 10/Mar/22 06:37;eejbyfeldt;The race condition only exists when broadcast is used for the MapOutputStatuses. So one workaround might be to increase the value of `spark.shuffle.mapOutput.minSizeForBroadcast`: https://github.com/apache/spark/blob/v3.2.1/core/src/main/scala/org/apache/spark/internal/config/package.scala#L1457-L1462 so that broadcast is not used. But I suspect that if you make it to large you might run into performance problems.;;;
Affects Version/s.1: 3.1.3
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Do not combine multiple Generate operators in the same WholeStageCodeGen node because it can  easily cause OOM failures if arrays are relatively large
Issue key: SPARK-44759
Issue id: 13546803
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tafranky@gmail.com
Creator: tafranky@gmail.com
Created: 10/Aug/23 09:23
Updated: 13/Aug/23 06:03
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1
Fix Version/s: 
Component/s: Deploy, Optimizer, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: This is an issue since the WSCG  implementation of the generate node. 

Because WSCG compute rows in batches , the combination of WSCG and the explode operation consume a lot of the dedicated executor memory. This is even more true when the WSCG node contains multiple explode nodes. This is the case when flattening a nested array.

The generate node used to flatten array generally  produces an amount of output rows that is significantly higher than the input rows.

the number of output rows generated is even drastically higher when flattening a nested array .

When we combine more that 1 generate node in the same WholeStageCodeGen  node, we run  a high risk of running out of memory for multiple reasons. 

1- As you can see from snapshots added in the comments ,  the rows created in the nested loop are saved in a writer buffer.  In this case because the rows were big , the job failed with an Out Of Memory Exception error .

2_ The generated WholeStageCodeGen result in a nested loop that for each row  , will explode the parent array and then explode the inner array.  The rows are accumulated in the writer buffer without accounting for the row size.

Please view the attached Spark Gui and Spark Dag 

In my case the wholestagecodegen includes 2 explode nodes. 

Because the array elements are large , we end up with an Out Of Memory error. 

 

I recommend that we do not merge  multiple explode nodes in the same whole stage code gen node . Doing so leads to potential memory issues.

In our case , the job execution failed with an  OOM error because the the WSCG executed  into a nested for loop . 

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 10/Aug/23 16:27;tafranky@gmail.com;image-2023-08-10-09-27-24-124.png;https://issues.apache.org/jira/secure/attachment/13062036/image-2023-08-10-09-27-24-124.png, 10/Aug/23 16:29;tafranky@gmail.com;image-2023-08-10-09-29-24-804.png;https://issues.apache.org/jira/secure/attachment/13062037/image-2023-08-10-09-29-24-804.png, 10/Aug/23 16:32;tafranky@gmail.com;image-2023-08-10-09-32-46-163.png;https://issues.apache.org/jira/secure/attachment/13062038/image-2023-08-10-09-32-46-163.png, 10/Aug/23 16:33;tafranky@gmail.com;image-2023-08-10-09-33-47-788.png;https://issues.apache.org/jira/secure/attachment/13062039/image-2023-08-10-09-33-47-788.png, 10/Aug/23 09:25;tafranky@gmail.com;wholestagecodegen_wc1_debug_wholecodegen_passed;https://issues.apache.org/jira/secure/attachment/13062030/wholestagecodegen_wc1_debug_wholecodegen_passed
Custom field (Affects version (Component)): 
Custom field (Attachment count): 5.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 10 16:34:45 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1jpe8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Aug/23 16:27;tafranky@gmail.com;WSCG  generated code that calls  generate_doConsume_0

!image-2023-08-10-09-27-24-124.png!;;;, 10/Aug/23 16:29;tafranky@gmail.com;WSCG  generated code for first Generate node 

!image-2023-08-10-09-29-24-804.png!;;;, 10/Aug/23 16:33;tafranky@gmail.com;WSCG  generated code for second Generate node 

!image-2023-08-10-09-32-46-163.png!;;;, 10/Aug/23 16:34;tafranky@gmail.com;Spark Dag for the use case . The failure is from the execution of WholeStageCodeGen(2)

!image-2023-08-10-09-33-47-788.png!;;;
Affects Version/s.1: 3.0.1, 3.2.2, 3.2.3, 3.2.4, 3.3.0, 3.3.1, 3.3.2, 3.4.0, 3.4.1
Attachment.1: 10/Aug/23 16:29;tafranky@gmail.com;image-2023-08-10-09-29-24-804.png;https://issues.apache.org/jira/secure/attachment/13062037/image-2023-08-10-09-29-24-804.png
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.11.0, EMR-6.11.1, EMR-6.12.0, EMR-6.13.0, EMR-6.14.0, EMR-6.15.0, EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Reject disk blocks when out of disk space
Issue key: SPARK-34337
Issue id: 13356310
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 02/Feb/21 22:25
Updated: 09/Aug/23 18:34
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Now that we have the ability to store shuffle blocks on dis-aggregated storage (when configured) we should add the option to reject storing blocks locally on an executor at a certain disk pressure threshold.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Feb 04 17:35:47 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0na5c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 4.0.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Feb/21 17:35;holden;Initially we should allow the user to configure a maximum amount of shuffle blocks to be stored. In the future we can try and use underlying FS info.;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Driver can't distribute task to executor because NullPointerException
Issue key: SPARK-35914
Issue id: 13386210
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Heltman
Creator: Heltman
Created: 28/Jun/21 08:25
Updated: 26/Jul/23 06:22
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.1, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: When use spark3 submit a spark job to yarn cluster, I get a problem. Once in a while, driver can't distribute any tasks to any executors, and the stage will stuck , the total spark job will stuck. Check driver log, I found NullPointerException. It's like a netty problem, I can confirm this problem only exist in spark3, because I use spark2 never happend.

 
{code:java}
// Error message
21/06/28 14:42:43 INFO TaskSetManager: Starting task 2592.0 in stage 1.0 (TID 3494) (worker39.hadoop, executor 84, partition 2592, RACK_LOCAL, 5006 bytes) taskResourceAssignments Map()
21/06/28 14:42:43 INFO TaskSetManager: Finished task 4155.0 in stage 1.0 (TID 3367) in 36670 ms on worker39.hadoop (executor 84) (3278/4249)
21/06/28 14:42:43 INFO TaskSetManager: Finished task 2283.0 in stage 1.0 (TID 3422) in 22371 ms on worker15.hadoop (executor 109) (3279/4249)
21/06/28 14:42:43 ERROR Inbox: Ignoring error
java.lang.NullPointerException
	at java.lang.String.length(String.java:623)
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:420)
	at java.lang.StringBuilder.append(StringBuilder.java:136)
	at org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$5(TaskSetManager.scala:483)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.scheduler.TaskSetManager.logInfo(TaskSetManager.scala:54)
	at org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:484)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:444)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:581)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:576)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:576)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:547)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:547)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.$anonfun$makeOffers$5(CoarseGrainedSchedulerBackend.scala:340)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$withLock(CoarseGrainedSchedulerBackend.scala:904)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers(CoarseGrainedSchedulerBackend.scala:332)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:157)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
21/06/28 14:42:43 INFO TaskSetManager: Finished task 2255.0 in stage 1.0 (TID 3419) in 23035 ms on worker15.hadoop (executor 116) (3280/4249)
21/06/28 14:42:43 ERROR Inbox: Ignoring error
java.lang.NullPointerException
	at java.lang.String.length(String.java:623)
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:420)
	at java.lang.StringBuilder.append(StringBuilder.java:136)
	at org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$5(TaskSetManager.scala:483)
	at org.apache.spark.internal.Logging.logInfo(Logging.scala:57)
	at org.apache.spark.internal.Logging.logInfo$(Logging.scala:56)
	at org.apache.spark.scheduler.TaskSetManager.logInfo(TaskSetManager.scala:54)
	at org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:484)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:444)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:397)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:392)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:392)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:383)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:581)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:576)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:576)
	at org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:547)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:547)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.$anonfun$makeOffers$5(CoarseGrainedSchedulerBackend.scala:340)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$$withLock(CoarseGrainedSchedulerBackend.scala:904)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$makeOffers(CoarseGrainedSchedulerBackend.scala:332)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:157)
	at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
	at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
	at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
.....

21/06/28 14:43:44 INFO TaskSetManager: Finished task 155.0 in stage 1.0 (TID 3486) in 64503 ms on worker02.hadoop (executor 59) (3427/4249)
21/06/28 14:43:44 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:45 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:46 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:47 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:48 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:49 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:49 INFO TaskSetManager: Finished task 189.0 in stage 1.0 (TID 3491) in 66320 ms on worker02.hadoop (executor 62) (3428/4249)
21/06/28 14:43:49 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:50 ERROR Inbox: Ignoring error
java.lang.NullPointerException
21/06/28 14:43:51 ERROR Inbox: Ignoring error
java.lang.NullPointerException{code}
 

 

The first time I found this problem in spark3.0.1, I checked spark source code, the NPE happended in  *org.apache.spark.scheduler.TaskSetManager.logInfo*, source code like below:
{code:java}
logInfo(s"Starting $taskName (TID $taskId, $host, executor ${info.executorId}, " +
    s"partition ${task.partitionId}, $taskLocality, ${serializedTask.limit()} bytes)")
{code}
I tried to confirm which variable is null, so I add log and repack spark-core like below:
{code:java}
// add log:
logInfo(s"zyh Starting $taskName")
logInfo(s"zyh (TID $taskId")
logInfo(s"zyh $host")
logInfo(s"zyh executor ${info.executorId}")
logInfo(s"zyh ${task.partitionId}")
logInfo(s"zyh $taskLocality,")
logInfo(s"zyh ${serializedTask.limit()} bytes)")

// repack command
./build/mvn -DskipTests -pl :spark-core_2.12 clean install{code}
Then I got *host* is null, try print:
{code:java}
println(host) // result is Some(null)
println(host.getClass.getName) // result is scala.Some
{code}
track host, add log everywhere to print it:

{{TaskSchedulerImpl.resourceOfferSingleTaskSet}} -> {{TaskSetManager.resourceOffer}}-> {{TaskSetManager.dequeueTask}} -> {{TaskSetManager.logInfo}}

Strange host is nomal ! But executorId become null... This two variable came from one case class  *WorkerOffer*

So I add log to print executorId everywhere, the problem disappered !!!
{code:java}
// TaskSchedulerImpl.resourceOfferSingleTaskSet
 private def resourceOfferSingleTaskSet(
      taskSet: TaskSetManager,
      maxLocality: TaskLocality,
      shuffledOffers: Seq[WorkerOffer],
      availableCpus: Array[Int],
      availableResources: Array[Map[String, Buffer[String]]],
      tasks: IndexedSeq[ArrayBuffer[TaskDescription]],
      addressesWithDescs: ArrayBuffer[(String, TaskDescription)]) : Boolean = {
    var launchedTask = false    // nodes and executors that are blacklisted for the entire application have already been
    // filtered out by this point
    for (i <- 0 until shuffledOffers.size) {
      val execId = shuffledOffers(i).executorId
      val host = shuffledOffers(i).host
      if (availableCpus(i) >= CPUS_PER_TASK &&
        resourcesMeetTaskRequirements(availableResources(i))) {
        try {
          val time = System.currentTimeMillis()
          // the first print
          // scalastyle:off println
          println(s"==zyh1== $time" + host.getClass.getName)
          println(s"==zyh1== $time" + execId.getClass.getName)
          for (task <- taskSet.resourceOffer(execId, host, maxLocality, availableResources(i))) {// TaskSetManager.resourceOffer
  def resourceOffer(
      execId: String,
      host: String,
      maxLocality: TaskLocality.TaskLocality,
      availableResources: Map[String, Seq[String]] = Map.empty)
    : Option[TaskDescription] =
  {
    val time = System.currentTimeMillis()
    // the second print
    // scalastyle:off println
    println(s"==zyh2== $time" + host.getClass.getName)
    println(s"==zyh2== $time" + execId.getClass.getName)
      
    val offerBlacklisted = taskSetBlacklistHelperOpt.exists { blacklist =>
      blacklist.isNodeBlacklistedForTaskSet(host) ||
        blacklist.isExecutorBlacklistedForTaskSet(execId)
    }
    if (!isZombie && !offerBlacklisted) {
      val curTime = clock.getTimeMillis()      var allowedLocality = maxLocality      if (maxLocality != TaskLocality.NO_PREF) {
        allowedLocality = getAllowedLocalityLevel(curTime)
        if (allowedLocality > maxLocality) {
          // We're not allowed to search for farther-away tasks
          allowedLocality = maxLocality
        }
      }      dequeueTask(execId, host, allowedLocality).map { case ((index, taskLocality, speculative)) =>
          
// TaskSetManager.dequeueTask
private def dequeueTask(
      execId: String,
      host: String,
      maxLocality: TaskLocality.Value): Option[(Int, TaskLocality.Value, Boolean)] = {
    // Tries to schedule a regular task first; if it returns None, then schedules
    // a speculative task
    val time = System.currentTimeMillis()
    // the third print
    // scalastyle:off println
    println(s"==zyh3== $time" + host.getClass.getName)
    println(s"==zyh3== $time" + execId.getClass.getName)
    dequeueTaskHelper(execId, host, maxLocality, false).orElse(
      dequeueTaskHelper(execId, host, maxLocality, true))
  }
          
// TaskSetManager.resourceOffer.dequeueTask，上面返回后进入map时再次打印
dequeueTask(execId, host, allowedLocality).map { case ((index, taskLocality, speculative)) =>
        // Found a task; do some bookkeeping and return a task description
        val time = System.currentTimeMillis()
        // // the fourth print
        // scalastyle:off println
        println(s"==zyh4== $time" + host.getClass.getName)
        println(s"==zyh4== $time" + execId.getClass.getName)
    
// TaskSetManager.logInfo
    // try catch NPE
        val taskName = s"task ${info.id} in stage ${taskSet.id}"
        try {
          logInfo(s"Starting $taskName (TID $taskId, $host, executor ${info.executorId}, " +
            s"partition ${task.partitionId}, $taskLocality, ${serializedTask.limit()} bytes)")
        } catch {
          case e: NullPointerException =>
            val time = System.currentTimeMillis()
            e.printStackTrace()
            // scalastyle:off println
            println("+++zyh+++" + host.getClass.getName + "  " + taskName.getClass.getName)
            println("+++zyh+++" + taskId.getClass.getName + "  " + info.executorId.getClass.getName)
            println("+++zyh+++" + task.partitionId.getClass.getName)
            println("+++zyh+++" + taskLocality.getClass.getName)
            println("+++zyh+++" + serializedTask.limit().getClass.getName)
        }
{code}
This problem make it's difficult to trust a spark job is running or not, only when I check spark web ui and found it's not any running task, I can kill it by hand. I test on spark3.0.1 3.1.1 3.1.2, it's same problem.
Environment: hadoop 2.6.0-cdh5.7.1

Spark 3.0.1, 3.1.1, 3.1.2
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 28/Jun/21 09:07;Heltman;stuck log.png;https://issues.apache.org/jira/secure/attachment/13027349/stuck+log.png, 28/Jun/21 09:07;Heltman;webui stuck.png;https://issues.apache.org/jira/secure/attachment/13027348/webui+stuck.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jul 26 06:22:05 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sczk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Jun/21 03:02;Heltman;I guess this problem is related to hadoop-version, I use CDH-5.7.1:hadoop-2.6.5, spark3 use hadoop-2.7. Because the other problem I found in spark about webui, it's caused by the version. I will try higher hadoop-version to confirm it.

[SPARK-35802] Error loading the stages/stage/<id> page in spark UI - ASF JIRA (apache.org);;;, 29/Jun/21 14:12;code_kr_dev_s;[~Heltman] Hi, I want to work upon this issue. If you are not working can I work upon it?;;;, 30/Jun/21 01:21;Heltman;Do what you wanna do, [~code_kr_dev_s], but this problem is difficult to recurrent, it's random occurrence. If you need any help, tell me!;;;, 26/Jul/23 06:22;bhargwa;Hey,

We are facing similar issue and we are using spark3.1.1 with hadoop3.2.
Is this issue resolved in further versions ? If yes, can you guys let me know the fixed version.
OR If there is any workaround to solve this issue, Please let me know.;;;
Affects Version/s.1: 3.1.1
Attachment.1: 28/Jun/21 09:07;Heltman;webui stuck.png;https://issues.apache.org/jira/secure/attachment/13027348/webui+stuck.png
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Jobs that have join & have .rdd calls get executed 2x when AQE is enabled.
Issue key: SPARK-44378
Issue id: 13543172
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: priyankar
Creator: priyankar
Created: 11/Jul/23 16:35
Updated: 12/Jul/23 19:26
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Submit
Due Date: 
Votes: 0
Labels: aqe
Description: We have a few spark scala jobs that are currently running in production. Most jobs typically use Dataset, Dataframes. There is a small code in our custom library code, that makes rdd calls example to check if the dataframe is empty: df.rdd.getNumPartitions == 0

When I enable aqe for these jobs, this .rdd is converted into a separate job of it's own and the entire dag is executed 2x, taking 2x more time. This does not happen when AQE is disabled. Why does this happen and what is the best way to fix the issue?

 

Sample code to reproduce the issue:

 

 
{code:java}
import org.apache.spark.sql._ 
  case class Record(
    id: Int,
    name: String
 )
 
    val partCount = 4
    val input1 = (0 until 100).map(part => Record(part, "a"))
 
    val input2 = (100 until 110).map(part => Record(part, "c"))
 
    implicit val enc: Encoder[Record] = Encoders.product[Record]
 
    val ds1 = spark.createDataset(
      spark.sparkContext
        .parallelize(input1, partCount)
    )
 
    va


l ds2 = spark.createDataset(
      spark.sparkContext
        .parallelize(input2, partCount)
    )
 
    val ds3 = ds1.join(ds2, Seq("id"))
    val l = ds3.count()
 
    val incomingPartitions = ds3.rdd.getNumPartitions
    log.info(s"Num partitions ${incomingPartitions}")
  {code}
 

Spark UI for the same job with AQE,  !Screenshot 2023-07-11 at 9.36.14 AM.png!

 

Spark UI for the same job without AQE:

 

!Screenshot 2023-07-11 at 9.36.19 AM.png!

 

This is causing unexpected regression in our jobs when we try to enable AQE for our jobs in production. We use spark 3.1 in production, but I can see the same behavior in spark 3.2 from the console as well

 

!image2.png!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 11/Jul/23 16:37;priyankar;Screenshot 2023-07-11 at 9.36.14 AM.png;https://issues.apache.org/jira/secure/attachment/13061243/Screenshot+2023-07-11+at+9.36.14+AM.png, 11/Jul/23 16:38;priyankar;Screenshot 2023-07-11 at 9.36.19 AM.png;https://issues.apache.org/jira/secure/attachment/13061244/Screenshot+2023-07-11+at+9.36.19+AM.png, 11/Jul/23 16:41;priyankar;image2.png;https://issues.apache.org/jira/secure/attachment/13061245/image2.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 3.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Scala
Custom field (Last public comment date): 2023-07-11 16:35:22.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1j308:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 11/Jul/23 16:38;priyankar;Screenshot 2023-07-11 at 9.36.19 AM.png;https://issues.apache.org/jira/secure/attachment/13061244/Screenshot+2023-07-11+at+9.36.19+AM.png
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: when shuffle hash join is enabled, q95 performance deteriorates
Issue key: SPARK-43526
Issue id: 13536408
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 16/May/23 13:16
Updated: 19/May/23 03:54
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Testing with 5TB dataset, the performance of q95 in tpcds deteriorates when shuffle hash join is enabled and the performance is better when sortMergeJoin is used.

 
Performance difference:  from 3.9min(sortMergeJoin) to 8.1min(shuffledHashJoin)
 

1. enable shuffledHashJoin, the execution plan is as follows:

!image-2023-05-16-21-28-44-163.png|width=935,height=64!

!image-2023-05-16-21-21-35-493.png|width=924,height=502!

2. disable shuffledHashJoin, the execution plan is as follows:

!image-2023-05-16-21-28-11-514.png|width=922,height=67!

!image-2023-05-16-21-22-16-170.png|width=934,height=477!

 

And when shuffledHashJoin is enabled, gc is very serious,

!image-2023-05-16-21-23-35-237.png|width=929,height=570!

 

but sortMergeJoin executes without this problem.

!image-2023-05-16-21-24-09-182.png|width=931,height=573!

 

Any suggestions on how to solve it？Thanks!

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 16/May/23 13:21;caican;image-2023-05-16-21-21-35-493.png;https://issues.apache.org/jira/secure/attachment/13058251/image-2023-05-16-21-21-35-493.png, 16/May/23 13:22;caican;image-2023-05-16-21-22-16-170.png;https://issues.apache.org/jira/secure/attachment/13058252/image-2023-05-16-21-22-16-170.png, 16/May/23 13:23;caican;image-2023-05-16-21-23-35-237.png;https://issues.apache.org/jira/secure/attachment/13058255/image-2023-05-16-21-23-35-237.png, 16/May/23 13:24;caican;image-2023-05-16-21-24-09-182.png;https://issues.apache.org/jira/secure/attachment/13058256/image-2023-05-16-21-24-09-182.png, 16/May/23 13:28;caican;image-2023-05-16-21-28-11-514.png;https://issues.apache.org/jira/secure/attachment/13058257/image-2023-05-16-21-28-11-514.png, 16/May/23 13:28;caican;image-2023-05-16-21-28-44-163.png;https://issues.apache.org/jira/secure/attachment/13058258/image-2023-05-16-21-28-44-163.png, 17/May/23 08:53;caican;image-2023-05-17-16-53-42-302.png;https://issues.apache.org/jira/secure/attachment/13058288/image-2023-05-17-16-53-42-302.png, 17/May/23 08:55;caican;image-2023-05-17-16-54-59-053.png;https://issues.apache.org/jira/secure/attachment/13058289/image-2023-05-17-16-54-59-053.png, 19/May/23 02:43;caican;image-2023-05-19-10-43-51-747.png;https://issues.apache.org/jira/secure/attachment/13058339/image-2023-05-19-10-43-51-747.png, 19/May/23 02:42;caican;shuffle1.png;https://issues.apache.org/jira/secure/attachment/13058337/shuffle1.png, 19/May/23 02:42;caican;sort1.png;https://issues.apache.org/jira/secure/attachment/13058336/sort1.png, 19/May/23 02:43;caican;sort2.png;https://issues.apache.org/jira/secure/attachment/13058338/sort2.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 12.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri May 19 03:54:45 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1hxf4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/May/23 04:13;yumwang;Why do you prefer shuffle hash join?;;;, 17/May/23 09:01;caican;[~yumwang] 
Tpcds tests show performance gains for most queries and we plan to use shuffledHashJoin preferentially to eliminate sort consumption when the small table meets a certain threshold, but q95 in tpcds has a serious performance regression and we are not sure if it can be turned on by default.

with shuffledHashJoin:

!image-2023-05-17-16-53-42-302.png|width=691,height=344!

sortMergeJoin is preferred:

!image-2023-05-17-16-54-59-053.png|width=722,height=319!;;;, 19/May/23 02:44;caican;gently ping [~yumwang] 

I find that the shuffle hash join is slower than the sort merge join because the sort node is added after two shuffle hash joins, and the number of data bars of the two shuffle hash joins expands a lot.

 

I overwrote q95, after closing shuffle hash join and adding sort operation after corresponding join nodes, q95 execution also became slow.

 

1. The execution plan before I rewrite q95 sql is as follows:

*Sort merge join*

!sort1.png|width=926,height=473!

*shuffle hash join*

!shuffle1.png|width=921,height=441!

 

2. The execution plan after I rewrite q95 sql is as follows:

*sort merge join*

!sort2.png|width=936,height=496!

 

The sort operation was added after the corresponding join nodes, and the execution was slower than shuffle hash join.

And it can be confirmed that the performance deteriorates after the shuffle hash join function is enabled because a large amount of data is sorted.

!image-2023-05-19-10-43-51-747.png|width=708,height=38!

 

*q95 sql with sort operation added*

 
{code:java}
 
set spark.sql.optimizer.excludedRules="org.apache.spark.sql.catalyst.optimizer.EliminateSorts";

set spark.sql.execution.removeRedundantSorts=false;

WITH
    ws_wh AS (
        SELECT
            ws1.ws_order_number,
            ws1.ws_warehouse_sk wh1,
            ws2.ws_warehouse_sk wh2
        FROM
            web_sales ws1,
            web_sales ws2
        WHERE
            ws1.ws_order_number=ws2.ws_order_number
            AND ws1.ws_warehouse_sk<>ws2.ws_warehouse_sk
        SORT BY
            ws1.ws_order_number
    ),
    tmp1 as (
        SELECT
            ws_order_number
        FROM
            ws_wh
    ),
    tmp2 as (
        SELECT
            wr_order_number
        FROM
            web_returns,
            ws_wh
        WHERE
            wr_order_number=ws_wh.ws_order_number
        SORT BY
            wr_order_number
    )
SELECT
    count(DISTINCT ws_order_number) AS `order count `,
    sum(ws_ext_ship_cost) AS `total shipping cost `,
    sum(ws_net_profit) AS `total net profit `
FROM
    web_sales ws1
    left semi join tmp1 on ws1.ws_order_number=tmp1.ws_order_number
    left semi join tmp2 on ws1.ws_order_number=tmp2.wr_order_number
    join date_dim on ws1.ws_ship_date_sk=date_dim.d_date_sk
    join customer_address on ws1.ws_ship_addr_sk=customer_address.ca_address_sk
    join web_site on ws1.ws_web_site_sk=web_site.web_site_sk
WHERE
    d_date BETWEEN '1999-02-01' AND (CAST('1999-02-01' AS DATE)+INTERVAL 60 DAY)
    AND ws1.ws_ship_date_sk=d_date_sk
    AND ws1.ws_ship_addr_sk=ca_address_sk
    AND ca_state='IL'
    AND ws1.ws_web_site_sk=web_site_sk
    AND web_company_name='pri'
ORDER BY
    count(DISTINCT ws_order_number)
LIMIT
    100{code}
 ;;;, 19/May/23 03:54;yumwang;[~caican] Thank you for investigation.;;;
Affects Version/s.1: 3.2.0
Attachment.1: 16/May/23 13:22;caican;image-2023-05-16-21-22-16-170.png;https://issues.apache.org/jira/secure/attachment/13058252/image-2023-05-16-21-22-16-170.png, 19/May/23 02:42;caican;sort1.png;https://issues.apache.org/jira/secure/attachment/13058336/sort1.png, 19/May/23 02:43;caican;sort2.png;https://issues.apache.org/jira/secure/attachment/13058338/sort2.png
EMR Versions: EMR-6.11.0, EMR-6.11.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Union does not propagate Metadata output
Issue key: SPARK-41498
Issue id: 13513183
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: fred-db
Creator: fred-db
Created: 12/Dec/22 12:41
Updated: 14/Mar/23 15:39
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.3.0, 3.3.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, the Union operator does not propagate any metadata output. This makes it impossible to access any metadata if a Union operator is used, even though the children have the exact same metadata output.
Example:

 
{code:java}
val df1 = spark.read.load(path1)
val df2 = spark.read.load(path2)
df1.union(df2).select("_metadata.file_path"). // <-- fails{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 14 15:39:03 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1dyy8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Dec/22 12:47;apachespark;User 'fred-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/38941;;;, 23/Dec/22 11:39;cloud_fan;Issue resolved by pull request 38941
[https://github.com/apache/spark/pull/38941];;;, 10/Mar/23 16:09;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/40371;;;, 10/Mar/23 18:31;dongjoon;This is reverted via https://github.com/apache/spark/commit/164db5ba3c39614017f5ef6428194a442d79b425;;;, 14/Mar/23 15:39;dongjoon;I removed the `Target Version`.;;;
Affects Version/s.1: 3.1.3
Attachment.1: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: delete uploaded file when job finish
Issue key: SPARK-42744
Issue id: 13527941
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ziqian hu
Creator: ziqian hu
Created: 10/Mar/23 08:07
Updated: 10/Mar/23 09:19
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: On kubernetes, spark use spark.kubernetes.file.upload.path to upload local files to 

Hadoop compatible file system. But spark do not delete those files at all.

In this issue, we add a configuration spark.kubernetes.uploaded.file.delete.on.termination to delete those files by driver when job finishs.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Mar 10 09:19:35 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ghhk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Mar/23 09:19;ziqian hu;add pr https://github.com/apache/spark/pull/40363;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: access apiserver by pod env
Issue key: SPARK-42742
Issue id: 13527924
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ziqian hu
Creator: ziqian hu
Created: 10/Mar/23 06:28
Updated: 10/Mar/23 06:57
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: When start spark on k8s，driver pod  use spark.kubernetes.driver.master to get apiserver address. This config  us  [https://kubernetes.default.svc|https://kubernetes.default.svc/] as default and do not care about the apiserver port.

In our case, apiserver port is not 443 will driver will throw connectException. As k8s doc mentioned （https://kubernetes.io/docs/tasks/run-application/access-api-from-pod/#directly-accessing-the-rest-api）, we can get master url by getting {{KUBERNETES_SERVICE_HOST}} and {{KUBERNETES_SERVICE_PORT_HTTPS}} environment variables from pod. So we add a new conf spark.kubernetes.driver.master.from.pod.env to allow driver get master url from env in cluster mode on k8s
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Mar 10 06:57:19 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ghds:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Mar/23 06:57;ziqian hu;Created PR [https://github.com/apache/spark/pull/40361] for this issue.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: timestamp type column analyze result is wrong
Issue key: SPARK-36604
Issue id: 13397941
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yghu
Creator: yghu
Created: 28/Aug/21 07:39
Updated: 02/Mar/23 19:08
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: when we create table with timestamp column type, the min and max data of the analyze result for the timestamp column is wrong

eg:

{code}
> select * from a;
{code}

{code}
2021-08-15 15:30:01
Time taken: 2.789 seconds, Fetched 1 row(s)
spark-sql> desc formatted a a;
col_name a
data_type timestamp
comment NULL
min 2021-08-15 07:30:01.000000
max 2021-08-15 07:30:01.000000
num_nulls 0
distinct_count 1
avg_col_len 8
max_col_len 8
histogram NULL
Time taken: 0.278 seconds, Fetched 10 row(s)
spark-sql> desc a;
a timestamp NULL
Time taken: 1.432 seconds, Fetched 1 row(s)
{code}

 

reproduce step:

{code}
create table a(a timestamp);
insert into a select '2021-08-15 15:30:01';
analyze table a compute statistics for columns a;
desc formatted a a;
select * from a;
{code}

Environment: Spark 3.1.1
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Mar 02 19:08:56 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0udag:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Aug/21 08:53;yghu;I'd like to work on this.;;;, 31/Aug/21 04:49;senthh;[~yghu] I tested this scenario in Spark2.4, but I don't see this issue is occurring.   Are you seeing this issue only in Spark 3.1.1? 

 

 
{panel}


 

_scala> spark.sql("create table c(a timestamp)")_

_res16: org.apache.spark.sql.DataFrame = []_

 __ 

_scala> spark.sql("insert into c select '2021-08-15 15:30:01'")_

_res17: org.apache.spark.sql.DataFrame = []_

 __ 

_scala> spark.sql("analyze table c compute statistics for columns a")_

_res18: org.apache.spark.sql.DataFrame = []_

 __ 

_scala> spark.sql("desc formatted c a").show(true)_

_+--------------+--------------------+_

_|     info_name|          info_value|_

_+--------------+--------------------+_

_|      col_name|                   a|_

_|     data_type|           timestamp|_

_|       comment|                NULL|_

_|           min|2021-08-15 15:30:...|_

_|           max|2021-08-15 15:30:...|_

_|     num_nulls|                   0|_

_|distinct_count|                   1|_

_|   avg_col_len|                   8|_

_|   max_col_len|                   8|_

_|     histogram|                NULL|_

_+--------------+--------------------+_

 
{panel}
 ;;;, 31/Aug/21 06:13;yghu;[~senthh]  i tested with spark2.4.5 also don't have this issue, i checked code  maybe it's caused by this commit: https://github.com/apache/spark/pull/23662/files;;;, 01/Sep/21 07:30;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33886;;;, 14/Apr/22 01:28;yghu;[~senthh] what's the session time zone?

i tested with spark 3.2.1 alse have the issue. The value's '2021-08-15 15:30:01', while the min/max value is 8 hours diff.

scala>  spark.sql("insert into c select '2021-08-15 15:30:01'")
22/04/14 09:23:36 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
res3: org.apache.spark.sql.DataFrame = []

scala> spark.sql("analyze table c compute statistics for columns a")
res4: org.apache.spark.sql.DataFrame = []                                       

scala> spark.sql("desc formatted c a").show(true)
+--------------+--------------------+
|     info_name|          info_value|
+--------------+--------------------+
|      col_name|                   a|
|     data_type|           timestamp|
|       comment|                NULL|
|           min|2021-08-15 07:30:...|
|           max|2021-08-15 07:30:...|
|     num_nulls|                   0|
|distinct_count|                   1|
|   avg_col_len|                   8|
|   max_col_len|                   8|
|     histogram|                NULL|
+--------------+--------------------+


scala> sql("set spark.sql.session.timeZone").show
+--------------------+-------------+
|                 key|        value|
+--------------------+-------------+
|spark.sql.session...|Asia/Shanghai|
+--------------------+-------------+;;;, 02/Mar/23 19:08;ritikam;Seems to be working correctly in  Spark 3.3.0

spark-sql> insert into a values(cast('2021-08-15 15:30:01' as timestamp)
         > );
23/03/02 11:04:11 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Time taken: 3.278 seconds
spark-sql> select * from a;
2021-08-15 15:30:01
Time taken: 0.782 seconds, Fetched 1 row(s)
spark-sql> analyze table a compute statistics for columns a;
Time taken: 1.882 seconds
spark-sql> desc formatted a a;
col_name        a
data_type       timestamp
comment NULL
min     2021-08-15 15:30:01.000000 -0700
max     2021-08-15 15:30:01.000000 -0700
num_nulls       0
distinct_count  1
avg_col_len     8
max_col_len     8
histogram       NULL
Time taken: 0.095 seconds, Fetched 10 row(s)
spark-sql> desc a;
a                       timestamp                                   
Time taken: 0.059 seconds, Fetched 1 row(s)
spark-sql>;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0

Summary: Make "SPARK-34674: Close SparkContext after the Main method has finished" configurable
Issue key: SPARK-42219
Issue id: 13521805
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: attilapiros
Reporter: attilapiros
Creator: attilapiros
Created: 27/Jan/23 19:59
Updated: 27/Jan/23 21:48
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.1.3, 3.2.0, 3.2.1, 3.2.2, 3.2.3, 3.3.0, 3.3.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: We run into an error after an upgrade from Spark 3.1 to Spark 3.2 cased by SPARK-34674 which closed the SparkContext right after the application start. This application was a spark job server built on top of springboot so all the job submits were outside of the main method.


Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jan 27 21:48:24 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1ffrc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Jan/23 20:18;attilapiros;I will open a PR regarding this;;;, 27/Jan/23 21:47;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/39775;;;, 27/Jan/23 21:48;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/39775;;;
Affects Version/s.1: 3.1.3
Attachment.1: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Spark streaming query scheduling synchronisation  with Trigger Interval
Issue key: SPARK-41665
Issue id: 13515090
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tprelle-ubi
Creator: tprelle-ubi
Created: 21/Dec/22 12:57
Updated: 21/Dec/22 13:59
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.0.3, 3.1.2, 3.2.2, 3.3.1
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Hi,
We detect a strange behavior on spark streaming when we set a trigger interval for example at 1 minutes all query will start at 0:00:00 0:01:00 0:02:00 no matter the start time of the query.
So all query are "sync", so it's can disturbed a cluster a cluster i do leads to spike of utilisation 
!image-2022-12-21-07-57-18-679.png!

For me the expected behavior should be like this

!image-2022-12-21-07-57-32-654.png!

 

It's because of this line 
[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/TriggerExecutor.scala#L98]

as now in intervalMS are long now / intervalMs * intervalMs will just cut in my case the second, as it's explicitely like this on the test ([https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/execution/streaming/ProcessingTimeExecutorSuite.scala#L36)]  i do not know if it's the expected behavior or it's juste because this line it's here since 6 years. So it's affecting all versions since 6 years. 

Regards

Thomas

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 21/Dec/22 12:57;tprelle-ubi;image-2022-12-21-07-57-18-679.png;https://issues.apache.org/jira/secure/attachment/13054046/image-2022-12-21-07-57-18-679.png, 21/Dec/22 12:57;tprelle-ubi;image-2022-12-21-07-57-32-654.png;https://issues.apache.org/jira/secure/attachment/13054047/image-2022-12-21-07-57-32-654.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-12-21 12:57:00.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1eal4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.0.3
Attachment.1: 21/Dec/22 12:57;tprelle-ubi;image-2022-12-21-07-57-32-654.png;https://issues.apache.org/jira/secure/attachment/13054047/image-2022-12-21-07-57-32-654.png
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: 'spark.archives' does not extract the archive file into the driver under client mode
Issue key: SPARK-36088
Issue id: 13389070
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: rickcheng
Creator: rickcheng
Created: 12/Jul/21 09:30
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Kubernetes, Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: When running spark in the k8s cluster, there are 2 deploy modes: cluster and client. After my test, in the cluster mode, *spark.archives* can extract the archive file to the working directory of the executors and driver. But in client mode, *spark.archives* can only extract the archive file to the working directory of the executors.

 

However, I need *spark.archives* to send the virtual environment tar file packaged by conda to both the driver and executors under client mode (So that the executor and the driver have the same python environment).

 

Why *spark.archives* does not extract the archive file into the working directory of the driver under client mode?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Dec 17 12:20:49 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sul4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Jul/21 08:10;gurwls223;does your driver run inside a pod or on a physical host?;;;, 19/Jul/21 08:12;gurwls223;You might have to call https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L393-L419 logic when {{isKubernetesClient}} is on. Are you interested in submitting a PR?;;;, 19/Jul/21 08:13;gurwls223;cc [~dongjoon] and [~holdenkarau] FYI;;;, 20/Jul/21 15:05;srowen;I think the idea is that in client mode you already have access to the file, presumably?;;;, 21/Jul/21 02:02;rickcheng;Hi, [~hyukjin.kwon]

Thanks for the comment. After my test, under client mode, the archive file will not extract to the driver's working directory no matter if the driver is in the pod or not. Thanks for pointing out the code, maybe I will consider making a PR.;;;, 21/Jul/21 02:10;rickcheng;Hi, [~srowen]

Thanks for the comment. I agree that in client mode, user can access the file in some cases. However, my original intention to raise this question was because I wanted to distribute the conda packaged environment （a tar.gz file） through *spark.archive* and extract it to the driver and executors. In this way, the driver and executors will have the same python environment. And in K8s, the driver may run in a pod and the tar.gz file may be in a remote place (e.g., HDFS). So I think it's also necessary to extract the archive file to the driver through spark.archive. Or maybe there is a better way to achieve this goal?;;;, 17/Dec/21 06:52;zhongjingxiong;In cluster mode, I hava another question that when I unzip python3.6.6.zip in pod , but no permission to execute, my execute operation as follows：

{code:sh}
spark-submit \
--archives ./python3.6.6.zip#python3.6.6 \
--conf "spark.pyspark.python=python3.6.6/python3.6.6/bin/python3" \
--conf "spark.pyspark.driver.python=python3.6.6/python3.6.6/bin/python3" \
--conf spark.kubernetes.container.image.pullPolicy=Always \
./examples/src/main/python/pi.py 100
{code}
;;;, 17/Dec/21 09:07;gurwls223;[~zhongjingxiong] can you try with tar.gz? zip doesn't keep the permissions by default.;;;, 17/Dec/21 09:12;gurwls223;[~rickcheng] can you try a fix? we can maybe try to download the files locally when we're in a pod. we can mimic the behaviour of spark.files on driver side.;;;, 17/Dec/21 12:20;zhongjingxiong;@hyukjin.kwon I make an issue at https://issues.apache.org/jira/browse/SPARK-37677, I think I can fix it.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: cloudpickle: ValueError: Cell is empty
Issue key: SPARK-36476
Issue id: 13394577
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: oliman
Creator: oliman
Created: 11/Aug/21 07:26
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: {code:java}
  File "/Users/tekumara/code/awesome-spark-app/.venv/lib/python3.7/site-packages/pyspark/serializers.py", line 437, in dumps
    return cloudpickle.dumps(obj, pickle_protocol)
  File "/Users/tekumara/code/awesome-spark-app/.venv/lib/python3.7/site-packages/pyspark/cloudpickle/cloudpickle_fast.py", line 101, in dumps
    cp.dump(obj)
  File "/Users/tekumara/code/awesome-spark-app/.venv/lib/python3.7/site-packages/pyspark/cloudpickle/cloudpickle_fast.py", line 540, in dump
    return Pickler.dump(self, obj)
  File "/Users/tekumara/.pyenv/versions/3.7.9/lib/python3.7/pickle.py", line 437, in dump
    self.save(obj)
  File "/Users/tekumara/.pyenv/versions/3.7.9/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/Users/tekumara/.pyenv/versions/3.7.9/lib/python3.7/pickle.py", line 789, in save_tuple
    save(element)
  File "/Users/tekumara/.pyenv/versions/3.7.9/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/Users/tekumara/code/awesome-spark-app/.venv/lib/python3.7/site-packages/pyspark/cloudpickle/cloudpickle_fast.py", line 722, in save_function
    *self._dynamic_function_reduce(obj), obj=obj
  File "/Users/tekumara/code/awesome-spark-app/.venv/lib/python3.7/site-packages/pyspark/cloudpickle/cloudpickle_fast.py", line 659, in _save_reduce_pickle5
    dictitems=dictitems, obj=obj
  File "/Users/tekumara/.pyenv/versions/3.7.9/lib/python3.7/pickle.py", line 638, in save_reduce
    save(args)
  File "/Users/tekumara/.pyenv/versions/3.7.9/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/Users/tekumara/.pyenv/versions/3.7.9/lib/python3.7/pickle.py", line 789, in save_tuple
    save(element)
  File "/Users/tekumara/.pyenv/versions/3.7.9/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/Users/tekumara/.pyenv/versions/3.7.9/lib/python3.7/pickle.py", line 774, in save_tuple
    save(element)
  File "/Users/tekumara/.pyenv/versions/3.7.9/lib/python3.7/pickle.py", line 504, in save
    f(self, obj) # Call unbound method with explicit self
  File "/Users/tekumara/code/awesome-spark-app/.venv/lib/python3.7/site-packages/dill/_dill.py", line 1226, in save_cell
    f = obj.cell_contents
ValueError: Cell is empty
{code}
Doesn't occur in Spark 3.0.0, so possibly introduced when cloudpickle was upgraded to 1.5.0 (see https://issues.apache.org/jira/browse/SPARK-32094).

Also doesn't occur in Spark 3.1.2 with python 3.8.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jan 27 00:38:34 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tsjc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Aug/21 07:30;oliman;Looks like a similar issue raised here https://github.com/cloudpipe/cloudpickle/issues/393;;;, 15/Aug/21 03:12;gurwls223;[~oliman]do you have a self-contained reproducer?;;;, 26/Jan/22 07:46;larroy;This seems to happen as an interaction with the package "dill" and only in Python 3.7

 

This was explained here and I verified the reproduction in my codebas: [https://stackoverflow.com/questions/69360462/conflict-between-dill-and-pickle-while-using-pyspark]

 

 

https://github.com/cloudpipe/cloudpickle/issues/393;;;, 27/Jan/22 00:38;gurwls223;Thanks man. Let's get this done in cloudpickle, and upgrade in Spark side.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Side effects between PySpark Pandas UDF and Numpy indexing
Issue key: SPARK-37449
Issue id: 13413382
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: carlosft
Creator: carlosft
Created: 23/Nov/21 12:12
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: applyInPandas, NumPy, Pandas, Pygeos, UDF
Description: Let's create a simple Pandas Dataframe with a single column named 'id' that contains a sequential range.
{code:java}
df = pd.DataFrame(np.arange(0,1000), columns=['id']){code}
Consider this function that selects the first 4 indexes of the 'id' column of an array.
{code:java}
def udf_example(df):
  
  some_index = np.array([0, 1, 2, 3])
  values = df['id'].values[some_index]
  
  df = pd.DataFrame(values, columns=['id'])
  return df{code}
If I apply this function in Pyspark I get this result:
{code:java}
schema = t.StructType([t.StructField('id', t.LongType(), True)])
df_spark = spark.createDataFrame(df).groupBy().applyInPandas(udf_example, schema)
display(df_spark)
# id
# 125
# 126
# 127
# 128
{code}
If I apply it in Python I get the correct and expected result:
{code:java}
udf_example(df)
# id
# 0
# 1
# 2
# 3
{code}
Using NumPy indexing operations inside a Pandas UDF in Spark causes side effects and unexpected results.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Nov 24 10:48:43 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0x0dc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Nov/21 01:35;gurwls223;{{applyInPandas}} does not maintain its index. Each pandas DataFrame in {{udf_example}} is a chunk of PySpark DataFrame that has a (default) index starting from one.

So, in PySpark, you would have an output of 0 ~ 3 for each chunk.;;;, 24/Nov/21 10:25;carlosft;You are right. I'm selecting the first 4 indexes of each partition. Each time I run the query partitions change, that's why I'm getting different results.

I would go as far as saying that this is not a bug. I was probably misusing the pandas udf.

Thank you for the clarification.

 ;;;, 24/Nov/21 10:48;carlosft;Sometimes there is no natural way to group a dataframe in even partitions before using a Pandas UDF (applyInPandas). This is useful for use cases that require applying vectorized functions to arbitrary chunks of the dataset.

In these situations the best workaround I found is grouping the dataframe through the partition id "spark_partition_id". This is similar to having a "mapPartitions" operation in spark dataframes.

Are there any plans to include this feature for PySpark dataframes?;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Incorrect Count when reading CSV file
Issue key: SPARK-40584
Issue id: 13483508
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tariqueanwer
Creator: tariqueanwer
Created: 27/Sep/22 17:07
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: I'm trying to read the below data from a CSV file and end up with a wrong count, although the dataframe contains all the records below. df_inputfile.count() prints 3 although it should have been 4.


{code:java}
B1123451020-502,"","{""m"": {""difference"": 60}}","","","",2022-02-12T15:40:00.783Z
B1456741975-266,"","{""m"": {""difference"": 60}}","","","",2022-02-04T17:03:59.566Z
B1789753479-460,"","",",","","",2022-02-18T14:46:57.332Z
B1456741977-123,"","{""m"": {""difference"": 60}}","","","",2022-02-04T17:03:59.566Z {code}
Here's the code:
{code:java}
df_inputfile = (spark.read.format("com.databricks.spark.csv")
                                     .option("inferSchema", "true")
                                     .option("header","false")                
                                     .option("quotedstring",'\"')
                                     .option("escape",'\"')
                                     .option("multiline","true")
                                     .option("delimiter",",")
                                     .load('<path to csv>'))

print(df_inputfile.count()) # Prints 3
print(df_inputfile.distinct().count()) # Prints 4 {code}
Adding a cache() statement before the count results in correct output. Removing the option 'escape' also results in a correct count. 
{noformat}
option("escape",'\"'){noformat}
It looks like this is happening because of the single comma in the 4th column of the 3rd row. Can someone please explain what's going on?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Oct 06 09:23:07 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18w60:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Sep/22 01:51;gurwls223;As a workaround, you can do:

{code}
df_inputfile = ...
df_inputfile  = DataFrame(spark._jsparkSession.internalCreateDataFrame(df_inputfile._jdf.queryExecution().toRdd(), df._jdf.schema(), False), df.sql_ctx)
print(...)
{code};;;, 28/Sep/22 03:56;tariqueanwer;Thank You for the workaround. It worked. Would you mind explaining the reason behind the baffling count and how does the workaround resolve it by roundtripping though an RDD?

Also why does adding a cache() statement resolve the issue?

 

 ;;;, 06/Oct/22 05:27;ivan.sadikov;Disabling "multiLine" also fixes the issue.

Seems to be an issue with the CSV file - when setting "unescapedQuoteHandling" to RAISE_ERROR although I did not debug in detail.
{code:java}
Cause: com.univocity.parsers.common.TextParsingException: Unescaped quote character '"' inside quoted value of CSV field. To allow unescaped quotes, set 'parseUnescapedQuotes' to 'true' in the CSV parser settings. Cannot parse CSV input.
[info] Internal state when error was thrown: line=2, column=3, record=1, charIndex=121, headers=[1, , {"m": {"difference": 60}}, , , , 2022-02-12T15:40:00.783Z]
[info]   at com.univocity.parsers.csv.CsvParser.handleValueSkipping(CsvParser.java:241)
[info]   at com.univocity.parsers.csv.CsvParser.handleUnescapedQuote(CsvParser.java:319) {code};;;, 06/Oct/22 09:23;tariqueanwer;The actual file contains multiline rows, so the "multiline" option cannot be disabled. The sample above is the smallest snippet of data which is resulting in this strange issue. ;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: pyspark dataframe methods (i.e. show()) won't run in VSCode debug console
Issue key: SPARK-40523
Issue id: 13482647
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: elipersonal
Creator: elipersonal
Created: 21/Sep/22 20:35
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: when debugging pyspark codes in VSCode using the python debugger, when execution is paused on a breakpoint, you can issue statement/expression in VSCode debug console to check dataframe's content, etc.

However, those statement related to df operation always gets stuck and then debugger throws a timeout error in the debug console. 

 

This issue is initially reported in stack overflow: [https://stackoverflow.com/questions/65739467/pyspark-dataframe-methods-i-e-show-can-not-be-printed-in-vs-code-debug-cons]

there are some workaround suggestions as well in that thread.

OS: win 10
VSCode: 1.64.0

Python extension in VScode: v2022.4.1
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 22 10:41:10 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18qyg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Sep/22 10:41;gurwls223;Is this a pyspark issue? or VSCode issue?;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Serie to Scalar pandas_udf in GroupedData.agg() breaks the following monotonically_increasing_id()
Issue key: SPARK-35745
Issue id: 13383506
Parent id: 
Issue Type: Bug
Status: Reopened
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Marsu_
Creator: Marsu_
Created: 11/Jun/21 21:21
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Hello,

I encountered an issue when using a Serie to Scalar `{{panda_udf}}` in `{{GroupedData.agg()}}` followed by `{{monotonically_increasing_id()}}`. I obtain duplicated ids. Actually, the partition offset in the id seems to be zero on all partitions. The problem is avoided by using `{{asNondeterministic}}`.

Minimal reproducing example
{code:java}
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql.functions import pandas_udf
import pandas as pd
from pyspark.sql.types import IntegerType

spark = SparkSession.builder\
.config("spark.sql.execution.arrow.pyspark.enabled", "true")\
.config("spark.sql.shuffle.partitions", "8")\
.master("local[4]").getOrCreate()

@pandas_udf(IntegerType())
def sum_pandas(vals: pd.Series) -> int:
    return int(vals.to_numpy().sum())

@pandas_udf(IntegerType())
def sum_pandas2(vals: pd.Series) -> int: 
    return int(vals.to_numpy().sum())

sum_pandas2 = sum_pandas2.asNondeterministic()

l = [(i%100,i) for i in range(2000)]

data = spark.createDataFrame(l, schema=["col1","col2"])
data.groupby("col1").agg(sum_pandas("col2").alias("sum"))\
.withColumn("group_id", F.monotonically_increasing_id()).show()

data = spark.createDataFrame(l, schema=["col1","col2"])
data.groupby("col1").agg(sum_pandas2("col2").alias("sum"))\
.withColumn("group_id", F.monotonically_increasing_id()).show(){code}
Output
{code:java}
+----+-----+--------+
|col1|  sum|group_id|
+----+-----+--------+
|   2|19040|       0|
|  12|19240|       1|
|  26|19520|       2|
|  28|19560|       3|
|  29|19580|       4|
|  30|19600|       5|
|  33|19660|       6|
|  42|19840|       7|
|  48|19960|       8|
|  67|20340|       9|
|  73|20460|      10|
|  88|20760|      11|
|  91|20820|      12|
|  93|20860|      13|
|   9|19180|       0|
|  11|19220|       1|
|  22|19440|       2|
|  32|19640|       3|
|  36|19720|       4|
|  40|19800|       5|
+----+-----+--------+
only showing top 20 rows

+----+-----+----------+
|col1|  sum|  group_id|
+----+-----+----------+
|   2|19040|         0|
|  12|19240|         1|
|  26|19520|         2|
|  28|19560|         3|
|  29|19580|         4|
|  30|19600|         5|
|  33|19660|         6|
|  42|19840|         7|
|  48|19960|         8|
|  67|20340|         9|
|  73|20460|        10|
|  88|20760|        11|
|  91|20820|        12|
|  93|20860|        13|
|   9|19180|8589934592|
|  11|19220|8589934593|
|  22|19440|8589934594|
|  32|19640|8589934595|
|  36|19720|8589934596|
|  40|19800|8589934597|
+----+-----+----------+
only showing top 20 rows
{code}
Environment: I was able to reproduce this with both

pyspark == ' 3.1.1'
 pyarrow == '3.0.0'
Python 3.7.10

and

pyspark == '3.1.2'
 pyarrow == '4.0.1'
Python 3.7.9
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 14 12:46:42 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rwbc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Jun/21 04:49;gurwls223;{quote}
The problem is avoided by using `asNondeterministic`.
{quote}

This is the correct way to avoid this problem.;;;, 14/Jun/21 12:42;Marsu_;> This is the correct way to avoid this problem.

How come this is the correct way to avoid this problem ? There shouldn't be a need for using asNondeterministic as the pandas udf IS deterministic ?!;;;, 14/Jun/21 12:46;Marsu_;The ticket has been marked as resolved because marking the udf function as non-deterministic solves the problem. However, the idf function IS deterministic and so, I don't understand how one could consider this to be a proper solution ?;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0

Summary: ClassCastException when trying to persist the result of a join between two Iceberg tables
Issue key: SPARK-37621
Issue id: 13416924
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: cipg
Creator: cipg
Created: 12/Dec/21 23:19
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I am gettin an error when I try to persist the results on a Join operation. Note that both tables to be joined and the output table are Iceberg tables.

SQL code to repro. 
{code}
String sqlJoin = String.format(
        "SELECT * from " +
                "((select %s from %s.%s where %s ) lllll " +
                "join (select %s from %s.%s where %s ) rrrrr " +
                "using (%s))",
        ........);
spark.sql(sqlJoin).writeTo("ciptest.ttt").option("write-format", "parquet").createOrReplace();
{code}

My exception stack is:
{code}
Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericInternalRow cannot be cast to org.apache.spark.sql.catalyst.expressions.UnsafeRow
	at org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$1.writeValue(UnsafeRowSerializer.scala:64)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:249)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:158)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at ….
{code}

Explain on the Sql statement gets the following plan:
{code}
== Physical Plan ==
Project [ ... ]
+- SortMergeJoin […], Inner
  :- Sort […], false, 0
  : +- Exchange hashpartitioning(…, 10), ENSURE_REQUIREMENTS, [id=#38]
  :   +- Filter (…)
  :    +- BatchScan[... ] left [filters=…]
  +- *(2) Sort […], false, 0
   +- Exchange hashpartitioning(…, 10), ENSURE_REQUIREMENTS, [id=#47]
     +- *(1) Filter (…)
      +- BatchScan[…] right [filters=…] 
{code}

Note that several variations of this fail. Besides the repro code listed above I have tried doing CTAS and trying to write the result into parquet files without making a table out of it.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Dec 14 00:24:18 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xlq0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): blue
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Dec/21 00:29;gurwls223;Is this an issue specific to icebug? or other sources in Spark too?;;;, 13/Dec/21 16:30;rdblue;[~hyukjin.kwon], this affects any source that doesn't always produce `UnsafeRow`. The problem is that certain parts of Spark assume that `UnsafeRow` will be passed even though the required interface is `InternalRow`. Rather than fixing that assumption, the community chose to ensure that there is always a projection added so that the conversion to unsafe happens. But if that projection is removed by other rules or is not added, then operators that assume `UnsafeRow` can fail.

The long-term fix is the same as always: eventually, Spark should use the declared type. A simpler fix is to find out why the projection is missing and update that. But then we'll see this problem come back later.;;;, 13/Dec/21 17:28;cipg;I was able to force Spark to add the extra projection like below. Now the code runs without that error.



{{String sqlJoin = String.format(}}
{{"SELECT * from " +}}
{{"((select 1.0 + rand() as 027318db716e, %s from %s.%s where %s ) " +}}
{{"join (select %s from %s.%s where %s ) " +}}
{{"using (%s))" +}}
{{"where 1.0 < 027318db716e",}}
{{...)}}

{{spark.sql(sqlJoin).drop("027318db716e").write().parquet(...);}};;;, 14/Dec/21 00:24;gurwls223;Thanks [~rdblue] for clarification. I have been just quickly looking through new JIRAs :-).;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: 100% CPU usage on Spark Thrift Server.
Issue key: SPARK-37254
Issue id: 13410796
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: rkchilaka
Creator: rkchilaka
Created: 09/Nov/21 10:33
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: We are trying to use Spark thrift server as a distributed sql query engine, the queries work when the resident memory occupied by Spark thrift server identified through HTOP is comparatively less than the driver memory. The same queries result in 100% cpu usage when the resident memory occupied by spark thrift server is greater than the configured driver memory and keeps running at 100% cpu usage. I am using incremental collect as false, as i need faster responses for exploratory queries. I am trying to understand the following points
 * Why isn't spark thrift server releasing back the memory, when there are no queries. 
 * What is causing spark thrift server to go into 100% cpu usage on all the cores, when spark thrift server's memory is greater than the driver memory (by 10% usually) and why are queries just stuck.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Nov 11 02:27:32 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wkfk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Nov/21 02:27;gurwls223;it would be much easier to investigate the issue if there're reproducible steps.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Use "concurrency" setting on Github Actions
Issue key: SPARK-35668
Issue id: 13382481
Parent id: 
Issue Type: Test
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yikunkero
Creator: yikunkero
Created: 07/Jun/21 13:23
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Project Infra
Due Date: 
Votes: 0
Labels: 
Description: We are using [cancel_duplicate_workflow_runs|https://github.com/apache/spark/blob/a70e66ecfa638cacc99b4e9a7c464e41ec92ad30/.github/workflows/cancel_duplicate_workflow_runs.yml#L1] job to cancel previous jobs when a new job is queued. Now, it has been supported by the github action by using ["concurrency"|https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#concurrency] syntax to make sure only a single job or workflow using the same concurrency group.

related: https://github.com/apache/arrow/pull/10416 and https://github.com/potiuk/cancel-workflow-runs
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jun 09 07:50:03 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rq00:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Jun/21 13:27;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32806;;;, 07/Jun/21 13:28;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/32806;;;, 08/Jun/21 03:11;gurwls223;Issue resolved by pull request 32806
[https://github.com/apache/spark/pull/32806];;;, 09/Jun/21 07:50;gurwls223;Reverted at https://github.com/apache/spark/commit/3be7b29cd8beaf6d34640dec12a10bc033deecdc;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Transient StackOverflowError on DataFrame from Catalyst QueryPlan
Issue key: SPARK-37609
Issue id: 13416647
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ravwojdyla
Creator: ravwojdyla
Created: 10/Dec/21 22:38
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: I sporadically observe a StackOverflowError from Catalyst's QueryPlan (for a relatively complicated query), below is a stacktrace from the {{count}} on that DF.  It's a bit troubling because it's a transient error, with enough retries (no change to code, probably some kind of cache?), I can get the op to work :(

{noformat}
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
~/miniconda3/envs/tr-dev/lib/python3.9/site-packages/pyspark/sql/dataframe.py in count(self)
    662         2
    663         """
--> 664         return int(self._jdf.count())
    665 
    666     def collect(self):

~/miniconda3/envs/tr-dev/lib/python3.9/site-packages/py4j/java_gateway.py in __call__(self, *args)
   1302 
   1303         answer = self.gateway_client.send_command(command)
-> 1304         return_value = get_return_value(
   1305             answer, self.gateway_client, self.target_id, self.name)
   1306 

~/miniconda3/envs/tr-dev/lib/python3.9/site-packages/pyspark/sql/utils.py in deco(*a, **kw)
    109     def deco(*a, **kw):
    110         try:
--> 111             return f(*a, **kw)
    112         except py4j.protocol.Py4JJavaError as e:
    113             converted = convert_exception(e.java_exception)

~/miniconda3/envs/tr-dev/lib/python3.9/site-packages/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    324             value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)
    325             if answer[1] == REFERENCE_TYPE:
--> 326                 raise Py4JJavaError(
    327                     "An error occurred while calling {0}{1}{2}.\n".
    328                     format(target_id, ".", name), value)

Py4JJavaError: An error occurred while calling o9123.count.
: java.lang.StackOverflowError
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:188)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
...
{noformat}
Environment: py:3.9
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 29 08:07:13 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xkhs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Dec/21 23:51;yumwang;How to reproduce this issue?;;;, 11/Dec/21 00:08;ravwojdyla;[~yumwang] I don't have a public code to share. Also don't have minimal reproduction code yet (don't have time to do it right now). The DF in this specific case had 162 columns, and I can't share the query plan without anonymising it (https://issues.apache.org/jira/browse/SPARK-37610). Anything else I could do in the meantime that would not require significant amount of work?;;;, 13/Dec/21 00:32;gurwls223;[~ravwojdyla] I don;t think people will dare to reproduce and debug for further investigation. it would be great to have minimised self-contained reproducer here.;;;, 13/Dec/21 20:27;ravwojdyla;[~hyukjin.kwon] yep, understand that, if I have some time to do this, I will try to figure out and post a reproduction code.;;;, 31/May/22 07:42;angerszhuuu;Same error in spark-3.1, query is simple, but so many nested columns, sometimes run into stackoverflow.

{code:java}
22/05/26 15:26:48 ERROR ApplicationMaster: User class threw exception: java.lang.StackOverflowError
java.lang.StackOverflowError
	at scala.collection.TraversableOnce.nonEmpty(TraversableOnce.scala:114)
	at scala.collection.TraversableOnce.nonEmpty$(TraversableOnce.scala:114)
	at scala.collection.AbstractTraversable.nonEmpty(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:358)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:192)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	
{code}

cc [~hyukjin.kwon][~yumwang];;;, 31/May/22 08:07;yumwang;[~angerszhuuu] How to reproduce this issue?;;;, 31/May/22 09:56;angerszhuuu;[~yumwang]Seems just an very complex table schema. Not reproduce every time. I am tell user to try  increase -Xss, to see if this way can resolve this probelm.;;;, 31/May/22 11:05;angerszhuuu;Increase -Xss  can resolve this. But we should better to refactor the current  code...;;;, 29/Aug/22 08:07;ravwojdyla;Experienced another issue like this, this time the query is fairly simple but there's a couple of thousands of columns in the DataFrame.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Anonymized/obfuscated query plan
Issue key: SPARK-37610
Issue id: 13416734
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ravwojdyla
Creator: ravwojdyla
Created: 11/Dec/21 00:07
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I would like to share a query plan for a specific issue(https://issues.apache.org/jira/browse/SPARK-37609), but can't without at least anonymising the column names. If I could call {{explain}} that would anonymise/obfuscate the column names that would be useful.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Dec 13 00:32:00 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xkjs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Dec/21 00:32;gurwls223;I think it's better to have workload specific script to address this. I doubt if we can have a generalized way to hide sensitive information for the whole query plan since it can contains arbitrary information.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: spark-hadoop-cloud broken on release and only published via 3rd party repositories
Issue key: SPARK-36936
Issue id: 13405117
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: colin.williams
Creator: colin.williams
Created: 06/Oct/21 05:09
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Input/Output
Due Date: 
Votes: 0
Labels: 
Description: The spark docmentation suggests using `spark-hadoop-cloud` to read / write from S3 in [https://spark.apache.org/docs/latest/cloud-integration.html] . However artifacts are currently published via only 3rd party resolvers in [https://mvnrepository.com/artifact/org.apache.spark/spark-hadoop-cloud] including Cloudera and Palantir.

 

Then apache spark documentation is providing a 3rd party solution for object stores including S3. Furthermore, if you follow the instructions and include one of the 3rd party jars IE the Cloudera jar with the spark 3.1.2 release and try to access object store, the following exception is returned.

 

```

Exception in thread "main" java.lang.NoSuchMethodError: 'void com.google.common.base.Preconditions.checkArgument(boolean, java.lang.String, java.lang.Object, java.lang.Object)'
 at org.apache.hadoop.fs.s3a.S3AUtils.lookupPassword(S3AUtils.java:894)
 at org.apache.hadoop.fs.s3a.S3AUtils.lookupPassword(S3AUtils.java:870)
 at org.apache.hadoop.fs.s3a.S3AUtils.getEncryptionAlgorithm(S3AUtils.java:1605)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:363)
 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
 at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
 at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
 at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)
 at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)
 at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)
 at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)
 at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)
 at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:519)
 at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:428)

```

It looks like there are classpath conflicts using the cloudera published `spark-hadoop-cloud` with spark 3.1.2, again contradicting the documentation.

Then the documented `spark-hadoop-cloud` approach to using object stores is poorly supported only by 3rd party repositories and not by the released apache spark whose documentation refers to it.

Perhaps one day apache spark will provide tested software so that developers can quickly and easily access cloud object stores using the documentation.
Environment: name:=spark-demo

version := "0.0.1"

scalaVersion := "2.12.12"

lazy val app = (project in file("app")).settings(
 assemblyPackageScala / assembleArtifact := false,
 assembly / assemblyJarName := "uber.jar",
 assembly / mainClass := Some("com.example.Main"),
 // more settings here ...
 )

resolvers += "Cloudera" at "https://repository.cloudera.com/artifactory/cloudera-repos/"

libraryDependencies += "org.apache.spark" %% "spark-sql" % "3.1.2" % "provided"
libraryDependencies += "org.apache.spark" %% "spark-hadoop-cloud" % "3.1.1.3.1.7270.0-253"
libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.1.1.7.2.7.0-184"
libraryDependencies += "com.amazonaws" % "aws-java-sdk-bundle" % "1.11.901"

libraryDependencies += "org.scalatest" %% "scalatest" % "3.0.1" % "test"

// test suite settings
fork in Test := true
javaOptions ++= Seq("-Xms512M", "-Xmx2048M", "-XX:MaxPermSize=2048M", "-XX:+CMSClassUnloadingEnabled")
// Show runtime of tests
testOptions in Test += Tests.Argument(TestFrameworks.ScalaTest, "-oD")

___________________________________________________________________________________________

 

import org.apache.spark.sql.SparkSession

object SparkApp {
 def main(args: Array[String]){
 val spark = SparkSession.builder().master("local")
 //.config("spark.jars.repositories", "https://repository.cloudera.com/artifactory/cloudera-repos/")
 //.config("spark.jars.packages", "org.apache.spark:spark-hadoop-cloud_2.12:3.1.1.3.1.7270.0-253")
 .appName("spark session").getOrCreate

 val jsonDF = spark.read.json("s3a://path-to-bucket/compact.json")
 val csvDF = spark.read.format("csv").load("s3a://path-to-bucket/some.csv")
 jsonDF.show()
 csvDF.show()
 }
}
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Oct 09 20:31:13 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vljs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Oct/21 06:45;gurwls223;cc [~sunchao] FYI;;;, 06/Oct/21 19:13;csun;[~colin.williams] which version of {{spark-hadoop-cloud}} you were using? I think the above error shouldn't happen if the version is the same as the Spark's version.

We've already started to publish {{spark-hadoop-cloud}} as part of the Spark release procedure, see SPARK-35844.;;;, 07/Oct/21 19:40;colin.williams;[~csun] when I see SPARK-35844 I see 3.2.0 version for the jar. That does not look to be published.



2021.10.07 12:39:03 INFO [warn] Note: Unresolved dependencies path:
2021.10.07 12:39:03 INFO [error] sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-hadoop-cloud_2.12:3.2.0
2021.10.07 12:39:03 INFO [error] Not found
2021.10.07 12:39:03 INFO [error] Not found
2021.10.07 12:39:03 INFO [error] not found: /home/colin/.ivy2/local/org.apache.spark/spark-hadoop-cloud_2.12/3.2.0/ivys/ivy.xml
2021.10.07 12:39:03 INFO [error] not found: https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.12/3.2.0/spark-hadoop-cloud_2.12-3.2.0.pom
2021.10.07 12:39:03 INFO [error] not found: https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/spark/spark-hadoop-cloud_2.12/3.2.0/spark-hadoop-cloud_2.12-3.2.0.pom;;;, 08/Oct/21 16:04;csun;[~colin.williams] Spark 3.2.0 is not released yet - it will be there soon.;;;, 09/Oct/21 20:31;colin.williams;But the Spark 3.1.2 documentation  [https://spark.apache.org/docs/3.1.2/cloud-integration.html] states:

<dependencyManagement>
 ...
 <dependency>
 <groupId>org.apache.spark</groupId>
 <artifactId>hadoop-cloud_2.12</artifactId>
 <version>${spark.version}</version>
 <scope>provided</scope>
 </dependency>
 ...
 </dependencyManagement>

 

For which I show an artifact for 3.1.2 does not exist.

 

 
 2021.10.09 13:34:47 INFO [error] (update) sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-hadoop-cloud_2.12:3.1.2
 2021.10.09 13:34:47 INFO [error] Not found
 2021.10.09 13:34:47 INFO [error] Not found
 2021.10.09 13:34:47 INFO [error] not found: /home/colin/.ivy2/local/org.apache.spark/spark-hadoop-cloud_2.12/3.1.2/ivys/ivy.xml
 2021.10.09 13:34:47 INFO [error] not found: [https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.12/3.1.2/spark-hadoop-cloud_2.12-3.1.2.pom]
 2021.10.09 13:34:47 INFO [error] not found: [https://repository.cloudera.com/artifactory/cloudera-repos/org/apache/spark/spark-hadoop-cloud_2.12/3.1.2/spark-hadoop-cloud_2.12-3.1.2.po];;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0

Summary: show api of Dataset should get as input the output method
Issue key: SPARK-36329
Issue id: 13392395
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: igreenfi
Creator: igreenfi
Created: 28/Jul/21 18:55
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: For now show is:
{code:scala}
def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }
{code}
it can be turn into:
{code:scala}
def show(numRows: Int, truncate: Boolean, out: String => Unit = println): Unit = if (truncate) {
    out(showString(numRows, truncate = 20))
  } else {
    out(showString(numRows, truncate = 0))
  }
{code}
so user will be able to send that to file/log...
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Aug 01 14:34:39 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tf2w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Jul/21 00:25;gurwls223;why don't you do it with collect(...).map(..transform..).foreach(println)?;;;, 01/Aug/21 14:34;igreenfi;# if you do it like that you should write it again and again.
 # it very handy to have it in the same format as show.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Writing to hiveserver throught jdbc throws ParseException
Issue key: SPARK-36325
Issue id: 13392289
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jrballesteros05
Creator: jrballesteros05
Created: 28/Jul/21 08:29
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: spark, spark-sql
Description: Hello everyone, I am new working on Spark and this is my first post. If I make a mistake please be kind to me but I have searched in the web and I haven't found anything related. If this bug is duplicated or something please feel free to close it and tell me where to look. 

I am working with Zeppelin, I got a dataframe from Solr API, I processed and I want to write to a table trough thrift  and read that new table from Apache SuperSet.

 

I have this df with this schema:
{code:java}
%spark
df_linux.printSchema()
root
 |-- time: string (nullable = false)
 |-- raw_log: string (nullable = false)
 |-- service_name: string (nullable = false)
 |-- hostname: string (nullable = false)
 |-- pid: string (nullable = false)
 |-- username: string (nullable = false)
 |-- source_ip: string (nullable = false)

{code}
 

And this content:

 
{code:java}
%spark
df_linux.show()
+--------------------+--------------------+------------+------------------+-----+--------+---------+
| time| raw_log|service_name| hostname| pid|username|source_ip|
+--------------------+--------------------+------------+------------------+-----+--------+---------+
|2021-07-28T07:41:53Z|Jul 28 07:41:52 s...| sshd[11611]|sa3secessuperset01|11611| debian| 10.0.9.3|
|2021-07-28T07:41:44Z|Jul 28 07:41:43 s...| sshd[11590]|sa3secessuperset01|11590| debian| 10.0.9.3|
|2021-07-27T08:46:11Z|Jul 27 08:46:10 s...| sshd[16954]|sa3secessuperset01|16954| debian| 10.0.9.3|
|2021-07-27T08:44:55Z|Jul 27 08:44:54 s...| sshd[16511]|sa3secessuperset01|16511| debian| 10.0.9.3|
|2021-07-27T08:30:03Z|Jul 27 08:30:02 s...| sshd[14511]|sa3secessuperset01|14511| debian| 10.0.9.3|
+--------------------+--------------------+------------+------------------+-----+--------+---------+
{code}
 

When I write the dataframe through jdbc I got this error:

 

 
{code:java}
df_linux.write.mode("overwrite")
 .format("jdbc")
 .option("driver","org.apache.hive.jdbc.HiveDriver")
 .option("url", "jdbc:hive2://sa3secessuperset01.a3sec.local:10000")
 .option("dbtable", "o365new")
 .option("createTableColumnTypes", "time VARCHAR(1024) NOT NULL, raw_log VARCHAR(1024) NOT NULL, service_name VARCHAR(1024), hostname VARCHAR(1024), pid VARCHAR(1024), username VARCHAR(1024), source_ip STRING")
 .save()

java.sql.SQLException: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException:
no viable alternative at input '("time"'(line 1, pos 22)== SQL ==
CREATE TABLE o365new ("time" varchar(1024) NOT NULL, "raw_log" varchar(1024) NOT NULL, "service_name" varchar(1024) NOT NULL, "hostname" varchar(1024) NOT NULL, "pid" varchar(1024) NOT NULL, "username" varchar(1024) NOT NULL, "source_ip" string NOT NULL)
----------------------^^^{code}
 

What I have seen it's the way it tries to create the table. If you run the generated SQL sentence in Beeline it would throw exactly the same error:

 
{code:java}
%hive
CREATE TABLE o365new ("time" varchar(1024) NOT NULL, "raw_log" varchar(1024) NOT NULL, "service_name" varchar(1024) NOT NULL, "hostname" varchar(1024) NOT NULL, "pid" varchar(1024) NOT NULL, "username" varchar(1024) NOT NULL, "source_ip" string NOT NULL);

org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.catalyst.parser.ParseException: 
no viable alternative at input '("time"'(line 2, pos 22{code}
 

 

Then I just removed the quotes and the table is created without any problem:

 
{code:java}
%hive
CREATE TABLE o365new (time varchar(1024) NOT NULL, raw_log varchar(1024) NOT NULL, service_name varchar(1024) NOT NULL, hostname varchar(1024) NOT NULL, pid varchar(1024) NOT NULL, username varchar(1024) NOT NULL, source_ip string NOT NULL)
{code}
 

 

So, the problem are the quotes, that's why I think this is a bug, but I don't know how to "override" the query like I do with "createTableColumnTypes". Maybe this is not the way to work and there is another approach but I don't know how to.

 

Best regards.

 

 

 

 
Environment: OS: Debian 10

Spark version: 3.1.2

Zeppelin Notebook: 0.9.0

Jdbc driver:  org.apache.hive:hive-jdbc:3.1.2  
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 02 15:01:20 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tefc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Jul/21 00:27;gurwls223;Hm, I think you should implement HiveDialet that extends JdbcDialect to address the different cases.;;;, 02/Aug/21 15:01;jrballesteros05;Hello, firstable thank you for your reply. I used the dialect but now I have another issue, and this one I don't know how to address it.

 

 
{code:java}
import org.apache.spark.sql.jdbc.{JdbcDialects, JdbcType, JdbcDialect}
import org.apache.spark.sql.types.StringType
import java.sql.Types
import org.apache.spark.sql.types.DataType
val HiveDialect = new JdbcDialect { 
override def canHandle(url: String): Boolean = url.startsWith("jdbc:hive2") || url.contains("hive2")
override def quoteIdentifier(colName: String): String ={ s"$colName" }
override def getJDBCType(dt: DataType): Option[JdbcType] = dt match {
 case StringType => Option(JdbcType("STRING", Types.VARCHAR))
 case _ => None
 }
}

JdbcDialects.registerDialect(HiveDialect)
df_linux.write.mode("overwrite")
 .format("jdbc")
 .option("driver","org.apache.hive.jdbc.HiveDriver")
 .option("url", "jdbc:hive2://sa3secessuperset01.a3sec.local:10000")
 .option("dbtable", "o365new")
 //.option("createTableColumnTypes", "_time VARCHAR(1024), raw_log VARCHAR(1024), service_name VARCHAR(1024), hostname VARCHAR(1024), pid VARCHAR(1024), username VARCHAR(1024), source_ip VARCHAR(1024)")
 .option("createTableColumnTypes", "time STRING, raw_log STRING, service_name STRING, hostname STRING, pid STRING, username STRING, source_ip STRING")
 .save()
{code}
 

I get this error:

 
{code:java}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 4 times, most recent failure: Lost task 0.3 in stage 32.0 (TID 423) (10.103.0.118 executor 2): java.sql.SQLFeatureNotSupportedException: Method not supported
 at org.apache.hive.jdbc.HivePreparedStatement.addBatch(HivePreparedStatement.java:78)
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:683)
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:856)
 at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:854)
 
{code}
 

Maybe jdbc is not the way to write throught thriftserver but I don't know how to do it. At the moment I am using another database but I really want to use the SparkSQL. If you think I should close this issue and maybe open as something else feel free to close the ticket.

 

 

 ;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Create the external hive table for HBase failed 
Issue key: SPARK-36860
Issue id: 13403416
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yimo_yym
Creator: yimo_yym
Created: 27/Sep/21 03:54
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: We use follow sql to create hive external table , which read from hbase
{code:java}
CREATE EXTERNAL TABLE if not exists dev.sanyu_spotlight_headline_material(
   rowkey string COMMENT 'HBase主键',
   content string COMMENT '图文正文')
USING HIVE   
ROW FORMAT SERDE
   'org.apache.hadoop.hive.hbase.HBaseSerDe'
 STORED BY
   'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
 WITH SERDEPROPERTIES (
   'hbase.columns.mapping'=':key, cf1:content'
)
 TBLPROPERTIES (
   'hbase.table.name'='spotlight_headline_material'
 );
{code}
But the sql failed in Spark 3.1.2, which throw this exception
{code:java}
21/09/27 11:44:24 INFO scheduler.DAGScheduler: Asked to cancel job group 26d7459f-7b58-4c18-9939-5f2737525ff2
21/09/27 11:44:24 ERROR thriftserver.SparkExecuteStatementOperation: Error executing query with 26d7459f-7b58-4c18-9939-5f2737525ff2, currentState RUNNING,
org.apache.spark.sql.catalyst.parser.ParseException:
Operation not allowed: Unexpected combination of ROW FORMAT SERDE 'org.apache.hadoop.hive.hbase.HBaseSerDe' and STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITHSERDEPROPERTIES('hbase.columns.mapping'=':key, cf1:content')(line 5, pos 0)
{code}
this check was introduced from this change: [https://github.com/apache/spark/pull/28026]

 

Could anyone gave the introduction how to create the external table for hbase in spark3 now ? 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 27/Sep/21 06:18;yimo_yym;image-2021-09-27-14-18-10-910.png;https://issues.apache.org/jira/secure/attachment/13034181/image-2021-09-27-14-18-10-910.png, 27/Sep/21 06:25;yimo_yym;image-2021-09-27-14-25-28-900.png;https://issues.apache.org/jira/secure/attachment/13034182/image-2021-09-27-14-25-28-900.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 29 05:26:24 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vb28:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Sep/21 05:29;sarutak;[~yimo_yym]
Spark doesn't support creating Hive table using storage handlers yet.
Please see also http://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html#specifying-storage-format-for-hive-tables
;;;, 27/Sep/21 06:25;yimo_yym;thanks, [~sarutak] , may I ask why spark doesn't support creating Hive table using storage handlers? 

it seems spark supported the stored by syntax, the data is already in CreateFileFormatContext.

!image-2021-09-27-14-18-10-910.png!

but the validateRowFormatFileFormat in AstBuilder later only checks the fileformat provided in stored as syntax, and as fileformat is null here, it throw out an exception.  Maybe stored by clause can be fixed by fix this check?

!image-2021-09-27-14-25-28-900.png!;;;, 29/Sep/21 05:26;gurwls223;Unless it's explicitly documented, nothing is supported officially.;;;
Affects Version/s.1: 
Attachment.1: 27/Sep/21 06:25;yimo_yym;image-2021-09-27-14-25-28-900.png;https://issues.apache.org/jira/secure/attachment/13034182/image-2021-09-27-14-25-28-900.png
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Incorrect writing the string, containing symbols like "\" to Hive 
Issue key: SPARK-36802
Issue id: 13402075
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Programmirus
Creator: Programmirus
Created: 19/Sep/21 20:53
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.3.0, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: After writing the strings, containing symbol like "\" to Hive, the result record in HiveTable doesn't contain that symbol. It happens when using the standart method of pyspark.sql.readwriter.DataFrameWriter saveAsTable as well as insertInto.

For example, running the query

spark.sql("select '\d\{4}' as code").write.saveAsTable('db.table')

I've got the next result in Hive:

spark.table('db.table').collect()[0][0]

>>"d\{4}"

But expected the next 

>> "\d\{4}"

Spark version : '2.3.0.2.6.5.0-292'

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Sep 26 21:48:03 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v2so:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Sep/21 03:57;gurwls223;Spark 2.x is EOL. mind trying and see if it works with Spark 3.x?;;;, 26/Sep/21 21:48;Programmirus;[~hyukjin.kwon], 3.x has the same problem.;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Pyspark UDF wrongly changes timestamps to UTC
Issue key: SPARK-33863
Issue id: 13347055
Parent id: 
Issue Type: Bug
Status: Reopened
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: nasirali
Creator: nasirali
Created: 20/Dec/20 23:32
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.1, 3.0.2, 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: *Problem*:

I have a dataframe with a ts (timestamp) column in UTC. If I create a new column using udf, pyspark udf wrongly changes timestamps into UTC time. ts (timestamp) column is already in UTC time. Therefore, pyspark udf should not convert ts (timestamp) column into UTC timestamp. 

I have used following configs to let spark know the timestamps are in UTC:

 
{code:java}
--conf spark.driver.extraJavaOptions=-Duser.timezone=UTC 
--conf spark.executor.extraJavaOptions=-Duser.timezone=UTC
--conf spark.sql.session.timeZone=UTC
{code}
Below is a code snippet to reproduce the error:

 
{code:java}
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StringType, TimestampType
import datetime

spark = SparkSession.builder.config("spark.sql.session.timeZone", "UTC").getOrCreate()

df = spark.createDataFrame([("usr1",17.00, "2018-02-10T15:27:18+00:00"),
                            ("usr1",13.00, "2018-02-11T12:27:18+00:00"),
                            ("usr1",25.00, "2018-02-12T11:27:18+00:00"),
                            ("usr1",20.00, "2018-02-13T15:27:18+00:00"),
                            ("usr1",17.00, "2018-02-14T12:27:18+00:00"),
                            ("usr2",99.00, "2018-02-15T11:27:18+00:00"),
                            ("usr2",156.00, "2018-02-22T11:27:18+00:00")
                            ],
                           ["user","id", "ts"])

df = df.withColumn('ts', df.ts.cast('timestamp'))
df.show(truncate=False)

def some_time_udf(i):
    if  datetime.time(5, 0)<=i.time() < datetime.time(12, 0):
        tmp= "Morning: " + str(i)
    elif  datetime.time(12, 0)<=i.time() < datetime.time(17, 0):
        tmp= "Afternoon: " + str(i)
    elif  datetime.time(17, 0)<=i.time() < datetime.time(21, 0):
        tmp= "Evening"
    elif  datetime.time(21, 0)<=i.time() < datetime.time(0, 0):
        tmp= "Night"
    elif  datetime.time(0, 0)<=i.time() < datetime.time(5, 0):
        tmp= "Night"
    return tmp

udf = F.udf(some_time_udf,StringType())
df.withColumn("day_part", udf(df.ts)).show(truncate=False)


{code}
 

Below is the output of the above code:
{code:java}
+----+-----+-------------------+----------------------------+
|user|id   |ts                 |day_part                    |
+----+-----+-------------------+----------------------------+
|usr1|17.0 |2018-02-10 15:27:18|Morning: 2018-02-10 09:27:18|
|usr1|13.0 |2018-02-11 12:27:18|Morning: 2018-02-11 06:27:18|
|usr1|25.0 |2018-02-12 11:27:18|Morning: 2018-02-12 05:27:18|
|usr1|20.0 |2018-02-13 15:27:18|Morning: 2018-02-13 09:27:18|
|usr1|17.0 |2018-02-14 12:27:18|Morning: 2018-02-14 06:27:18|
|usr2|99.0 |2018-02-15 11:27:18|Morning: 2018-02-15 05:27:18|
|usr2|156.0|2018-02-22 11:27:18|Morning: 2018-02-22 05:27:18|
+----+-----+-------------------+----------------------------+
{code}
Above output is incorrect. You can see ts and day_part columns don't have same timestamps. Below is the output I would expect:

 
{code:java}
+----+-----+-------------------+----------------------------+
|user|id   |ts                 |day_part                    |
+----+-----+-------------------+----------------------------+
|usr1|17.0 |2018-02-10 15:27:18|Afternoon: 2018-02-10 15:27:18|
|usr1|13.0 |2018-02-11 12:27:18|Afternoon: 2018-02-11 12:27:18|
|usr1|25.0 |2018-02-12 11:27:18|Morning: 2018-02-12 11:27:18|
|usr1|20.0 |2018-02-13 15:27:18|Afternoon: 2018-02-13 15:27:18|
|usr1|17.0 |2018-02-14 12:27:18|Afternoon: 2018-02-14 12:27:18|
|usr2|99.0 |2018-02-15 11:27:18|Morning: 2018-02-15 11:27:18|
|usr2|156.0|2018-02-22 11:27:18|Morning: 2018-02-22 11:27:18|
+----+-----+-------------------+----------------------------+{code}
 
Environment: MAC/Linux

Standalone cluster / local machine
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 24 01:51:46 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0low0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Dec/20 07:37;gurwls223;[~nasirali] Can you show expected results and actual results of {{df.show()}}?;;;, 23/Dec/20 19:39;viirya;It is unclear what the issue is. "Pyspark UDF changes timestamps to UTC" does it mean the UDF wrongly changes timestamps to UTC?;;;, 08/Jan/21 01:30;nasirali;[~hyukjin.kwon] and [~viirya] I have simplified example code, added/revised details, and also added output and expected output. Please let me know if you need more information.;;;, 23/Feb/21 02:55;nasirali;[~hyukjin.kwon] and [~viirya] any update on this issue?;;;, 08/May/21 00:00;nasirali;[~hyukjin.kwon] and [~viirya] This bug exist in all the 3.x.x versions of Pyspark. Any update or suggestion?;;;, 04/Jun/21 02:52;dc-heros;Have this issues resolved, I couldn't reproduce it

 ;;;, 24/Jun/21 01:51;nasirali;Issue is not resolved. ;;;, 24/Jun/21 01:51;nasirali;[~dc-heros] Could you please share the output you got when you ran the above code?;;;
Affects Version/s.1: 3.0.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: The log could not get the correct line number
Issue key: SPARK-36781
Issue id: 13401634
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: chenxusheng
Creator: chenxusheng
Created: 16/Sep/21 10:19
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.6, 3.0.3, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: INFO 18:16:46 [Thread-1] org.apache.spark.internal.Logging$class.logInfo({color:#FF0000}Logging.scala:54{color}) MemoryStore cleared
 INFO 18:16:46 [Thread-1] org.apache.spark.internal.Logging$class.logInfo({color:#FF0000}Logging.scala:54{color}) BlockManager stopped
 INFO 18:16:46 [Thread-1] org.apache.spark.internal.Logging$class.logInfo({color:#FF0000}Logging.scala:54{color}) BlockManagerMaster stopped
 INFO 18:16:46 [dispatcher-event-loop-0] org.apache.spark.internal.Logging$class.logInfo({color:#FF0000}Logging.scala:54{color}) OutputCommitCoordinator stopped!
 INFO 18:16:46 [Thread-1] org.apache.spark.internal.Logging$class.logInfo({color:#FF0000}Logging.scala:54{color}) Successfully stopped SparkContext
 INFO 18:16:46 [Thread-1] org.apache.spark.internal.Logging$class.logInfo({color:#FF0000}Logging.scala:54{color}) Shutdown hook called

all are : {color:#FF0000}Logging.scala:54{color}

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Sep 28 09:37:12 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v02o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Sep/21 04:40;gurwls223;Spark 2.x is EOL. Can you check if it works w/ Spark 3.x?;;;, 17/Sep/21 05:06;chenxusheng;[~hyukjin.kwon]  Yes, when I use Spark3.0, the problem still exists;;;, 28/Sep/21 09:37;senthh;[~chenxusheng] Could you please share the sample code to simulate this issue?;;;
Affects Version/s.1: 3.0.3
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Spark Support for MS Sql JDBC connector with Kerberos/Keytab
Issue key: SPARK-36765
Issue id: 13401406
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dts
Creator: dts
Created: 15/Sep/21 14:12
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 3.1.2
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Hi Team,

 

We are using the Spark-3.0.2 to connect to MS SqlServer with the following instruction  

Also tried with the Spark-3.1.2 Version,

 
 1) download mssql-jdbc-9.4.0.jre8.jar
 2) Generated Keytab using kinit
 3) Validate Keytab using klist
 4) Run the spark job with jdbc_library, principal and keytabs passed

.config("spark.driver.extraClassPath", spark_jar_lib) \
.config("spark.executor.extraClassPath", spark_jar_lib) \


 5) connection_url = "jdbc:sqlserver://{}:{};databaseName={};integratedSecurity=true;authenticationSchema=JavaKerberos"\
 .format(jdbc_host_name, jdbc_port, jdbc_database_name)

Note: without integratedSecurity=true;authenticationSchema=JavaKerberos it looks for the usual username/password option to connect

6) passing the following options during spark read.
 .option("principal", database_principal) \
 .option("files", database_keytab) \
 .option("keytab", database_keytab) \
  
 tried with files and keytab, just files, and with all above 3 parameters
  
 We are unable to connect to SqlServer from Spark and getting the following error shown below. 
  
 A) Wanted to know if anybody was successful Spark to SqlServer? (as I see the previous Jira has been closed)
 https://issues.apache.org/jira/browse/SPARK-12312
 https://issues.apache.org/jira/browse/SPARK-31337
  
 B) If yes, could you let us know if there are any additional configs needed for Spark to connect to SqlServer please?
 Appreciate if we can get inputs to resolve this error.
  
  
 Full Stack Trace

{code}
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: This driver is not configured for integrated authentication.         at com.microsoft.sqlserver.jdbc.SQLServerConnection.terminate(SQLServerConnection.java:1352)         at com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:2329)         at com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:1905)         at com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:41)         at com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:1893)         at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:4575)         at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1400)         at com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:1045)         at com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:817)         at com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:700)         at com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:842)         at org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)         at org.apache.spark.sql.execution.datasources.jdbc.connection.SecureConnectionProvider.getConnection(SecureConnectionProvider.scala:44)         at org.apache.spark.sql.execution.datasources.jdbc.connection.MSSQLConnectionProvider.org$apache$spark$sql$execution$datasources$jdbc$connection$MSSQLConnectionProvider$$super$getConnection(MSSQLConnectionProvider.scala:69)         at org.apache.spark.sql.execution.datasources.jdbc.connection.MSSQLConnectionProvider$$anon$1.run(MSSQLConnectionProvider.scala:69)         at org.apache.spark.sql.execution.datasources.jdbc.connection.MSSQLConnectionProvider$$anon$1.run(MSSQLConnectionProvider.scala:67)         at java.base/java.security.AccessController.doPrivileged(Native Method)         at java.base/javax.security.auth.Subject.doAs(Subject.java:423)         at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)         ... 23 more Caused by: java.lang.UnsatisfiedLinkError: no sqljdbc_auth in java.library.path: [/usr/java/packages/lib, /usr/lib64, /lib64, /lib, /usr/lib]         at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2660)         at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:827)         at java.base/java.lang.System.loadLibrary(System.java:1871)         at com.microsoft.sqlserver.jdbc.AuthenticationJNI.<clinit>(AuthenticationJNI.java:32)         at com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:1902)
{code}

Environment: Unix Redhat Environment
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Sep 17 11:55:45 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uyo0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Sep/21 04:41;gurwls223;cc [~gaborgsomogyi] FYI;;;, 17/Sep/21 07:50;gaborgsomogyi;It was long time ago when I've done that and AFAIR it took me almost a month to make it work so definitely a horror task!
My knowledge is cloudy because it was not yesterday but I remember something like this:

The exception generally indicates that the driver can not find the appropriate sqljdbc_auth lib in the JVM library path.  To correct the problem, one can use use the java -D option to specify the "java.library.path" system property value.  Worth to mention full path must be set as path, otherwise it was not working.

All in all I've faced at least 5-6 different issues which were extremely hard to address. Hope others need less time to solve the issues.
;;;, 17/Sep/21 11:55;jakubpawlowski;As per documentation on JDBC driver, sqljdbc_auth lib should not be needed and authentication should happen using pure java libraries. This library was needed only for older versions of the driver.

[https://docs.microsoft.com/en-us/sql/connect/jdbc/using-kerberos-integrated-authentication-to-connect-to-sql-server?view=sql-server-ver15]

I could make it a vanilla java code work, but spark is creating programmatically the jaas configuration so maybe that's where something gets broken..?;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: spark should support distribute directory to cluster
Issue key: SPARK-36518
Issue id: 13395403
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yghu
Creator: yghu
Created: 16/Aug/21 06:59
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Deploy
Due Date: 
Votes: 0
Labels: 
Description: Spark now only supports distribute files to cluster, but in some scenario, we need upload a directory to cluster.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Aug 17 08:16:48 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0txmw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Aug/21 05:01;gurwls223;I think you can use --archive for this purpose.;;;, 17/Aug/21 06:24;yghu;[~hyukjin.kwon] ya, i know this parameter, but this will have one extra layer of directory structure, i just only want uploading one directory structure as it is in local or dfs. ;;;, 17/Aug/21 06:27;gurwls223;Can you try {{SparkContext.addFile("...", recursive = true)}}?;;;, 17/Aug/21 06:49;yghu;[~hyukjin.kwon] we should support it like using --files command;;;, 17/Aug/21 08:16;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33760;;;
Affects Version/s.1: 3.1.1
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Keep file permissions for .tar archives
Issue key: SPARK-38632
Issue id: 13435266
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gurwls223
Creator: 
Created: 23/Mar/22 04:51
Updated: 12/Dec/22 17:50
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: SPARK-38631 disallowed file permissions for .tar archives to work around a security issue. We should restore it back.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-03-23 04:51:22.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10qg8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.1
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: spark-3.1.2以cluster模式提交作业报 Could not initialize class com.github.luben.zstd.ZstdOutputStream
Issue key: SPARK-41013
Issue id: 13495481
Parent id: 
Issue Type: Bug
Status: Reopened
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yutiantian005
Creator: yutiantian005
Created: 04/Nov/22 10:10
Updated: 01/Dec/22 15:12
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: libzstd-jni, spark.shuffle.mapStatus.compression.codec, zstd
Description: 使用spark-3.1.2版本以cluster模式提交作业，报

Could not initialize class com.github.luben.zstd.ZstdOutputStream。具体日志如下：

Exception in thread "map-output-dispatcher-0" Exception in thread "map-output-dispatcher-2" java.lang.ExceptionInInitializerError: Cannot unpack libzstd-jni: No such file or directory at java.io.UnixFileSystem.createFileExclusively(Native Method) at java.io.File.createTempFile(File.java:2024) at com.github.luben.zstd.util.Native.load(Native.java:97) at com.github.luben.zstd.util.Native.load(Native.java:55) at com.github.luben.zstd.ZstdOutputStream.<clinit>(ZstdOutputStream.java:16) at org.apache.spark.io.ZStdCompressionCodec.compressedOutputStream(CompressionCodec.scala:223) at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:910) at org.apache.spark.ShuffleStatus.$anonfun$serializedMapStatus$2(MapOutputTracker.scala:233) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.ShuffleStatus.withWriteLock(MapOutputTracker.scala:72) at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:230) at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:466) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Exception in thread "map-output-dispatcher-7" Exception in thread "map-output-dispatcher-5" java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.ZstdOutputStream at org.apache.spark.io.ZStdCompressionCodec.compressedOutputStream(CompressionCodec.scala:223) at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:910) at org.apache.spark.ShuffleStatus.$anonfun$serializedMapStatus$2(MapOutputTracker.scala:233) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.ShuffleStatus.withWriteLock(MapOutputTracker.scala:72) at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:230) at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:466) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Exception in thread "map-output-dispatcher-4" Exception in thread "map-output-dispatcher-3" java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.ZstdOutputStream at org.apache.spark.io.ZStdCompressionCodec.compressedOutputStream(CompressionCodec.scala:223) at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:910) at org.apache.spark.ShuffleStatus.$anonfun$serializedMapStatus$2(MapOutputTracker.scala:233) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.ShuffleStatus.withWriteLock(MapOutputTracker.scala:72) at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:230) at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:466) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.ZstdOutputStream at org.apache.spark.io.ZStdCompressionCodec.compressedOutputStream(CompressionCodec.scala:223) at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:910) at org.apache.spark.ShuffleStatus.$anonfun$serializedMapStatus$2(MapOutputTracker.scala:233) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.ShuffleStatus.withWriteLock(MapOutputTracker.scala:72) at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:230) at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:466) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)

但是同样的代码，以client模式提交可以正常执行。

以cluster模式提交作业暂时的解决办法是在spark-default.conf 中配置spark.shuffle.mapStatus.compression.codec lz4 作业可以正常提交。

想咨询下cluster模式，在shuffle 过程中使用zstd压缩为什么会不能正常使用呢？

有任何思路提供的大佬将不胜感激。
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Dec 01 15:12:50 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1axsg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 05/Nov/22 01:57;yumwang;Could you test the Spark 3.3.1?;;;, 07/Nov/22 11:39;yutiantian005;[~yumwang]  spark-3.3.1 test appears to generate the same error.

Are there any parameters that need to be configured？

The error log is as follows

19:09:45.882 [map-output-dispatcher-0] ERROR org.apache.spark.MapOutputTrackerMaster - null java.lang.reflect.InvocationTargetException: null at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_181] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_181] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_181] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_181] at org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:87) ~[spark-core_2.12-3.3.1.jar:3.3.1] at org.apache.spark.MapOutputTracker$.serializeOutputStatuses(MapOutputTracker.scala:1492) ~[spark-core_2.12-3.3.1.jar:3.3.1] at org.apache.spark.ShuffleStatus.$anonfun$serializedMapStatus$2(MapOutputTracker.scala:338) ~[spark-core_2.12-3.3.1.jar:3.3.1] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?] at org.apache.spark.ShuffleStatus.withWriteLock(MapOutputTracker.scala:77) ~[spark-core_2.12-3.3.1.jar:3.3.1] at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:335) ~[spark-core_2.12-3.3.1.jar:3.3.1] at org.apache.spark.MapOutputTrackerMaster$MessageLoop.handleStatusMessage(MapOutputTracker.scala:729) ~[spark-core_2.12-3.3.1.jar:3.3.1] at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:746) ~[spark-core_2.12-3.3.1.jar:3.3.1] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_181] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_181] Caused by: java.lang.ExceptionInInitializerError: Cannot unpack libzstd-jni-1.5.2-1: No such file or directory at java.io.UnixFileSystem.createFileExclusively(Native Method) ~[?:1.8.0_181] at java.io.File.createTempFile(File.java:2024) ~[?:1.8.0_181] at com.github.luben.zstd.util.Native.load(Native.java:99) ~[zstd-jni-1.5.2-1.jar:1.5.2-1] at com.github.luben.zstd.util.Native.load(Native.java:55) ~[zstd-jni-1.5.2-1.jar:1.5.2-1] at com.github.luben.zstd.ZstdOutputStreamNoFinalizer.<clinit>(ZstdOutputStreamNoFinalizer.java:18) ~[zstd-jni-1.5.2-1.jar:1.5.2-1] at com.github.luben.zstd.RecyclingBufferPool.<clinit>(RecyclingBufferPool.java:18) ~[zstd-jni-1.5.2-1.jar:1.5.2-1] at org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:221) ~[spark-core_2.12-3.3.1.jar:3.3.1] ... 15 more 19:09:45.883 [map-output-dispatcher-2] ERROR org.apache.spark.MapOutputTrackerMaster - null java.lang.reflect.InvocationTargetException: null at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_181] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_181] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_181] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_181] at org.apache.spark.io.CompressionCodec$.createCodec(CompressionCodec.scala:87) ~[spark-core_2.12-3.3.1.jar:3.3.1] at org.apache.spark.MapOutputTracker$.serializeOutputStatuses(MapOutputTracker.scala:1492) ~[spark-core_2.12-3.3.1.jar:3.3.1] at org.apache.spark.ShuffleStatus.$anonfun$serializedMapStatus$2(MapOutputTracker.scala:338) ~[spark-core_2.12-3.3.1.jar:3.3.1] at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?] at org.apache.spark.ShuffleStatus.withWriteLock(MapOutputTracker.scala:77) ~[spark-core_2.12-3.3.1.jar:3.3.1] at org.apache.spark.ShuffleStatus.serializedMapStatus(MapOutputTracker.scala:335) ~[spark-core_2.12-3.3.1.jar:3.3.1] at org.apache.spark.MapOutputTrackerMaster$MessageLoop.handleStatusMessage(MapOutputTracker.scala:729) ~[spark-core_2.12-3.3.1.jar:3.3.1] at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:746) ~[spark-core_2.12-3.3.1.jar:3.3.1] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_181] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_181] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_181] Caused by: java.lang.NoClassDefFoundError: Could not initialize class com.github.luben.zstd.RecyclingBufferPool at org.apache.spark.io.ZStdCompressionCodec.<init>(CompressionCodec.scala:221) ~[spark-core_2.12-3.3.1.jar:3.3.1] ... 15 more 19:09:45.883 [map-output-dispatcher-1] ERROR org.apache.spark.MapOutputTrackerMaster - null;;;, 14/Nov/22 09:02;yutiantian005;最终原因：由于配置了 -Djava.io.tmpdir=/data01/spark/tmp，并且/data01/spark/tmp 在某些节点上并不是777权限，导致cluster模式，driver跑在某些节点上时，解压zstd压缩包到/data01/spark/tmp下没有权限，导致失败，报了如上的错误。;;;, 01/Dec/22 15:12;srowen;Can you clarify the issue? this doesn't look like it directly relates to Spark, but, the error message is truncated. We need to see the underlying cause;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Spark catalog and Delta tables
Issue key: SPARK-37648
Issue id: 13417556
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: h.liashchuk
Creator: h.liashchuk
Created: 14/Dec/21 21:38
Updated: 24/Nov/22 00:31
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: I'm using Spark with Delta tables, while tables are created, there are no columns in the table.

Steps to reproduce:
1. Start spark-shell 
{code:java}
spark-shell --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" --conf "spark.sql.legacy.parquet.int96RebaseModeInWrite=LEGACY"{code}
2. Create delta table
{code:java}
spark.range(10).write.format("delta").option("path", "tmp/delta").saveAsTable("delta"){code}
3. Make sure table exists 
{code:java}
spark.catalog.listTables.show{code}
4. Find out that columns are not
{code:java}
spark.catalog.listColumns("delta").show{code}

This is critical for Delta integration with different BI tools such as Power BI or Tableau, as they are querying spark catalog for the metadata and we are getting errors that no columns are found. 
Discussion can be found in Delta repository - https://github.com/delta-io/delta/issues/695
Environment: Spark version 3.1.2
Scala version 2.12.10
Hive version 2.3.7
Delta version 1.0.0
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Nov 18 15:04:14 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xpmg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Dec/21 02:36;chengpan;We have a workaround for this issue in Apache Kyuubi (Incubating), [https://github.com/apache/incubator-kyuubi/pull/1476]

Kyuubi can be considered as a more powerful Spark Thrift Server, it's worth a try.;;;, 18/Nov/22 15:04;clarknova9@gmail.com;Any update here? This continues to be an issue in 3.3.1;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: HadoopRDD#getPartitions fetches FileSystem Delegation Token for every partition
Issue key: SPARK-36328
Issue id: 13392311
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: prabhujoseph
Creator: prabhujoseph
Created: 28/Jul/21 10:11
Updated: 15/Nov/22 02:47
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark Job creates a separate JobConf for every RDD (every hive table partition) in HadoopRDD#getPartitions.

{code}
  override def getPartitions: Array[Partition] = {
    val jobConf = getJobConf()
    // add the credentials here as this can be called before SparkContext initialized
    SparkHadoopUtil.get.addCredentials(jobConf)
{code}

Hadoop FileSystem fetches FileSystem Delegation Token and sets into the Credentials which is part of JobConf. On further requests, will reuse the token from the Credentials if already exists.

{code}
       if (serviceName != null) { // fs has token, grab it
      final Text service = new Text(serviceName);
      Token<?> token = credentials.getToken(service);
      if (token == null) {
        token = getDelegationToken(renewer);
        if (token != null) {
          tokens.add(token);
          credentials.addToken(service, token);
        }
      }
    }
{code}

 But since Spark Job creates a new JobConf (which will have a new Credentials) for every hive table partition, the token is not reused and gets fetched for every partition. This is slowing down the query as each delegation token has to go through KDC and SSL handshake on Secure Clusters.

*Improvement:*

Spark can add the credentials from previous JobConf into the new JobConf to reuse the FileSystem Delegation Token similar to how the User Credentials are added into JobConf after construction.

{code}
     val jobConf = getJobConf()
    // add the credentials here as this can be called before SparkContext initialized
    SparkHadoopUtil.get.addCredentials(jobConf)
{code}



*Repro*

{code}
beeline>
create table parttable (key char(1), value int) partitioned by (p int);
insert into table parttable partition(p=100) values ('d', 1), ('e', 2), ('f', 3);
insert into table parttable partition(p=200) values ('d', 1), ('e', 2), ('f', 3);
insert into table parttable partition(p=300) values ('d', 1), ('e', 2), ('f', 3);

spark-sql>
select value, count(*) from parttable group by value
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): SPARK-41073
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Aug 07 12:53:11 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tek8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Jul/21 15:11;Shockang;I'm working on it.;;;, 07/Aug/21 12:52;apachespark;User 'Shockang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33674;;;, 07/Aug/21 12:53;apachespark;User 'Shockang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33674;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Spark Submitter generates a ConfigMap with the same name
Issue key: SPARK-41060
Issue id: 13500278
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Serhii Nesterov
Creator: Serhii Nesterov
Created: 09/Nov/22 03:12
Updated: 11/Nov/22 00:03
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.3.0, 3.3.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: *Description of the issue:*

There's a problem with submitting spark jobs to K8s cluster: the library generates and reuses the same name for config maps (for drivers and executors). Ideally, for each job 2 config maps should be created: for a driver and an executor. However, the library creates only one driver config map for all jobs (in some cases it generates only one executor map for all jobs in the same manner). So, if I run 5 jobs, then only one driver config map will be generated and used for every job.  During those runs we experience issues when deleting pods from the cluster: executors pods are endlessly created and immediately terminated overloading cluster resources.

 

*The reason of the issue:*

This problem occurs because of the *KubernetesClientUtils* class in which we have *configMapNameExecutor* and *configMapNameDriver* as constants. It seems to be incorrect and should be urgently fixed. I've prepared some changes for review to fix the issue (tested in the cluster of our project).

 

*Steps to reproduce the issue:*

 
 # Create a *KubernetesClientApplication* object.
 # Submit at least 2 jobs (sequentially or using *Thread* for running in parallel).

 

*The results of my observations according to the steps are as follows:*
 # Spark 3.1.2 - The same config map in K8S will be overwritten which means all the jobs will point to the same config map.
 # Spark 3.3.* -  For the first job a new config map will be created. For other jobs an exception will be thrown (the K8S Fabric library does not allow to create a new config map with the existing name).
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 11/Nov/22 00:03;Serhii Nesterov;Screenshot 2022-11-09 015432.png;https://issues.apache.org/jira/secure/attachment/13052071/Screenshot+2022-11-09+015432.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): scala
Custom field (Last public comment date): Fri Nov 11 00:03:29 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1brds:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Nov/22 03:48;apachespark;User '19Serhii99' has created a pull request for this issue:
https://github.com/apache/spark/pull/38574;;;, 09/Nov/22 03:48;apachespark;User '19Serhii99' has created a pull request for this issue:
https://github.com/apache/spark/pull/38574;;;, 11/Nov/22 00:03;Serhii Nesterov;After applying the fixes from the pull request config maps are created correctly:

!Screenshot 2022-11-09 015432.png!;;;
Affects Version/s.1: 3.3.0
Attachment.1: 
EMR Versions: EMR-6.10.0, EMR-6.10.1, EMR-6.4.0, EMR-6.5.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Make it easier to convert numpy arrays to Spark Dataframes
Issue key: SPARK-37697
Issue id: 13418661
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: douglas.moore@databricks.com
Creator: douglas.moore@databricks.com
Created: 20/Dec/21 16:50
Updated: 02/Nov/22 21:06
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: Make it easier to convert numpy arrays to dataframes.


Often we receive errors:

 
{code:java}
df = spark.createDataFrame(numpy.arange(10))
Can not infer schema for type: <class 'numpy.int64'>
{code}
 

OR
{code:java}
df = spark.createDataFrame(numpy.arange(10.))
Can not infer schema for type: <class 'numpy.float64'>
{code}
 

Today (Spark 3.x) we have to:
{code:java}
spark.createDataFrame(pd.DataFrame(numpy.arange(10.))) {code}
Make this easier with a direct conversion from Numpy arrays to Spark Dataframes.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 01/Nov/22 02:49;douglas.moore@databricks.com;image-2022-10-31-22-49-37-356.png;https://issues.apache.org/jira/secure/attachment/13051657/image-2022-10-31-22-49-37-356.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Nov 02 21:06:06 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xwfs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Dec/21 20:22;ddavies1;Hey Douglas,
 
I've definitely been caught by numpy types a few times in some of our spark workflows, so would be keen to solve in PySpark directly also.
 
Do you want to be able to create a DataFrame from a list of numpy numbers directly? This is supported in Pandas, but I think it's not possible to do this even with native Python types in Spark (e.g. my understanding is that the input to createDataFrame is required to be an iterable of rows), so maybe there's a discussion around supporting that. For example, running the below:
{code:java}
df = spark.createDataFrame([1,2,3,4,5]){code}
Fails with:
{code:java}
TypeError: Can not infer schema for type: <class 'int'> {code}
The more common issue I've come across though is something like the following not being supported:
{code:java}
df = spark.createDataFrame([np.arange(10), np.arange(10)]){code}
(I.e. where each row could be a numpy list and/or the overall input list of rows is wrapped in a numpy array also).
 
I'd be happy to take the work on for this PR- whether the first possible case, or the second (it would be my first contribution, so if you/ anyone else think this is more complex than I'm currently estimating below, let me know).
 
For creating a dataframe with a flat iterable, this looks like it would be an addition of a createDataFrame function around [here|https://github.com/apache/spark/blob/master/python/pyspark/sql/session.py#L700]
 
For the second problem- i.e. where the input model remains the same, but rows are provided as numpy arrays; I'd be keen to re-use numpy's ndarray tolist() function here. Not only does this push the underlying array object into Python lists, which PySpark already supports, but it also has the benefit of converting list items of the numpy-specific types to Python native scalars. From a brief glance I've taken, it looks like this would need to be taken into account in three places:
 
 - In the set of prepare functions in session.py [here|https://github.com/apache/spark/blob/master/python/pyspark/sql/session.py#L912]
 - In the conversion function [here|https://github.com/apache/spark/blob/master/python/pyspark/sql/types.py#L1447]
 - In the schema inference function [here|https://github.com/apache/spark/blob/master/python/pyspark/sql/types.py#L1280]
 
This would work for inputs where rows are numpy arrays of any type; but a bit more work would be needed to make a row like the following work:
 
{code:java}
[1, 2, numpy.int64(3)]{code}

 

Hope that all makes sense- let me know which of the two problems you are more interested in solving.
 
I'd also be keen to get a review from someone of whether any of my solutions made sense;;;, 31/Oct/22 21:00;XinrongM;Hi, we have NumPy input support https://issues.apache.org/jira/browse/SPARK-39405 in Spark 3.4.0.;;;, 01/Nov/22 02:58;douglas.moore@databricks.com;[~XinrongM] can confirm the one dimensional case works on spark 3.4.0 and not 3.3.x !
{code:java}
df = spark.createDataFrame(numpy.arange(10))
df = spark.createDataFrame(numpy.arange(10.)){code}
 

The two dimensional case needs  a little more work:
{code:java}
df = spark.createDataFrame(np.reshape(np.arange(100.),[10,10]))

ValueError: Shape of passed values is (10, 10), indices imply (10, 2){code}
 

 

Whereas with Pandas, we can get that 2d array into a Spark DataFrame.

 
{code:java}
spark.createDataFrame(pd.DataFrame(np.reshape(np.arange(100.),[10,10]))).display(){code}
 

!image-2022-10-31-22-49-37-356.png|width=776,height=233!;;;, 01/Nov/22 16:58;douglas.moore@databricks.com;Sorry, I don't know what's going on...

This works: 
{noformat}
spark.createDataFrame(np.array([[1, 2], [3, 4]])).show(){noformat}
and this works:
{code:java}
spark.createDataFrame(np.arange(4).reshape([2,2])).show(){code}
Both have same type() and same .shape

However, scale up to 10x10 and it doesn't work:
{code:java}
spark.createDataFrame(np.arange(100).reshape([10,10])).show(){code}
Error:
{code:java}
ValueError: Shape of passed values is (10, 10), indices imply (10, 2)---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<command-3974870967402265> in <cell line: 1>()
----> 1 spark.createDataFrame(np.arange(100).reshape([10,10])).show()

/databricks/spark/python/pyspark/instrumentation_utils.py in wrapper(*args, **kwargs)
     46             start = time.perf_counter()
     47             try:
---> 48                 res = func(*args, **kwargs)
     49                 logger.log_success(
     50                     module_name, class_name, function_name, time.perf_counter() - start, signature

/databricks/spark/python/pyspark/sql/session.py in createDataFrame(self, data, schema, samplingRatio, verifySchema)
   1208                     )
   1209 
-> 1210             data = pd.DataFrame(data, columns=column_names)
   1211 
   1212         if has_pandas and isinstance(data, pd.DataFrame):

/databricks/python/lib/python3.9/site-packages/pandas/core/frame.py in __init__(self, data, index, columns, dtype, copy)
    670                 )
    671             else:
--> 672                 mgr = ndarray_to_mgr(
    673                     data,
    674                     index,

/databricks/python/lib/python3.9/site-packages/pandas/core/internals/construction.py in ndarray_to_mgr(values, index, columns, dtype, copy, typ)
    322     )
    323 
--> 324     _check_values_indices_shape_match(values, index, columns)
    325 
    326     if typ == "array":

/databricks/python/lib/python3.9/site-packages/pandas/core/internals/construction.py in _check_values_indices_shape_match(values, index, columns)
    391         passed = values.shape
    392         implied = (len(index), len(columns))
--> 393         raise ValueError(f"Shape of passed values is {passed}, indices imply {implied}")
    394 
    395 

ValueError: Shape of passed values is (10, 10), indices imply (10, 2) {code};;;, 01/Nov/22 17:05;douglas.moore@databricks.com;This fails with the same value error:
{code:java}
spark.createDataFrame(np.array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
       [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
       [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
       [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],
       [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],
       [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])).show() {code};;;, 01/Nov/22 17:07;douglas.moore@databricks.com;This works: 
{code:java}
spark.createDataFrame(np.arange(100).reshape([50,2])).show() {code};;;, 01/Nov/22 22:15;XinrongM;Thanks [~douglas.moore@databricks.com] , your queries should work when [https://github.com/apache/spark/pull/38473] is in. The Jira is https://issues.apache.org/jira/browse/SPARK-40990.;;;, 02/Nov/22 21:06;XinrongM;The commit is in.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Kryo DataWritingSparkTaskResult registration error
Issue key: SPARK-38088
Issue id: 13426149
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: majsiel
Creator: majsiel
Created: 02/Feb/22 09:07
Updated: 22/Sep/22 07:56
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark 3.1.2, Scala 2.12

I'm registering classes with _sparkConf.registerKryoClasses(Array( ..._ method. Inside there are spark structured streaming code. Following settings are added as well:

sparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
sparkConf.set("spark.kryo.registrationRequired", "true")

Unfortunately, during execution following error is thrown:

Caused by: java.lang.IllegalArgumentException: Class is not registered: org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTaskResult
Note: To register this class use: kryo.register(org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTaskResult.class); 

As far as I can see in [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/WriteToDataSourceV2Exec.scala] 

class org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTaskResult is private (private[v2] case class DataWritingSparkTaskResult) therefore not available to register.

 

!image-2022-02-02-10-09-14-858.png!

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 02/Feb/22 09:09;majsiel;image-2022-02-02-10-09-14-858.png;https://issues.apache.org/jira/secure/attachment/13039597/image-2022-02-02-10-09-14-858.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 22 07:56:46 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z6h4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Sep/22 07:56;linzesu;[~majsiel] Hi, I'm facing the same situation. Did you manage to get around this?;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Abort result stage directly when it failed caused by FetchFailed
Issue key: SPARK-40455
Issue id: 13481692
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 15/Sep/22 11:41
Updated: 15/Sep/22 12:03
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.0, 3.1.2, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Here's a very serious bug：

When result stage failed caused by FetchFailedException,  the previous condition to determine whether result stage retries are allowed is {color:#ff0000}numMissingPartitions < resultStage.numTasks{color}. 

 

If this condition holds on retry, but the other tasks at the current result stage are not killed, when result stage was resubmit, it would got wrong partitions to recalculation.
{code:java}
// DAGScheduler#submitMissingTasks
 
// Figure out the indexes of partition ids to compute.
val partitionsToCompute: Seq[Int] = stage.findMissingPartitions() {code}
It is possible that the number of partitions to be recalculated is smaller than the actual number of partitions at result stage
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 15 12:03:39 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18l40:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Sep/22 12:03;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/37899;;;, 15/Sep/22 12:03;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/37899;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: SQL configuration spark.sql.mapKeyDedupPolicy not always applied
Issue key: SPARK-40388
Issue id: 13480586
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: praetp
Creator: praetp
Created: 08/Sep/22 06:57
Updated: 09/Sep/22 12:25
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I have set spark.sql.mapKeyDedupPolicy to LAST_WIN.

However, I had still one failure where I got
{quote}Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 1201.0 failed 4 times, most recent failure: Lost task 7.3 in stage 1201.0 (TID 1011313) (ip-10-1-34-47.eu-west-1.compute.internal executor 228): java.lang.RuntimeException: Duplicate map key domain was found, please check the input data. If you want to remove the duplicated keys, you can set spark.sql.mapKeyDedupPolicy to LAST_WIN so that the key inserted at last takes precedence.
{quote}
We are confident we set the right configuration in SparkConf (we can find it on the Spark UI -> Environment).

It is our impression this configuration is not propagated reliably to the executors.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Sep 09 12:25:06 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18ebs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Sep/22 12:25;Zing;Hi [~praetp] , can you provide a way to reproduce this bug?;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Error while creating dataset in Java spark-3.x using Encoders bean with Dense Vector. (Issue arises when updating spark from 2.4 to 3.x)
Issue key: SPARK-40074
Issue id: 13476727
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: anujgrgv
Creator: anujgrgv
Created: 15/Aug/22 00:28
Updated: 22/Aug/22 01:06
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.2, 3.3.0
Fix Version/s: 
Component/s: Java API, ML, SQL
Due Date: 
Votes: 0
Labels: 
Description: Encountered a compatibility issue while upgrading spark from 2.4 to 3.x (also scala is upgraded from 2.11 to 2.12). 

This java code below used to work with spark 2.4 but when migrated to 3.x it gives the error (mentioned below) I have done my own research but couldn't find a solution or any related information.
 

 
{code:java|title=Code.java|borderStyle=solid}
public void test() {

final SparkSession spark = SparkSession.builder()
.appName("Test")
.getOrCreate();

DenseClass denseFactor1 = new DenseClass( new DenseVector( new double[]{0.13, 0.24}));

DenseClass denseFactor2 = new DenseClass( new DenseVector( new double[]{0.24, 0.32}));

final List<DenseClass> inputsNew = Arrays.asList(denseFactor1, denseFactor2);

final Dataset<DenseClass> denseVectorDf = spark.createDataset(inputsNew, Encoders.bean(DenseClass.class));

denseVectorDf.printSchema();
}


public static class DenseClass implements Serializable

{ private org.apache.spark.ml.linalg.DenseVector denseVector; }{code}
The error occurs while creating the dataset *denseVectorDf* .

Error
 
{noformat}
}}
{{org.apache.spark.sql.AnalysisException: Cannot up cast `denseVector` from struct<> to struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.
The type path of the target object is:
 - field (class: "org.apache.spark.ml.linalg.DenseVector", name: "denseVector")
You can either add an explicit cast to the input data or choose a higher precision type of the field in the target object}}

{{{noformat}
I have tried to use _double_ instead of dense vector and it works just fine, but fails on using the dense vector with encoders bean.

 

StackOverflow link for the issue: [https://stackoverflow.com/questions/73313660/error-while-creating-dataset-in-java-spark-3-x-using-encoders-bean-with-dense-ve]

 
Environment: Scala 2.12

Spark 3.x
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): java, scala
Custom field (Last public comment date): 2022-08-15 00:28:22.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17qps:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Apache Commons Crypto doesn't support Java 11
Issue key: SPARK-37751
Issue id: 13419521
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: fengsp
Creator: fengsp
Created: 27/Dec/21 09:38
Updated: 19/Aug/22 00:53
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Security
Due Date: 
Votes: 0
Labels: 
Description: For kubernetes, we are using Java 11 in docker, [https://github.com/apache/spark/blob/v3.2.0/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile:]
{code:java}
ARG java_image_tag=11-jre-slim
{code}
We have a simple app:
{code:scala}
object SimpleApp {
  def main(args: Array[String]) {
    val session = SparkSession.builder.getOrCreate
  
    // the size of demo.csv is 5GB
    val rdd = session.read.option("header", "true").option("inferSchema", "true").csv("/data/demo.csv").rdd
    val lines = rdd.repartition(200)
    val count = lines.count()
  }
}
{code}
 
Enable AES-based encryption for RPC connection by the following config:
{code:java}
--conf spark.authenticate=true
--conf spark.network.crypto.enabled=true
{code}
This would cause the following error:
{code:java}
java.lang.IllegalArgumentException: Frame length should be positive: -6119185687804983867
	at org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)
	at org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:150)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.apache.spark.network.crypto.TransportCipher$DecryptionHandler.channelRead(TransportCipher.java:190)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Unknown Source) {code}

The error disappears in 8-jre-slim. It seems that Apache Commons Crypto 1.1.0 only works with Java 8: [https://commons.apache.org/proper/commons-crypto/download_crypto.cgi]
Environment: Spark 3.2.0 on kubernetes
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Aug 19 00:53:16 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y1q0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Aug/22 19:48;benoit_roy;Hello, we have also encountered this issue after upgrading to Java11 (we also migrated to Spark 3.3.0 - standalone), so this also affects Spark 3.3.0 version.

Any suggestions how we can resolve this? - aside from _spark.network.crypto.enabled_ ?;;;, 19/Aug/22 00:53;qiyuangong;Hi [~benoit_roy] . Some hot fix for this issue:
 # Change back to Java 8 if possible.
 # Use Kernel 5.4 or higher. We found this reduce the possibility of this error. 

 ;;;
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: SkewJoin--Stream side skew in BroadcastJoin
Issue key: SPARK-39921
Issue id: 13474127
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wangzhun
Creator: wangzhun
Created: 29/Jul/22 06:54
Updated: 04/Aug/22 11:26
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: There is data skew in BroadcastJoin. Since LocalShuffle cannot cover all cases, we can refer to the solution of SortMergeJoin skew to consider solving the data skew problem in BroadcastJoin.
h3. senairo

!1.png!

!2.png!
h3. Effect

!4.png!

!3.png!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 29/Jul/22 06:57;wangzhun;1.png;https://issues.apache.org/jira/secure/attachment/13047357/1.png, 29/Jul/22 06:57;wangzhun;2.png;https://issues.apache.org/jira/secure/attachment/13047358/2.png, 29/Jul/22 06:57;wangzhun;3.png;https://issues.apache.org/jira/secure/attachment/13047359/3.png, 29/Jul/22 06:57;wangzhun;4.png;https://issues.apache.org/jira/secure/attachment/13047360/4.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 4.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 04 11:26:46 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z17ark:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Aug/22 08:11;yumwang;+1. Would you like to file a PR？;;;, 04/Aug/22 09:45;wangzhun;Yes, PR is in preparation;;;, 04/Aug/22 11:26;apachespark;User 'wang-zhun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37406;;;, 04/Aug/22 11:26;apachespark;User 'wang-zhun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37406;;;
Affects Version/s.1: 
Attachment.1: 29/Jul/22 06:57;wangzhun;2.png;https://issues.apache.org/jira/secure/attachment/13047358/2.png
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Filesystem closed should not be considered as corrupt files
Issue key: SPARK-39389
Issue id: 13448553
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Bone An
Creator: Bone An
Created: 06/Jun/22 10:00
Updated: 27/Jul/22 21:30
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.7, 3.1.2, 3.2.1
Fix Version/s: 
Component/s: Input/Output
Due Date: 
Votes: 0
Labels: correctness
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-39901
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 06 10:22:49 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12z6o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Jun/22 10:22;apachespark;User 'boneanxs' has created a pull request for this issue:
https://github.com/apache/spark/pull/36775;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, Unknown

Summary: Big SQL query leads to StackOverFlowError
Issue key: SPARK-39801
Issue id: 13471941
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tomerr90s
Creator: tomerr90s
Created: 17/Jul/22 21:27
Updated: 17/Jul/22 23:45
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: I have the following DataFrame with the following catalog (using SHC by hortonworks) trying to read an HBase table:
{code:java}
private static final String CATALOG = "{"
                                           + "\"table\":{\"namespace\":\"A_NAME_SPACE\", \"name\":\"A_TABLE_NAME\", \"tableCoder\":\"PrimitiveType\"},"
                                           + "\"rowkey\":\"name\","
                                           + "\"columns\":{"
                                           + "\"name\":{\"cf\":\"rowkey\", \"col\":\"name\", \"type\":\"string\"},"
                                           + "\"m\":{\"cf\":\"d\", \"col\":\"m\", \"type\":\"string\"}"
                                           + "}"
                                           + "}";

System.out.println(
    sparkSession.read()
        .option(HBaseTableCatalog.tableCatalog(), CATALOG)
        .format("org.apache.spark.sql.execution.datasources.hbase")
        .load()
        .filter(String.join(" OR ", keysSQLs))
        .count()
);{code}
Im using the Java API.

keysSQLs contains about 10K elements that look like so:
{code:java}
"(name > '100,00000000000' AND name < '100,99999999999')" {code}
With different keys.

So the final SQL condition is something like:
{code:java}
(name > '100,00000000000' AND name < '100,99999999999') OR (name > '200,00000000000' AND name < '200,99999999999') OR (name > '300,00000000000' AND name < '300,99999999999') OR (name > '400,00000000000' AND name < '400,99999999999') ... {code}
Im getting the following stacktrace on 3.0.3:
{code:java}
java.lang.StackOverflowError
	at scala.collection.generic.Growable.$anonfun$$plus$plus$eq$1(Growable.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:184)
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:47)
	at scala.collection.generic.GenericCompanion.apply(GenericCompanion.scala:53)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$expressionReorder$5.applyOrElse(Canonicalize.scala:80)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$$anonfun$expressionReorder$5.applyOrElse(Canonicalize.scala:80)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.gatherCommutative(Canonicalize.scala:61)
	at org.apache.spark.sql.catalyst.expressions.Canonicalize$.$anonfun$gatherCommutative$1(Canonicalize.scala:61)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)
	at scala.collection.immutable.List.foreach(List.scala:392) {code}
And this stacktrace on 3.1.2 (sometimes I get the same stacktrace as in 3.0.3):
{code:java}
java.lang.StackOverflowError
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.expressions.Or.equals(predicates.scala:680)
	at org.apache.spark.sql.catalyst.trees.TreeNode.fastEquals(TreeNode.scala:156)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapTreeNode$1(TreeNode.scala:262)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$withNewChildren$2(TreeNode.scala:289)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:276)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:230)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:229)
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:228)
	at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$canonicalized$1(Expression.scala:229) {code}
I tried increasing the size of the stack significantly (-Xss256M) but it just leads to Spark taking forever (stopped it after 40minutes) and it doesnt seem like its starting the DF calculations.

When limiting the number of elements in the SQL, too 500 for example, everything works fine.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-07-17 21:27:35.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z16xag:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Spark Shuffle Index Cache ignore the weight of index Path
Issue key: SPARK-39569
Issue id: 13463002
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: chenzhejia
Creator: chenzhejia
Created: 23/Jun/22 15:05
Updated: 23/Jun/22 15:05
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.0, 3.1.2
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: We had the same OOMs problem with [SPARK-33206|https://issues.apache.org/jira/browse/SPARK-33206]. This PR fixed the incorrect weight calculation problem when ExternalShuffle caches ShuffleIndexInformation, but we noticed that the key was ignored, of which type is filePath
 
shuffleIndexCache = CacheBuilder.newBuilder()
      .maximumWeight(JavaUtils.byteStringAsBytes(indexCacheSize))
      .weigher((Weigher<String, ShuffleIndexInformation>)
        (filePath, indexInfo) -> indexInfo.getRetainedMemorySize())
      .build(indexCacheLoader);
 
in our case the length of the index path could be greater than 100, e.g. /data/data2/yarn/nm/usercache/hive/appcache/application_1654741161919_1249246/blockmgr-6b0f7db0-7d55-4270-ad3d-42fe70b5694e/37/shuffle_0_1794_0.index
. This is causing a lot of memory usage in jmap dump. Should we consider the key size when calculating the weight in order to get a more accurate result?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-06-23 15:05:12.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z15fig:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: setPredictionCol for OneVsRest does not persist when saving model to disk
Issue key: SPARK-39544
Issue id: 13454564
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kobakhit
Creator: kobakhit
Created: 21/Jun/22 12:32
Updated: 21/Jun/22 12:38
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: ML
Due Date: 
Votes: 0
Labels: 
Description: The naming of rawPredcitionCol in OneVsRest does not persist after saving and loading a trained model. This becomes an issue when I try to stack multiple One Vs Rest models in a pipeline. Code example below. 
{code:java}
from pyspark.ml.classification import LinearSVC, OneVsRest, OneVsRestModel

data_path = "/sample_multiclass_classification_data.txt"
df = spark.read.format("libsvm").load(data_path)
lr = LinearSVC(regParam=0.01)

# set the name of rawPrediction column
ovr = OneVsRest(classifier=lr, rawPredictionCol = 'raw_prediction')
print(ovr.getRawPredictionCol())

model = ovr.fit(df)model_path = 'temp' + "/ovr_model"

# save and read back in
model.write().overwrite().save(model_path)
model2 = OneVsRestModel.load(model_path)
model2.getRawPredictionCol()

Output:
raw_prediction
'rawPrediction' {code}
 

 
Environment: Python 3.6

Spark 3.2
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-06-21 12:32:11.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z13zfs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.0.1
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: leveldbjni doesn't work in Apple Silicon on macOS
Issue key: SPARK-35782
Issue id: 13384047
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dbtsai
Creator: dbtsai
Created: 16/Jun/21 04:43
Updated: 16/Jun/22 18:56
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 1
Labels: 
Description: leveldbjni doesn't contain the native library for Apple Silicon on macOS. We will need to build native library for Apple Silicon on macOS, and cut a new release so Spark can use it.

However, it is not maintained for a long time, and the last release was in 2016. Per [discussion|http://apache-spark-developers-list.1001551.n3.nabble.com/leveldbjni-dependency-td30146.html] in spark dev mailing list, other platform also runs into the same support issue. Perhaps, we should we consider racksdb as replacement.

Note, here is the rocksdb task to support Apple Silicon, https://github.com/facebook/rocksdb/issues/7720
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): SPARK-36019
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-35781
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 16 18:56:05 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rznk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Jun/21 08:40;yikunkero;I'm not sure the common arm64 native can meet Apple Silicon arch or not? See realted:

[https://github.com/apache/spark/pull/26636]

https://issues.apache.org/jira/browse/SPARK-27721

 ;;;, 17/Jun/21 16:36;dbtsai;[~yikunkero] It will only work for Apple Silicon on Linux but not macOS. For macOS, we need to recompile for this specific OS.;;;, 16/Jun/22 18:56;dongjoon;We provide RocksDB alternative for this issue.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Add log files rolling policy for driver running in cluster mode with spark standalone cluster
Issue key: SPARK-36252
Issue id: 13391242
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jhu
Creator: jhu
Created: 22/Jul/21 04:06
Updated: 15/Jun/22 03:47
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: For a long running driver in cluster mode, there is no rolling policy, the log (stdout/stderr) may accupy lots of space, user needs an external tool to clean the old logs, it's not user friendly. 

For executor, following 5 configurations is used to control the log file rolling policy:
{code:java}
spark.executor.logs.rolling.maxRetainedFiles
spark.executor.logs.rolling.enableCompression
spark.executor.logs.rolling.maxSize
spark.executor.logs.rolling.strategy
spark.executor.logs.rolling.time.interval
{code}

For driver running in cluster mode:
1. reuse the executor settings
2. similar to executor: add following configurations (only works for stderr/stdout for driver in cluster mode)
{code:java}
spark.driver.logs.rolling.maxRetainedFiles
spark.driver.logs.rolling.enableCompression
spark.driver.logs.rolling.maxSize
spark.driver.logs.rolling.strategy
spark.driver.logs.rolling.time.interval
{code}

#2 seems better, do you agree?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jun 15 03:47:59 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0t7yo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Jun/22 03:47;apachespark;User 'jhu-chang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36872;;;, 15/Jun/22 03:47;apachespark;User 'jhu-chang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36872;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Week of month from a date is missing in spark3 for return values of 1 to 6
Issue key: SPARK-38571
Issue id: 13434136
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: KevinAppelBofa
Creator: KevinAppelBofa
Created: 16/Mar/22 14:31
Updated: 14/Jun/22 00:35
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In Spark2 we could use the date_format function with either the W or F flags to compute week of month from a date.  These are computing two different items, the W is having values from 1 to 6 and the F is having values from 1 to 5

Sample code and output of expected
df1 = spark.createDataFrame(
    [
        (1, date(2014, 3, 7)),
        (2, date(2014, 3, 8)),
        (3, date(2014, 3, 30)),
        (4, date(2014, 3, 31)),
        (5, date(2015, 3, 7)),
        (6, date(2015, 3, 8)),
        (7, date(2015, 3, 30)),
        (8, date(2015, 3, 31)),
    ],
    schema="a long, b date",
)
df1 = df1.withColumn("WEEKOFMONTH1-6", F.date_format(F.col("b"), "W"))
df1 = df1.withColumn("WEEKOFMONTH1-5", F.date_format(F.col("b"), "F"))
df1.show()



{+}--{-}{-}{+}--------{-}++{-}------------{-}{-}-------------+                                                                                                                        
| a|        b|WEEKOFMONTH1-6|WEEKOFMONTH1-5|

{+}--{-}{-}{+}--------{-}++{-}------------{-}{-}-------------+
| 1|2014-03-07|            2|            1|
| 2|2014-03-08|            2|            2|
| 3|2014-03-30|            6|            5|
| 4|2014-03-31|            6|            5|
| 5|2015-03-07|            1|            1|
| 6|2015-03-08|            2|            2|
| 7|2015-03-30|            5|            5|
| 8|2015-03-31|            5|            5|

{+}--{-}{-}{+}--------{-}++{-}------------{-}{-}-------------+

 

With the Spark3 having the spark.sql.legacy.timeParserPolicy set to 
EXCEPTION by default this throws an error:
Caused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: W, Please use the SQL function EXTRACT instead
 
However from the EXTRACT function there is nothing available that is extracting the week of month for the values 1 to 6
 
The Spark3 mentions they define our own patterns  located at [https://spark.apache.org/docs/3.2.1/sql-ref-datetime-pattern.html] that are implemented via DateTimeFormatter under the hood: 
[https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html]
 
That site is listing both W and F for week of month
 W week-of-month number 4
 F week-of-month number 3
 
However only F is implemented on the datetime pattern reference
 
Is there another way we can compute this week of month for values 1 to 6 by still using the builtin with Spark3?  Currently we have to set the spark.sql.legacy.timeParserPolicy to LEGACY in order to run this
 
Thank you,
 
Kevin
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): SPARK-39433
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-03-16 14:31:09.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10jhs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: TaskSchedulerImpl should quickly ignore task finished event if its task was  finished state
Issue key: SPARK-39287
Issue id: 13446761
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 25/May/22 10:16
Updated: 25/May/22 11:46
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed May 25 11:46:21 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12o6w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/May/22 11:45;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36665;;;, 25/May/22 11:46;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36665;;;
Affects Version/s.1: 3.1.1
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, Unknown

Summary: Hive client should not gather statistic by default.
Issue key: SPARK-39043
Issue id: 13442039
Parent id: 
Issue Type: Task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 27/Apr/22 10:22
Updated: 19/May/22 15:12
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When use `InsertIntoHiveTable`, when insert overwrite partition, it will call
Hive.loadPartition(), in this method, when `hive.stats.autogather` is true(default is true)

 

{code:java}
// Some comments here
public String getFoo()
      if (oldPart == null) {
        newTPart.getTPartition().setParameters(new HashMap<String,String>());
        if (this.getConf().getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {
          StatsSetupConst.setBasicStatsStateForCreateTable(newTPart.getParameters(),
              StatsSetupConst.TRUE);
        }

public static void setBasicStatsStateForCreateTable(Map<String, String> params, String setting) {
  if (TRUE.equals(setting)) {
    for (String stat : StatsSetupConst.supportedStats) {
      params.put(stat, "0");
    }
  }
  setBasicStatsState(params, setting);
} 

public static final String[] supportedStats = {NUM_FILES,ROW_COUNT,TOTAL_SIZE,RAW_DATA_SIZE};
{code}




Then it set default rowNum as 0, but since spark will update numFiles and rawSize, so rowNum remain 0.

This impact other system like presto's CBO.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri May 06 09:37:58 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11v7c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Apr/22 10:57;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36377;;;, 06/May/22 09:37;cloud_fan;Issue resolved by pull request 36377
[https://github.com/apache/spark/pull/36377];;;
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: FsUrlStreamHandlerFactory deadlock with Hadoop 2 profile
Issue key: SPARK-39094
Issue id: 13443026
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: jzhuge
Creator: jzhuge
Created: 03/May/22 20:48
Updated: 11/May/22 16:47
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.0.3, 3.1.2, 3.2.1
Fix Version/s: 
Component/s: Spark Submit
Due Date: 
Votes: 0
Labels: 
Description: This is another Spark deadlock caused by setting Hadoop FsUrlStreamHandlerFactory, similar to SPARK-31922 and SPARK-26961. We use Spark 2.4.8 and Hadoop 2.7.3 with a few cherry picks.
{code:java}
"spark-listener-group-shared":
waiting to lock monitor 0x0000563048a4c7b8 (object 0x0000000680d8b560, a org.apache.hadoop.conf.Configuration),
which is held by "main"
"main":
waiting to lock monitor 0x00007fd919ac26f8 (object 0x00000006802b6d68, a java.lang.Object),
which is held by "spark-listener-group-shared" {code}
 
{code:java}
"main":
at java.net.URL.getURLStreamHandler(URL.java:1242)
waiting to lock <0x00000006802b6d68> (a java.lang.Object)
at java.net.URL.<init>(URL.java:617)
at java.net.URL.<init>(URL.java:508)
at java.net.URL.<init>(URL.java:457)
at java.net.JarURLConnection.parseSpecs(JarURLConnection.java:175)
at java.net.JarURLConnection.<init>(JarURLConnection.java:158)
at sun.net.www.protocol.jar.JarURLConnection.<init>(JarURLConnection.java:81)
at sun.net.www.protocol.jar.Handler.openConnection(Handler.java:41)
at java.net.URL.openConnection(URL.java:1027)
at java.net.URL.openStream(URL.java:1093)
at java.util.ServiceLoader.parse(ServiceLoader.java:304)
at java.util.ServiceLoader.access$200(ServiceLoader.java:185)
at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)
at java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)
at java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)
at javax.xml.parsers.FactoryFinder$1.run(FactoryFinder.java:293)
at java.security.AccessController.doPrivileged(Native Method)
at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:289)
at javax.xml.parsers.FactoryFinder.find(FactoryFinder.java:267)
at javax.xml.parsers.DocumentBuilderFactory.newInstance(DocumentBuilderFactory.java:119)
at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2563)
at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2539)
at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2452)
locked <0x0000000680d8b560> (a org.apache.hadoop.conf.Configuration)
at org.apache.hadoop.conf.Configuration.get(Configuration.java:1028)
at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1078)
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2236)
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)
at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.createURLStreamHandler(FsUrlStreamHandlerFactory.java:74)
at java.net.URL.getURLStreamHandler(URL.java:1190)
at java.net.URL.<init>(URL.java:617)
at java.net.URL.<init>(URL.java:508)
at java.net.URL.<init>(URL.java:457)
at java.net.JarURLConnection.parseSpecs(JarURLConnection.java:175)
at java.net.JarURLConnection.<init>(JarURLConnection.java:158)
at sun.net.www.protocol.jar.JarURLConnection.<init>(JarURLConnection.java:81)
at sun.net.www.protocol.jar.Handler.openConnection(Handler.java:41)
at java.net.URL.openConnection(URL.java:1027)
at java.net.URL.openStream(URL.java:1093)
at java.util.ServiceLoader.parse(ServiceLoader.java:304)
at java.util.ServiceLoader.access$200(ServiceLoader.java:185)
at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)
at java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)
at java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)
at javax.xml.parsers.FactoryFinder$1.run(FactoryFinder.java:293)
at java.security.AccessController.doPrivileged(Native Method)
at javax.xml.parsers.FactoryFinder.findServiceProvider(FactoryFinder.java:289)
at javax.xml.parsers.FactoryFinder.find(FactoryFinder.java:267)
at javax.xml.parsers.DocumentBuilderFactory.newInstance(DocumentBuilderFactory.java:119)
at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2563)
at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2539)
at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2452)
locked <0x00000006802d9a58> (a org.apache.hadoop.conf.Configuration)
at org.apache.hadoop.conf.Configuration.get(Configuration.java:1028)
at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:59)
at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:130)
at org.apache.spark.sql.SparkSession$$anonfun$sharedState$1.apply(SparkSession.scala:130)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:130)
locked <0x0000000680dd97b8> (a org.apache.spark.sql.SparkSession)
at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:129)
at org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:326)
at org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1115)
at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:154)
at org.apache.spark.sql.SparkSession$$anonfun$sessionState$2.apply(SparkSession.scala:153)
at scala.Option.getOrElse(Option.scala:121)
at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:153)
locked <0x0000000680dd97b8> (a org.apache.spark.sql.SparkSession)
at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:150)
at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:55)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:317)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:168)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:856)
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:184)
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:931)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:940)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

"spark-listener-group-shared":
at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2448)
waiting to lock <0x0000000680d8b560> (a org.apache.hadoop.conf.Configuration)
at org.apache.hadoop.conf.Configuration.get(Configuration.java:1028)
at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1078)
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2236)
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2654)
at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.createURLStreamHandler(FsUrlStreamHandlerFactory.java:74)
at java.net.URL.getURLStreamHandler(URL.java:1255)
locked <0x00000006802b6d68> (a java.lang.Object)
at java.net.URL.<init>(URL.java:617)
at java.net.URL.<init>(URL.java:508)
at java.net.URL.<init>(URL.java:457)
at javax.crypto.JceSecurity.<clinit>(JceSecurity.java:229)
at javax.crypto.Cipher.getInstance(Cipher.java:518)
at sun.security.ssl.JsseJce.getCipher(JsseJce.java:213)
at sun.security.ssl.SSLCipher.isTransformationAvailable(SSLCipher.java:483)
at sun.security.ssl.SSLCipher.<init>(SSLCipher.java:472)
at sun.security.ssl.SSLCipher.<clinit>(SSLCipher.java:81)
at sun.security.ssl.CipherSuite.<clinit>(CipherSuite.java:69)
at sun.security.ssl.SSLContextImpl.getApplicableSupportedCipherSuites(SSLContextImpl.java:345)
at sun.security.ssl.SSLContextImpl.access$100(SSLContextImpl.java:46)
at sun.security.ssl.SSLContextImpl$AbstractTLSContext.<clinit>(SSLContextImpl.java:577)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:264)
at java.security.Provider$Service.getImplClass(Provider.java:1703)
at java.security.Provider$Service.newInstance(Provider.java:1661)
at sun.security.jca.GetInstance.getInstance(GetInstance.java:236)
at sun.security.jca.GetInstance.getInstance(GetInstance.java:164)
at javax.net.ssl.SSLContext.getInstance(SSLContext.java:156)
at org.apache.http.conn.ssl.SSLContexts.createDefault(SSLContexts.java:58)
at org.apache.http.conn.ssl.SSLSocketFactory.getSocketFactory(SSLSocketFactory.java:171)
at org.apache.http.impl.conn.SchemeRegistryFactory.createDefault(SchemeRegistryFactory.java:53)
at org.apache.http.impl.client.AbstractHttpClient.createClientConnectionManager(AbstractHttpClient.java:266)
at org.apache.http.impl.client.AbstractHttpClient.getConnectionManager(AbstractHttpClient.java:437)
locked <0x0000000680dd0000> (a org.apache.http.impl.client.DefaultHttpClient)
at org.apache.http.impl.client.AbstractHttpClient.createHttpContext(AbstractHttpClient.java:246)
at org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:771)
locked <0x0000000680dd0000> (a org.apache.http.impl.client.DefaultHttpClient)
at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:118)
at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
...
at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:55)
at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:91)
at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll(AsyncEventQueue.scala:92)
at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:92)
at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1327)
at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82) {code}
 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): SPARK-31692
Inward issue link (Reference): SPARK-26961, HADOOP-16159, SPARK-31933
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed May 11 16:47:50 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z1219c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/May/22 21:22;jzhuge;SPARK-31692 seemed to introduce the issue.;;;, 03/May/22 21:24;jzhuge;A possible fix is to initialize Hadoop configuration then use it set Hadoop FsUrlStreamHandlerFactory. I will put up an PR shortly.;;;, 11/May/22 09:24;LuciferYang;Does spark 3.2.x have this issue? Spark 2.4.x  should have EOL;;;, 11/May/22 16:35;jzhuge;It affects all Spark versions with Hadoop 2 profile which sets the version to 2.7.4.

It is ok with Hadoop 3 profile which sets version to 3.3.2.

 

Base on my assessment in HADOOP-16159, the issue is fixed in Hadoop since 2.10.2, 3.1.1, and 3.2.0.;;;, 11/May/22 16:47;jzhuge;Since this can be fixed by upgrading Hadoop, I will not submit a PR. Here is the patch if anyone is interested:

 
{noformat}
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala b/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala
--- a/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala    (revision ed5039e5155e693ccc9b643462fbc11c125b506e)
+++ b/sql/core/src/main/scala/org/apache/spark/sql/internal/SharedState.scala    (revision e93460b9502e69ac4fd7ed310048df46ac469847)
@@ -42,7 +42,14 @@
  */
 private[sql] class SharedState(val sparkContext: SparkContext) extends Logging {
 
-  SharedState.setFsUrlStreamHandlerFactory(sparkContext.conf, sparkContext.hadoopConfiguration)
+  // SPARK-39094: Ensure Hadoop configuration initialized before setting FsUrlStreamHandlerFactory
+  private val initializedHadoopConf = {
+    // HACK: Call Configuration.size() to initialize the Configuration object
+    sparkContext.hadoopConfiguration.size()
+    sparkContext.hadoopConfiguration
+  }
+
+  SharedState.setFsUrlStreamHandlerFactory(sparkContext.conf, initializedHadoopConf)
 
   // Load hive-site.xml into hadoopConf and determine the warehouse path we want to use, based on
   // the config from both hive and Spark SQL. Finally set the warehouse config value to sparkConf.
{noformat};;;
Affects Version/s.1: 3.0.3
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, Unknown

Summary: dynamicPartitionOverwrite can direct rename to targetPath instead of partition path one by one
Issue key: SPARK-36563
Issue id: 13396684
Parent id: 13396683.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 23/Aug/21 12:47
Updated: 06/May/22 10:12
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: dynamicPartitionOverwrite can direct rename to targetPath instead of partition path one by one
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Aug 23 12:52:42 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u5jk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Aug/21 12:52;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33811;;;
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Make spark stagingDir can be congfigurable
Issue key: SPARK-36579
Issue id: 13397040
Parent id: 13396683.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 25/Aug/21 02:29
Updated: 06/May/22 10:12
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Make spark stagingDir can be congfigurable
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 25 03:17:31 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|hzzwy8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Aug/21 03:16;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33828;;;, 25/Aug/21 03:17;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33828;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Reuse compatible executors for stage-level scheduling
Issue key: SPARK-36699
Issue id: 13400116
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: xiaochang-wu
Creator: xiaochang-wu
Created: 09/Sep/21 02:12
Updated: 06/May/22 10:12
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Current stage-level scheduling allocated separated set of executors for different profiles. This approach simplified implementation, however is a waste of executor resources when the existing executors have enough resources to run the following tasks.

We proposed to reuse executors by defining a "compatible" executor concept: two executors binding to different resource profiles are compatible only when the executorResources (cores in particular if not defining custom resources) are the same, but taskResources can be different.  When the executors are compatible, the tasks can be allocated to any of them even when in the different profiles.  Users defining profiles should make sure the different taskResources are properly specified against the same executorResources.

The typical user scenario is for different stages, user wants to use different core number for the task with same executor resources. For instance in CPU machine learning scenario, to achieve the best performance, given the same executor resources, when in ETL stage, user will allocate 1 core per task and many tasks, and in the following CPU training stage, user will use more cores per task and less tasks. Reusing executors allows better CPU resource utilization and better performance.

The first PR will focus on reusing executors with same cores without custom resources. A SparkConf option is defined to change the default behavior which is not reusing executors.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 09 02:44:57 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uqpk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 09/Sep/21 02:44;apachespark;User 'xwu99' has created a pull request for this issue:
https://github.com/apache/spark/pull/33941;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Spark SQL - Combination of HAVING and SORT not resolved correctly
Issue key: SPARK-39022
Issue id: 13441748
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: lukas.grasmann
Creator: lukas.grasmann
Created: 26/Apr/22 08:43
Updated: 27/Apr/22 15:43
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.1, 3.4.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Example: Given a simple relation {{test}} with two relevant columns {{hotel}} and {{price}} where {{hotel}} is a unique identifier of a hotel and {{price}} is the cost of a night's stay. We would then like to order the {{{}hotel{}}}s by their cumulative prices but only for hotels where the cumulative price is higher than {{{}150{}}}.
h2. Current Behavior

To achieve the goal specified above, we give a simple query that works in most common database systems. Note that we only retrieve {{hotel}} in the {{SELECT ... FROM}} statement which means that the aggregate has to be removed from the result attributes using a {{Project}} node.
{code:scala}
sqlcontext.sql("SELECT hotel FROM test GROUP BY hotel HAVING sum(price) > 150 ORDER BY sum(price)").show{code}
Currently, this yields an {{AnalysisException}} since the aggregate {{sum(price)}} in {{Sort}} is not resolved correctly. Note that the child of {{Sort}} is a (premature) {{Project}} node which only provides {{hotel}} as its output. This prevents the aggregate values from being passed to {{{}Sort{}}}.
{code:scala}
org.apache.spark.sql.AnalysisException: Column 'price' does not exist. Did you mean one of the following? [test.hotel]; line 1 pos 75;
'Sort ['sum('price) ASC NULLS FIRST], true
+- Project [hotel#17]
   +- Filter (sum(cast(price#18 as double))#22 > cast(150 as double))
      +- Aggregate [HOTEL#17], [hotel#17, sum(cast(price#18 as double)) AS sum(cast(price#18 as double))#22]
         +- SubqueryAlias test
            +- View (`test`, [hotel#17,price#18])
               +- Relation [hotel#17,price#18] csv
{code}
The {{AnalysisException}} itself, however, is not caused by the introduced {{Project}} as can be seen in the following example. Here, {{sum(price)}} is part of the result and therefore *not* removed using a {{Project}} node.
{code:scala}
sqlcontext.sql("SELECT hotel, sum(price) FROM test GROUP BY hotel HAVING sum(price) > 150 ORDER BY sum(price)").show{code}
Resolving the aggregate {{sum(price)}} (i.e., resolving it to the aggregate introduced by the {{Aggregate}} node) is still not successful even if there is no {{{}Project{}}}. Spark still throws the following {{AnalysisException}} which is similar to the exception from before. It follows that there is a second error in the analyzer that still prevents successful resolution even if the problem regarding the {{Project}} node is fixed.
{code:scala}
org.apache.spark.sql.AnalysisException: Column 'price' does not exist. Did you mean one of the following? [sum(price), test.hotel]; line 1 pos 87;
'Sort ['sum('price) ASC NULLS FIRST], true
+- Filter (sum(price)#24 > cast(150 as double))
   +- Aggregate [HOTEL#17], [hotel#17, sum(cast(price#18 as double)) AS sum(price)#24]
      +- SubqueryAlias test
         +- View (`test`, [hotel#17,price#18])
            +- Relation [hotel#17,price#18] csv
{code}
 

This error occurs (at least) in Spark versions 3.1.2, 3.2.1, as well as the latest version from the GitHub {{master}} branch.
h2. Current Workaround

The issue can currently be worked around by using a subquery to first retrieve only the hotels which fulfill the condition and then ordering them in the outer query:
{code:sql}
SELECT hotel, sum_price FROM
    (SELECT hotel, sum(price) AS sum_price FROM test GROUP BY hotel HAVING sum(price) > 150) sub
ORDER BY sum_price;
{code}
h2. Proposed Solution(s)

The first change fixes the (premature) insertion of {{Project}} before a {{Sort}} by moving the {{Project}} up in the plan such that the {{Project}} is then parent of the {{Sort}} instead of vice versa. This does not change the results of the computations since both {{Sort}} and {{Project}} do not add or remove tuples from the result.

There are two potential side-effects to this solution:
 * May change some plans generated by DataFrame/DataSet which previously also produced similar errors such that they now yield a result instead. However, this is unlikely to produce unexpected/undesired results (see above).
 * Moving the projection might reduce performance for {{Sort}} since the input is potentially bigger.

{code:scala}
object PreventPrematureProjections extends Rule[LogicalPlan] {
    def apply(plan: LogicalPlan): LogicalPlan = plan.resolveOperatorsUp {
        case sort@Sort(_, _,
            project@Project(_,
                filter@Filter(_,
                    aggregate: Aggregate
                )
            )
        ) =>
        
        project.copy(
            child = sort.copy(
                child = filter.copy(
                    child = aggregate
                )
            )
        )
    }
}
{code}
 

To solve the second problem with aggregates not being resolved, we introduce a new case for {{ResolveAggregateFunctions}} as the second change. The newly introduced code is similar to the cases already in place. Here, we ensure that aggregate functions of the plan can also be resolved if there is a {{Filter}} (introduced by resolving an {{UnresolvedHaving}} node generated by parsing the original query) "between" the {{Sort}} and the {{Aggregate}} nodes.
{code:scala}
case Sort(sortOrder, global, filter@Filter(_, agg: Aggregate)) if agg.resolved =>
    val maybeResolved = sortOrder.map(_.child).map(resolveExpressionByPlanOutput(_, agg))
    resolveOperatorWithAggregate(maybeResolved, agg, (newExprs, newChild) => {
        val newSortOrder = sortOrder.zip(newExprs).map {
        case (sortOrder, expr) => sortOrder.copy(child = expr)
        }
        Sort(newSortOrder, global, filter.copy(child = newChild))
    })
{code}
h2. Changed Behavior

The behavior of one of the TCPDS v2.7 plan stability suite tests changed. For {{{}tpcds-v2.7.0/q6.sql{}}}, the resolved plan is now slightly different since the resolution has changed. This should, however, not affect the results of the query as it consists of only two attributes ({{{}state{}}} and {{{}cnt{}}}).

 

Old {{{}explain.txt{}}}:

 
{code:java}
(37) HashAggregate [codegen id : 8]
Input [2]: [ca_state#2, count#27]
Keys [1]: [ca_state#2]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#29]
Results [3]: [ca_state#2 AS state#30, count(1)#29 AS cnt#31, ca_state#2]

(38) Filter [codegen id : 8]
Input [3]: [state#30, cnt#31, ca_state#2]
Condition : (cnt#31 >= 10)

(39) TakeOrderedAndProject
Input [3]: [state#30, cnt#31, ca_state#2]
Arguments: 100, [cnt#31 ASC NULLS FIRST, ca_state#2 ASC NULLS FIRST], [state#30, cnt#31]{code}
 

New {{{}explain.txt{}}}:

 
{code:java}
(37) HashAggregate [codegen id : 8]
Input [2]: [ca_state#2, count#27]
Keys [1]: [ca_state#2]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#29]
Results [2]: [ca_state#2 AS state#30, count(1)#29 AS cnt#31]

(38) Filter [codegen id : 8]
Input [2]: [state#30, cnt#31]
Condition : (cnt#31 >= 10)

(39) TakeOrderedAndProject
Input [2]: [state#30, cnt#31]
Arguments: 100, [cnt#31 ASC NULLS FIRST, state#30 ASC NULLS FIRST], [state#30, cnt#31]{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 26/Apr/22 08:43;lukas.grasmann;explain_new.txt;https://issues.apache.org/jira/secure/attachment/13042940/explain_new.txt, 26/Apr/22 08:43;lukas.grasmann;explain_old.txt;https://issues.apache.org/jira/secure/attachment/13042941/explain_old.txt
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 27 15:43:06 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11teo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Apr/22 10:09;lukas.grasmann;I am working on a Pull Request for this issue in case the solution described is ok.;;;, 27/Apr/22 15:17;apachespark;User 'Lukas-Grasmann' has created a pull request for this issue:
https://github.com/apache/spark/pull/36378;;;, 27/Apr/22 15:18;apachespark;User 'Lukas-Grasmann' has created a pull request for this issue:
https://github.com/apache/spark/pull/36378;;;, 27/Apr/22 15:43;lukas.grasmann;This is my first contribution. How do I assign this issue to myself?;;;
Affects Version/s.1: 3.2.1
Attachment.1: 26/Apr/22 08:43;lukas.grasmann;explain_old.txt;https://issues.apache.org/jira/secure/attachment/13042941/explain_old.txt
EMR Versions: EMR-6.12.0, EMR-6.4.0, EMR-6.5.0, EMR-6.7.0

Summary: Max iterations reached in Operator Optimization w/left_anti or left_semi join and nested structures
Issue key: SPARK-37222
Issue id: 13410346
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ssmith
Creator: ssmith
Created: 06/Nov/21 01:56
Updated: 26/Apr/22 15:44
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: Optimizer
Due Date: 
Votes: 0
Labels: 
Description: The query optimizer never reaches a fixed point when optimizing the query below. This manifests as a warning:

> WARN: Max iterations (100) reached for batch Operator Optimization before Inferring Filters, please set 'spark.sql.optimizer.maxIterations' to a larger value.

But the suggested fix won't help. The actual problem is that the optimizer fails to make progress on each iteration and gets stuck in a loop.

In practice, Spark logs a warning but continues on and appears to execute the query successfully, albeit perhaps sub-optimally.

To reproduce, paste the following into the Spark shell. With Spark 3.1.2 and 3.2.0 but not 3.0.1 it will throw an exception:
{noformat}
case class Nested(b: Boolean, n: Long)
case class Table(id: String, nested: Nested)
case class Identifier(id: String)

locally {
  System.setProperty("spark.testing", "true") // Fail instead of logging a warning
  val df = List.empty[Table].toDS.cache()
  val ids = List.empty[Identifier].toDS.cache()
  df.join(ids, Seq("id"), "left_anti") // also fails with "left_semi"
    .select('id, 'nested("n"))
    .explain()
}
{noformat}
Looking at the query plan as the optimizer iterates in {{RuleExecutor.execute()}}, here's an example of the plan after 49 iterations:
{noformat}
Project [id#2, _gen_alias_108#108L AS nested.n#28L]
+- Join LeftAnti, (id#2 = id#18)
   :- Project [id#2, nested#3.n AS _gen_alias_108#108L]
   :  +- InMemoryRelation [id#2, nested#3], StorageLevel(disk, memory, deserialized, 1 replicas)
   :        +- LocalTableScan <empty>, [id#2, nested#3]
   +- InMemoryRelation [id#18], StorageLevel(disk, memory, deserialized, 1 replicas)
         +- LocalTableScan <empty>, [id#18]
{noformat}
And here's the plan after one more iteration. You can see that all that has changed is new aliases for the column in the nested column: "{{_gen_alias_108#108L}}" to "{{_gen_alias_109#109L}}".
{noformat}
Project [id#2, _gen_alias_109#109L AS nested.n#28L]
+- Join LeftAnti, (id#2 = id#18)
   :- Project [id#2, nested#3.n AS _gen_alias_109#109L]
   :  +- InMemoryRelation [id#2, nested#3], StorageLevel(disk, memory, deserialized, 1 replicas)
   :        +- LocalTableScan <empty>, [id#2, nested#3]
   +- InMemoryRelation [id#18], StorageLevel(disk, memory, deserialized, 1 replicas)
         +- LocalTableScan <empty>, [id#18]
{noformat}
The optimizer continues looping and tweaking the alias until it hits the max iteration count and bails out.

Here's an example that includes a stack trace:
{noformat}
$ bin/spark-shell
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.2.0
      /_/

Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 11.0.12)
Type in expressions to have them evaluated.
Type :help for more information.

scala> :paste
// Entering paste mode (ctrl-D to finish)

case class Nested(b: Boolean, n: Long)
case class Table(id: String, nested: Nested)
case class Identifier(id: String)

locally {
  System.setProperty("spark.testing", "true") // Fail instead of logging a warning
  val df = List.empty[Table].toDS.cache()
  val ids = List.empty[Identifier].toDS.cache()
  df.join(ids, Seq("id"), "left_anti") // also fails with "left_semi"
    .select('id, 'nested("n"))
    .explain()
}

// Exiting paste mode, now interpreting.

java.lang.RuntimeException: Max iterations (100) reached for batch Operator Optimization before Inferring Filters, please set 'spark.sql.optimizer.maxIterations' to a larger value.
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:246)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
  at scala.collection.immutable.List.foreach(List.scala:431)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:138)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:134)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:130)
  at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:148)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:166)
  at org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:163)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:163)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$simpleString$2(QueryExecution.scala:220)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$.append(QueryPlan.scala:600)
  at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:220)
  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:247)
  at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:228)
  at org.apache.spark.sql.Dataset.$anonfun$explain$1(Dataset.scala:543)
  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.Dataset.explain(Dataset.scala:543)
  at org.apache.spark.sql.Dataset.explain(Dataset.scala:567)
  ... 47 elided
{noformat}
Environment: I've reproduced the error on Spark 3.1.2, 3.2.0, and with the current branch-3.2 HEAD (git commit 966c90c0b5) as of November 5, 2021.

The problem does not occur with Spark 3.0.1.

 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-37696
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 26/Apr/22 14:53;nchammas;plan-log.log;https://issues.apache.org/jira/secure/attachment/13042958/plan-log.log
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Apr 26 15:14:57 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0whsg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Nov/21 02:17;ssmith;Plan at the start of one iteration:
{noformat}
Project [id#2, _gen_alias_110#110L AS nested.n#28L]
+- Join LeftAnti, (id#2 = id#18)
   :- Project [id#2, nested#3.n AS _gen_alias_110#110L]
   :  +- InMemoryRelation [id#2, nested#3], StorageLevel(disk, memory, deserialized, 1 replicas)
   :        +- LocalTableScan <empty>, [id#2, nested#3]
   +- InMemoryRelation [id#18], StorageLevel(disk, memory, deserialized, 1 replicas)
         +- LocalTableScan <empty>, [id#18]
{noformat}
After {{org.apache.spark.sql.catalyst.optimizer.PushDownLeftSemiAntiJoin}} (moved {{Join}} down)
{noformat}
Project [id#2, _gen_alias_110#110L AS nested.n#28L]
+- Project [id#2, nested#3.n AS _gen_alias_110#110L]
   +- Join LeftAnti, (id#2 = id#18)
      :- InMemoryRelation [id#2, nested#3], StorageLevel(disk, memory, deserialized, 1 replicas)
      :     +- LocalTableScan <empty>, [id#2, nested#3]
      +- InMemoryRelation [id#18], StorageLevel(disk, memory, deserialized, 1 replicas)
            +- LocalTableScan <empty>, [id#18]
{noformat}
After {{org.apache.spark.sql.catalyst.optimizer.ColumnPruning}} the plan looks the same but plan objects are different.

After {{org.apache.spark.sql.catalyst.optimizer.CollapseProject}} (moved {{Join}} up, added {{Project [id18]}})
{noformat}
Project [id#2, _gen_alias_111#111L AS nested.n#28L]
+- Join LeftAnti, (id#2 = id#18)
   :- Project [id#2, nested#3.n AS _gen_alias_111#111L]
   :  +- InMemoryRelation [id#2, nested#3], StorageLevel(disk, memory, deserialized, 1 replicas)
   :        +- LocalTableScan <empty>, [id#2, nested#3]
   +- Project [id#18]
      +- InMemoryRelation [id#18], StorageLevel(disk, memory, deserialized, 1 replicas)
            +- LocalTableScan <empty>, [id#18]
{noformat}
After {{org.apache.spark.sql.catalyst.optimizer.FoldablePropagation}} the plan looks the same but plan objects are different.

After {{org.apache.spark.sql.catalyst.optimizer.RemoveNoopOperators}} (removed {{Project [id18]}})
{noformat}
Project [id#2, _gen_alias_111#111L AS nested.n#28L]
+- Join LeftAnti, (id#2 = id#18)
   :- Project [id#2, nested#3.n AS _gen_alias_111#111L]
   :  +- InMemoryRelation [id#2, nested#3], StorageLevel(disk, memory, deserialized, 1 replicas)
   :        +- LocalTableScan <empty>, [id#2, nested#3]
   +- InMemoryRelation [id#18], StorageLevel(disk, memory, deserialized, 1 replicas)
         +- LocalTableScan <empty>, [id#18]
{noformat}
Plan at the end of one iteration:
{noformat}
Project [id#2, _gen_alias_112#112L AS nested.n#28L]
+- Join LeftAnti, (id#2 = id#18)
   :- Project [id#2, nested#3.n AS _gen_alias_112#112L]
   :  +- InMemoryRelation [id#2, nested#3], StorageLevel(disk, memory, deserialized, 1 replicas)
   :        +- LocalTableScan <empty>, [id#2, nested#3]
   +- InMemoryRelation [id#18], StorageLevel(disk, memory, deserialized, 1 replicas)
         +- LocalTableScan <empty>, [id#18]
{noformat};;;, 25/Apr/22 14:20;nchammas;Thanks for the detailed report, [~ssmith]. I am hitting this issue as well on Spark 3.2.1, and your minimal test case also reproduces the issue for me.

How did you break down the optimization into its individual steps like that? That was very helpful.

I was able to use your breakdown to work around the issue by excluding {{{}PushDownLeftSemiAntiJoin{}}}:
{code:java}
spark.conf.set(
  "spark.sql.optimizer.excludedRules",
  "org.apache.spark.sql.catalyst.optimizer.PushDownLeftSemiAntiJoin"
){code}
If I run that before running the problematic query (including your test case), it seems to work around the issue.;;;, 26/Apr/22 15:14;nchammas;I've found a helpful log setting that causes Spark to print out detailed information about how exactly a plan is transformed during optimization:
{code:java}
spark.conf.set("spark.sql.planChangeLog.level", "warn") {code}
Here's the log generated by enabling this setting and running Shawn's example: [^plan-log.log]

To confirm what Shawn noted in his comment above, it looks like the chain of events that results in a loop is as follows:
 # ColumnPruning
 # FoldablePropagation _<loop starts here>_
 # RemoveNoopOperators
 # PushDownLeftSemiAntiJoin
 # ColumnPruning
 # CollapseProject
 # _<back to the start of the loop with FoldablePropagation>_

What seems to be the problem is that ColumnPruning inserts some Project operators which are then removed successively by CollapseProject, RemoveNoopOperators, and PushDownLeftSemiAntiJoin.

These rules go back and forth, undoing each other's work, until {{spark.sql.optimizer.maxIterations}} is exhausted.;;;
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0

Summary: Call to MetricsSystem#getServletHandlers() may take place before MetricsSystem becomes running
Issue key: SPARK-38998
Issue id: 13441196
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yuzhihong@gmail.com
Creator: yuzhihong@gmail.com
Created: 22/Apr/22 16:18
Updated: 22/Apr/22 16:32
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Sometimes the following exception is observed:
{code}
java.lang.IllegalArgumentException: requirement failed: Can only call getServletHandlers on a running MetricsSystem
    at scala.Predef$.require(Predef.scala:281)
    at org.apache.spark.metrics.MetricsSystem.getServletHandlers(MetricsSystem.scala:92)
    at org.apache.spark.SparkContext.<init>(SparkContext.scala:597)
    at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
{code}
It seems somehow the MetricsSystem was stopped in between start() and getServletHandlers() calls.
SparkContext should check the MetricsSystem becomes running before calling getServletHandlers()
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-22 16:18:33.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11q1s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Spark  Executors  status always is KILLED
Issue key: SPARK-38930
Issue id: 13440136
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: sorin99
Creator: sorin99
Created: 18/Apr/22 08:50
Updated: 18/Apr/22 09:17
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.1.3
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In  standalone deploy mode, try run spark org.apache.spark.examples.SparkPi or other spark program ,the ui always show Executors  status is killed

spark worker error log :

22/04/18 17:08:27 INFO Worker: Asked to kill executor app-20220418170822-0039/0
22/04/18 17:08:27 INFO ExecutorRunner: Runner thread for executor app-20220418170822-0039/0 interrupted
22/04/18 17:08:27 INFO ExecutorRunner: Killing process!
22/04/18 17:08:27 DEBUG SizeBasedRollingPolicy: 55 + 18896 > 1073741824
22/04/18 17:08:27 DEBUG RollingFileAppender: Closed file /opt/spark/work/app-20220418170822-0039/0/stderr
22/04/18 17:08:27 DEBUG RollingFileAppender: Closed file /opt/spark/work/app-20220418170822-0039/0/stdout
22/04/18 17:08:27 INFO ExecutorRunner: exitCode:Some(143)
22/04/18 17:08:27 INFO Worker: Executor app-20220418170822-0039/0 finished with state KILLED exitStatus 143

 

haved patch  [https://github.com/apache/spark/pull/12012]

run SparkPi commad:

/opt/app/applications/bd-spark/bin/run-example  -{-}class org.apache.spark.examples.SparkPi{-}  -master spark://10.205.90.120:7077,10.205.90.131:7077 --deploy-mode cluster --driver-java-options "-Dlog4j.configuration=[file:/opt/app/applications/bd-spark/conf/log4j.properties|file://opt/app/applications/bd-spark/conf/log4j.properties]" --conf spark.executor.extraJavaOptions="-Dlog4j.configuration=[file:/opt/app/applications/bd-spark/conf/log4j.properties|file://opt/app/applications/bd-spark/conf/log4j.properties]"

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 18/Apr/22 09:17;sorin99;driver.log;https://issues.apache.org/jira/secure/attachment/13042569/driver.log, 18/Apr/22 08:50;sorin99;spark-default.conf;https://issues.apache.org/jira/secure/attachment/13042564/spark-default.conf, 18/Apr/22 08:50;sorin99;spark-env.sh;https://issues.apache.org/jira/secure/attachment/13042565/spark-env.sh, 18/Apr/22 08:50;sorin99;spark-ui.png;https://issues.apache.org/jira/secure/attachment/13042566/spark-ui.png, 18/Apr/22 08:59;sorin99;stderr;https://issues.apache.org/jira/secure/attachment/13042568/stderr
Custom field (Affects version (Component)): 
Custom field (Attachment count): 5.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Important
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-18 08:50:19.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11jwg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.3
Attachment.1: 18/Apr/22 08:50;sorin99;spark-default.conf;https://issues.apache.org/jira/secure/attachment/13042564/spark-default.conf
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: StreamingQuery.processAllAvailable() blocks forever on queries containing mapGroupsWithState(GroupStateTimeout.ProcessingTimeTimeout())
Issue key: SPARK-38917
Issue id: 13439882
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tchristman
Creator: tchristman
Created: 15/Apr/22 18:38
Updated: 18/Apr/22 00:02
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: StreamingQuery.processAllAvailable() blocks forever when called on queries containing a mapGroupsWithState operation configured with GroupStateTimeout.ProcessingTimeTimeout().

 

I think processAllAvailable() should unblock when all incoming data has been processed AND when all existing groupStates do not have a current timeout specified.

 

Sample code to demonstrate this failure follows:
{code:java}
def demoSparkProcessAllAvailableBug() : Unit = {
    val localSpark = SparkSession
      .builder()
      .master("local[*]")
      .appName("demoSparkProcessAllAvailableBug")
      .config("spark.driver.host", "localhost")
      .getOrCreate()

    import localSpark.implicits._

    val demoDataStream = MemoryStream[BugDemo.NameNumberData](1, localSpark.sqlContext)
    demoDataStream.addData(BugDemo.NameNumberData("Alice", 1))
    demoDataStream.addData(BugDemo.NameNumberData("Bob", 2))
    demoDataStream.addData(BugDemo.NameNumberData("Alice", 3))
    demoDataStream.addData(BugDemo.NameNumberData("Bob", 4))

    // StreamingQuery.processAllAvailable() is successful when executing against NoTimeout,
    // but blocks forever when executing against EventTimeTimeout
    val timeoutTypes = List(GroupStateTimeout.NoTimeout(), GroupStateTimeout.ProcessingTimeTimeout())

    for (timeoutType <- timeoutTypes) {
      val totalByName = demoDataStream.toDF()
        .as[BugDemo.NameNumberData]
        .groupByKey(_.Name)
        .mapGroupsWithState(timeoutType)(BugDemo.summateRunningTotal)

      val totalByNameQuery = totalByName
        .writeStream
        .format("console")
        .outputMode("update")
        .start()

      println(s"${timeoutType} query starting to processAllAvailable()")
      totalByNameQuery.processAllAvailable()
      println(s"${timeoutType} query completed processAllAvailable()")

      totalByNameQuery.stop()
    }
  }
}

object BugDemo {
  def summateRunningTotal(name: String, input: Iterator[NameNumberData], groupState: GroupState[RunningTotal]): NameNumberData = {
    var currentTotal: Int = if (groupState.exists) {
      groupState.get.Total
    } else {
      0
    }

    for (nameNumberData <- input) {
      currentTotal += nameNumberData.Number
    }

    groupState.update(RunningTotal(currentTotal))

    NameNumberData(name, currentTotal)
  }

  case class NameNumberData(
    Name: String,
    Number: Integer
  )

  case class RunningTotal(
    Total: Integer
  )
} {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Apr 15 21:50:25 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11ic8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Apr/22 21:50;kabhwan;Would you mind attaching thread dump, please? That would help many of contributors who may want to look into this.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Bug in async commit of Kafka offset in DirectKafkaInputDStream
Issue key: SPARK-38824
Issue id: 13438445
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zauberer
Creator: zauberer
Created: 07/Apr/22 17:51
Updated: 16/Apr/22 01:22
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.0.0, 3.0.3, 3.1.0, 3.1.2, 3.2.1
Fix Version/s: 
Component/s: DStreams
Due Date: 
Votes: 0
Labels: 
Description: I added a few debug statements at the following lines and found few issues.

1. At line 254 of override def compute(validTime: Time): Option[KafkaRDD[K, V]] in DirectKafkaInputDStream.scala:

System.out.print("Called commitAll at time " + validTime + " " +
commitQueue.toArray.mkString("Array(", ", ", ")") + "\n")

2. At line 454 of test("offset recovery from kafka") in DirectKafkaStreamSuite.scala:

print("Called commitAsync at " + time + " " + offsets.mkString("Array(", ", ", ")") + "\n")


This shows that the commitAll call is not properly handled. Since, it is called inside compute function. There is a chance that during last RDD, we will miss the last offset. In the current example we have missed the offset commit of range 8->10.

Can someone confirm if this is a design choice or a bug?

The current log is something like this.

Called commitAll at time 1645548063100 ms Array()
Called commitAll at time 1645548063200 ms Array()
Called commitAll at time 1645548063300 ms Array()
Called commitAll at time 1645548063400 ms Array()
Called commitAll at time 1645548063500 ms Array()
Called commitAll at time 1645548063600 ms Array()
Called commitAll at time 1645548063700 ms Array()
Called commitAll at time 1645548063800 ms Array()
Called commitAll at time 1645548063900 ms Array()
Called commitAll at time 1645548064000 ms Array()
Called commitAll at time 1645548064100 ms Array()
Called commitAll at time 1645548064200 ms Array()
Called commitAsync at 1645548063100 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [0 -> 4]))
Called commitAsync at 1645548063200 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAll at time 1645548064300 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [0 -> 4]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548063300 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548063400 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548063500 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAll at time 1645548064400 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548063600 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548063700 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548063800 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548063900 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAll at time 1645548064500 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548064000 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548064100 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548064200 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548064300 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAll at time 1645548064600 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 4]))
Called commitAsync at 1645548064400 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 8]))
Called commitAsync at 1645548064500 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [8 -> 8]))
Called commitAsync at 1645548064600 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [8 -> 8]))
Called commitAll at time 1645548064700 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [4 -> 8]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [8 -> 8]), OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [8 -> 8]))
Called commitAsync at 1645548064700 ms Array(OffsetRange(topic: 'recoveryfromkafka', partition: 0, range: [8 -> 10]))

Regards,

Souvik Paul
GitHub: @paulsouri
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Apr 16 01:22:43 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z119io:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Apr/22 07:11;dongjoon;cc [~viirya];;;, 16/Apr/22 01:21;viirya;Hmm, I think I can get the idea you want to point out. The offsets to commit are put into the queue by commitAsync, and then are picked up by commitAll in next compute call at DirectKafkaInputDStream. So if the stream job is stopped, the last offsets might be processed but not committed because no more compute call after.

;;;, 16/Apr/22 01:22;viirya;Ideally, I think we might need to call commitAll in stop method of DirectKafkaInputDStream.;;;
Affects Version/s.1: 3.0.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, Unknown

Summary: When ConfigMap creation fails, Spark driver starts but fails to start executors
Issue key: SPARK-38794
Issue id: 13438014
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: seth.horrigan
Creator: seth.horrigan
Created: 05/Apr/22 22:34
Updated: 05/Apr/22 22:41
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: When running Spark in Kubernetes client mode, all executors assume that a ConfigMap exactly matching `KubernetesClientUtils.configMapNameExecutor` will exist (see [https://github.com/apache/spark/blob/02a055a42de5597cd42c1c0d4470f0e769571dc3/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala#L98])

If the ConfigMap creation fails, [https://github.com/apache/spark/blob/02a055a42de5597cd42c1c0d4470f0e769571dc3/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/scheduler/cluster/k8s/KubernetesClusterSchedulerBackend.scala#L80], (due to the Kubernetes control plane being temporarily unavailable or the permissions of the serviceaccount being insufficient to create a ConfigMap), the driver will start fully, then will wait for executors that will forever fail to start due to "MountVolume.SetUp failed for volume \"spark-conf-volume-exec\" : configmap \"spark-exec-...-conf-map\" not found" 

 

Either the driver start-up should fail with an error, or the driver should retry the attempt to create the ConfigMap

--

To reproduce the problem when the Kubernetes control plane is not experiencing issues, start Spark in client mode, but do not give the Kubernetes ServiceAccount permission to create ConfigMap. The driver pod will start successfully, but the executor pods will terminate upon creation, and the driver will not create new executors.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-38079
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-04-05 22:34:39.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z116vk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0

Summary: Web UI add heap dump 
Issue key: SPARK-38758
Issue id: 13437183
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: cutiechi
Creator: cutiechi
Created: 01/Apr/22 09:05
Updated: 01/Apr/22 10:54
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.1
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: The current Web UI can dump threads, so I want to add memory dump
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Apr 01 10:54:22 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z111ts:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Apr/22 10:54;apachespark;User 'cutiechi' has created a pull request for this issue:
https://github.com/apache/spark/pull/36037;;;
Affects Version/s.1: 3.2.1
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0

Summary: Using EquivalentExpressions getEquivalentExprs function instead of getExprState at SubexpressionEliminationSuite
Issue key: SPARK-38754
Issue id: 13437124
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: lijiahong
Creator: lijiahong
Created: 01/Apr/22 03:48
Updated: 01/Apr/22 04:01
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.0, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Using EquivalentExpressions getEquivalentExprs function instead of getExprState at SubexpressionEliminationSuite, as spark in branch 3.1 has no getExprState fcuntion in EquivalentExpressions
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Apr 01 04:01:01 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z111go:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Apr/22 04:00;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/36033;;;, 01/Apr/22 04:01;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/36033;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Repartition by Column that is Int not working properly only on particular numbers. (11, 33)
Issue key: SPARK-38653
Issue id: 13435703
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: John Engelhart
Creator: John Engelhart
Created: 25/Mar/22 03:12
Updated: 27/Mar/22 01:04
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: My understanding is when you call .repartition(a column). For each unique key in that field. The data will go to that partition. There should never be two keys repartitioned to the same part. That behavior is true with a String column. That behavior is also true with an Int column except on certain numbers. In my use case. The magic numbers 11 and 33.
{code:java}
//Int based column repartition
spark.sparkContext.parallelize(Seq(1, 11, 33)).toDF("collectionIndex").
repartition($"collectionIndex").write.mode("overwrite").parquet("path")
//Produces two part files
//String based column repartition
spark.sparkContext.parallelize(Seq("1", "11", "33")).toDF("collectionIndex").
repartition($"collectionIndex").write.mode("overwrite").parquet("path1")
//Produces three part files {code}
 
{code:java}
//Not working as expected
spark.read.parquet("path/part-00000...").distinct.show
spark.read.parquet("path/part-00001...").distinct.show

//Working as expected
spark.read.parquet("path1/part-00000...").distinct.show
spark.read.parquet("path1/part-00001...").distinct.show
spark.read.parquet("path1/part-00002...").distinct.show {code}
!image-2022-03-24-22-16-44-560.png!

This problem really manifested itself when doing something like
{code:java}
spark.sparkContext.parallelize(Seq(1, 11, 33)).toDF("collectionIndex"). repartition($"collectionIndex").write.mode("overwrite").partitionBy("collectionIndex").parquet("path") {code}
Because you end up with incorrect partitions where the data is commingled. 
Environment: This was running on EMR 6.4.0 using Spark 3.1.2 in an EMR Notebook writing to S3
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-03-25 03:12:44.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10t4o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: display the number of empty partitions on spark ui
Issue key: SPARK-38559
Issue id: 13434001
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 16/Mar/22 02:51
Updated: 16/Mar/22 03:16
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL, Web UI
Due Date: 
Votes: 0
Labels: 
Description: When demoting join from broadcast-hash to smj, i think it is necessary to display the number of empty partitions on spark ui.

Otherwise, users might wonder why SMJ is used when joining a small table. Displaying the number of empty partitions is useful for users to understand changes to the execution plan.

Before updated the ui:

!image-2022-03-16-10-56-46-446.png!

After updated the ui, display the number of empty partitions:

!image-2022-03-16-11-07-39-182.png!

 

 

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 16/Mar/22 02:56;caican;image-2022-03-16-10-56-46-446.png;https://issues.apache.org/jira/secure/attachment/13041192/image-2022-03-16-10-56-46-446.png, 16/Mar/22 03:07;caican;image-2022-03-16-11-07-39-182.png;https://issues.apache.org/jira/secure/attachment/13041199/image-2022-03-16-11-07-39-182.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Mar 16 03:16:40 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10ins:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Mar/22 03:16;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/35867;;;, 16/Mar/22 03:16;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/35867;;;
Affects Version/s.1: 
Attachment.1: 16/Mar/22 03:07;caican;image-2022-03-16-11-07-39-182.png;https://issues.apache.org/jira/secure/attachment/13041199/image-2022-03-16-11-07-39-182.png
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Automatically calculate the upper and lower bounds of partitions when no specified partition related params
Issue key: SPARK-38444
Issue id: 13432591
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 08/Mar/22 08:31
Updated: 08/Mar/22 08:50
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: when access rdbms, such as mysql, if partitionColumn, lowerBound, upperBound, numPartitions are not specified, by default only one partition to scan database is working. 

It makes load data from database slow and makes it difficult for users to configure multiple parameters to improve parallelism.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Mar 08 08:50:37 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10a0g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Mar/22 08:49;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/35764;;;, 08/Mar/22 08:50;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/35764;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Support to delete matched rows from jdbc tables
Issue key: SPARK-38431
Issue id: 13432335
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 07/Mar/22 06:12
Updated: 07/Mar/22 08:42
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The Spark SQL cannot perform delete opration when it accesses the RDBMS. I think that It's not friendly.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Mar 07 08:42:03 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z108g0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Mar/22 08:42;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/35748;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: SQLConf.get flaky causes NON-default spark session settings being lost
Issue key: SPARK-38328
Issue id: 13430652
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: andrea0pica
Creator: andrea0pica
Created: 25/Feb/22 10:15
Updated: 25/Feb/22 14:14
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: config, spark-sql
Description: We have been using two different setups and the bug is present in both:
1 Spark 3.1.2
* spark on k8s mode
* delta 1.0.0 (for spark 3.1.2)
* spark.sql.datetime.java8API.enabled=true (NON-default spark session setting)

2 Spark 3.2.1
* spark on k8s mode
* delta 1.1.0 (for spark 3.2.1)
* spark.sql.datetime.java8API.enabled=true (NON-default spark session setting)

It happens that our delta merge calls randomly fail with the error "java.lang.RuntimeException: java.time.Instant is not a valid external type for schema of timestamp",
as if the support for java.time.Instant were not enabled despite the specific spark.sql.datetime.java8API.enabled=true spark session setting.

Sometimes, simply retrying the failed calls (with no change) solves the failures, but the frequency of the error is variable and so is the number of required retries.

The error stack points to spark libraries (not delta library, not our code), namely catalyst:
```
2022-02-03 11:22:53,695 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 23.0 (TID 1542) (10.251.129.64 executor 1): java.lang.RuntimeException: Error while encoding: java.lang.RuntimeException: java.time.Instant is not a valid external type for schema of timestamp
[...similar rows removed...]
if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.sql.catalyst.util.DateTimeUtils$, TimestampType, fromJavaTimestamp, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true]), 1, SRC_VALID_FROM_DTTM), TimestampType), true, false) AS SRC_VALID_FROM_DTTM#1749
[...similar rows removed...]
at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:213)
at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:195)
at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:512)
at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage17.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)
at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
at org.apache.spark.scheduler.Task.run(Task.scala:131)
at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.time.Instant is not a valid external type for schema of timestamp
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.StaticInvoke_1$(Unknown Source)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$Serializer.apply(ExpressionEncoder.scala:209)
... 19 more
```

Tracking the error in spark sources lead us to a piece of code which calls SQLConf.get.datetimeJava8ApiEnabled to select the proper datatype.

[objects.scala](https://github.com/apache/spark/blob/v3.1.2/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/objects/objects.scala)
```
private lazy val errMsg = s" is not a valid external type for schema of ${expected.simpleString}"
```

[RowEncoder.scala](https://github.com/apache/spark/blob/v3.1.2/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/encoders/RowEncoder.scala)
```
def externalDataTypeFor(dt: DataType): DataType = dt match {
case _ if ScalaReflection.isNativeType(dt) => dt
case TimestampType =>
if (SQLConf.get.datetimeJava8ApiEnabled) {
ObjectType(classOf[java.time.Instant])
} else {
ObjectType(classOf[java.sql.Timestamp])
}
case DateType =>
if (SQLConf.get.datetimeJava8ApiEnabled) {
ObjectType(classOf[java.time.LocalDate])
} else {
ObjectType(classOf[java.sql.Date])
}
```

The result of SQLConf.get depends on the existence of an active SparkSession, and this leads to non-default spark session settings sometimes being lost by executors, depending on when they are started.

[SQLConf.get](https://github.com/apache/spark/blob/v3.1.2/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L149)
```
* Returns the active config object within the current scope. If there is an active SparkSession,
* the proper SQLConf associated with the thread's active session is used. If it's called from
* tasks in the executor side, a SQLConf will be created from job local properties, which are set
* and propagated from the driver side, unless a `SQLConf` has been set in the scope by
* `withExistingConf` as done for propagating SQLConf for operations performed on RDDs created
* from DataFrames.
```

We inferred that `SQLConf.get` (getConf) could be flaky, and this would be the reason for our random error above.

In fact, using a custom spark where the above `if` on SQLConf.get.datetimeJava8ApiEnabled is short-circuited for Instant or, better, changing the default in SQLConf.scala solves the issue:

[original SQLConf.scala](https://github.com/apache/spark/blob/v3.1.2/sql/catalyst/src/main/scala/org/apache/spark/sql/internal/SQLConf.scala#L2613)

our change:
```
val DATETIME_JAVA8API_ENABLED = buildConf("spark.sql.datetime.java8API.enabled")
.doc("If the configuration property is set to true, java.time.Instant and " +
"java.time.LocalDate classes of Java 8 API are used as external types for " +
"Catalyst's TimestampType and DateType. If it is set to false, java.sql.Timestamp " +
"and java.sql.Date are used for the same purpose.")
.version("3.0.0")
.booleanConf
.createWithDefault(true) // our test change
// .createWithDefault(false)
```

Browsing spark jira, we found that there are a number of issues (some open, some closed) all pointing to similar problems in SQLConf.get.

* [SPARK-23894](https://issues.apache.org/jira/browse/SPARK-23894) Flaky Test: BucketedWriteWithoutHiveSupportSuite
```
_So how come sometimes its defined? Note that activeSession is an Inheritable thread local. Normally the executor threads are created before activeSession is defined, so they don't inherit anything. But a threadpool is free to create more threads at any time. And when they do, then suddenly the new executor threads will inherit the active session from their parent, a thread in the driver with the activeSession defined._
```
* [SPARK-22938](https://issues.apache.org/jira/browse/SPARK-22938) Assert that SQLConf.get is accessed only on the driver.
```
_Assert if code tries to access SQLConf.get on executor._
_This can lead to hard to detect bugs, where the executor will read fallbackConf, falling back to default config values, ignoring potentially changed non-default configs._
_If a config is to be passed to executor code, it needs to be read on the driver, and passed explicitly._
```
* [SPARK-30798](https://issues.apache.org/jira/browse/SPARK-30798) Scope Session.active in QueryExecution
```
_SparkSession.active is a thread local variable that points to the current thread's spark session. It is important to note that the SQLConf.get method depends on SparkSession.active. In the current implementation it is possible that SparkSession.active points to a different session which causes various problems. Most of these problems arise because part of the query processing is done using the configurations of a different session._
```
* [SPARK-30223](https://issues.apache.org/jira/browse/SPARK-30223) queries in thrift server may read wrong SQL configs
```
_The Spark thrift server creates many SparkSessions to serve requests, and the thrift server serves requests using a single thread. One thread can only have one active SparkSession, so SQLCong.get can't get the proper conf from the session that runs the query._
```
* [SPARK-35252](https://issues.apache.org/jira/browse/SPARK-35252) PartitionReaderFactory's Implemention Class of DataSourceV2: sqlConf parameter is null
```
_The driver construct a RDD instance(DataSourceRDD), the sqlConf parameter pass to the MyPartitionReaderFactory is not null.__{{But when the executor deserialize the RDD, the sqlConf parameter is null.}}_
```
* [SPARK-35324](https://issues.apache.org/jira/browse/SPARK-35324) Spark SQL configs not respected in RDDs

_I think the difference might have to do with the fact that in the RDD case the config isn't in the local properties of the TaskContext._

_* Stepping through the debugger, I see that both RDD and Dataset decide on using or not using the legacy date formatter in DateFormatter.getFormatter._
_* Then in SQLConf.get, both cases find a TaskContext and no existingConf. So they create a new ReadOnlySQLConf from the TaskContext object._
_* RDD and Dataset code path differ in the local properties they find on the TaskContext here. The Dataset code path has spark.sql.legacy.timeParserPolicy in the local properties, but the RDD path doesn't. The ReadOnlySQLConf is created from the local properties, so in the RDD path the resulting config object doesn't have an override for spark.sql.legacy.timeParserPolicy._
 
 
Environment: We have used two configurations:
1 Spark 3.1.2
* spark on k8s mode

* delta 1.0.0 (for spark 3.1.2)
* spark.sql.datetime.java8API.enabled=true (NON-default spark session setting)
2 Spark 3.2.1
* spark on k8s mode
* delta 1.1.0 (for spark 3.2.1)
* spark.sql.datetime.java8API.enabled=true (NON-default spark session setting)
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): SPARK-22938, SPARK-30223, SPARK-23894, SPARK-35324, SPARK-35252, SPARK-30798
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Feb 25 10:19:04 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zy3c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Feb/22 10:19;andrea0pica;The problem was enlightened from the issues linked here but it has never been completely solved.;;;
Affects Version/s.1: 3.2.1
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0

Summary: The Rank windows to be ordered is not necessary in a query
Issue key: SPARK-38280
Issue id: 13429829
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 22/Feb/22 05:59
Updated: 22/Feb/22 07:37
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Feb 22 07:37:00 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zt20:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 22/Feb/22 07:37;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/35605;;;
Affects Version/s.1: 3.0.1
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, Unknown

Summary: Cannot resolve attribute with table reference
Issue key: SPARK-36768
Issue id: 13401426
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: rshkv
Creator: rshkv
Created: 15/Sep/21 16:06
Updated: 21/Feb/22 16:03
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.7, 3.0.3, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Spark seems in some cases unable to resolve attributes that contain multi-part names where the first parts reference a table. Here's a repro:
{code:python}
>>> spark.range(3).toDF("col").write.parquet("testdata")

# Single name part attribute is fine
>>> spark.sql("SELECT col FROM parquet.testdata").show()
+---+
|col|
+---+
|  1|
|  0|
|  2|
+---+

# Name part with the table reference fails
>>> spark.sql("SELECT parquet.testdata.col FROM parquet.testdata").show()

AnalysisException: cannot resolve '`parquet.testdata.col`' given input columns: [col]; line 1 pos 7;
'Project ['parquet.testdata.col]
+- Relation[col#50L] parquet
{code}

The expected behavior is that {{parquet.testdata.col}} is recognized as referring to attribute {{col}} in {{parquet.testdata}} (you'd expect {{AttributeSeq.resolve}} matches [this case|https://github.com/apache/spark/blob/b665782f0d3729928be4ca897ec2eb990b714879/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala#L214-L239]).
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 15 16:35:07 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uysg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Sep/21 16:13;rshkv;This also reproduces on master at time of writing.;;;, 15/Sep/21 16:31;rshkv;In the debugger I see [on this line|https://github.com/apache/spark/blob/b665782f0d3729928be4ca897ec2eb990b714879/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala#L227] {{collectMatches}} doesn't produce any matches because {{qualified3Part}} is an empty map. And it seems to be an empty map because the {{"col"}} attribute in this {{AttributeSeq}} has an empty qualifiers.

On the other hand, if you do
{code:sql}
SELECT t.col FROM parquet.testdata t
{code}
the {{"col"}} attribute in the {{AttributeSeq}} has a {{"t"}} as attribute. And thus we get matches [on this line|https://github.com/apache/spark/blob/b665782f0d3729928be4ca897ec2eb990b714879/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/package.scala#L253] when filtering for the {{"t"}} qualifier.

Naively, that makes we wonder why in the {{"parquet.testdata.col"}} case {{"parquet.testdata"}} is not part of the {{"col"}} attribute's qualifier, but when we alias the table the alias is included as qualifier.;;;, 15/Sep/21 16:35;rshkv;In case you wonder why we care or why we can't just re-write our query with an alias: Those queries without aliases are generated and are meant to be compatible with both Spark SQL and another SQL database (where they work).;;;
Affects Version/s.1: 3.0.3
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: The columns in state schema should be relaxed to be nullable
Issue key: SPARK-38205
Issue id: 13428313
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: kabhwan
Creator: kabhwan
Created: 14/Feb/22 11:20
Updated: 15/Feb/22 22:52
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.1, 3.3.0
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Starting from SPARK-27237, Spark validates the schema of state across query runs to make sure it doesn't fall into more weird issue like SIGSEGV on the runtime.

The comparison logic is reasonable in terms of nullability; it has below matrices:
||existing schema||new schema||allowed||
|nullable|nullable|O|
|nullable|non-nullable|O|
|non-nullable|nullable|X|
|non-nullable|non-nullable|O|

What we miss here is, the nullability of the column can be changed in the optimizer (mostly nullable to non-nullable), and the optimization about nullability could be applied differently with any simple changes.

So this scenario is hypothetically possible:

1. At the first run of the query, optimizer marks some columns from nullable to non-nullable, and it goes to the schema of the state. (state schema has a column with non-nullable)
2. At the second run of the query (possibly with code modification or upgrading Spark version), optimizer no longer marks such columns from nullable to non-nullable, and it goes with comparison of the schema of the state (existing vs new), comparing non-nullable (existing) vs nullable (new), which is NOT allowed.

In terms of storage view for state store, it is not required to determine the column as non-nullable vs nullable. Interface-wise, state store has no concept of schema; so it is safe to relax such constraint, and open the chance for optimizer to do whatever it wants and doesn't break stateful operators.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Feb 15 22:52:36 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zjrc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Feb/22 22:52;kabhwan;I realized the output schema should also be nullable (since the operator will produce output from state), and now puzzled whether there may be cases I’m going to break the existing query (DSv2 sink may check the nullability when writing).

I guess another way is never changing the nullability on optimizer and keep the nullability check in state. I would rely on less invasive way if there is one, since the lifetime of streaming query is long, across Spark versions, and compatibility is a major concern.;;;
Affects Version/s.1: 3.2.1
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Undefined link function causing error in GLM that uses Tweedie family
Issue key: SPARK-38027
Issue id: 13424778
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zamir.evan@gmail.com
Creator: zamir.evan@gmail.com
Created: 25/Jan/22 20:49
Updated: 14/Feb/22 12:18
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: ML
Due Date: 
Votes: 0
Labels: GLM, pyspark
Description: I am trying to use the GLM regression with a Tweedie distribution so I can model insurance use cases. I have set up a very simple example adapted from the docs:


{code:python}
    def create_fake_losses_data(self):
        df = self._spark.createDataFrame([
            ("a", 100.0, 12, 1, Vectors.dense(0.0, 0.0)),
            ("b", 0.0, 12, 1, Vectors.dense(1.0, 2.0)),
            ("c", 0.0, 12, 1, Vectors.dense(0.0, 0.0)),
            ("d", 2000.0, 12, 1, Vectors.dense(1.0, 1.0)), ], ["user", "label", "offset", "weight", "features"])
        logging.info(df.collect())
        setattr(self, 'fake_data', df)
        try:
            glr = GeneralizedLinearRegression(
                family="tweedie", variancePower=1.5, linkPower=-1, offsetCol='offset')
            glr.setRegParam(0.3)
            model = glr.fit(df)
            logging.info(model)
        except Py4JJavaError as e:
            print(e)
        return self
{code}

This causes the following error:

*py4j.protocol.Py4JJavaError: An error occurred while calling o99.toString.
: java.util.NoSuchElementException: Failed to find a default value for link*
        at org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:756)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.ml.param.Params.getOrDefault(params.scala:756)
        at org.apache.spark.ml.param.Params.getOrDefault$(params.scala:753)
        at org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:41)
        at org.apache.spark.ml.param.Params.$(params.scala:762)
        at org.apache.spark.ml.param.Params.$$(params.scala:762)
        at org.apache.spark.ml.PipelineStage.$(Pipeline.scala:41)
        at org.apache.spark.ml.regression.GeneralizedLinearRegressionModel.toString(GeneralizedLinearRegression.scala:1117)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)


I was under the assumption that the default value for link is None, if not defined otherwise.
 
 
Environment: Running on Mac OS X Monterey
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Feb 14 12:18:05 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yy3c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Jan/22 22:33;zamir.evan@gmail.com;Looking into this further I think the issue is arising upon serializing the model either logging it or persisting it to disk. From my logs:

2022-01-25 14:21:33,664 root ERROR An error occurred while calling o1538.toString.
: java.util.NoSuchElementException: Failed to find a default value for link
	at org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:756)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.ml.param.Params.getOrDefault(params.scala:756)
	at org.apache.spark.ml.param.Params.getOrDefault$(params.scala:753)
	at org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:41)
	at org.apache.spark.ml.param.Params.$(params.scala:762)
	at org.apache.spark.ml.param.Params.$$(params.scala:762)
	at org.apache.spark.ml.PipelineStage.$(Pipeline.scala:41)
	at org.apache.spark.ml.regression.GeneralizedLinearRegressionModel.toString(GeneralizedLinearRegression.scala:1117)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
;;;, 14/Feb/22 12:18;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35514;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Python UDFs freeze when libraries with non-standard imports are imported in the namespace
Issue key: SPARK-38143
Issue id: 13427284
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: michaelcredera
Creator: michaelcredera
Created: 08/Feb/22 21:05
Updated: 08/Feb/22 21:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: When importing libraries with nontraditional imports (e.g. an `__init__.py` that doesn't just explicitly import its submodules, but instead uses a function or a custom loader) PySpark UDFs will freeze.

Two libraries that demonstrate this behavior:
 * [snscrape|https://github.com/JustAnotherArchivist/snscrape/blob/master/snscrape/modules/__init__.py]
 * [transformers|https://github.com/huggingface/transformers/blob/master/src/transformers/__init__.py]

The attached `example.zip` archive contains a module that demonstrates this behavior. To reproduce:

```bash
$ pip install transformers snscrape
$ unzip example.zip
$ cd example
$ pip install --editable .
$ example
```

This example module uses relative imports everywhere, with no circular imports.

To demonstrate a fix to this issue, simply erase `import_stuff.py`:

```bash
$ echo "print('erased')" > example/import_stuff.py
```

Then rerun the example:

```bash
$ example
```

My hypothesis is that some libraries take a long time to import. transformers can take up to 4 seconds to import, and snscrape up to 1 second. I think this import performance is causing Spark execution of our UDFs to just slow down to an unusable level.

*Note:* For some reason, changing the import behavior fixes the Spark performance issue, even though the import performance itself doesn't change. 

So, it's some combination of "library that takes a long time to import" and "library with weird imports" that breaks Spark UDFs. I have no clue what Spark internals cause this to be true, but I can demonstrate it with the `snscrape` library with a small modification to their import behavior [demonstrated here|https://github.com/michaelcredera/snscrape/blob/feature/update-imports/snscrape/modules/__init__.py]. This fork of `snscrape` has no associated UDF performance issues, despite the time to import the library being constant.

This is something we can avoid in our code by deferring the import of these "weird" libraries, but seems like odd behavior that I'd like to understand better. If anyone can help me get some insight into why this might be the case in Spark/PySpark, I would greatly appreciate it!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 08/Feb/22 21:06;michaelcredera;example.zip;https://issues.apache.org/jira/secure/attachment/13039790/example.zip
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-02-08 21:05:52.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zdfk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0

Summary: Incorrect scope when using named_windows in CTEs
Issue key: SPARK-37500
Issue id: 13414477
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: laurikoobas
Creator: laurikoobas
Created: 30/Nov/21 08:22
Updated: 08/Feb/22 07:57
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: This works, but shouldn't. The named_window is described outside the CTE that uses it.


{code:sql}
with step_1 as (
  select *
  , min(a) over w1 as min_a_over_w1
  from (select 1 as a, 2 as b, 3 as c)
)
select *
from step_1
window w1 as (partition by b order by c)
{code}


 

 
Environment: Databricks Runtime 9.0, 9.1, 10.0
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Feb 08 07:57:01 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0x74w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Dec/21 22:09;amaliujia;I can take a look on this bug.;;;, 04/Dec/21 06:08;amaliujia;I am thinking this is no longer valid? Running on 3.2 spark-cli:


{code:java}
spark-sql> 
         > with step_1 as (
         >   select *
         >   , min(a) over w1 as min_a_over_w1
         >   from (select 1 as a, 2 as b, 3 as c)
         > )
         > select *
         > from step_1
         > window w1 as (partition by b order by c)
         > ;
Error in query: Window specification w1 is not defined in the WINDOW clause.
{code}
;;;, 08/Feb/22 07:57;laurikoobas;This particular case is fixed indeed, but I believe it was fixed in a bad way.

This works: (notice the JOIN is commented out):
{noformat}
create or replace temporary view test_temp_view as 
with step_1 as (
  select *
  , min(a) over w2 as min_a_over_w1
  from (select 1 as a, 2 as b, 3 as c)
  window w2 as (partition by b order by c)
)
, step_2 as (
  select *
  from (select 1 as e, 2 as f, 3 as g)
  --join step_1 on true
  window w1 as (partition by f order by g)
)
select *
from step_2
{noformat}
This doesn't:
{noformat}
create or replace temporary view test_temp_view as 
with step_1 as (
  select *
  , min(a) over w2 as min_a_over_w1
  from (select 1 as a, 2 as b, 3 as c)
  window w2 as (partition by b order by c)
)
, step_2 as (
  select *
  from (select 1 as e, 2 as f, 3 as g)
  join step_1 on true
  window w1 as (partition by f order by g)
)
select *
from step_2
{noformat}
It seems that JOIN-ng one CTE with a named window to another CTE with a named window will overwrite/clear some scope.

Somehow it also ONLY applies when creating a view, but not when just running the query.;;;
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Executor metrics are missing on prometheus sink
Issue key: SPARK-38117
Issue id: 13426751
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: zoli
Creator: zoli
Created: 05/Feb/22 14:22
Updated: 05/Feb/22 14:23
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Setting up prometheus sink in this way:
{code:java}
-c spark.ui.prometheus.enabled=true
-c spark.executor.processTreeMetrics.enabled=true
-c spark.metrics.conf=/spark/conf/metric.properties{code}
{_}*metric.properties:*{_}{{{{}}{}}}
{code:java}
*.sink.prometheusServlet.class=org.apache.spark.metrics.sink.PrometheusServlet
*.sink.prometheusServlet.path=/metrics/prometheus{code}

Result:

Both of these endpoints have some metrics
{code:java}
<driver-ip>:4040/metrics/prometheus 
<driver-ip>:4040/metrics/executors/prometheus{code}

{{But the executor one}} misses metrics under the executor namespace described here: [https://spark.apache.org/docs/3.1.2/monitoring.html#component-instance--executor]
So everything is missing from {{bytesRead.count}} to {{threadpool.startedTasks}}

There are neither error nor warn level entries in the driver/executor logs.

By changing to ConsoleSink  I can see all the necessary metrics:
{code:java}
*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink
*.sink.console.period=10
*.sink.console.unit=seconds{code}

{{ }}
{{Something is wrong with the spark-prometheus integration}}
Environment: versions: Spark3.1.2, K8s v19
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-02-05 14:22:38.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0za6o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: AvroSerializer can cause java.lang.ClassCastException at run time
Issue key: SPARK-38091
Issue id: 13426260
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Zhen-hao
Creator: Zhen-hao
Created: 02/Feb/22 19:02
Updated: 03/Feb/22 09:38
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: Avro, serializers
Description: {{{}AvroSerializer{}}}'s implementation, at least in {{{}newConverter{}}}, was not 100% based on the {{nternalRow}} and {{SpecializedGetters}} interface. It assumes many implementation details of the interface. 

For example, in 

{code}
      case (TimestampType, LONG) => avroType.getLogicalType match {
          // For backward compatibility, if the Avro type is Long and it is not logical type
          // (the `null` case), output the timestamp value as with millisecond precision.
          case null | _: TimestampMillis => (getter, ordinal) =>
            DateTimeUtils.microsToMillis(timestampRebaseFunc(getter.getLong(ordinal)))
          case _: TimestampMicros => (getter, ordinal) =>
            timestampRebaseFunc(getter.getLong(ordinal))
          case other => throw new IncompatibleSchemaException(errorPrefix +
            s"SQL type ${TimestampType.sql} cannot be converted to Avro logical type $other")
        }
{code}

it assumes the {{InternalRow}} instance encodes {{TimestampType}} as {{{}java.lang.Long{}}}. That's true for {{Unsaferow}} but not for {{{}GenericInternalRow{}}}. 

Hence the above code will end up with runtime exceptions when used on an instance of {{{}GenericInternalRow{}}}, which is the case for Python UDF. 

I didn't get time to dig deeper than that. I got the impression that Spark's optimizer(s) will turn a row into a {{UnsafeRow}} and Python UDF doesn't involve the optimizer(s) and hence each row is a {{{}GenericInternalRow{}}}. 

It would be great if someone can correct me or offer a better explanation. 

 

To reproduce the issue, 

{{git checkout master}} and {{git cherry-pick --no-commit}} [this-commit|https://github.com/Zhen-hao/spark/commit/1ffe8e8f35273b2f3529f6c2d004822f480e4c88]

and run the test {{{}org.apache.spark.sql.avro.AvroSerdeSuite{}}}.

 

You will see runtime exceptions like the following one

\\{code}

Serialize DecimalType to Avro BYTES with logical type decimal *** FAILED ***
  java.lang.ClassCastException: class java.math.BigDecimal cannot be cast to class org.apache.spark.sql.types.Decimal (java.math.BigDecimal is in module java.base of loader 'bootstrap'; org.apache.spark.sql.types.Decimal is in unnamed module of loader 'app')
  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getDecimal(rows.scala:45)
  at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getDecimal$(rows.scala:45)
  at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getDecimal(rows.scala:195)
  at org.apache.spark.sql.avro.AvroSerializer.$anonfun$newConverter$10(AvroSerializer.scala:136)
  at org.apache.spark.sql.avro.AvroSerializer.$anonfun$newConverter$10$adapted(AvroSerializer.scala:135)
  at org.apache.spark.sql.avro.AvroSerializer.$anonfun$newStructConverter$2(AvroSerializer.scala:283)
  at org.apache.spark.sql.avro.AvroSerializer.serialize(AvroSerializer.scala:60)
  at org.apache.spark.sql.avro.AvroSerdeSuite.$anonfun$new$5(AvroSerdeSuite.scala:82)
  at org.apache.spark.sql.avro.AvroSerdeSuite.$anonfun$new$5$adapted(AvroSerdeSuite.scala:67)
  at org.apache.spark.sql.avro.AvroSerdeSuite.$anonfun$withFieldMatchType$2(AvroSerdeSuite.scala:217)
\\{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Feb 02 20:45:57 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0z75s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Feb/22 19:04;Zhen-hao;Can someone tell me how to let Jira render markdown? ;;;, 02/Feb/22 19:08;apachespark;User 'Zhen-hao' has created a pull request for this issue:
https://github.com/apache/spark/pull/35379;;;, 02/Feb/22 20:45;xkrogen;[~Zhen-hao] for formatting you need to use the Atlassian markup: [https://jira.atlassian.com/secure/WikiRendererHelpAction.jspa?section=all]
Basically replace ` ... ` with \{{ ... }} and replace ``` ... ``` with \{code} ... \{code};;;
Affects Version/s.1: 3.0.1
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, Unknown

Summary: Race condition in withHiveState and limited logic in IsolatedClientLoader result in ClassNotFoundException
Issue key: SPARK-37771
Issue id: 13419793
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ivan.sadikov
Creator: ivan.sadikov
Created: 29/Dec/21 01:48
Updated: 02/Feb/22 21:18
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.0, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: There is a race condition between creating a Hive client and loading classes that do not appear in shared prefixes config. For example, we confirmed that the code fails for the following configuration:
{code:java}
spark.sql.hive.metastore.version 0.13.0
spark.sql.hive.metastore.jars maven
spark.sql.hive.metastore.sharedPrefixes <string that does not include com.amazonaws prefix>
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem{code}
And code: 
{code:java}
-- Prerequisite commands to set up the table
-- drop table if exists ivan_test_2;
-- create table ivan_test_2 (a int, part string) using csv location 's3://bucket/hive-test' partitioned by (part);
-- insert into ivan_test_2 values (1, 'a'); 

-- Command that triggers failure
ALTER TABLE ivan_test_2 ADD PARTITION (part='b') LOCATION 's3://bucket/hive-test'{code}
 

Stacktrace (line numbers might differ):
{code:java}
21/12/22 04:37:05 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider
21/12/22 04:37:05 DEBUG IsolatedClientLoader: shared class: org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
21/12/22 04:37:05 DEBUG IsolatedClientLoader: hive class: com.amazonaws.auth.EnvironmentVariableCredentialsProvider - null
21/12/22 04:37:05 ERROR S3AFileSystem: Failed to initialize S3AFileSystem for path s3://bucket/hive-test
java.io.IOException: From option fs.s3a.aws.credentials.provider java.lang.ClassNotFoundException: Class com.amazonaws.auth.EnvironmentVariableCredentialsProvider not found
    at org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:725)
    at org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:688)
    at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:411)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)
    at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
    at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
    at org.apache.hadoop.hive.metastore.Warehouse.getFs(Warehouse.java:112)
    at org.apache.hadoop.hive.metastore.Warehouse.getDnsPath(Warehouse.java:144)
    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createLocationForAddedPartition(HiveMetaStore.java:1993)
    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.add_partitions_core(HiveMetaStore.java:1865)
    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.add_partitions_req(HiveMetaStore.java:1910)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
    at com.sun.proxy.$Proxy58.add_partitions_req(Unknown Source)
    at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.add_partitions(HiveMetaStoreClient.java:457)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
    at com.sun.proxy.$Proxy59.add_partitions(Unknown Source)
    at org.apache.hadoop.hive.ql.metadata.Hive.createPartitions(Hive.java:1514)
    at org.apache.spark.sql.hive.client.Shim_v0_13.createPartitions(HiveShim.scala:773)
    at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createPartitions$1(HiveClientImpl.scala:683)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:346)
    at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$retryLocked$1(HiveClientImpl.scala:247)
    at org.apache.spark.sql.hive.client.HiveClientImpl.synchronizeOnObject(HiveClientImpl.scala:283)
    at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:239)
    at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:326)
    at org.apache.spark.sql.hive.client.HiveClientImpl.createPartitions(HiveClientImpl.scala:676)
    at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$createPartitions$1(PoolingHiveClient.scala:345)
    at org.apache.spark.sql.hive.client.PoolingHiveClient.$anonfun$createPartitions$1$adapted(PoolingHiveClient.scala:344)
    at org.apache.spark.sql.hive.client.PoolingHiveClient.withHiveClient(PoolingHiveClient.scala:112)
    at org.apache.spark.sql.hive.client.PoolingHiveClient.createPartitions(PoolingHiveClient.scala:344)
    at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createPartitions$1(HiveExternalCatalog.scala:1170)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$2(HiveExternalCatalog.scala:150)
    at org.apache.spark.sql.hive.HiveExternalCatalog.maybeSynchronized(HiveExternalCatalog.scala:111)
    at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$1(HiveExternalCatalog.scala:149)
    at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:148)
    at org.apache.spark.sql.hive.HiveExternalCatalog.createPartitions(HiveExternalCatalog.scala:1152)
    at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createPartitions(ExternalCatalogWithListener.scala:213)
    at org.apache.spark.sql.catalyst.catalog.SessionCatalogImpl.createPartitions(SessionCatalog.scala:1552)
    at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.$anonfun$run$16(ddl.scala:530)
    at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.$anonfun$run$16$adapted(ddl.scala:529)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand.run(ddl.scala:529)
    ...
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: Class com.amazonaws.auth.EnvironmentVariableCredentialsProvider not found
    at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)
    at org.apache.hadoop.fs.s3a.S3AUtils.loadAWSProviderClasses(S3AUtils.java:722)
    ... 118 more{code}
 

Based on the stacktrace, we call {{withHiveState}} method which sets {{IsolatedClientLoader.classLoader}} class loader for Hadoop configuration. All of the Hadoop configuration and file system code runs as a closure within {{{}withHiveState{}}}.

Hadoop configuration uses the set class loader to load classes with {{conf.getClassByNameOrNull()}} method. When isolated class loader tries to load the class and the class is not shared (com.amazonaws prefix is not shared), it will only look up the jars that are loaded with the class loader without checking the base class loader.

To mitigate the issue, you need to set:
 * {{spark.sql.hive.metastore.sharedPrefixes <string that includes com.amazonaws prefix>}}

Or update credentials providers to not load {{{}com.amazonaws.auth.EnvironmentVariableCredentialsProvider{}}}.
 
I think we should revisit the mechanism of sharing classes in IsolatedClassLoader and maybe only enforcing so that Hive classes are loaded first instead of in isolation.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): HADOOP-17372
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Feb 02 21:18:53 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y3eg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.3.0
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 07/Jan/22 19:24;stevel@apache.org;probably related to HADOOP-17372, which makes sure the hive classloader isn't picked up for class lookups in the config

try with hadoop 3.3.1 binaries;;;, 02/Feb/22 17:15;stevel@apache.org;[~ivan.sadikov] -any update here?;;;, 02/Feb/22 21:18;ivan.sadikov;I could not manage to work around the issue with Hadoop 3.3.1 binaries, it still persists. Shared prefixes config works; however, I found there are more issues with IsolatedClassLoader which might need to be fixed, e.g. the incorrect parent class loader is passed to IsolatedClassLoader in certain situations - I am debugging this now.

No updates on the fix yet, workaround with the config works, and the issue is not blocking me at the moment.;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Cannot fetch remote jar correctly
Issue key: SPARK-37962
Issue id: 13423447
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: cutiechi
Creator: cutiechi
Created: 19/Jan/22 09:18
Updated: 19/Jan/22 09:23
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: When my Jar link address is encoded, the Jar cannot be pulled correctly

Log:

!image-2022-01-19-17-18-24-795.png!

 

My static file server(tomcat) log:

!image-2022-01-19-17-21-53-011.png!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 19/Jan/22 09:18;cutiechi;image-2022-01-19-17-18-24-795.png;https://issues.apache.org/jira/secure/attachment/13039060/image-2022-01-19-17-18-24-795.png, 19/Jan/22 09:21;cutiechi;image-2022-01-19-17-21-53-011.png;https://issues.apache.org/jira/secure/attachment/13039061/image-2022-01-19-17-21-53-011.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jan 19 09:23:20 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ypwg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Jan/22 09:23;cutiechi;The root cause was that the link I had encoded was decoded;;;
Affects Version/s.1: 3.2.0
Attachment.1: 19/Jan/22 09:21;cutiechi;image-2022-01-19-17-21-53-011.png;https://issues.apache.org/jira/secure/attachment/13039061/image-2022-01-19-17-21-53-011.png
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Error reading old dates when AQE is enabled in Spark 3.1. Works when AQE is disabled
Issue key: SPARK-37898
Issue id: 13422528
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gasparms
Creator: gasparms
Created: 13/Jan/22 13:09
Updated: 13/Jan/22 13:14
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Hi guys, 
 
I was testing an spark job that fail when I encountered something that is not consistent among different spark versions.  I reduced my code to be replicated easily with a simple spark-shell. Note: Code logic probably does not make sense :)
 
The following snippet:
 
 - Works with Spark 3.1.2 and 3.1.3-rc  when AQE disabled
 - Fails with Spark 3.1.2 and 3.1.3-rc  when AQE enabled
 - Works with Spark 3.2.0 always
{code:java}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.expressions.Window


spark.conf.set("spark.sql.legacy.parquet.int96RebaseModeInRead", "LEGACY")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "LEGACY")

val dataset = spark.read.parquet("/tmp/parquet-output")

val window = Window.orderBy(dataset.col("date").desc)
val resultDataset = dataset.withColumn("rankedFilterOverPartition", dense_rank().over(window)).filter("rankedFilterOverPartition = 1").drop("rankedFilterOverPartition")
println(resultDataset.rdd.getNumPartitions){code}
 
Previously I wrote data with this snippet and Spark 2.2 to write data in the path /tmp/parquet-output.
 
{code:java}
import spark.implicits._
import java.sql.Timestamp
import org.apache.spark.sql.functions._

case class MyCustomClass(id_col: Int, date: String, timestamp_col: java.sql.Timestamp)

val dataset = Seq(MyCustomClass(1, "0001-01-01", Timestamp.valueOf("1000-01-01 10:00:00")), MyCustomClass(2, "0001-01-01", Timestamp.valueOf("1000-01-01 10:00:00"))).toDF
 
dataset.select($"id_col", $"date".cast("date"), $"timestamp_col").write.mode("overwrite").parquet("/tmp/parquet-output"){code}
 
The error is:
{code:java}
scala> println(resultDataset.rdd.getNumPartitions) 22/01/13 13:45:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
22/01/13 13:45:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
22/01/13 13:45:17 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+'s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.datetimeRebaseModeInRead to 'LEGACY' to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.datetimeRebaseModeInRead to 'CORRECTED' to read the datetime values as it is.
at org.apache.spark.sql.execution.datasources.DataSourceUtils$.newRebaseExceptionInRead(DataSourceUtils.scala:147)
at org.apache.spark.sql.execution.datasources.DataSourceUtils.newRebaseExceptionInRead(DataSourceUtils.scala){code}
 

¿It's possible fix it for 3.1 branch? 

 
Regards
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-01-13 13:09:26.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yk8o:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Option "--files" with local:// prefix is not honoured for Spark on kubernetes
Issue key: SPARK-35715
Issue id: 13383244
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Pardhu
Creator: Pardhu
Created: 10/Jun/21 16:52
Updated: 13/Jan/22 07:00
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.2, 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: When we provide a local file as a dependency using "--files" option, the file is not getting copied to work directories of executors.
h5. Example 1:

 
{code:java}
$SPARK_HOME/bin/spark-submit --master k8s://https://<ip-address-k8s> \ 
--deploy-mode cluster \ 
--name spark-pi \ 
--class org.apache.spark.examples.SparkPi \ 
--conf spark.executor.instances=1 \ 
--conf spark.kubernetes.container.image=<spark-3.1.2-image> \ 
--conf spark.kubernetes.driver.pod.name=sparkdriverpod \ 
--files local:///etc/xattr.conf \ 
local:///opt/spark/examples/jars/spark-examples_2.12-3.1.2.jar 1000000
{code}
 
h6. Content of Spark Executor work-dir:

 
{code:java}
~$ kubectl exec -n default spark-pi-22de6279f6bec01c-exec-1 ls /opt/spark/work-dir/
spark-examples_2.12-3.1.2.jar
{code}
 

We can notice here that the file _/etc/xattr.conf_ is *NOT* copied to  _/opt/spark/work-dir/ ._

 
----
 

{{Instead of using "–files", if we use "--jars" option the file is getting copied as expected.}}
h5. Example 2:
{code:java}
$SPARK_HOME/bin/spark-submit --master k8s://https://<ip-address-k8s> \ 
--deploy-mode cluster \ 
--name spark-pi \ 
--class org.apache.spark.examples.SparkPi \ 
--conf spark.executor.instances=1 \ 
--conf spark.kubernetes.container.image=<spark-3.1.2-image> \ 
--conf spark.kubernetes.driver.pod.name=sparkdriverpod \ 
--jars local:///etc/xattr.conf \ 
local:///opt/spark/examples/jars/spark-examples_2.12-3.1.2.jar 1000000


{code}
h6. Content of Spark Executor work-dir:

 
{code:java}
~$ kubectl exec -n default spark-pi-22de6279f6bec01c-exec-1 ls /opt/spark/work-dir/
spark-examples_2.12-3.1.2.jar

xattr.conf

{code}
We can notice here that the file _/etc/xattr.conf_ *IS COPIED* to  _/opt/spark/work-dir/ ._

 

I tested this with versions *3.1.2* and *3.0.2*. It is behaving the same way in both cases.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jan 13 07:00:26 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rup4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 10/Jun/21 16:58;xkrogen;Not sure about k8s, but at least for YARN this is expected -- the {{local:}} prefix indicates that the file(s) shouldn't be copied because they're already present on the local filesystems. The {{file:}} scheme is supposed to be used to indicate something that's currently on your local FS but should be distributed for you. Actually, from my understanding, it looks like the bug is when you use {{jars}}, which shouldn't be copying anything since you've used the {{local}} scheme .. But again, not sure if k8s is supposed to work differently. Hopefully someone with more experience there can chime in.;;;, 13/Jan/22 07:00;zhongjingxiong;It seems that spark 3 does not support the schema using local as the path. You can try file:///etc/xattr.conf;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Built-in ORC reader cannot read data file in sub-directories created by Hive Tez
Issue key: SPARK-37749
Issue id: 13419493
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: IAmAdele
Creator: IAmAdele
Created: 27/Dec/21 07:09
Updated: 30/Dec/21 03:37
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Input/Output, SQL
Due Date: 
Votes: 0
Labels: 
Description: A Partitioned Hive Table is created and load data in HDP 3.1.4. The Hive engine is Tez, and the storage format is ORC. The data direcotry is like:

table1/statt_dt=2021-12-08/-ext-10000/000000_0

 

The result of SparkSQL which is "select * from table1" does not include the data of partition 2021-12-08.

 
Environment: HDP 3.1.4
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-27 07:09:13.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y1js:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Support pushing down a dynamic partition pruning from one join to other joins
Issue key: SPARK-37616
Issue id: 13416873
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 12/Dec/21 09:03
Updated: 12/Dec/21 11:55
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Support pushing down a dynamic partition pruning from one join to other joins
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Dec 12 11:55:57 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xleo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Dec/21 11:55;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34871;;;, 12/Dec/21 11:55;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34871;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: DatasourceV2 `exists ... select *` column push down
Issue key: SPARK-37595
Issue id: 13416249
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: wangzhun
Creator: wangzhun
Created: 09/Dec/21 09:42
Updated: 09/Dec/21 09:49
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: The datasourcev2 table is very slow when executing TPCDS, because `exists ... select *` will not push down the cropped columns to the data source

 

Add test in `org.apache.spark.sql.connector.DataSourceV2SQLSuite`
{code:java}
test("datasourcev2 exists") {
    val t1 = s"${catalogAndNamespace}t1"
    withTable(t1) {
      sql(s"CREATE TABLE $t1 (col1 string, col2 string) USING $v2Format")
      val t2 = s"${catalogAndNamespace}t2"
      withTable(t2) {
        sql(s"CREATE TABLE $t2 (col1 string, col2 string) USING $v2Format")
        val query = sql(s"select * from $t1 where not exists" +
            s"(select * from $t2 where t1.col1=t2.col1)").queryExecution
        // scalastyle:off println
        println(query.executedPlan)
      }
    }
  }


AdaptiveSparkPlan isFinalPlan=false
+- BroadcastHashJoin [col1#17], [col1#19], LeftSemi, BuildRight, false
   :- Project [col1#17, col2#18]
   :  +- BatchScan[col1#17, col2#18] class org.apache.spark.sql.connector.catalog.InMemoryTable$InMemoryBatchScan RuntimeFilters: []
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#28]
      +- Project [col1#19]
         +- BatchScan[col1#19, col2#20] class org.apache.spark.sql.connector.catalog.InMemoryTable$InMemoryBatchScan RuntimeFilters: []


Expectation is `BatchScan[col1#19] class org.apache.spark.sql.connector.catalog.InMemoryTable$InMemoryBatchScan RuntimeFilters: []` {code}
Reason `Batch("Early Filter and Projection Push-Down" V2ScanRelationPushDown` is executed before `Batch("RewriteSubquery"`, parallel datasourceV2 does not support `FileSourceStrategy`
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-09 09:42:37.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xi1k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: MapType supports orderable semantics
Issue key: SPARK-34819
Issue id: 13366643
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: EdisonWang
Creator: EdisonWang
Created: 22/Mar/21 06:06
Updated: 07/Dec/21 06:24
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Comparable/orderable semantics for map types is useful in some scenarios, and it's implemented in hive/presto. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): SPARK-37560
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): SPARK-18134
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri May 14 15:39:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0p148:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Mar/21 02:35;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/31967;;;, 14/May/21 15:39;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/32552;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Unexpected NullPointerException when Aggregator.finish returns null
Issue key: SPARK-37547
Issue id: 13415434
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: andreiharbunou
Creator: andreiharbunou
Created: 04/Dec/21 23:29
Updated: 04/Dec/21 23:29
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: I'm migrating existing code (Java 8) from Spark 2.4 to Spark 3 and I see NullPointerException when an Aggregator returns null in finish method for a custom class.

I've created simple snippet to repro the issue.
{code:java}
public class SparkTest {
  public static void main(String[] args) {
    SparkConf conf = new SparkConf().setAppName("name").setMaster("local[*]");
    SparkSession spark = SparkSession.builder().config(conf).getOrCreate();
    List<String> data = Arrays.asList("1", "2", "3");
    Dataset<String> dataset = spark.createDataset(data, Encoders.STRING());
    Dataset<Row> aggDataset = dataset.groupBy("value").agg(new EntityAggregator().toColumn().name("agg"));
    aggDataset.show();
  }
} {code}
{code:java}
public class EntityAggregator extends Aggregator<Row, EntityAgg, EntityAgg> { public EntityAgg zero() { return new EntityAgg(0l); } 
public EntityAgg reduce(EntityAgg agg, Row row) { return agg; } 
public EntityAgg merge(EntityAgg e1, EntityAgg e2) { return e1; } 
public Encoder<EntityAgg> bufferEncoder() { return Encoders.bean(EntityAgg.class); } 
public Encoder<EntityAgg> outputEncoder() { return Encoders.bean(EntityAgg.class); } 
public EntityAgg finish(EntityAgg reduction) { return null; } 
}
{code}
{code:java}
public class EntityAgg {
  private long field;
  public EntityAgg() { }
  public EntityAgg(long field) { this.field = field; }
  public long getField() { return field; }
  public void setField(long field) { this.field = field; }
} {code}
Expected behavior is to print table like this
{noformat}
+-----+----+
|value| agg|
+-----+----+
|    3|null|
|    1|null|
|    2|null|
+-----+----+
{noformat}
This code works fine for 2.4 but fails with the following stacktrace for Spark 3 (I tested for 3.1.2 and 3.2.0)
{noformat}
Caused by: java.lang.NullPointerException
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(generated.java:49)
    at org.apache.spark.sql.execution.aggregate.AggregationIterator.$anonfun$generateResultProjection$5(AggregationIterator.scala:259)
    at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:85)
    at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.next(ObjectAggregationIterator.scala:32)
    at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:346)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
    at org.apache.spark.scheduler.Task.run(Task.scala:131)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748){noformat}
Another observation, that if I change EntityAgg to String in Aggregator then It works fine.

I've found a test in github that should check for this behavior. [https://github.com/apache/spark/blob/branch-3.1/sql/core/src/test/scala/org/apache/spark/sql/DatasetAggregatorSuite.scala#L338] 

I haven't found similar issue so please point me to open ticket if there is any.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Java
Custom field (Last public comment date): 2021-12-04 23:29:24.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xd0w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Optimize the dynamic partitioning prune rules to avoid inserting unnecessary predicates to improve performance
Issue key: SPARK-37542
Issue id: 13415368
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 04/Dec/21 05:46
Updated: 04/Dec/21 06:51
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, the dynamic partition pruning rule will insert a predicate on the filterable table using the filter from the other side of the join and a custom wrapper called DynamicPruning，and the predicate will be re-optimized by the AQE or non-AQE.

But, sometimes the predicate may be unnecessary if the join can NOT reuse broadcastExchange or it is not benefit，and it will be dropped by the rules of  the AQE or non-AQE.

We should optimize the dynamic partitioning pruning rule to avoid inserting unnecessary predicates to improve performance.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Dec 04 06:50:56 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xcm8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Dec/21 06:50;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34805;;;
Affects Version/s.1: 3.0.1
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Support Relation With LateralView
Issue key: SPARK-37519
Issue id: 13415014
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: blackpig
Creator: blackpig
Created: 02/Dec/21 10:32
Updated: 03/Dec/21 03:35
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: {code:java}
CREATE TABLE person (id INT, name STRING, age INT, class INT, address STRING);
INSERT INTO person VALUES
    (100, 'John', 30, 1, 'Street 1'),
    (200, 'Mary', NULL, 1, 'Street 2'),
    (300, 'Mike', 80, 3, 'Street 3'),
    (400, 'Dan', 50, 4, 'Street 4');
SELECT *
FROM person AS P1
LATERAL VIEW EXPLODE(ARRAY(30, 60)) CC1 AS C_AGE1
LEFT JOIN person P2
LATERAL VIEW EXPLODE(ARRAY(50)) CC2 AS C_AGE2 ON  P1.ID = P2.ID  AND CC1.C_AGE1=P2.AGE;
{code}
{code:java}
Error msg:
LEFT JOIN person P2 ^^^ LATERAL VIEW EXPLODE(ARRAY(50)) CC2 AS C_AGE2 ON  P1.ID = P2.ID  AND CC1.C_AGE1=P2.AGE  
      .........
        at java.lang.Thread.run(Thread.java:748) Caused by: org.apache.spark.sql.catalyst.parser.ParseException:  mismatched input 'LEFT' expecting {<EOF>, ';'}(line 4, pos 0) {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-12-02 10:32:35.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0xafs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: With enough resources, the task may still be permanently pending
Issue key: SPARK-37488
Issue id: 13414308
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Guiyankuang
Creator: Guiyankuang
Created: 29/Nov/21 13:56
Updated: 29/Nov/21 14:27
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Scheduler, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: {code:java}
// The online environment is actually hive partition data imported to tidb, the code logic can be simplified as follows
    SparkSession testApp = SparkSession.builder()
        .master("local[*]")
        .appName("test app")
        .enableHiveSupport()
        .getOrCreate();
    Dataset<Row> dataset = testApp.sql("select * from default.test where dt = '20211129'");
    dataset.persist(StorageLevel.MEMORY_AND_DISK());
    dataset.count();
{code}

I have observed that tasks are permanently blocked and reruns can always be reproduced.

Since it is only reproducible online, I use the arthas runtime to see the status of the function entries and returns within the TaskSetManager.
https://gist.github.com/guiyanakuang/431584f191645513552a937d16ae8fbd

NODE_LOCAL level, because the persist function is called, the pendingTasks.forHost has a collection of pending tasks, but it points to the machine where the block of partitioned data is located, and since the only resource spark gets is the driver. In this case, it cannot be scheduled. getAllowedLocalityLevel gives the wrong runlevel, so it cannot be run with TaskLocality.Any

The task pending permanently because the scheduling time is very short and it is too late to raise the runlevel with a timeout.

Environment: Spark 3.1.2，Default Configuration
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Nov 29 14:27:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0x63c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Nov/21 14:27;apachespark;User 'guiyanakuang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34743;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Graceful termination of Spark Structured Streaming queries
Issue key: SPARK-36240
Issue id: 13391094
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Ehnalis
Creator: Ehnalis
Created: 21/Jul/21 09:06
Updated: 26/Nov/21 08:19
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 1
Labels: 
Description: Spark Streaming provides a way to gracefully stop the streaming application using the configuration parameter {{spark.streaming.stopGracefullyOnShutdown}}. The configuration states:
{quote}If {{true}}, Spark shuts down the {{StreamingContext}} gracefully on JVM shutdown rather than immediately.
{quote}
This effectively stops the job generation (see {{JobGenerator}} of Spark Streaming) and lets the current {{Job}} (corresponding to a micro-batch) be finished instead of canceling the active job itself.

Some applications may require graceful stopping so that their output would remain consistent - an output that is written out halfway poses a lot of problems for applications that would require "exactly-once" semantics.

There is no support in Structured Streaming to gracefully stop queries/streaming applications.

Naive solutions found on the web propose checking whether the queries are active using {{query.isActive}} or checking query state directly and then attempting to call {{stop()}} at the right time. In Structured Streaming, with the current implementation, {{stop()}} cancels the job group that may lead to inconsistent output, because it still depends on the timing of the cancellation.

_Proposed solution:_

Strictly speaking in the context of the micro-batch execution model, a {{StreamingQuery}} that we want to gracefully stop would be of implementation \{{MicroBatchExecution. }}The motivation is similar to that of the Streaming Context's gracefulness: stop the "job generation" and then wait for any active job to finish, instead of canceling the jobs.

The micro-batch scheduling is managed by a {{ProcessingTimeExecutor}} of the {{MicroBatchExecution}} class.

 
{code:java}
private val triggerExecutor = trigger match {
  case t: ProcessingTimeTrigger => ProcessingTimeExecutor(t, triggerClock)
  case OneTimeTrigger => OneTimeExecutor()
  case _ => throw new IllegalStateException(s"Unknown type of trigger: $trigger")
}
{code}
The following while-true is being run be the job generation mechanism. The {{triggerHandler}} is a UDF that generates the micro-batches.
{code:java}
override def execute(triggerHandler: () => Boolean): Unit = {
  while (true) {
    val triggerTimeMs = clock.getTimeMillis
    val nextTriggerTimeMs = nextBatchTime(triggerTimeMs)
    val terminated = !triggerHandler()
    if (intervalMs > 0) {
      val batchElapsedTimeMs = clock.getTimeMillis - triggerTimeMs
      if (batchElapsedTimeMs > intervalMs) {
        notifyBatchFallingBehind(batchElapsedTimeMs)
      }
      if (terminated) {
        return
      }
      clock.waitTillTime(nextTriggerTimeMs)
    } else {
      if (terminated) {
        return
      }
    }
  }
}
{code}
Here, upon a {{gracefulStop()}} signal from the queries could essentially signal {{ProcessingTimeExecutor}} to stop triggering new batches.

Then another mechanism is required that would await until any current job is finished. Then, it would call {{stop()}} and then the {{SparkSession}} may be stopped as well.
Environment:  

 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Nov 26 08:19:15 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0t71s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Jul/21 09:08;Ehnalis;Based on suggestions and guidance of the Spark community, we would be happy to implement this feature to Spark Structured Streaming.;;;, 21/Jul/21 09:24;kabhwan;Hi, just FYI, fix version is set when the PR is submitted and merged and the issue is marked as fixed. Please leave it as empty.;;;, 26/Nov/21 08:19;dingyufei;This feature is expected to be available in which version of Spark？;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: `with as` clause got inconsistent results
Issue key: SPARK-37382
Issue id: 13412683
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: caican
Creator: caican
Created: 19/Nov/21 03:12
Updated: 22/Nov/21 09:46
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: In Spark3.1, the `with as` clause in the same SQL is executed multiple times,  got different results

`

with tab as (
 select 'Withas' as name, rand() as rand_number
)
select name, rand_number
from tab
union all
select name, rand_number
from tab

`

!spark3.1.png!

But In spark2.3, it got consistent results

`

with tab as (
 select 'Withas' as name, rand() as rand_number
)
select name, rand_number
from tab
union all
select name, rand_number
from tab

`

!spark2.3.png!

Why does Spark3.1.2 return different results?

Has anyone encountered this problem?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 22/Nov/21 09:43;caican;spark2.3.png;https://issues.apache.org/jira/secure/attachment/13036438/spark2.3.png, 22/Nov/21 09:42;caican;spark3.1.png;https://issues.apache.org/jira/secure/attachment/13036437/spark3.1.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Nov 22 09:46:23 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ww28:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Nov/21 07:59;victor-wong;The images are broken, please have a look:);;;, 19/Nov/21 08:44;zhenw;https://issues.apache.org/jira/browse/SPARK-36447 related?;;;, 22/Nov/21 09:40;caican;[~zhenw] Thank you for your reply, i will test it out.;;;, 22/Nov/21 09:46;caican;[~victor-wong] Does the images display nomally now?;;;
Affects Version/s.1: 
Attachment.1: 22/Nov/21 09:42;caican;spark3.1.png;https://issues.apache.org/jira/secure/attachment/13036437/spark3.1.png
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: split function behave differently between spark 2.3 and spark 3.2
Issue key: SPARK-37344
Issue id: 13411934
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: oceaneast
Creator: oceaneast
Created: 16/Nov/21 02:00
Updated: 16/Nov/21 10:40
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: incorrect
Description: while use split function in sql, it behave differently between 2.3 and 3.2, which cause incorrect problem.

we can use this sql to reproduce this problem:

 

create table split_test ( id int,name string)

insert into split_test values(1,"abc;def")

explain extended select split(name,'\\\\;') from split_test

 

spark3:

spark-sql> Explain extended select split(name,'\\\\;') from split_test;

== Parsed Logical Plan ==

'Project [unresolvedalias('split('name, \\;), None)]

+- 'UnresolvedRelation [split_test], [], false

 

spark2:

 

spark-sql> Explain extended select split(name,'\\\\;') from split_test;

== Parsed Logical Plan ==

'Project [unresolvedalias('split('name, \;), None)]

+- 'UnresolvedRelation split_test

 

It looks like the deal of escape is different
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Nov 16 10:40:32 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0wrg8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Nov/21 02:17;angerszhuuu;for same SQL  
{code}
explain extended select split('dawdawdawd','\\\\;');

{code}
In hive 1.2
{code}
OK
ABSTRACT SYNTAX TREE:

TOK_QUERY
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            TOK_FUNCTION
               split
               'dawdawdawd'
               '\\\;'
{code}

In hive 3
{code}
OK
ABSTRACT SYNTAX TREE:

TOK_QUERY
   TOK_INSERT
      TOK_DESTINATION
         TOK_DIR
            TOK_TMP_FILE
      TOK_SELECT
         TOK_SELEXPR
            TOK_FUNCTION
               split
               'dawdawdawd'
               '\\\\;'
{code}

So it should be caused by hive's code.;;;, 16/Nov/21 10:40;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34616;;;, 16/Nov/21 10:40;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34616;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: MapStatus adds localDirs to avoid the rpc request by method getHostLocalDirs when shuffle reading
Issue key: SPARK-37006
Issue id: 13406520
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: csbliss
Creator: csbliss
Created: 14/Oct/21 09:08
Updated: 22/Oct/21 11:08
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: When executing the ShuffleBlockFetcherIterator.fetchHostLocalBlocks method, in order to obtain the hostLocalDirs value, we need to send an RPC request through ExternalBlockStoreClient or NettyBlockTransferService. Then get shuffle data according to blockId and localDirs.

We can add localDir to the BlockManagerId class of MapStatus, so that we can get localDir directly when fetch host-local blocks without sending RPC requests.

The benefits are:
1. No need to send RPC request localDirs value when fetchHostLocalBlocks;
2. When the external shuffle service is enabled, there is no need to register ExecutorShuffleInfo in the ExternalShuffleBlockResolver class, nor to save the ExecutorShuffleInfo data in the ExternalShuffleBlockResolver class through leveldb.
3. Also, there is no need to cache host-local dirs in the HostLocalDirManager class.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Oct 22 11:08:07 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vu6w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Oct/21 09:12;csbliss;hi [~cloud_fan]，can you review this issue for me?;;;, 15/Oct/21 01:57;csbliss;Or whether we can generate localDirs based on appId and execId, just like DiskBlockManager.getFile, so that we don't need to save localDirs in MapStatus, just add appId to MapStatus;;;, 22/Oct/21 11:08;csbliss;hi [~Ngone51], can you review this issue for me?;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Publicise UpperCaseCharStream
Issue key: SPARK-37016
Issue id: 13406703
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: dohongdayi
Creator: dohongdayi
Created: 15/Oct/21 04:55
Updated: 22/Oct/21 10:14
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.2.3, 2.3.4, 2.4.8, 3.0.3, 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Many Spark extension projects are copying `UpperCaseCharStream` because it is private beneath `parser` package, such as:

[Delta Lake|https://github.com/delta-io/delta/blob/625de3b305f109441ad04b20dba91dd6c4e1d78e/core/src/main/scala/io/delta/sql/parser/DeltaSqlParser.scala#L290]

[Hudi|https://github.com/apache/hudi/blob/3f8ca1a3552bb866163d3b1648f68d9c4824e21d/hudi-spark-datasource/hudi-spark/src/main/scala/org/apache/spark/sql/parser/HoodieCommonSqlParser.scala#L112]

[Iceberg|https://github.com/apache/iceberg/blob/c3ac4c6ca74a0013b4705d5bd5d17fade8e6f499/spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala#L175]

[Submarine|https://github.com/apache/submarine/blob/2faebb8efd69833853f62d89b4f1fea1b1718148/submarine-security/spark-security/src/main/scala/org/apache/submarine/spark/security/parser/UpperCaseCharStream.scala#L31]

[Kyuubi|https://github.com/apache/incubator-kyuubi/blob/8a5134e3223844714fc58833a6859d4df5b68d57/dev/kyuubi-extension-spark-common/src/main/scala/org/apache/kyuubi/sql/zorder/ZorderSparkSqlExtensionsParserBase.scala#L108]

[Spark-ACID|https://github.com/qubole/spark-acid/blob/19bd6db757677c40f448e85c74d9995ba97d5942/src/main/scala/com/qubole/spark/datasources/hiveacid/sql/catalyst/parser/ParseDriver.scala#L13]

We can publicise `UpperCaseCharStream` to eliminate code duplication.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Oct 22 10:14:42 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vvbc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Oct/21 05:20;apachespark;User 'dohongdayi' has created a pull request for this issue:
https://github.com/apache/spark/pull/34290;;;, 22/Oct/21 10:14;dohongdayi;Anyone care about this issue?;;;
Affects Version/s.1: 2.3.4
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Improve error message when use parquet vectorize reader
Issue key: SPARK-37035
Issue id: 13406978
Parent id: 13407412.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 18/Oct/21 04:17
Updated: 20/Oct/21 09:18
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Vectorized reader won't show which file read failed.

 
None-vectorize parquet reader 
{code}
cutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details:
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:193)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file hdfs://path/to/failed/file
	at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:251)
	at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)
	... 15 more
{code}


Vectorize parquet reader
{code}
21/10/15 18:01:54 WARN TaskSetManager: Lost task 1881.0 in stage 16.0 (TID 10380, ip-10-130-169-140.idata-server.shopee.io, executor 168): TaskKilled (Stage cancelled)
: An error occurred while calling o362.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 963 in stage 17.0 failed 4 times, most recent failure: Lost task 963.3 in stage 17.0 (TID 10351, ip-10-130-75-201.idata-server.shopee.io, executor 99): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainIntegerDictionary
	at org.apache.parquet.column.Dictionary.decodeToLong(Dictionary.java:49)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetDictionary.decodeToLong(ParquetDictionary.java:36)
	at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.getLong(OnHeapColumnVector.java:364)
	at org.apache.spark.sql.execution.vectorized.MutableColumnarRow.getLong(MutableColumnarRow.java:120)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anonfun$doExecute$2$$anonfun$apply$2.apply(DataSourceScanExec.scala:351)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anonfun$doExecute$2$$anonfun$apply$2.apply(DataSourceScanExec.scala:349)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Oct 18 04:43:52 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vx0g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Oct/21 04:19;angerszhuuu;raise a pr soon
;;;, 18/Oct/21 04:43;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34308;;;
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Demote BroadcastJoin causes performance regression and increases OOM risks
Issue key: SPARK-36443
Issue id: 13393818
Parent id: 13407393.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Qin Yao
Creator: Qin Yao
Created: 06/Aug/21 03:22
Updated: 19/Oct/21 22:02
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description:  
h2. A test case

Use bin/spark-sql with local mode and all other default settings with 3.1.2 to run the case below
{code:sql}
// Some comments here
set spark.sql.shuffle.partitions=20;
set spark.sql.adaptive.enabled=true;
-- set spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin=0; -- (default 0.2)enable this for not demote bhj
set spark.sql.autoBroadcastJoinThreshold=200;
SELECT
  l.id % 12345 k,
  sum(l.id) sum,
  count(l.id) cnt,
  avg(l.id) avg,
  min(l.id) min,
  max(l.id) max
from (select id % 3 id from range(0, 1e8, 1, 100)) l
  left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id
GROUP BY 1;
{code}
 
 1. demote bhj w/ nonEmptyPartitionRatioForBroadcastJoin comment out

 
| |
||[Job Id ▾|http://localhost:4040/jobs/?&completedJob.sort=Job+Id&completedJob.desc=false&completedJob.pageSize=100#completed]||[Description|http://localhost:4040/jobs/?&completedJob.sort=Description&completedJob.pageSize=100#completed]||[Submitted|http://localhost:4040/jobs/?&completedJob.sort=Submitted&completedJob.pageSize=100#completed]||[Duration|http://localhost:4040/jobs/?&completedJob.sort=Duration&completedJob.pageSize=100#completed]||Stages: Succeeded/Total||Tasks (for all stages): Succeeded/Total||
|4|SELECT l.id % 12345 k, sum(l.id) sum, count(l.id) cnt, avg(l.id) avg, min(l.id) min, max(l.id) max from (select id % 3 id from range(0, 1e8, 1, 100)) l left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id GROUP BY 1[main at NativeMethodAccessorImpl.java:0|http://localhost:4040/jobs/job/?id=4]|2021/08/06 17:31:37|71 ms|1/1 (4 skipped)|3/3 (205 skipped) 
  |
|3|SELECT l.id % 12345 k, sum(l.id) sum, count(l.id) cnt, avg(l.id) avg, min(l.id) min, max(l.id) max from (select id % 3 id from range(0, 1e8, 1, 100)) l left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id GROUP BY 1[main at NativeMethodAccessorImpl.java:0|http://localhost:4040/jobs/job/?id=3]|2021/08/06 17:31:18|19 s|1/1 (3 skipped)|4/4 (201 skipped) 
  |
|2|SELECT l.id % 12345 k, sum(l.id) sum, count(l.id) cnt, avg(l.id) avg, min(l.id) min, max(l.id) max from (select id % 3 id from range(0, 1e8, 1, 100)) l left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id GROUP BY 1[main at NativeMethodAccessorImpl.java:0|http://localhost:4040/jobs/job/?id=2]|2021/08/06 17:31:18|87 ms|1/1 (1 skipped)|1/1 (100 skipped) 
  |
|1|SELECT l.id % 12345 k, sum(l.id) sum, count(l.id) cnt, avg(l.id) avg, min(l.id) min, max(l.id) max from (select id % 3 id from range(0, 1e8, 1, 100)) l left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id GROUP BY 1[main at NativeMethodAccessorImpl.java:0|http://localhost:4040/jobs/job/?id=1]|2021/08/06 17:31:16|2 s|1/1|100/100 
  |
|0|SELECT l.id % 12345 k, sum(l.id) sum, count(l.id) cnt, avg(l.id) avg, min(l.id) min, max(l.id) max from (select id % 3 id from range(0, 1e8, 1, 100)) l left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id GROUP BY 1[main at NativeMethodAccessorImpl.java:0|http://localhost:4040/jobs/job/?id=0]|2021/08/06 17:31:15|2 s|1/1|100/100 |

2. set nonEmptyPartitionRatioForBroadcastJoin to 0 to tell spark not to demote bhj

 
||[Job Id (Job Group) ▾|http://localhost:4040/jobs/?&completedJob.sort=Job+Id+%28Job+Group%29&completedJob.desc=false&completedJob.pageSize=100#completed]||[Description|http://localhost:4040/jobs/?&completedJob.sort=Description&completedJob.pageSize=100#completed]||[Submitted|http://localhost:4040/jobs/?&completedJob.sort=Submitted&completedJob.pageSize=100#completed]||[Duration|http://localhost:4040/jobs/?&completedJob.sort=Duration&completedJob.pageSize=100#completed]||Stages: Succeeded/Total||Tasks (for all stages): Succeeded/Total||
|5|SELECT l.id % 12345 k, sum(l.id) sum, count(l.id) cnt, avg(l.id) avg, min(l.id) min, max(l.id) max from (select id % 3 id from range(0, 1e8, 1, 100)) l left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id GROUP BY 1[main at NativeMethodAccessorImpl.java:0|http://localhost:4040/jobs/job/?id=5]|2021/08/06 18:25:15|29 ms|1/1 (2 skipped)|3/3 (200 skipped) 
  |
|4|SELECT l.id % 12345 k, sum(l.id) sum, count(l.id) cnt, avg(l.id) avg, min(l.id) min, max(l.id) max from (select id % 3 id from range(0, 1e8, 1, 100)) l left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id GROUP BY 1[main at NativeMethodAccessorImpl.java:0|http://localhost:4040/jobs/job/?id=4]|2021/08/06 18:25:13|2 s|1/1 (1 skipped)|100/100 (100 skipped) 
  |
|3 (700fefe1-8446-4761-9be2-b68ed6e84c11)|broadcast exchange (runId 700fefe1-8446-4761-9be2-b68ed6e84c11)[$anonfun$withThreadLocalCaptured$1 at FutureTask.java:266|http://localhost:4040/jobs/job/?id=3]|2021/08/06 18:25:13|54 ms|1/1 (2 skipped)|1/1 (101 skipped) 
  |
|2|SELECT l.id % 12345 k, sum(l.id) sum, count(l.id) cnt, avg(l.id) avg, min(l.id) min, max(l.id) max from (select id % 3 id from range(0, 1e8, 1, 100)) l left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id GROUP BY 1[main at NativeMethodAccessorImpl.java:0|http://localhost:4040/jobs/job/?id=2]|2021/08/06 18:25:13|88 ms|1/1 (1 skipped)|1/1 (100 skipped) 
  |
|1|SELECT l.id % 12345 k, sum(l.id) sum, count(l.id) cnt, avg(l.id) avg, min(l.id) min, max(l.id) max from (select id % 3 id from range(0, 1e8, 1, 100)) l left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id GROUP BY 1[main at NativeMethodAccessorImpl.java:0|http://localhost:4040/jobs/job/?id=1]|2021/08/06 18:25:10|2 s|1/1|100/100 
  |
|0|SELECT l.id % 12345 k, sum(l.id) sum, count(l.id) cnt, avg(l.id) avg, min(l.id) min, max(l.id) max from (select id % 3 id from range(0, 1e8, 1, 100)) l left join (SELECT max(id) as id, id % 2 gid FROM range(0, 1000, 2, 100) group by gid) r ON l.id = r.id GROUP BY 1[main at NativeMethodAccessorImpl.java:0|http://localhost:4040/jobs/job/?id=0]|2021/08/06 18:25:10|3 s|1/1|100/100
  |

The clause `select id % 3 id from range(0, 1e8, 1, 100)) l ` here produces highly compressed shuffle map output and 17/20 empty partitions at the reduced side, where is also the AQE reOptimize point for DynamicJoinSelection.
{code:java}
Exchange

shuffle records written: 100,000,000
shuffle write time total (min, med, max )
891 ms (2 ms, 5 ms, 33 ms )
records read: 100,000,000
local bytes read total (min, med, max )
10.0 MiB (3.3 MiB, 3.4 MiB, 3.4 MiB )
fetch wait time total (min, med, max )
0 ms (0 ms, 0 ms, 0 ms )
remote bytes read: 0.0 B
local blocks read: 300
remote blocks read: 0
data size total (min, med, max )
1525.9 MiB (15.3 MiB, 15.3 MiB, 15.3 MiB )
remote bytes read to disk: 0.0 B
shuffle bytes written total (min, med, max )
10.0 MiB (102.3 KiB, 102.3 KiB, 102.3 KiB )
{code}
 

In the case 1), the bhj is demoted and the `coalesce partitions rule` successfully coalesces these 'small' partitions even set *spark.sql.adaptive.advisoryPartitionSizeInBytes=1m*. See,

 

!screenshot-1.png!

Then, as you can see at the smj phase, the former coalesce and the latter expansion cause performance regression

 
{code:java}
// code placeholder
Sort

sort time total (min, med, max (stageId: taskId))
166 ms (0 ms, 55 ms, 57 ms (stage 7.0: task 203))
peak memory total (min, med, max (stageId: taskId))
315.1 MiB (64.0 KiB, 105.0 MiB, 105.0 MiB (stage 7.0: task 201))
spill size total (min, med, max (stageId: taskId))
1845.0 MiB (0.0 B, 615.0 MiB, 615.0 MiB (stage 7.0: task 201)
{code}
 

 
|1|202|0|SUCCESS| |driver| | |2021-08-06 17:31:18|18 s|4 s|3.0 ms|10.0 ms| | |105.3 MiB|1.0 ms|91 B / 1|3.4 MiB / 33333333|615 MiB|4.5 MiB| |
|2|203|0|SUCCESS| |driver| | |2021-08-06 17:31:18|19 s|4 s|4.0 ms|10.0 ms| | |105.3 MiB|1.0 ms|89 B / 1|3.4 MiB / 33333333|615 MiB|4.5 MiB| |
|0|201|0|SUCCESS| |driver| | |2021-08-06 17:31:18|17 s|4 s|6.0 ms|10.0 ms| | |105.3 MiB|1.0 ms|70 B / 1|3.3 MiB / 33333334|615 MiB|4.4 MiB|

 

In the case 2), the bhj mode increases task numbers which will casue extra schedule overhead and running unnecessary empty tasks, but it avoid the oom risk and the performance regression described  above.
h2. A real-world case, in which the expansion of the data increases the oom risk to a very high level. 

 

 

!image-2021-08-06-11-24-34-122.png!
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 06/Aug/21 03:24;Qin Yao;image-2021-08-06-11-24-34-122.png;https://issues.apache.org/jira/secure/attachment/13031537/image-2021-08-06-11-24-34-122.png, 06/Aug/21 09:57;Qin Yao;image-2021-08-06-17-57-15-765.png;https://issues.apache.org/jira/secure/attachment/13031553/image-2021-08-06-17-57-15-765.png, 06/Aug/21 09:55;Qin Yao;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13031552/screenshot-1.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 3.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 02 16:19:46 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tnuo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Sep/21 16:19;dongjoon;Thank you for the details.;;;
Affects Version/s.1: 
Attachment.1: 06/Aug/21 09:57;Qin Yao;image-2021-08-06-17-57-15-765.png;https://issues.apache.org/jira/secure/attachment/13031553/image-2021-08-06-17-57-15-765.png
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: fixing "SQL column nullable setting not retained as part of spark read" issue
Issue key: SPARK-36996
Issue id: 13406312
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: senthh
Creator: senthh
Created: 13/Oct/21 09:18
Updated: 13/Oct/21 10:53
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.0, 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Sql 'nullable' columns are not retaining 'nullable' type as it is while reading from Spark read using jdbc format.

 

SQL :

------------

 

mysql> CREATE TABLE Persons(Id int NOT NULL, FirstName varchar(255), LastName varchar(255), Age int);

 

mysql> desc Persons;
+-----------+--------------+------+-----+---------+-------+
| Field | Type | Null | Key | Default | Extra |
+-----------+--------------+------+-----+---------+-------+
| Id | int | NO | | NULL | |
| FirstName | varchar(255) | YES | | NULL | |
| LastName | varchar(255) | YES | | NULL | |
| Age | int | YES | | NULL | |
+-----------+--------------+------+-----+---------+-------+

 

But in Spark  we get all the columns as "Nullable":

=============

scala> val df = spark.read.format("jdbc").option("database","Test_DB").option("user", "root").option("password", "").option("driver", "com.mysql.cj.jdbc.Driver").option("url", "jdbc:mysql://localhost:3306/Test_DB").option("dbtable", "Persons").load()
df: org.apache.spark.sql.DataFrame = [Id: int, FirstName: string ... 2 more fields]

scala> df.printSchema()
root
 |-- Id: integer (nullable = true)
 |-- FirstName: string (nullable = true)
 |-- LastName: string (nullable = true)
 |-- Age: integer (nullable = true)

=============

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Oct 13 10:53:06 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vsxk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Oct/21 09:21;senthh;Based on further analysis, Spark is hard-coding "nullable" as "true" always. This change has been inccluded due to "https://issues.apache.org/jira/browse/SPARK-19726".

 

 ;;;, 13/Oct/21 09:21;senthh;I m working on this;;;, 13/Oct/21 10:40;apachespark;User 'senthh' has created a pull request for this issue:
https://github.com/apache/spark/pull/34272;;;, 13/Oct/21 10:50;senthh;We need to consider 2 scenarios

 
 # maintain NULLABLE value as per SQL metadata for non timestamp columns
 # set NULLABLE as true(always) for timestamp columns

 

 ;;;, 13/Oct/21 10:53;senthh;Sample Output after this changes:

SQL :

mysql> CREATE TABLE Persons(Id int NOT NULL, FirstName varchar(255), LastName varchar(255), Age int);

 

mysql> desc Persons;
+-----------+--------------+------+-----+---------+-------+
| Field | Type | Null | Key | Default | Extra |
+-----------+--------------+------+-----+---------+-------+
| Id | int | NO | | NULL | |
| FirstName | varchar(255) | YES | | NULL | |
| LastName | varchar(255) | YES | | NULL | |
| Age | int | YES | | NULL | |
+-----------+--------------+------+-----+---------+-------+

----------++-----------++----------------+

Spark:

scala> val df = spark.read.format("jdbc").option("database","Test_DB").option("user", "root").option("password", "").option("driver", "com.mysql.cj.jdbc.Driver").option("url", "jdbc:mysql://localhost:3306/Test_DB").option("dbtable", "Persons").load()
 df: org.apache.spark.sql.DataFrame = [Id: int, FirstName: string ... 2 more fields]

scala> df.printSchema()
 root
 |-- Id: integer (nullable = false)
 |-- FirstName: string (nullable = true)
 |-- LastName: string (nullable = true)
 |-- Age: integer (nullable = true)

 

 

And for TIMESTAMP columns

 

SQL:
create table timestamp_test(id int(11), time_stamp timestamp not null default current_timestamp);

SPARK:

scala> val df = spark.read.format("jdbc").option("database","Test_DB").option("user", "root").option("password", "").option("driver", "com.mysql.cj.jdbc.Driver").option("url", "jdbc:mysql://localhost:3306/Test_DB").option("dbtable", "timestamp_test").load()
df: org.apache.spark.sql.DataFrame = [id: int, time_stamp: timestamp]

scala> df.printSchema()
root
|-- id: integer (nullable = true)
|-- time_stamp: timestamp (nullable = true);;;
Affects Version/s.1: 3.1.0
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: ignoreCorruptFiles does not work when schema change from int to string when a file having more than X records
Issue key: SPARK-36983
Issue id: 13406071
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mikevn
Creator: mikevn
Created: 12/Oct/21 06:24
Updated: 13/Oct/21 06:33
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Precondition:

Spark 3.1 run locally on my Macbook Pro(16G Ram,i7, 2015)

In folder A having two parquet files
 * File 1: have some columns and one of them is column C1 with data type Int and have only one record
 * File 2: Same schema with File 1 except column C1  having data type String and having>= X records

X depends on the capacity of your computer, my case is 36, you can increase the number of row to find X.

Read file 1 to get schema of file 1.

Read folder A with schema of file 1.

Expected: Read successfully, file 2 will be ignored as the data type of column C1 changed to string.

Actual: File 2 seems to be not ignored and get error:

 `WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (192.168.1.78 executor driver): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainBinaryDictionary WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2) (192.168.1.78 executor driver): java.lang.UnsupportedOperationException: org.apache.parquet.column.values.dictionary.PlainValuesDictionary$PlainBinaryDictionary at org.apache.parquet.column.Dictionary.decodeToInt(Dictionary.java:45)`

 

If i remove one record from file2. It works well  
{code:java}
spark.conf.set('spark.sql.files.ignoreCorruptFiles', True)
schema1 = StructType([
 StructField("program_sk", IntegerType(), True),
 StructField("client_sk", IntegerType(), True),
])

sample_data = [(1, 17)]
df1 = spark.createDataFrame(sample_data, schema1)

schema2 = StructType([
 StructField("program_sk", IntegerType(), True),
 StructField("client_sk", StringType(), True),
])
sample_data = [(1, "19999"), (2, "3332"), (3, "199999"), (4, "3333"),
 (2, "19999"), (2, "3332"), (3, "199999"), (4, "3333"),
 (3, "19999"), (2, "3332"), (3, "199999"), (4, "3333"),
 (4, "19999"), (2, "3332"), (3, "199999"), (4, "3333"),
 (5, "19999"), (2, "3332"), (3, "199999"), (4, "3333"),
 (6, "19999"), (2, "3332"), (3, "199999"), (4, "3333"),
 (7, "19999"), (2, "3332"), (3, "199999"), (4, "3333"),
 (8, "19999"), (2, "3332"), (3, "199999"), (4, "3333"),
 (9, "19999"), (2, "3332"), (3, "199999"), (4, "3333"),
 ]
df2 = spark.createDataFrame(sample_data, schema2)
file_save_path = 's3://xxx-data-dev/adp_data_lake/test_ignore_corrupt/'

df1.write \
 .mode('overwrite') \
 .format('parquet') \
 .save(f'{file_save_path}')

df2.write \
 .mode('append') \
 .format('parquet') \
 .save(f'{file_save_path}')

df = spark.read.schema(schema1).parquet(file_save_path)
df.show(){code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-10-12 06:24:40.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vrfs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Support sql overwrite a path that is also being read from when partitionOverwriteMode is dynamic
Issue key: SPARK-36727
Issue id: 13400621
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: blackpig
Creator: blackpig
Created: 12/Sep/21 09:00
Updated: 11/Oct/21 03:20
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: {code:java}
// non-partitioned table overwrite
CREATE TABLE tbl (col1 INT, col2 STRING) USING PARQUET;
INSERT OVERWRITE TABLE tbl SELECT 0,1;
INSERT OVERWRITE TABLE tbl SELECT * FROM tbl;

// partitioned table static overwrite
CREATE TABLE tbl (col1 INT, col2 STRING) USING PARQUET PARTITIONED BY (pt1 INT);
INSERT OVERWRITE TABLE tbl PARTITION(p1=2021) SELECT 0 AS col1,1 AS col2;
INSERT OVERWRITE TABLE tbl PARTITION(p1=2021) SELECT col1, col2 FROM WHERE p1=2021;

{code}
When we run the above query, an error will be throwed "Cannot overwrite a path that is also being read from"

We need to support this operation when the spark.sql.sources.partitionOverwriteMode is dynamic
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-09-12 09:00:09.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0utts:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Renamed columns of parquet tables become NULL
Issue key: SPARK-36959
Issue id: 13405659
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: andor.toth
Creator: andor.toth
Created: 08/Oct/21 17:22
Updated: 08/Oct/21 17:22
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: If a column of a Parquet table gets renamed in Hive metastore, then Spark (SQL) is unable to read the relevant values, and all of the renamed column becomes NULL.

The problem could be reproduced with the following SQL queries:

 
{noformat}
create table tmp.parquet_table1 (i1 int, s1 string) stored as parquet;
insert into tmp.parquet_table1 values (1, "AAA"), (2, "BBB"), (3, "CCC");
select * from tmp.parquet_table1;
+---+---+
| i1| s1|
+---+---+
| 1|AAA|
| 2|BBB|
| 3|CCC|
+---+---+
alter table tmp.parquet_table1 replace columns (i2 int, s1 string);
select * from tmp.parquet_table1;
+----+---+
| i2| s1|
+----+---+
|null|AAA|
|null|BBB|
|null|CCC|
+----+---+ 
{noformat}
Notice, that column `i1` is renamed to `i2`, which became NULL afterwards. 

{{I have used Impala to create the table, and insert the values, but Spark (SQL) could be used as well. "alter table ... replace columns ..." could only be executed outside of Spark (Impala or Hive).}}

{{No [configuration option|https://spark.apache.org/docs/latest/configuration.html] helps, that I am aware of.}}

{{With Spark 2.4.8, this works correctly.}}
Environment: Executors are running on Hadoop YARN (Cloudera CDH5 5.16.2-2) cluster, with 12 nodes.

Driver runs a CentOS 8 (Release), Python 3.6.8, pip is used to install pyspark in fresh virtual environment. SPARK_HOME is set to virtual environment's pyspark directory (.../venv/lib/python3.6/site-packages/pyspark).

HADOOP_CONF_DIR is set where the Hive client configuration resides.

Spark config:
{noformat}
(spark.jars,)
(spark.app.name,org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver)
(spark.sql.hive.metastore.jars,maven)
(spark.submit.pyFiles,)
(spark.submit.deployMode,client)
(spark.master,yarn)
(spark.sql.hive.metastore.version,1.1.0){noformat}
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-10-08 17:22:00.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vowg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Add product function to SQL function
Issue key: SPARK-36957
Issue id: 13405598
Parent id: 
Issue Type: Task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 08/Oct/21 10:08
Updated: 08/Oct/21 13:12
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Add product function to SQL function
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Oct 08 13:12:01 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0voio:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Oct/21 13:11;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34223;;;, 08/Oct/21 13:12;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34223;;;
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Exception when trying to access Row field using getAs method
Issue key: SPARK-36947
Issue id: 13405439
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: amavrommatis
Creator: amavrommatis
Created: 07/Oct/21 15:03
Updated: 08/Oct/21 00:26
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: catalyst, row, sql
Description: I have an input dataframe *df* with the following schema:
{code:java}
|-- origin: string (nullable = true)
|-- product: struct (nullable = true)
|    |-- id: integer (nullable = true){code}
 

when I try to select the first 20 rows of the id column I execute:
{code:java}
df.select("product.id").show(20, false)
{code}
 

and I manage to get the result. But when I execute the following: 
{code:java}
df.map(_.getAs[Int]("product.id")).show(20, false){code}
 

I get the following error:
{code:java}
java.lang.IllegalArgumentException: Field "product.id" does not exist.{code}
 
Environment: Spark 3.1.2 (but this also may affect other versions as well)
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): scala
Custom field (Last public comment date): 2021-10-07 15:03:11.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vnjc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Add AQE Planning Times to SQL Metrics
Issue key: SPARK-36911
Issue id: 13404557
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mikechen
Creator: mikechen
Created: 01/Oct/21 22:28
Updated: 04/Oct/21 16:13
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Add metrics for durations of "reOptimize", "generate explainString" and "createQueryStages" to AdaptiveSparkPlanExec metrics to make it easier to see overhead of AQE for a query
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Oct 04 16:13:47 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vi3s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Oct/21 16:13;apachespark;User 'ChenMichael' has created a pull request for this issue:
https://github.com/apache/spark/pull/34170;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Enable Dependabot for improving security posture of the dependencies
Issue key: SPARK-36916
Issue id: 13404709
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: turris-nivasan
Creator: turris-nivasan
Created: 03/Oct/21 19:27
Updated: 03/Oct/21 19:28
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Project Infra
Due Date: 
Votes: 0
Labels: 
Description: h3. Why are the changes needed?

[https://docs.github.com/en/code-security/supply-chain-security/keeping-your-dependencies-updated-automatically]

Having knowledge about vulnerabilities of the dependencies helps the project owners decide on their dependencies security posture to make decisions.

If the project decides to get updates only on security updates and not on any version updates then setting these options would not open any PR 's {{open-pull-requests-limit: 0}}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Oct 03 19:28:44 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vj1k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Oct/21 19:28;apachespark;User 'naveensrinivasan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34165;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: structured streaming support backpressure for kafka source
Issue key: SPARK-36857
Issue id: 13403355
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: baizhendong
Creator: baizhendong
Created: 26/Sep/21 10:36
Updated: 26/Sep/21 10:37
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.1.2
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: Spark streaming support backpressure for kafka, but in structured streaming, not support backpressure for kafka. Can someone explain why not support it?
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-09-26 10:36:31.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vaow:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Kryo Buffer underflow error in Spark 3.1
Issue key: SPARK-36787
Issue id: 13401752
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: rravi-sift
Creator: rravi-sift
Created: 17/Sep/21 00:51
Updated: 20/Sep/21 04:06
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark JavaPairRDD processing fails with error

{code:java}
com.esotericsoftware.kryo.KryoException: Buffer underflow.
Serialization trace:
topologyInfo_ (org.apache.spark.storage.BlockManagerId)
loc (org.apache.spark.scheduler.HighlyCompressedMapStatus)
	at com.esotericsoftware.kryo.io.Input.require(Input.java:199)
	at com.esotericsoftware.kryo.io.Input.readVarInt(Input.java:373)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:127)
	at com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:693)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:804)
	at com.twitter.chill.TraversableSerializer.read(Traversable.scala:43)
	at com.twitter.chill.TraversableSerializer.read(Traversable.scala:21)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:734)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:543)
	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:734)
	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)
	at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:543)
	at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:816)
	at org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:397)
	at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:103)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:75)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1996)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

Kryo log shows this right before the error, could this be a mulithreading issue in spark!
{code}
0:15 TRACE: [kryo] Read field: loc (org.apache.spark.scheduler.HighlyCompressedMapStatus) pos=534
00:15 TRACE: [kryo] Read field: loc (org.apache.spark.scheduler.HighlyCompressedMapStatus) pos=726
00:15 TRACE: [kryo] Read class 15: org.apache.spark.storage.BlockManagerId
00:15 TRACE: [kryo] Read class 15: org.apache.spark.storage.BlockManagerId
00:15 TRACE: [kryo] Read field: topologyInfo_ (org.apache.spark.storage.BlockManagerId) pos=801
00:15 TRACE: [kryo] Read field: topologyInfo_ (org.apache.spark.storage.BlockManagerId) pos=609
{code}
Environment: Dataproc Image: 2.0.20-debian10
Apache Spark:	3.1.2
 Scala: 2.12.14
 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Sep 20 04:06:19 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v0sw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Sep/21 04:06;dongjoon;Do you think you can provide a reproducer, [~rravi-sift]?;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Improve run-time performance for window function first and last against UnboundedFollowing window ROWS frame
Issue key: SPARK-36770
Issue id: 13401449
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Guibin
Creator: Guibin
Created: 15/Sep/21 19:08
Updated: 16/Sep/21 17:36
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.0, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: *Context*

The *UnboundedFollowingWindowFunctionFrame* has the time complexity of *O(N^2)*, N is the number of rows in the current partition, more specific the complexity is O(N* (N - 1)/2).

What happens internally in UnboundedFollowingWindowFunctionFrame:
 In the window frame, while processing each incoming row, it will go through current row till the end of partition to do re-calculation. This process will be repeated on each incoming row, which causes the high run-time complexity.

But UnboundedPrecedingWindowFunctionFrame has much better time complexity O(N), N is the number of rows in the current partition.

 

*What is the idea of the improvement?*

Give the big time complexity difference between UnboundedFollowingWindowFunctionFrame and UnboundedPrecedingWindowFunctionFrame, we can do following conversions to improve the time complexity of first() and last() from O(N^2) to O(N)
{code:java}
case 1:
first() OVER(PARTITION BY colA ORDER BY colB ASC ROWS CURRENT ROW AND UNBOUNDED FOLLOWING) 
converts to 
last()  OVER(PARTITION BY colA ORDER BY colB DEAC ROWS UNBOUNDED PRECEDING AND CURRENT ROW)

case 2:
last()  OVER(PARTITION BY colA ORDER BY colB ASC ROWS CURRENT ROW AND UNBOUNDED FOLLOWING) 
converts to 
first() OVER(PARTITION BY colA ORDER BY colB DESC ROWS UNBOUNDED PRECEDING AND CURRENT ROW)

{code}
 

*Summary*

Replace "UNBOUNDED FOLLOWING" with "UNBOUNDED PRECEDING", and flip the ORDER BY for the window functions first() and last() for ROWS.

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 15 22:21:38 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uyxk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Sep/21 22:21;apachespark;User 'guibin' has created a pull request for this issue:
https://github.com/apache/spark/pull/34010;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Hadoop config map not mount
Issue key: SPARK-36039
Issue id: 13388285
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: cutiechi
Creator: cutiechi
Created: 08/Jul/21 04:36
Updated: 10/Sep/21 00:58
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: When i spec --conf spark.kubernetes.hadoop.configMapName in submit args, i see driver pod success mount this config, but exectuor not, **I looked at the source code and found that it was caused by no hadoop conf steps added.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jul 08 06:34:53 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0spqw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Jul/21 05:27;apachespark;User 'cutiechi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33254;;;, 08/Jul/21 05:27;apachespark;User 'cutiechi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33254;;;, 08/Jul/21 06:08;apachespark;User 'cutiechi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33256;;;, 08/Jul/21 06:09;apachespark;User 'cutiechi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33256;;;, 08/Jul/21 06:34;apachespark;User 'cutiechi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33257;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Delete appName from StreamingSource 
Issue key: SPARK-36360
Issue id: 13392760
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: mrcl
Creator: mrcl
Created: 30/Jul/21 13:03
Updated: 10/Sep/21 00:55
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: DStreams
Due Date: 
Votes: 0
Labels: 
Description: The StreamingSource includes the appName in its sourceName. However, the appName should not be handled by the StreamingSource. It is already handled by the MetricsSystem. See all other MetricSources, e.g. ExecutorMetricsSource.

Why is this important? See this part from the [documentation|https://spark.apache.org/docs/latest/monitoring.html#metrics]:

??Often times, users want to be able to track the metrics across apps for driver and executors, which is hard to do with application ID (i.e. {{spark.app.id}}) since it changes with every invocation of the app. For such use cases, a custom namespace can be specified for metrics reporting using {{spark.metrics.namespace}} configuration property. If, say, users wanted to set the metrics namespace to the name of the application, they can set the {{spark.metrics.namespace}} property to a value like {{$\{spark.app.name}}}. This value is then expanded appropriately by Spark and is used as the root namespace of the metrics system.??

This is only possible if the MetricsSystem handles the namespace which it does. But the StreamingSource additionally adds the appName in its sourceName, thus there is no way to configure a namespace that does not include the appName.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 04 07:08:40 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0thc0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Jul/21 13:37;apachespark;User 'mrclneumann' has created a pull request for this issue:
https://github.com/apache/spark/pull/33592;;;, 04/Aug/21 07:08;apachespark;User 'mrclneumann' has created a pull request for this issue:
https://github.com/apache/spark/pull/33632;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Implement spark-shell idle timeouts
Issue key: SPARK-36693
Issue id: 13399929
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gyogal
Creator: gyogal
Created: 08/Sep/21 08:27
Updated: 08/Sep/21 08:39
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Shell
Due Date: 
Votes: 0
Labels: 
Description: Many customers have been asking if there is a setting they can use to kill idle spark-shell since they can't really go to each developers desk and force them to use Cntr+D or exit() when their work is over. Our response so far has been to use dynamic allocation so that it will release the executors after the specified timeout.

However this is not always an ideal solution since the shell process would still be there, though AM would be occupying a very small resource and the user still needs to kill the idle spark shell via CM> Applications > spark-shell > Kill or run 'kill -9' on the OS to remove those. It would be nice to have a property in Spark (and exposed in CM) which deals with idle spark-shells, just like we have in beeline and let's leave it to the admins to see if they want the idle spark-shell timeout to be set as 1 day or a week.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 08 08:39:58 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0upk0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Sep/21 08:39;apachespark;User 'gyogal' has created a pull request for this issue:
https://github.com/apache/spark/pull/33936;;;, 08/Sep/21 08:39;apachespark;User 'gyogal' has created a pull request for this issue:
https://github.com/apache/spark/pull/33936;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Spark on K8s: Driver pod keeps running when executor allocator terminates with Fatal exception
Issue key: SPARK-36577
Issue id: 13397012
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: gargv
Creator: gargv
Created: 24/Aug/21 20:09
Updated: 29/Aug/21 01:45
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In Spark on Kubernetes, the class ExecutorPodsSnapshotsStoreImpl creates a thread which is responsible for creating new executor pods. The thread catches all 'NonFatal' exceptions, logs them and ignores these NonFatal exceptions. However, from Fatal exceptions, it only handles {color:#20999d}IllegalArgumentException {color}exception and terminates the driver pod in that case. Other Fatal exceptions are not handled at all, which means that if such a Fatal exception occurs, the executor creation thread abruptly terminates, while the main thread keeps running. Thus, the Spark application/job would keep running indefinitely without making any progress.

To fix this, 2 of the possible options are:

*Option#1*: Fail the driver pod whenever any Fatal exception happens. However, this approach has following disadvantages:
 # A few number of executors may have already been created when this Fatal exception happens. These executors can still take the job to completion, although slower than expected as all executors were not launched. Thus, we would fail the job instead of letting the job succeed slowly.
 # JVM can sometimes recover from Fatal exceptions on its own. Thus, we are not giving a chance to driver pod to recover from failure, rather we are killing it on first occurrence of {{Fatal}} exception.

*Option#2*: Fail the driver pod only when there are 0 executors running currently

In this approach, we fail the driver pod only when number of currently running executors is 0. This is so that we don’t kill a job which can potentially complete. Thus, 1 single running executor can keep the driver pod from dying. This may mean that job may make very slow progress when actual number of requested executors is very large.

*Option#3*: Expose configurations which allows customers to control this behavior.

spark.kubernetes.driver.executor-creation.terminate-on-fatal-exception=<true|false>
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-24 20:09:32.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u7kg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.0.0
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: ExecutorClassLoader no longer works with Http based Class Servers
Issue key: SPARK-36599
Issue id: 13397515
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: vinothkr
Creator: vinothkr
Created: 26/Aug/21 10:12
Updated: 27/Aug/21 16:59
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: There are 2 primary issues,
 * If the classServer uri is `{{http://host:port}}` instead of `{{http://host:port/}}` the getPath on the URI object returns empty string and subsequently causes Path creation to fail with Path cannot be empty++
 * This is a regression issue and transitively a hadoop issue too, the Http Filesystem fails with URI not absolute error unlike other filesystems when the path doesn't have scheme or authority. This used to work when there was http specific implementation and no longer work with Filesystem based implementation

https://issues.apache.org/jira/browse/HADOOP-17870
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Aug 27 16:59:39 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uao0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Aug/21 10:21;apachespark;User 'yellowflash' has created a pull request for this issue:
https://github.com/apache/spark/pull/33849;;;, 27/Aug/21 16:59;stevel@apache.org;I thought things had been fixed up so Hadoop's HTTP binding was not used through classloaders. If that isn't the case, that is something to fix.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Spark HistoryPage may show 'NotFound' in some multiple attempts cases
Issue key: SPARK-36582
Issue id: 13397086
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Elixir Kook
Creator: Elixir Kook
Created: 25/Aug/21 07:21
Updated: 25/Aug/21 11:05
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.1.2
Fix Version/s: 
Component/s: Web UI
Due Date: 
Votes: 0
Labels: 
Description: Current historypage show a attemptId column in case of hasMultipleAttempts is true.

if hasMultipleAttempts is false, remove the attemptId column.

 

But, applications in yarn could be failed even not logged to spark application history.

 

(application_1628518360417_0029 is normal case. Don't have to consider it) 

*application_1628518360417_0028*'s attemptId is 2, but size of attempt in spark history is 1.

(First try failed to starting spark driver in cluster mode, and second one was succeeded.)

For this case, the attemptId column is needed.

 
{code:java}
[
{
"id": "application_1628518360417_0029",
"name": "Spark Pi",
"attempts": [
{
"attemptId": "1",
"startTime": "2021-08-25T05:20:15.521GMT",
"endTime": "2021-08-25T05:20:30.398GMT",
"lastUpdated": "2021-08-25T05:20:30.475GMT",
"duration": 14877,
"sparkUser": "elixir-kook",
"completed": true,
"appSparkVersion": "2.4.5",
"endTimeEpoch": 1629868830398,
"lastUpdatedEpoch": 1629868830475,
"startTimeEpoch": 1629868815521
}
]
},
{
"id": "application_1628518360417_0028",
"name": "Spark Pi",
"attempts": [
{
"attemptId": "2",
"startTime": "2021-08-25T05:19:22.850GMT",
"endTime": "2021-08-25T05:19:44.662GMT",
"lastUpdated": "2021-08-25T05:19:44.726GMT",
"duration": 21812,
"sparkUser": "elixir-kook",
"completed": true,
"appSparkVersion": "2.4.5",
"endTimeEpoch": 1629868784662,
"lastUpdatedEpoch": 1629868784726,
"startTimeEpoch": 1629868762850
}
]
},
....
]
{code}
  !Screen Shot 2021-08-25 at 2.35.30 PM.png!

 

attemptId should be exist.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 25/Aug/21 11:04;Elixir Kook;Screen Shot 2021-08-25 at 2.35.30 PM.png;https://issues.apache.org/jira/secure/attachment/13032419/Screen+Shot+2021-08-25+at+2.35.30+PM.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-25 07:21:35.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u80w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Improve InsertIntoHadoopFsRelation file commit logic
Issue key: SPARK-36562
Issue id: 13396683
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 23/Aug/21 12:46
Updated: 24/Aug/21 05:17
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Improve InsertIntoHadoopFsRelation file commit logic
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-23 12:46:20.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0u5jc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: Skip Retrieving keytab with SparkFiles.get if keytab found in the CWD of Yarn Container
Issue key: SPARK-36493
Issue id: 13394864
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: xuzikun2003
Creator: xuzikun2003
Created: 12/Aug/21 10:09
Updated: 17/Aug/21 22:15
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.0, 3.1.2
Fix Version/s: 3.1.3
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Currently we have the logic to deal with the JDBC keytab provided by the "--files" option

{{if (keytabParam != null && FilenameUtils.getPath(keytabParam).isEmpty)}}
 \{{{}}
 {{}}{{val result = SparkFiles.get(keytabParam)}}
 {{}}{{logDebug(s"Keytab path not found, assuming --files, file name used on executor: $result")}}
 {{}}{{result}}
 {{}}} {{else {}}
 {{}}{{logDebug("Keytab path found, assuming manual upload")}}
 {{}}{{keytabParam}}
 {{}}}

Spark has already created the soft link for any file submitted by the "--files" option. Here is an example.

testusera1.keytab -> /var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/filecache/12/testusera1.keytab

 

So there is no need to call the SparkFiles.get to absolute path of the keytab file. We can directly use the variable `keytabParam` as the keytab file path.

 

Moreover, SparkFiles.get will get a wrong path of keytab for the driver in cluster mode. In cluster mode, the keytab is available at the following location for both the driver and executors

{{/var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/container_1628584679772_0030_01_000001/testusera1.keytab}}

but SparkFiles.get brings the following wrong location for the driver

/var/opt/hadoop/temp/nm-local-dir/usercache/testusera1/appcache/application_1628584679772_0003/spark-8fb0f437-c842-4a9f-9612-39de40082e40/userFiles-5075388b-0928-4bc3-a498-7f6c84b27808/testusera1.keytab

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Aug 12 12:59:42 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tub4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): gaborgsomogyi
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 3.1.2
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 12/Aug/21 12:59;xuzikun2003;A PR is submitted for this issue

https://github.com/apache/spark/pull/33726;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Infering schema from JSON file shall respect ignoreCorruptFiles and handle IOE 
Issue key: SPARK-36477
Issue id: 13394599
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Qin Yao
Creator: Qin Yao
Created: 11/Aug/21 09:25
Updated: 11/Aug/21 10:00
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 11 10:00:13 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tso8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Aug/21 10:00;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33706;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Add the support in Spark for having group by map datatype column for the scenario that works in Hive
Issue key: SPARK-36452
Issue id: 13394087
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: saurabhc100
Creator: saurabhc100
Created: 08/Aug/21 13:11
Updated: 08/Aug/21 13:44
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Add the support in Spark for having group by map datatype column for the scenario that works in Hive.

In hive the below scenario works 
{code:java}
describe extended complex2;
OK
id                  string 
c1                  map<int, string>   
Detailed Table Information Table(tableName:complex2, dbName:default, owner:abc, createTime:1627994412, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:id, type:string, comment:null), FieldSchema(name:c1, type:map<int,string>, comment:null)], location:/user/hive/warehouse/complex2, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat,serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1})

select * from complex2;
OK
1 {1:"u"}
2 {1:"u",2:"uo"}
1 {1:"u",2:"uo"}
Time taken: 0.363 seconds, Fetched: 3 row(s)

Working Scenario in Hive -: 

select id, c1, count(*) from complex2 group by id, c1;
OK
1 {1:"u"} 1
1 {1:"u",2:"uo"} 1
2 {1:"u",2:"uo"} 1
Time taken: 1.621 seconds, Fetched: 3 row(s)

Failed Scenario in Hive -: failed when map type is present in aggregated expression 

select id, max(c1), count(*) from complex2 group by id, c1;

FAILED: UDFArgumentTypeException Cannot support comparison of map<> type or complex type containing map<>.
{code}
But in spark this scenario where the group by map column failed for this scenario where the map column is used in the select without any aggregation
{code:java}
scala> spark.sql("select id,c1, count(*) from complex2 group by id, c1").show
org.apache.spark.sql.AnalysisException: expression spark_catalog.default.complex2.`c1` cannot be used as a grouping expression because its data type map<int,string> is not an orderable data type.;
Aggregate [id#1, c1#2], [id#1, c1#2, count(1) AS count(1)#3L]
+- SubqueryAlias spark_catalog.default.complex2
 +- HiveTableRelation [`default`.`complex2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#1, c1#2], Partition Cols: []]
at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:50)
{code}
There is need to add the this scenario where grouping expression can have map type if aggregated expression does not have the that map type reference. This helps in migrating the user from hive to Spark.

After the code change 
{code:java}
scala> spark.sql("select id,c1, count(*) from complex2 group by id, c1").show
+---+-----------------+--------+                                                
| id|               c1|count(1)|
+---+-----------------+--------+
|  1|         {1 -> u}|       1|
|  2|{1 -> u, 2 -> uo}|       1|
|  1|{1 -> u, 2 -> uo}|       1|
+---+-----------------+--------+
 {code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Aug 08 13:28:11 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tpig:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Aug/21 13:27;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/33679;;;, 08/Aug/21 13:28;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/33679;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Add SQL metrics to AdaptiveSparkPlanExec for BHJs and Skew joins
Issue key: SPARK-36416
Issue id: 13393537
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ekoifman
Creator: ekoifman
Created: 04/Aug/21 17:14
Updated: 04/Aug/21 20:30
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Add {{"num broadcast joins conversions"}} and {{"num skew join conversions"}}
 metrics to {{AdaptiveSparkPlanExec}} so that it's easy to get a sense of how much impact AQE had on an a complex query.

It's also useful for systems that collect metrics for later analysis of AQE effectiveness in large production deployment.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 04 18:46:55 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tm48:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Aug/21 17:35;xkrogen;+1 this would be very helpful!;;;, 04/Aug/21 18:45;apachespark;User 'ekoifman' has created a pull request for this issue:
https://github.com/apache/spark/pull/33641;;;, 04/Aug/21 18:46;apachespark;User 'ekoifman' has created a pull request for this issue:
https://github.com/apache/spark/pull/33641;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Add Test  Coverage to meet viewFs(Hadoop federation) scenario
Issue key: SPARK-36412
Issue id: 13393434
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: senthh
Creator: senthh
Created: 04/Aug/21 10:09
Updated: 04/Aug/21 10:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Create coverage Test to meet viewFs(Hadoop federation) scenario.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Aug 04 10:10:05 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tlhc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Aug/21 10:10;senthh;I am working on this;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Spark submit REST cluster/standalone mode - launching an s3a jar with STS
Issue key: SPARK-35974
Issue id: 13387077
Parent id: 
Issue Type: Bug
Status: Reopened
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: toopt4
Creator: toopt4
Created: 01/Jul/21 14:25
Updated: 29/Jul/21 23:38
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: {code:java}
/var/lib/spark-2.4.8-bin-hadoop2.7/bin/spark-submit --master spark://myhost:6066 --conf spark.hadoop.fs.s3a.access.key='redact1' --conf spark.executorEnv.AWS_ACCESS_KEY_ID='redact1' --conf spark.driverEnv.AWS_ACCESS_KEY_ID='redact1' --conf spark.hadoop.fs.s3a.secret.key='redact2' --conf spark.executorEnv.AWS_SECRET_ACCESS_KEY='redact2' --conf spark.driverEnv.AWS_SECRET_ACCESS_KEY='redact2' --conf spark.hadoop.fs.s3a.session.token='redact3' --conf spark.executorEnv.AWS_SESSION_TOKEN='redact3' --conf spark.driverEnv.AWS_SESSION_TOKEN='redact3' --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider --conf spark.driver.extraJavaOptions='-DAWS_ACCESS_KEY_ID=redact1 -DAWS_SECRET_ACCESS_KEY=redact2 -DAWS_SESSION_TOKEN=redact3' --conf spark.executor.extraJavaOptions='-DAWS_ACCESS_KEY_ID=redact1 -DAWS_SECRET_ACCESS_KEY=redact2 -DAWS_SESSION_TOKEN=redact3' --total-executor-cores 4 --executor-cores 2 --executor-memory 2g --driver-memory 1g --name lin1 --deploy-mode cluster --conf spark.eventLog.enabled=false --class com.yotpo.metorikku.Metorikku s3a://mybuc/metorikku_2.11.jar -c s3a://mybuc/spark_ingestion_job.yaml
{code}
running the above command give below stack trace:

 
{code:java}
 Exception from the cluster:\njava.nio.file.AccessDeniedException: s3a://mybuc/metorikku_2.11.jar: getFileStatus on s3a://mybuc/metorikku_2.11.jar: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: xx; S3 Extended Request ID: /1qj/yy=), S3 Extended Request ID: /1qj/yy=\n\
org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:158)
org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)
org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1542)
org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)
org.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1463)
org.apache.hadoop.fs.s3a.S3AFileSystem.isFile(S3AFileSystem.java:2030)
org.apache.spark.util.Utils$.fetchHcfsFile(Utils.scala:747)
org.apache.spark.util.Utils$.doFetchFile(Utils.scala:723)
org.apache.spark.util.Utils$.fetchFile(Utils.scala:509)
org.apache.spark.deploy.worker.DriverRunner.downloadUserJar(DriverRunner.scala:155)
org.apache.spark.deploy.worker.DriverRunner.prepareAndRunDriver(DriverRunner.scala:173)
org.apache.spark.deploy.worker.DriverRunner$$anon$1.run(DriverRunner.scala:92){code}
all the ec2s in the spark cluster only have access to s3 via STS tokens. The jar itself reads csvs from s3 using the tokens, and everything works if either 1. i change the commandline to point to local jars on the ec2 OR 2. use port 7077/client mode instead of cluster mode. But it seems the jar itself can't be launched off s3, as if the tokens are not being picked up properly.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jul 29 23:37:46 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sic0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Jul/21 18:38;dongjoon;Could you try to use Apache Spark 3.1.2, please, [~toopt4], because Apache Spark 2.4 is EOL. It seems that the log shows `spark-2.3.4-bin-hadoop2.7` and the affected version is 2.4.6. Both are too old.;;;, 01/Jul/21 18:38;dongjoon;Free free to reopen this with the updated information with Spark 3.;;;, 03/Jul/21 00:44;toopt4;v2.4.8 is less than 2 months old;;;, 03/Jul/21 14:56;dongjoon;v2.4.8 is EOL.;;;, 03/Jul/21 14:57;dongjoon;Please reopen this with the up-to-date result with Spark 3.;;;, 03/Jul/21 14:58;dongjoon;According to our website, 
- https://spark.apache.org/versioning-policy.html

{code}
For example, 2.4.0 was released in November 2nd 2018 and had been maintained for 31 months until 2.4.8 was released on May 2021. 2.4.8 is the last release and no more 2.4.x releases should be expected even for bug fixes.
{code}

If you are reporting something with Spark 2.x, it's the same with the report with Spark 1.6, [~toopt4].;;;, 29/Jul/21 23:37;toopt4;same issue on spark 3.1.2:

 
{code:java}
{
  "action" : "SubmissionStatusResponse",
  "driverState" : "ERROR",
  "message" : "Exception from the cluster:\njava.nio.file.AccessDeniedException: s3a://redact/ingestion-0.5.2-SNAPSHOT.jar: getFileStatus on s3a://redact/ingestion-0.5.2-SNAPSHOT.jar: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: hidden; S3 Extended Request ID: hideit), S3 Extended Request ID: hideit\n\torg.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:158)\n\torg.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\torg.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1542)\n\torg.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\n\torg.apache.hadoop.fs.FileSystem.isFile(FileSystem.java:1463)\n\torg.apache.hadoop.fs.s3a.S3AFileSystem.isFile(S3AFileSystem.java:2030)\n\torg.apache.spark.util.Utils$.fetchHcfsFile(Utils.scala:799)\n\torg.apache.spark.util.Utils$.doFetchFile(Utils.scala:776)\n\torg.apache.spark.util.Utils$.fetchFile(Utils.scala:541)\n\torg.apache.spark.deploy.worker.DriverRunner.downloadUserJar(DriverRunner.scala:162)\n\torg.apache.spark.deploy.worker.DriverRunner.prepareAndRunDriver(DriverRunner.scala:180)\n\torg.apache.spark.deploy.worker.DriverRunner$$anon$2.run(DriverRunner.scala:99)",
  "serverSparkVersion" : "3.1.2",
  "submissionId" : "driver-20210729233253-0001",
  "success" : true,
  "workerHostPort" : "10.redact:17537",
  "workerId" : "worker-20210729232355-10.redact-17537"
}
 {code};;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: NoClassDefFoundError for org.slf4j.impl.StaticLoggerBinder in org.apache.spark.Logging#isLog4j12 when using SLF4J/Logback 2.x
Issue key: SPARK-36316
Issue id: 13392169
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: ian.springer
Creator: ian.springer
Created: 27/Jul/21 16:51
Updated: 27/Jul/21 16:51
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: When using SLF4J 2.x, I hit the following exception:

 
java.lang.NoClassDefFoundError: org/slf4j/impl/StaticLoggerBinder
Caused by: java.lang.ClassNotFoundException: org.slf4j.impl.StaticLoggerBinder
 

This is because org.slf4j.impl.StaticLoggerBinder no longer exists in SLF4J 2.x (see [http://www.slf4j.org/codes.html#StaticLoggerBinder).] Ideally, Spark should not have a hard dependency on an SFL4J 1.x impl classes. 

 

Perhaps reflection or NoClassDefFoundError try-catch blocks could be used in the logger detection code, so both SLF4J 1.x and 2.x could be supported at runtime.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-27 16:51:35.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tdoo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Extend SPARK-SQL to support REPEATABLE TABLESAMPLE clause. 
Issue key: SPARK-35934
Issue id: 13386560
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: miccagiann
Creator: miccagiann
Created: 29/Jun/21 16:20
Updated: 26/Jul/21 13:21
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: features
Description: Extend Spark-SQL so as to support the `REPEATABLE TABLESAMPLE` clause per SQL-standard:
{code:sql}
SELECT * FROM `foo` TABLESAMPLE (50.0 PERCENT) REPEATABLE (1234) {code}
Certain applications need to return repeatable/deterministic results when applying TABLESAMPLE clause.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jul 26 13:21:07 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sf5c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): srowen
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Jul/21 13:21;miccagiann;[~hyukjin.kwon] [~sowen] Can I assign this issue to myself and contribute the work back to the community? It appears that I cannot assign myself to this issue and I also cannot set the status of this Jira item to `In Progress`.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: [SQL] Add bitmap functions for Spark SQL
Issue key: SPARK-36118
Issue id: 13389325
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: apachespark
Reporter: master_chief
Creator: master_chief
Created: 13/Jul/21 05:57
Updated: 20/Jul/21 06:30
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Bitmaps are used more and more widely, and many frameworks have native support, such as Clickhouse. Maybe spark can also integrate this capability.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 13 06:13:05 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sw5s:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Jul/21 06:13;apachespark;User 'ReachInfi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33314;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Kubernetes: Allow different namespaces for Driver and Executors
Issue key: SPARK-35702
Issue id: 13383010
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: cth
Creator: cth
Created: 09/Jun/21 17:28
Updated: 08/Jul/21 04:59
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: Currently the Kubernetes namespace of driver AND executors always needs to be identical - specified with {{spark.kubernetes.namespace}}.
 Especially when developing in Kubernetes, this is a harsh restriction: In a _notebook_ Namespace I want to run spark in driver in client mode for interaction, however executors should get their own dedicated resources in a dedicated namespace with different policies and different lifecycles.

This could be solved by adding the following two additional configuration options:
{code:java}
 spark.kubernetes.executor.pod.namespace
 spark.kubernetes.driver.pod.namespace
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jul 08 04:59:40 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rt94:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Jul/21 18:44;holden;If you're running in client mode (e.g. notebook) does this matter?;;;, 08/Jul/21 04:59;cth;[~holden] yes it does matter, as different namespaces can have different policies. If working in a Notebook in Kubernetes, users typically don't have CREATE / DELETE Verbs unlocked for Pods and Configmaps in the Notebook namespace, in order to not interfere with other users Pods.
 A Serviceaccount in another executor namespace however allows users to spawn executors there.

There is acutally a workaround possible, which is however a bit messy and uncomplete with the following settings:
 spark.master: k8s://https://<kubernetes-api-ip>:443
 spark.driver.host: Pod IP of Notebook / Driver in Namespace 1
 spark.kubernetes.namespace: Namespace 2
 spark.kubernetes.image: Image to use for executors

This however breakes the [Client Mode Executor Pod Garbage Collection|https://spark.apache.org/docs/latest/running-on-kubernetes.html#client-mode-executor-pod-garbage-collection] as the driver is expected to be in spark.kubernetes.namespace.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Push down Filter in case of filter having child as TypedFilter
Issue key: SPARK-36027
Issue id: 13387964
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: saurabhc100
Creator: saurabhc100
Created: 06/Jul/21 15:03
Updated: 06/Jul/21 15:11
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: In case of Filter having child as TypedFilter, Pushdown of Filters does not take place

 
{code:java}
scala> def testUdfFunction(r: String): Boolean = {
 | r.equals("hello")
 | }
testUdfFunction: (r: String)Boolean

val df= spark.read.parquet("/testDir/testParquetSize/Parquetgzip/")
df: org.apache.spark.sql.DataFrame = [_1: string, _2: string ... 1 more field]
{code}
 

df.filter(x => testUdfFunction(x.getAs("_1"))).filter("_2<='id103855'").queryExecution.executedPlan

 
{code:java}
Filter (isnotnull(_2#1) AND (_2#1 <= id103855))
+- *(1) Filter $line20.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3184/1455948476@5ce4af92.apply
 +- *(1) ColumnarToRow
 +- FileScan parquet [_1#0,_2#1,_3#2] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/testDir/testParquetSize/Parquetgzip], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_1:string,_2:string,_3:string>
 
{code}
df.filter(x => testUdfFunction(x.getAs("_1"))).filter("_2<='id103855'").queryExecution.optimizedPlan
{code:java}
Filter (isnotnull(_2#1) AND (_2#1 <= id103855))
+- TypedFilter $line22.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$Lambda$3191/320569017@37a2806c, interface org.apache.spark.sql.Row, [StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true)], createexternalrow(_1#0.toString, _2#1.toString, _3#2.toString, StructField(_1,StringType,true), StructField(_2,StringType,true), StructField(_3,StringType,true))
 +- Relation[_1#0,_2#1,_3#2] parquet{code}
 

There is need to add this change to push down the filter for this scenario

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 06 15:07:17 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sns0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Jul/21 15:07;apachespark;User 'SaurabhChawla100' has created a pull request for this issue:
https://github.com/apache/spark/pull/33232;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: in spark3.1.2 version The canCast method of type of char/varchar needs to be consistent with StringType
Issue key: SPARK-36005
Issue id: 13387432
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: biaobiao.sun
Creator: biaobiao.sun
Created: 03/Jul/21 03:41
Updated: 04/Jul/21 01:41
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: In https://github.com/apache/spark/pull/32109 this pr, we introduced the char/varchar type,

As described in this issue:

To be safe, this PR doesn't add char/varchar type to the query engine(expression input check, internal row framework, codegen framework, etc.). We will replace char/varchar type by string type with metadata (Attribute.metadata or StructField.metadata) that includes the original type string before it goes into the query engine. That said, the existing code will not see char/varchar type but only string type.


so The canCast method of type of char/varchar needs to be consistent with StringType
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Jul 03 07:52:21 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ski8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Jul/21 07:51;apachespark;User 'zheniantoushipashi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33201;;;, 03/Jul/21 07:52;apachespark;User 'zheniantoushipashi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33201;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Allow predicate for pyspark.sql.functions.array_sort
Issue key: SPARK-35970
Issue id: 13386992
Parent id: 
Issue Type: Wish
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: sramanan
Creator: sramanan
Created: 01/Jul/21 08:43
Updated: 01/Jul/21 08:43
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 1
Labels: 
Description: Currently, both the Python API and the Scala API for the SQL function `array_sort` do not take a predicate boolean function/lambda expression as a second argument. Hence, we have to resort to `expression` or `selectExpression` and use the DSL for the predicate function. It would be nice to allow this, just like all the higher-order functions.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-01 08:43:30.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sht4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: working with spring boot, spark context will stopped while application is started.
Issue key: SPARK-35949
Issue id: 13386696
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: sunpe
Creator: sunpe
Created: 30/Jun/21 07:11
Updated: 01/Jul/21 01:04
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark 3.1.2 with springboot, on client mode, the spark context while stopped while the application is started.

 

 
{code:java}
//代码占位符
@Bean
@ConditionalOnMissingBean(SparkSession.class)
public SparkSession sparkSession(SparkConf conf) {
    return SparkSession.builder()
            .enableHiveSupport()
            .config(conf)
            .getOrCreate();
}{code}
 
{quote} 21/06/30 12:03:38 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)
 21/06/30 12:03:38 INFO Utils: Using initial executors = 1, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances
 21/06/30 12:03:38 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)
 21/06/30 12:03:39 INFO WelcomePageHandlerMapping: Adding welcome page template: index
 21/06/30 12:03:40 INFO Http11NioProtocol: Starting ProtocolHandler ["http-nio-9000"]
 21/06/30 12:03:40 INFO TomcatWebServer: Tomcat started on port(s): 9000 (http) with context path ''
 21/06/30 12:03:40 INFO SpringApplication: Started application in 525.411 seconds (JVM running for 529.958)
 21/06/30 12:03:40 INFO AbstractConnector: Stopped Spark@3e1d19ea\{HTTP/1.1, (http/1.1)}
 Unknown macro: \{0.0.0.0}
 21/06/30 12:03:40 INFO SparkUI: Stopped Spark web UI at 
 21/06/30 12:03:40 INFO YarnClientSchedulerBackend: Interrupting monitor thread
 21/06/30 12:03:40 INFO YarnClientSchedulerBackend: Shutting down all executors
 21/06/30 12:03:40 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
 21/06/30 12:03:40 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
 21/06/30 12:03:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
 21/06/30 12:03:40 INFO MemoryStore: MemoryStore cleared
 21/06/30 12:03:40 INFO BlockManager: BlockManager stopped
 21/06/30 12:03:40 INFO BlockManagerMaster: BlockManagerMaster stopped
 21/06/30 12:03:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
 21/06/30 12:03:40 INFO SparkContext: Successfully stopped SparkContext
 21/06/30 12:03:40 INFO [/]: Initializing Spring DispatcherServlet 'dispatcherServlet'
 21/06/30 12:03:40 INFO DispatcherServlet: Initializing Servlet 'dispatcherServlet'
 21/06/30 12:03:40 INFO DispatcherServlet: Completed initialization in 1 ms
{quote}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jun 30 09:19:51 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sfzk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Jun/21 08:00;apachespark;User 'sunpe' has created a pull request for this issue:
https://github.com/apache/spark/pull/33151;;;, 30/Jun/21 09:19;apachespark;User 'sunpe' has created a pull request for this issue:
https://github.com/apache/spark/pull/33154;;;, 30/Jun/21 09:19;apachespark;User 'sunpe' has created a pull request for this issue:
https://github.com/apache/spark/pull/33154;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: [Deploy] Upgrade Apache Curator Dependency to 4.2.0
Issue key: SPARK-35954
Issue id: 13386835
Parent id: 
Issue Type: Dependency upgrade
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: nrousseau
Creator: nrousseau
Created: 30/Jun/21 15:25
Updated: 30/Jun/21 16:00
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Deploy
Due Date: 
Votes: 0
Labels: 
Description: +Abstract :+ as a Spark Cluster Administrator, I want to connect spark masters deployed in HA/mode to Zookeeper over SSL/TLS, so that my network traffic is ciphered between y components. ([https://spark.apache.org/docs/latest/spark-standalone.html#standby-masters-with-zookeeper]  )

 

With the release of Hadoop 3.3.1, ZKFC libraries and their dependencies were updated : it is now possible to connect ZKFC to ZooKeeper over TLS.

 

+Note:+ TLS is possible with Zookeeper Server Version >= 3.5.6 ([https://docs.confluent.io/platform/current/installation/versions-interoperability.html#zk] )

 

Spark 3.2.0 aims to support Hadoop 3.3.1 ; this Hadoop release bundles the following shared libraries :
 * curator-client-4.2.0.jar
 * curator-framework-4.2.0.jar
 * curator-recipes-4.2.0.jar
 * zookeeper-3.5.6.jar
 * zookeeper-jute-3.5.6.jar

 

Currently, Spark dependency is set to 2.13.0 for the Currator framework ([https://github.com/apache/spark/blob/master/pom.xml#L127).|https://github.com/apache/spark/blob/master/pom.xml#L127)]

 

It would be great to update "curator-*" dependencies to 4.2.0 in order to be compatible with shared jars of the hadoop stack.

Moreover, it will allow administrators to connect Spark Masters to ZooKeeper over TLS.

 

Some patches will be required, such as :
 * [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkCuratorUtil.scala#L51]

 

I will try to prepare a MR for this.

 

 
Environment: * OS: Linux
 * JAVA: 1.8.0_292
 * {color:#FF0000}*hadoop-3.3.1*{color}

 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jun 30 15:28:17 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sgu8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 30/Jun/21 15:28;nrousseau;Since it is my first contribution, do not hesitate to point me guidelines about this issue :);;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Ability to override Yarn Cluster Submit Class with Configuration
Issue key: SPARK-35931
Issue id: 13386536
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: akbordia
Creator: akbordia
Created: 29/Jun/21 14:20
Updated: 29/Jun/21 14:59
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core, YARN
Due Date: 
Votes: 0
Labels: 
Description: Currently, YarnClusterApplication class is used to initiate Yarn Client and submit application to Yarn cluster. In case of a custom variant of Yarn cluster, there may be significant (non-generic) changes required in Yarn client. User should be able to override YarnClusterApplication via config.  Using this configuration, they can add custom implementation of YarnClusterApplication (as a separate jar) which can invoke a custom Yarn client.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jun 29 14:59:49 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sf00:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Jun/21 14:34;apachespark;User 'akshatb1' has created a pull request for this issue:
https://github.com/apache/spark/pull/33134;;;, 29/Jun/21 14:34;apachespark;User 'akshatb1' has created a pull request for this issue:
https://github.com/apache/spark/pull/33134;;;, 29/Jun/21 14:59;apachespark;User 'akshatb1' has created a pull request for this issue:
https://github.com/apache/spark/pull/33135;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: spark.driver.log.dfsDir with hdfs scheme failed
Issue key: SPARK-35902
Issue id: 13385952
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: yghu
Creator: yghu
Created: 26/Jun/21 01:49
Updated: 27/Jun/21 00:01
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: when i set spark.driver.log.dfsDir value with hdfs scheme path, it throw an exception:

spark.driver.log.persistToDfs.enabled = true

spark.driver.log.dfsDir = hdfs://hacluster/spark2xdriverlogs1

 

2021-06-25 14:56:45,786 | ERROR | main | Could not persist driver logs to dfs | org.apache.spark.util.logging.DriverLogger.logError(Logging.scala:94)
 java.lang.IllegalArgumentException: Pathname /opt/client811/Spark2x/spark/hdfs:/hacluster/spark2xdriverlogs1 from /opt/client811/Spark2x/spark/hdfs:/hacluster/spark2xdriverlogs1 is not a valid DFS filename.
 at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:252)
 at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1375)
 at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1372)
 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
 at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1389)
 at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1364)
 at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2410)
 at org.apache.spark.deploy.SparkHadoopUtil$.createFile(SparkHadoopUtil.scala:528)
 at org.apache.spark.util.logging.DriverLogger$DfsAsyncWriter.init(DriverLogger.scala:118)
 at org.apache.spark.util.logging.DriverLogger$DfsAsyncWriter.<init>(DriverLogger.scala:104)
 at org.apache.spark.util.logging.DriverLogger.startSync(DriverLogger.scala:72)
 at org.apache.spark.SparkContext.$anonfun$postApplicationStart$1(SparkContext.scala:2688)
 at org.apache.spark.SparkContext.$anonfun$postApplicationStart$1$adapted(SparkContext.scala:2688)
 at scala.Option.foreach(Option.scala:407)
 at org.apache.spark.SparkContext.postApplicationStart(SparkContext.scala:2688)
 at org.apache.spark.SparkContext.<init>(SparkContext.scala:640)
 at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2814)
 at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:947)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:941)
 at org.apache.spark.repl.Main$.createSparkSession(Main.scala:106)
 at $line3.$read$$iw$$iw.<init>(<console>:15)
 at $line3.$read$$iw.<init>(<console>:42)
 at $line3.$read.<init>(<console>:44)
 at $line3.$read$.<init>(<console>:48)
 at $line3.$read$.<clinit>(<console>)
 at $line3.$eval$.$print$lzycompute(<console>:7)
 at $line3.$eval$.$print(<console>:6)
 at $line3.$eval.$print(<console>)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
 at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
 at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
 at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
 at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
 at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
 at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
 at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
 at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
 at scala.tools.nsc.interpreter.IMain.$anonfun$quietRun$1(IMain.scala:224)
 at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:214)
 at scala.tools.nsc.interpreter.IMain.quietRun(IMain.scala:224)
 at org.apache.spark.repl.SparkILoop.$anonfun$initializeSpark$2(SparkILoop.scala:83)
 at scala.collection.immutable.List.foreach(List.scala:392)
 at org.apache.spark.repl.SparkILoop.$anonfun$initializeSpark$1(SparkILoop.scala:83)
 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
 at scala.tools.nsc.interpreter.ILoop.savingReplayStack(ILoop.scala:99)
 at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:83)
 at org.apache.spark.repl.SparkILoop.$anonfun$process$4(SparkILoop.scala:165)
 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
 at scala.tools.nsc.interpreter.ILoop.$anonfun$mumly$1(ILoop.scala:168)
 at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:214)
 at scala.tools.nsc.interpreter.ILoop.mumly(ILoop.scala:165)
 at org.apache.spark.repl.SparkILoop.loopPostInit$1(SparkILoop.scala:153)
 at org.apache.spark.repl.SparkILoop.$anonfun$process$10(SparkILoop.scala:221)
 at org.apache.spark.repl.SparkILoop.withSuppressedSettings$1(SparkILoop.scala:189)
 at org.apache.spark.repl.SparkILoop.startup$1(SparkILoop.scala:201)
 at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:236)
 at org.apache.spark.repl.Main$.doMain(Main.scala:78)
 at org.apache.spark.repl.Main$.main(Main.scala:58)
 at org.apache.spark.repl.Main.main(Main.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:993)
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:183)
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:206)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:93)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1072)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1081)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

 
Environment: Spark3.1.1 Hadoop 3.1.1
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jun 27 00:01:47 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sbe8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Jun/21 01:50;yghu;I'd like to work on this.;;;, 26/Jun/21 02:16;dongjoon;Thank you for reporting, [~yghu]. Go for it!;;;, 27/Jun/21 00:01;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33104;;;
Affects Version/s.1: 3.1.1
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Query performance degradation additional predicate and UDF call for explode
Issue key: SPARK-35882
Issue id: 13385692
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: johnhb
Creator: johnhb
Created: 24/Jun/21 17:49
Updated: 25/Jun/21 07:41
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: This issue cannot be seen in 3.0.1, but has been introduced since and observed in 3.1.1 and 3.1.2. I have a reproduce for this issue here: [https://github.com/johnbateman/spark-udf-slowdown] just change the sbt file between 3.1.2 and 3.0.1 to observe the difference in performance. It is a rather silly example but it demonstrates the issue.

Physical plan for 3.0.1, it executes on my machine in about 40 seconds.

 
{code:java}
== Physical Plan == Generate explode(fib#3), [id#1L, fib#3], false, [fib2#7] 
+- *(1) Project [id#1L, UDF(cast(id#1L as int)) AS fib#3] 
  +- *(1) Range (1, 500000, step=1, splits=8)
{code}
 

Physical plan for 3.1.2, it executes on my machine in about 4.7 min.

 
{code:java}
== Physical Plan ==
Generate (4)
+- * Project (3)
   +- * Filter (2)
      +- * Range (1)

(1) Range [codegen id : 1]
Output [1]: [id#2L]
Arguments: Range (1, 500000, step=1, splits=Some(8))

(2) Filter [codegen id : 1]
Input [1]: [id#2L]
Condition : ((size(UDF(cast(id#2L as int)), true) > 0) AND isnotnull(UDF(cast(id#2L as int))))

(3) Project [codegen id : 1]
Output [2]: [id#2L, UDF(cast(id#2L as int)) AS fib#4]
Input [1]: [id#2L]

(4) Generate
Input [2]: [id#2L, fib#4]
Arguments: explode(fib#4), [id#2L, fib#4], false, [fib2#11]{code}
 

You can see that there is an additional predicate generated in step 2, I can also confirm that the UDF is now called multiple times instead of once. I am aware that this is to be expected sometimes, but it is a change that has resulted in performance degradation particularly for expensive UDFs. Obviously, there is something specific to this query (ie the explode) that seems to be responsible for this predicate and UDF issue occurring, but I am not sure what that is.

For reference, this is the same issue (https://issues.apache.org/jira/browse/SPARK-35787)
Environment: Present on local ubuntu machine. Also CentOS VMs.
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): SPARK-35787
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jun 25 07:41:49 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s9sg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Jun/21 07:41;vdrasutis;Maybe it could be related to https://issues.apache.org/jira/browse/SPARK-35767 ?;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0

Summary: Unify IntervalUtils.castStringToDTInterval with parser
Issue key: SPARK-35891
Issue id: 13385776
Parent id: 13234582.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 25/Jun/21 06:35
Updated: 25/Jun/21 06:35
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-06-25 06:35:36.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sab4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Unify IntervalUtils.castStringToYMInterval with parser
Issue key: SPARK-35890
Issue id: 13385775
Parent id: 13234582.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: angerszhuuu
Creator: angerszhuuu
Created: 25/Jun/21 06:35
Updated: 25/Jun/21 06:35
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-06-25 06:35:00.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0saaw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Remove await (syncMode) in ChunkFetchRequestHandler
Issue key: SPARK-35865
Issue id: 13385470
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Baohe Zhang
Creator: Baohe Zhang
Created: 23/Jun/21 17:40
Updated: 23/Jun/21 17:41
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.1.2
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: SPARK-24355 introduces syncMode to mitigate the issue of sasl timeout by throting the max number of threads for sending responses of chunk fetch requests. But it causes severe performance degradation because the throughput of handling chunk fetch requests is reduced. SPARK-30623 makes the async and sync mode configurable and makes the async mode the default. 

SPARK-30512 uses a dedicated boss event loop to mitigate the sasl timeout issue and we rarely see sasl timeout issues with async mode in our production clusters today. 

Few days ago we accidentally turned on sync mode on one cluster and we observed severe shuffle performance degradation. As a result, We benchmarked the performance comparison between async and sync mode and *we suggest removing sync mode in the code base* as it seems not to provide any benefits today. We would like to share the benchmark result and hear the opinion from the community.

 

benchmark on job's run time (sync mode is 2x - 3x slower):
 YARN cluster setup: 6 nodes, 18 executors, each executor has 1 core and 3 GB memory, each node manager has 1GB heap size.

shuffle stages: 5GB shuffle data (400M key-value records), 1000 map tasks and 1000 reduce tasks.

results: shuffle read 5GB data, async mode takes 2-3 mins and sync mode takes 6 mins.

 

benchmark on metrics of external shuffle service:
 YARN cluster setup: 4 nodes in total. I set 2 nodes as async mode and 2 nodes as sync mode, shuffling 2.5 GB data.

results: in openblockreuqestslatencymillis_ratemean and some other metrics, the nodes in sync mode are 3x - 4x higher than nodes in async mode. I attached some screenshots of the metrics.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 23/Jun/21 17:40;Baohe Zhang;openblock-compare.png;https://issues.apache.org/jira/secure/attachment/13027206/openblock-compare.png, 23/Jun/21 17:40;Baohe Zhang;openblock.png;https://issues.apache.org/jira/secure/attachment/13027207/openblock.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-06-23 17:40:00.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s8f4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 23/Jun/21 17:40;Baohe Zhang;openblock.png;https://issues.apache.org/jira/secure/attachment/13027207/openblock.png
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Should skip retrieving driver pod in client mode
Issue key: SPARK-35828
Issue id: 13384675
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Qin Yao
Creator: Qin Yao
Created: 19/Jun/21 18:39
Updated: 19/Jun/21 19:00
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.2, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: 

{code:java}
bin/spark-submit  \
--conf spark.kubernetes.file.upload.path=./ \
--deploy-mode client \
--master k8s://https://kubernetes.docker.internal:6443 \
--conf spark.kubernetes.container.image=yaooqinn/spark:v20210619 \
-c spark.kubernetes.context=docker-for-desktop_1 \
--conf spark.kubernetes.executor.podNamePrefix=sparksql \
--conf spark.dynamicAllocation.shuffleTracking.enabled=true \
--conf spark.dynamicAllocation.enabled=true \
--conf spark.kubernetes.driver.pod.name=abc \
--class org.apache.spark.examples.SparkPi examples/jars/spark-examples_2.12-3.2.0-SNAPSHOT.jar
{code}

When `spark.kubernetes.driver.pod.name` is specific, we now get the driver pod for whatever the deploy mode is, while the driver pod only exists in cluster mode. So we should skip retrieving it instead of get the following error.

{code:java}
21/06/19 16:18:49 ERROR SparkContext: Error initializing SparkContext.
org.apache.spark.SparkException: No pod was found named abc in the cluster in the namespace default (this was supposed to be the driver pod.).
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$driverPod$2(ExecutorPodsAllocator.scala:81)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$driverPod$1(ExecutorPodsAllocator.scala:79)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.<init>(ExecutorPodsAllocator.scala:76)
	at org.apache.spark.scheduler.cluster.k8s.KubernetesClusterManager.createSchedulerBackend(KubernetesClusterManager.scala:118)
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2969)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:559)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2686)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:948)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:942)
	at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30)
	at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Jun 19 19:00:09 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s3io:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Jun/21 18:59;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/32979;;;, 19/Jun/21 19:00;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/32979;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: HighlyCompressedMapStatus should record accurately the size of skewed shuffle blocks
Issue key: SPARK-35596
Issue id: 13381477
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: exmy
Creator: exmy
Created: 01/Jun/21 14:57
Updated: 17/Jun/21 03:13
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.2, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: HighlyCompressedMapStatus currently cannot record accurately the size of shuffle blocks which much greater than other block but small than `spark.shuffle.accurateBlockThreshold`, which is likely to lead OOM when fetch shuffle blocks. We have to tune some extra properties like `spark.reducer.maxReqsInFlight` to prevent it, so it is better to fix it in HighlyCompressedMapStatus.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 17 03:12:35 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rjtc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Jun/21 15:44;apachespark;User 'exmy' has created a pull request for this issue:
https://github.com/apache/spark/pull/32733;;;, 17/Jun/21 03:12;podongfeng;I think I encounted a similar case in 3.0.2:

 

in a skewed stage, the matrics of shuffle read size are:

min: 13.2M, 25th: 32.4M, median: 48.2M, 75th: 80.6M, Max:13.2G

 

while in \{OptimizeSkewedJoin}, the statistic shows the left side are even and all partitions are of size 82M


 55986 Optimizing skewed join for [cast(batch_id#340 as bigint)|#340 as bigint)], [batch_id#466L|#466L], LeftOuter. 55987 Left side (canSplit=true) partitions size info: 55988 median size: 85996393, max size: 85996393, min size: 85953247, avg size: 85995559 55989 Right side (canSplit=false) partitions size info: 55990 median size: 648168, max size: 648168, min size: 648168, avg size: 648168
  ;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Put blocks only on disk while migrating RDD cached data
Issue key: SPARK-35754
Issue id: 13383692
Parent id: 13069723.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: abhishek_tiwari
Creator: abhishek_tiwari
Created: 14/Jun/21 09:32
Updated: 14/Jun/21 10:23
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: While migrating cached data we might drop already cached blocks if enough memory is not available on the peer block manager.
We should add a conf to put migrating blocks only on disk
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 14 10:23:10 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rxgo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Jun/21 10:22;apachespark;User 'q2w' has created a pull request for this issue:
https://github.com/apache/spark/pull/32902;;;, 14/Jun/21 10:23;apachespark;User 'q2w' has created a pull request for this issue:
https://github.com/apache/spark/pull/32902;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: [K8s] Spark executors memory request should be allowed to deviate from limit
Issue key: SPARK-35723
Issue id: 13383331
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: cth
Creator: cth
Created: 11/Jun/21 06:59
Updated: 11/Jun/21 06:59
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 1
Labels: 
Description: Currently the driver and executor memor requests always equals the limit.
As stated in [SPARK-23825|https://issues.apache.org/jira/browse/SPARK-23825], this is a reasonable default and is especially important for the driver.

For executors however, it might be usefull for users to deviate from this default for executors.

In typical development environments on K8s, the namespace quotas are an upper bound to the memory request that is possible.
The limits however can be much higher. For development, spark is often run in client mode. While the driver should request the memory it needs, we want to leverage all the resources of the cluster whith executors if they are free - and can live with an executor maybe beeing killed eventually. 

Thus I propose the introduction of {{spark.{driver,executor}.limit.memory}} similar to the {{spark.{driver,executor}.limit.cpu}}.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-06-11 06:59:43.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rv8g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Decommission executors in batches to avoid overloading network by block migrations.
Issue key: SPARK-35627
Issue id: 13381878
Parent id: 13069723.0
Issue Type: Sub-task
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: abhishek_tiwari
Creator: abhishek_tiwari
Created: 03/Jun/21 10:18
Updated: 03/Jun/21 10:22
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Currenlty, each executor is asked to starts offloading rdd and shuffle blocks as soon it is decommissioned. This can overload the network bandwidth of the application.

 

We should limit the number of executors migrating rdd and shuffle blocks at one time. This would be configurable to enable users to decommission executors conservatively or aggressively based on the use cases.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 03 10:22:15 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rma0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Jun/21 10:21;apachespark;User 'q2w' has created a pull request for this issue:
https://github.com/apache/spark/pull/32766;;;, 03/Jun/21 10:22;apachespark;User 'q2w' has created a pull request for this issue:
https://github.com/apache/spark/pull/32766;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: CTE With clause not working when using JDBC connection
Issue key: SPARK-35597
Issue id: 13381485
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Randallssv
Creator: Randallssv
Created: 01/Jun/21 15:35
Updated: 03/Jun/21 05:38
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: When using a JDBC data source, the "With" CTE function generates invalid SQL syntax when called

In the below example you can assume that SOURCE_CONNECTION, SQL_DRIVER and TABLE are all correctly defined.

 
SQLServerException: Incorrect syntax near the keyword 'WITH'.
at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:262) at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1632) at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(SQLServerPreparedStatement.java:602) at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecCmd.doExecute(SQLServerPreparedStatement.java:524) at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7418) at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3272) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:247) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:222) at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeQuery(SQLServerPreparedStatement.java:446) at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61) at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226) at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35) at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:385) at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:424) at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:391) at scala.Option.getOrElse(Option.scala:189) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:391) at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:264) at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:439)
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jun 01 17:42:59 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rjv4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Jun/21 17:42;Stelyus;What's your query ? It seems not related to Spark as your query is not understood by your JDBC driver;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: View result are not consistent after a modification inside a struct of the table
Issue key: SPARK-34528
Issue id: 13360747
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: tprelle-ubi
Creator: tprelle-ubi
Created: 24/Feb/21 21:09
Updated: 25/Feb/21 10:39
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.2, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: After [https://github.com/apache/spark/pull/31368] work to simplify hive view resolution
I found a bug because Hive allow you to change the order inside a struct

1) You create a table in hive with a struct:
 CREATE table test_struct (id int, sub STRUCT <a :INT, b:STRING>);
2) You insert data into it :
INSERT INTO TABLE test_struct select 1, named_struct("a",1,"b","v1");
3) Create a view on top of it :
CREATE view test_view_struct as select id, sub from test_view_struct
4) Change the table struct reodoring the struct
ALTER TABLE test_struct CHANGE COLUMN sub sub STRUCT < b:STRING,a :INT>;
5) Spark can not anymore query the view because struct in spark it's based on the position not on the name of the column.
If the changement it's castable you can even have a silent failed
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Feb 24 21:21:20 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0o1d4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 24/Feb/21 21:21;apachespark;User 'tprelle' has created a pull request for this issue:
https://github.com/apache/spark/pull/31639;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Use GenericData as Avro serialization data model can improve Avro write/read performance
Issue key: SPARK-34336
Issue id: 13356299
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: Baohe Zhang
Creator: Baohe Zhang
Created: 02/Feb/21 21:48
Updated: 02/Feb/21 22:06
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Input/Output, SQL
Due Date: 
Votes: 0
Labels: 
Description: We found that using "org.apache.avro.generic.GenericData" as Avro serialization data model in Avro writer can significantly improve Avro write performance and slightly improve Avro read performance.

This optimization was originally put up by [~samkhan]  in this PR https://github.com/apache/spark/pull/29354.

We re-evaluated the change "Use GenericData instead of ReflectData when writing Avro data" in that PR and verified it can provide performance improvement in Avro write/read benchmarks.

The base branch is today(2/2/21)'s branch-3.1.

Besides current Avro read/write benchmarks, I also ran some extra benchmarks for nested structs and arrays read/write, these benchmarks were put up in this PR https://github.com/apache/spark/pull/29352 but haven't been merged.

Benchmark results are added in the comment.

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 02/Feb/21 21:50;Baohe Zhang;base_read.txt;https://issues.apache.org/jira/secure/attachment/13019877/base_read.txt, 02/Feb/21 21:50;Baohe Zhang;base_write.txt;https://issues.apache.org/jira/secure/attachment/13019878/base_write.txt, 02/Feb/21 21:50;Baohe Zhang;generic_data_read.txt;https://issues.apache.org/jira/secure/attachment/13019879/generic_data_read.txt, 02/Feb/21 21:50;Baohe Zhang;generic_data_write.txt;https://issues.apache.org/jira/secure/attachment/13019880/generic_data_write.txt, 02/Feb/21 21:50;Baohe Zhang;read_comparison.png;https://issues.apache.org/jira/secure/attachment/13019876/read_comparison.png, 02/Feb/21 21:49;Baohe Zhang;write_comparison.png;https://issues.apache.org/jira/secure/attachment/13019875/write_comparison.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 6.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Feb 02 22:06:40 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0na2w:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 02/Feb/21 21:50;Baohe Zhang;Column chart comparison on avg time:

Avro write:
 !write_comparison.png! 

Avro read:
 !read_comparison.png! ;;;, 02/Feb/21 21:51;Baohe Zhang;Full benchmark results are added as txt attachments.;;;, 02/Feb/21 21:56;xkrogen;Thanks for bringing this up [~Baohe Zhang], I came across PR 29354 and was concerned that it would fall by the wayside now that [~samkhan] was no longer at Verizon Media :) I am not a committer so cannot give binding +1 but will be happy to help review and do what I can to get this in (y) ;;;, 02/Feb/21 22:06;apachespark;User 'baohe-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/31446;;;
Affects Version/s.1: 
Attachment.1: 02/Feb/21 21:50;Baohe Zhang;base_write.txt;https://issues.apache.org/jira/secure/attachment/13019878/base_write.txt
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Avoid migrating un-needed shuffle files
Issue key: SPARK-34280
Issue id: 13355269
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Major
Resolution: 
Assignee: 
Reporter: holden
Creator: holden
Created: 28/Jan/21 20:04
Updated: 31/Jan/21 05:12
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: In Spark 3.1 we introduced shuffle migrations. However, it is possible that a shuffle file will still exist after it is no longer needed. I've only observed this in a back port branch with SQL, so I'll do some more digging.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jan 31 05:12:13 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0n3qg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/Jan/21 05:12;attilapiros;It would be interesting to know whether the context cleaner was running for those blocks beforehand.

If you have the log it would be nice to see whether it has any of these: 
 - ["Error cleaning shuffle"|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/ContextCleaner.scala#L223-L237]
 - ["Error deleting data"|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/IndexShuffleBlockResolver.scala#L107-L121]

If not re-run it along with the logger "org.apache.spark.ContextCleaner" set to DEBUG level.

(I guess this not much help for you Holden but who knows who else reads this and looks for answers to similar questions.)
;;;
Affects Version/s.1: 3.1.1
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: pandas fixed width file support
Issue key: SPARK-36392
Issue id: 13393266
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: john.ayoub
Creator: john.ayoub
Created: 03/Aug/21 13:06
Updated: 18/Jul/23 22:55
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: please add support for the fixed width api in pandas to koalas. [reference|https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_fwf.html]
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Jul 18 22:55:44 UTC 2023
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tkg8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Oct/22 13:26;john.ayoub;[~itholic] [~hyukjin.kwon] hello, any update on when we could potentially get pandas fixed width file support added to spark?;;;, 27/Oct/22 01:16;gurwls223;I think it won'd be super difficult to implement this as far as I can tell. [~itholic] mind taking a look please?;;;, 27/Oct/22 05:00;itholic;Thanks for the reminder. Let me take a look this weekend, and try to add it if possible.;;;, 27/Oct/22 12:14;john.ayoub;Awesome, thank you!;;;, 10/Nov/22 08:37;gsdionis;[~itholic] how is this going? Happy to give a hand!;;;, 10/Nov/22 09:23;itholic;[~gsdionis] Sure, please feel free to work on this! I roughly get through, it's not that hard to implement, but recently I'm mainly focusing on other components so couldn't take a look into very detail.;;;, 29/Dec/22 13:21;john.ayoub;[~itholic] sorry to be a pest again but wanted to check if there has been any progress here? Happy New Year :);;;, 02/Jan/23 03:10;itholic;I think there is no related updated so far. [~gsdionis] do you happen to have any progress on this ticket ?;;;, 18/Jul/23 16:51;john.ayoub;[~itholic] Hello, any update on this ticket?;;;, 18/Jul/23 22:55;itholic;Not update yet here. [~gsdionis] , are you still interested in working on this ticket? Let me just work on if there is not responding until this weekend.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Spark 3.1.1 Internet Explorer 11 compatibility issues
Issue key: SPARK-35821
Issue id: 13384636
Parent id: 
Issue Type: Umbrella
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jobitmathew
Creator: jobitmathew
Created: 19/Jun/21 08:20
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Web UI
Due Date: 
Votes: 0
Labels: 
Description: Spark UI-Executor tab is empty in IE11

Spark UI-Stages DAG visualization is empty in IE11

other tabs looks Ok.

Spark job history shows completed and incomplete applications list .But when we go inside each application same issue may be there.

Attaching some screenshots
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 21/Jun/21 10:43;jobitmathew;Executortab_Chrome.png;https://issues.apache.org/jira/secure/attachment/13027111/Executortab_Chrome.png, 21/Jun/21 10:43;jobitmathew;Executortab_IE.PNG;https://issues.apache.org/jira/secure/attachment/13027110/Executortab_IE.PNG, 21/Jun/21 10:42;jobitmathew;dag_IE.PNG;https://issues.apache.org/jira/secure/attachment/13027109/dag_IE.PNG, 21/Jun/21 10:42;jobitmathew;dag_chrome.png;https://issues.apache.org/jira/secure/attachment/13027108/dag_chrome.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 4.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 21 10:46:24 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s3a0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Jun/21 03:18;gurwls223;[~jobitmathew] can you fill the JIRA description? ;;;, 20/Jun/21 03:18;gurwls223;cc [~sarutak] FYI;;;, 20/Jun/21 03:19;gurwls223;It would also be great to show some screenshots;;;, 20/Jun/21 04:30;sarutak;[~tsudukim] I remember you have hit this issue a few years ago. Do you know the cause of this issue?;;;, 21/Jun/21 07:54;tsudukim;Yes, our project met the problems like not showing the DAG area in the history page on IE11 several years ago, but we thought that those are just because of IE which has not enough compatibility with HTML5 or something, so we just avoided them by using Firefox. We didn't investigate the cause.;;;, 21/Jun/21 10:46;jobitmathew;[~hyukjin.kwon] I attached some screen shots .Could you please have a look;;;
Affects Version/s.1: 3.1.2
Attachment.1: 21/Jun/21 10:43;jobitmathew;Executortab_IE.PNG;https://issues.apache.org/jira/secure/attachment/13027110/Executortab_IE.PNG
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: Pyspark throws AnalysisException with incorrect error message when using .grouping() or .groupingId() (AnalysisException: grouping() can only be used with GroupingSets/Cube/Rollup;)
Issue key: SPARK-38983
Issue id: 13440938
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: Kimmel
Creator: Kimmel
Created: 21/Apr/22 14:07
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.1
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: cube, error_message_improvement, exception-handling, grouping, rollup
Description: h1. In a nutshell

Pyspark emits an incorrect error message when committing a type error with the results of the {{grouping()}} function.
h1. Code to reproduce

{{print(spark.version) # My environment, Azure DataBricks, defines spark automatically.}}
{{from pyspark.sql import functions as f}}
{{{}from pyspark.sql import types as t{}}}{{{}l = [{}}}
{{  ('a',),}}
{{  ('b',),}}
{{]}}
{{s = t.StructType([}}
{{  t.StructField('col1', t.StringType())}}
{{])}}
{{df = spark.createDataFrame(l, s)}}
{{{}df.display(){}}}{{{}( # This expression raises an AnalysisException(){}}}
{{  df}}
{{  .cube(f.col('col1'))}}
{{  .agg(f.grouping('col1') & f.lit(True))}}
{{  .collect()}}
{{)}}
h1. Expected results

The code produces an {{AnalysisException()}} with error message along the lines of:
{{AnalysisException: cannot resolve '(GROUPING(`col1`) AND true)' due to data type mismatch: differing types in '(GROUPING(`col1`) AND true)' (int and boolean).;}}
h1. Actual results

The code throws an {{AnalysisException()}} with error message
{{AnalysisException: grouping() can only be used with GroupingSets/Cube/Rollup;}}

Python provides the following traceback:
{{---------------------------------------------------------------------------}}
{{AnalysisException                         Traceback (most recent call last)}}
{{<command-2283735107422632> in <module>}}
{{     15 }}
{{     16 ( # This expression raises an AnalysisException()}}
{{---> 17   df}}
{{     18   .cube(f.col('col1'))}}
{{{}     19   .agg(f.grouping('col1') & f.lit(True)){}}}{{{}/databricks/spark/python/pyspark/sql/group.py in agg(self, *exprs){}}}
{{    116             # Columns}}
{{    117             assert all(isinstance(c, Column) for c in exprs), "all exprs should be Column"}}
{{--> 118             jdf = self._jgd.agg(exprs[0]._jc,}}
{{    119                                 _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))}}
{{{}    120         return DataFrame(jdf, self.sql_ctx){}}}{{{}/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in {_}{{_}}call{{_}}{_}(self, *args){}}}
{{   1302 }}
{{   1303         answer = self.gateway_client.send_command(command)}}
{{-> 1304         return_value = get_return_value(}}
{{   1305             answer, self.gateway_client, self.target_id, self.name)}}
{{   1306 }}{{/databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw)}}
{{    121                 # Hide where the exception came from that shows a non-Pythonic}}
{{    122                 # JVM exception message.}}
{{--> 123                 raise converted from None}}
{{    124             else:}}
{{{}    125                 raise{}}}{{{}AnalysisException: grouping() can only be used with GroupingSets/Cube/Rollup;{}}}
{{'Aggregate [cube(col1#548)|#548)], [col1#548, (grouping(col1#548) AND true) AS (grouping(col1) AND true)#551|#548, (grouping(col1#548) AND true) AS (grouping(col1) AND true)#551]}}
{{+- LogicalRDD [col1#548|#548], false}}
h1. Workaround

_Note:_ The reason I opened this ticket is that, when the user makes a particular type error, the resulting error message is misleading. The code snippet below shows how to fix that type error. It does not address the false-error-message bug, which is the focus of this ticket.

Cast the result of {{.grouping()}} to boolean type. That is, know _ab ovo_ that {{.grouping()}} produces an integer 0 or 1 rather than a boolean True or False.

{{(  # This expression does not raise an AnalysisException()}}
{{  df}}
{{  .cube(f.col('col1'))}}
{{  .agg(f.grouping('col1').cast(t.BooleanType()) & f.lit(True))}}
{{  .collect()}}
{{)}}
h1. Additional notes

The same error occurs if {{.cube()}} is replaced with {{.rollup()}} in "Code to reproduce".

The same error occurs if {{.grouping()}} is replaced with {{.grouping_id()}} in "Code to reproduce".
h1. Related tickets

https://issues.apache.org/jira/browse/SPARK-22748
h1. Relevant documentation
 * [Spark SQL GROUPBY, ROLLUP, and CUBE semantics|https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-groupby.html]
 * [DataFrame.cube()|https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.cube.html]
 * [DataFrame.rollup()|https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.rollup.html]
 * [DataFrame.agg()|https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.agg.html]
 * [functions.grouping()|https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.grouping.html]
 * [functions.grouping_id()|https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.grouping_id.html]

 
Environment: I have reproduced this error in two environments. I would be happy to answer questions about either.
h1. Environment 1

I first encountered this error on my employer's Azure Databricks cluster, which runs Spark version 3.1.2. I have limited access to cluster configuration information, but I can ask if it will help.
h1. Environment 2

I reproduced the error by running the same code in the Pyspark shell from Spark 3.2.1 on my Chromebook (i.e. Crostini Linux). I have more access to environment information here. Running {{spark-submit --version}} produced the following output:

{{Welcome to Spark version 3.2.1}}
{{Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.14}}
{{Branch HEAD}}
{{Compiled by user hgao on 2022-01-20T19:26:14Z}}
{{Revision 4f25b3f71238a00508a356591553f2dfa89f8290}}
{{Url https://github.com/apache/spark}}
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python, scala
Custom field (Last public comment date): Mon Apr 25 13:56:06 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11ogg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 25/Apr/22 00:46;gurwls223;Is the issue about error message? or are you saying str and bool should work?;;;, 25/Apr/22 13:56;Kimmel;Thanks for your comment, [~hyukjin.kwon] . This issue is about the misleading error message. I edited the ticket to clarify.;;;
Affects Version/s.1: 3.2.1
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0

Summary: Spark API to apply same function to multiple columns
Issue key: SPARK-36858
Issue id: 13403375
Parent id: 
Issue Type: New Feature
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: LvffY
Creator: LvffY
Created: 26/Sep/21 16:41
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Hi

My team and I have regularly need to apply the same function to multiple columns at once.

For example, we want to remove all non alphanumerical characters to each columns of our dataframes. 

When we hit this use case first, some people in my team were using this kind of code : 


{code:java}
val colListToClean = .... ## Generate some list, could be very long.
val dfToClean: DataFrame = ... ## This is the dataframe we want to clean
def cleanFunction(colName: String): Column = ... ## Write some function to manipulate column based on its name.
val dfCleaned = colListToClean.foldLeft(dfToClean)((df, colName) => df.withColumn(colName, cleanFunction(colName)){code}

This kind of code when applied on a large set of columns overloaded our driver (because a Dataframe is generated for each column to clean).

Based on this issue, we developed some code to add two functions : 


 * One to apply the same function to multiple columns
 * One to rename multiple columns based on a Map. 

 

I wonder if your ever ask your team to add such kind of API ? If you did, had you any kind of issue regarding the implementation ? If you didn't, is this any idea you could add to Spark ? 

Best regards, 

 

LvffY

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): scala
Custom field (Last public comment date): Thu Oct 14 04:39:15 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vat4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Sep/21 05:24;gurwls223;Can't we simply do this in a for loop?;;;, 06/Oct/21 13:31;LvffY;[~hyukjin.kwon] How would you do this ? 

From my point, if you make a `df.withColumn` in a for loop, it will end in the same execution plan (so probably in the same problem at the end, no ? ;;;, 07/Oct/21 01:13;gurwls223;you could use var. e.g.)

{code}
var df = ...
colListToClean.foreach { c => df = df.withColumn(c, func(...)) }
{code}

or actually what you did with foldLeft looks making sense too. What API do you have on your mind on this?;;;, 14/Oct/21 04:39;LvffY;Honestly, I feel a little dumb to not to have think at it earlier ... 

I fix our implementation and it is way better ! :)

To be clear, our method is like that : 


{code:java}
def withColumns(cols: Seq[String], columnTransform: String => Column, nameTransform: String => String = identity): DataFrame = { 
  // See https://issues.apache.org/jira/browse/SPARK-36858 
  cols.foreach((colName: String) => df = df.withColumn(nameTransform(colName), columnTransform(colName))) 
  df 
}
{code}

I think the method signature could easily be improved, and we could discuss about it.

Based on your comment, this ticket could probably change in "Add a question in some Tutorial" to avoid some noobies to fell into the trap I mention.

Of course, if Spark implements this method with some nice API it could be more easier to avoid this trap :) ;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Support Java Class with circular references
Issue key: SPARK-33598
Issue id: 13343246
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jacklzg
Creator: jacklzg
Created: 30/Nov/20 07:48
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Java API
Due Date: 
Votes: 2
Labels: 
Description: If the target Java data class has a circular reference, Spark will fail fast from creating the Dataset or running Encoders.

 

For example, with protobuf class, there is a reference with Descriptor, there is no way to build a dataset from the protobuf class.

From this line

{color:#7a869a}Encoders.bean(ProtoBuffOuterClass.ProtoBuff.class);{color}

 

It will throw out immediately

 
{quote}Exception in thread "main" java.lang.UnsupportedOperationException: Cannot have circular references in bean class, but got the circular reference of class class com.google.protobuf.Descriptors$Descriptor
{quote}
 

Can we add  a parameter, for example, 

 
{code:java}
Encoders.bean(Class<T> clas, List<Fields> fieldsToIgnore);{code}
````

or

 
{code:java}
Encoders.bean(Class<T> clas, boolean skipCircularRefField);{code}
 

 which subsequently, instead of throwing an exception @ [https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L556], it instead skip the field.

 
{code:java}
if (seenTypeSet.contains(t)) {
if(skipCircularRefField)
  println("field skipped") //just skip this field
else throw new UnsupportedOperationException( s"cannot have circular references in class, but got the circular reference of class $t")
}
{code}
 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Oct 11 14:13:17 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0l1eg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Dec/20 04:21;gurwls223;[~jacklzg] areyou able to test in the higher versions of Spark? I remember this was fixed in the upstream.;;;, 09/Sep/21 13:02;leoneuwald;Hi [~hyukjin.kwon] [~jacklzg].

I tested with Spark 3.1.2 and it still happening. I can share an example or try to fix if you are interested.

Thank you!;;;, 31/Aug/22 04:27;santokhsdg;*Facing same exception, Spark Version 3.2.2*

*Using avro mvn plugin to generate java class from below avro schema,*

 

{color:#ff0000}*Exception in thread "main" java.lang.UnsupportedOperationException: Cannot have circular references in bean class, but got the circular reference of class class org.apache.avro.Schema*{color}{*}{{*}}

*AVRO SCHEMA* 

[
{
"type": "record",
"namespace":"kafka.avro.schema.nested",
"name": "Address",
"fields": [

{ "name": "streetaddress", "type": "string"}

,

{"name": "city", "type": "string" }

]
},
{
"type": "record",
"name": "person",
"namespace":"kafka.avro.schema.nested",
"fields": [

{ "name": "firstname", "type": "string"}

,

{ "name": "lastname", "type": "string" }

,

{ "name": "address", "type": ["null","Address"] }

]
}
]

*-------CODE --------*

import kafka.avro.schema.nested.person;
import org.apache.spark.api.java.function.MapFunction;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.streaming.OutputMode;
import org.apache.spark.sql.streaming.StreamingQuery;
import org.apache.spark.sql.streaming.StreamingQueryException;
import org.apache.spark.sql.streaming.Trigger;
import za.co.absa.abris.avro.functions.*;
import za.co.absa.abris.config.AbrisConfig;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.concurrent.TimeoutException;

public class KafkaAvroStreamingAbris {

public static void main(String[] args) throws IOException, StreamingQueryException, TimeoutException {

SparkSession spark = SparkSession.builder()
.appName("AvroApp")
.master("local")
.getOrCreate();

Dataset df = spark.readStream()
.format("kafka")
.option("kafka.bootstrap.servers", "127.0.0.1:9092")
.option("subscribe", "person")
.option("startingOffsets", "earliest")
.load();

Dataset df2 = df
.select(za.co.absa.abris.avro.functions.from_avro(
org.apache.spark.sql.functions.col("value"),
za.co.absa.abris.config.AbrisConfig
.fromConfluentAvro().downloadReaderSchemaByLatestVersion()
.andTopicNameStrategy("person",false)
.usingSchemaRegistry("http://localhost:8089")).as("data"));

Dataset df3 = df2.map((MapFunction<Row,person>) row->

{ String rr = row.toString(); return null; }

, Encoders.bean(person.class));

StreamingQuery streamingQuery = df2
.writeStream()
.queryName("Kafka-Write")
.format("console")
.outputMode(OutputMode.Append())
.trigger(Trigger.ProcessingTime(Long.parseLong("2000")))
.start();

streamingQuery.awaitTermination();

}
};;;, 11/Oct/22 11:13;viveck_s@yahoo.com;Facing the same circular reference error in Google Protobuf object with spark version 3.3.0

Would be nice if we have an api to allow skip of circular reference that seems to be quite common.;;;, 11/Oct/22 11:23;viveck_s@yahoo.com;Also can the priority of this Jira be increased from current 3-Minor (that has workaround ??) to higher priority.

(I don't see any obvious workarounds mentioned here either);;;, 11/Oct/22 11:49;jacklzg;There is a PR here: [add Support Java Class with circular references by 1zg12 · Pull Request #37738 · apache/spark (github.com)|https://github.com/apache/spark/pull/37738];;;, 11/Oct/22 14:13;viveck_s@yahoo.com;Thanks [~jacklzg] , I see the PR, however it seems there are failing tests with this change and of course someone on the Apache Spark PMC / Committer has to review and get it to a mergeable state.

 

But yes, I agree, even I am facing issue converting a Dataset<Row> into a Dataset<My_GoogleProtobufGeneratedObject> due to the circular reference of class com.google.protobuf.Descriptors$Descriptor that is present in any Google protobuf generated code.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Introduce a config variable for the incrementalCollects row batch size
Issue key: SPARK-36816
Issue id: 13402493
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: okoenecke
Creator: okoenecke
Created: 21/Sep/21 14:54
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: After enabling *_spark.sql.thriftServer.incrementalCollects_* Thrift will execute queries in batches (as intended). Unfortunately the batch size cannot be configured as it seems to be hardcoded [here|https://github.com/apache/spark/blob/6699f76fe2afa7f154b4ba424f3fe048fcee46df/sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/thrift/ThriftCLIServiceClient.java#L404]. It would be useful to configure that value to be able to adjust it to your environment.

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Sep 27 08:49:21 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v5dk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 26/Sep/21 03:56;gurwls223;Can you show the reproducible steps, current input/output and expected intput/output?;;;, 27/Sep/21 08:49;okoenecke;I am running a Thrift Server {{/spark/sbin/start-thriftserver.sh}} with {{--conf spark.sql.thriftServer.incrementalCollect=true}} to prevent OutOfMemory Exceptions. Querying data results in batched result sets (as intended) with log messages like this:
{code:bash}
21/09/27 08:25:33 INFO SparkExecuteStatementOperation: Returning result set with 1000 rows from offsets [932000, 933000) with 50f346c0-02d4-40a2-a73c-30d326d2aae{code}
I'd like to be able to configure the value of {{1000 rows }}to be able to adjust that value to our server capacity. Result would look like this:
{code:java}
21/09/27 08:25:33 INFO SparkExecuteStatementOperation: Returning result set with 10000 rows from offsets [932000, 942000) with 50f346c0-02d4-40a2-a73c-30d326d2aae{code};;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Spark session does not update number of files for partition
Issue key: SPARK-40430
Issue id: 13481575
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: ffcms
Creator: ffcms
Created: 14/Sep/22 20:30
Updated: 25/Oct/22 19:49
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: When a spark session has already queried data from a table and partition and new files are inserted into the partition externally, the spark session keeps the outdated number of files and does not return the new records.
If the data is inserted into a new partition, the problem will not occur.

Steps to reproduce the behavior:

Open a Spark session
Query a count in a table
Open another spark session
insert data into an existing partition
Check the count again in the first session


I expect to see the inserted records.
Environment: I'm using spark 3.1.2 on AWS EMR and AWS Glue as catalog.
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 14/Sep/22 20:31;ffcms;session 1.png;https://issues.apache.org/jira/secure/attachment/13049304/session+1.png, 14/Sep/22 20:31;ffcms;session 2.png;https://issues.apache.org/jira/secure/attachment/13049303/session+2.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 2.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Oct 25 19:49:56 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z18ke0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Oct/22 03:47;ivan.sadikov;Can you try FSCK REPAIR TABLE command on your table if you use metastore or run REFRESH (https://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-refresh.html)?

Metadata is cached so it is likely you need to refresh the table to get the updates.;;;, 25/Oct/22 19:49;ffcms;It works with MSCK REPAIR TABLE.
Is there a way to force always update metadata in spark session, or do i have to know that the metadata is out of date and run MSCK REPAIR TABLE?

Thanks [~ivan.sadikov];;;
Affects Version/s.1: 3.2.0
Attachment.1: 14/Sep/22 20:31;ffcms;session 2.png;https://issues.apache.org/jira/secure/attachment/13049303/session+2.png
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1

Summary: why spark-X.X.X-bin-without-hadoop.tgz does not provide spark-hive_X.jar (and spark-hive-thriftserver_X.jar)
Issue key: SPARK-37130
Issue id: 13408678
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: pduroux
Creator: pduroux
Created: 27/Oct/21 12:42
Updated: 19/Aug/22 04:02
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Deploy
Due Date: 
Votes: 0
Labels: 
Description: Hi,

As my deployment is having its own Hadoop(+Hive) installed, I have tried to install Spark using  its bundle without Hadoop. I suspect that some jars are missing that are present in the corresponding spark-X.X.X-bin-hadoop3.2.tgz.

After comparing their contents both spark-hive_2.12-X.X.X.jar and spark-hive-thriftserver_2.12-X.X.X.jar are not in the spark-X.X.X-bin-without---hadoop.tgz. And I don't know if some others should also be there.

Thanks,

Patrice

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Aug 19 04:02:30 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0w7i8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Oct/21 12:49;pduroux;ps: a diff output

{{$ diff spark-without.lst spark-with.lst }}
{{667a668}}
{{> /jars/activation-1.1.1.jar}}
{{671a673}}
{{> /jars/antlr-runtime-3.5.2.jar}}
{{684a687}}
{{> /jars/bonecp-0.8.0.RELEASE.jar}}
{{689a693}}
{{> /jars/commons-cli-1.2.jar}}
{{694a699}}
{{> /jars/commons-dbcp-1.4.jar}}
{{695a701}}
{{> /jars/commons-lang-2.6.jar}}
{{696a703}}
{{> /jars/commons-logging-1.1.3.jar}}
{{698a706}}
{{> /jars/commons-pool-1.5.4.jar}}
{{701a710,717}}
{{> /jars/curator-client-2.13.0.jar}}
{{> /jars/curator-framework-2.13.0.jar}}
{{> /jars/curator-recipes-2.13.0.jar}}
{{> /jars/datanucleus-api-jdo-4.2.4.jar}}
{{> /jars/datanucleus-core-4.1.17.jar}}
{{> /jars/datanucleus-rdbms-4.1.19.jar}}
{{> /jars/derby-10.14.2.0.jar}}
{{> /jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar}}
{{704c720,739}}
{{< /jars/gson-2.8.6.jar}}
{{---}}
{{> /jars/gson-2.2.4.jar}}
{{> /jars/guava-14.0.1.jar}}
{{> /jars/hadoop-client-api-3.3.1.jar}}
{{> /jars/hadoop-client-runtime-3.3.1.jar}}
{{> /jars/hadoop-shaded-guava-1.1.1.jar}}
{{> /jars/hadoop-yarn-server-web-proxy-3.3.1.jar}}
{{> /jars/HikariCP-2.5.1.jar}}
{{> /jars/hive-beeline-2.3.9.jar}}
{{> /jars/hive-cli-2.3.9.jar}}
{{> /jars/hive-common-2.3.9.jar}}
{{> /jars/hive-exec-2.3.9-core.jar}}
{{> /jars/hive-jdbc-2.3.9.jar}}
{{> /jars/hive-llap-common-2.3.9.jar}}
{{> /jars/hive-metastore-2.3.9.jar}}
{{> /jars/hive-serde-2.3.9.jar}}
{{> /jars/hive-service-rpc-3.1.2.jar}}
{{> /jars/hive-shims-0.23-2.3.9.jar}}
{{> /jars/hive-shims-2.3.9.jar}}
{{> /jars/hive-shims-common-2.3.9.jar}}
{{> /jars/hive-shims-scheduler-2.3.9.jar}}
{{705a741}}
{{> /jars/hive-vector-code-gen-2.3.9.jar}}
{{708a745,747}}
{{> /jars/htrace-core4-4.1.0-incubating.jar}}
{{> /jars/httpclient-4.5.13.jar}}
{{> /jars/httpcore-4.4.14.jar}}
{{712a752}}
{{> /jars/jackson-core-asl-1.9.13.jar}}
{{715a756}}
{{> /jars/jackson-mapper-asl-1.9.13.jar}}
{{724a766,767}}
{{> /jars/javax.jdo-3.2.0-m3.jar}}
{{> /jars/javolution-5.5.1.jar}}
{{727a771}}
{{> /jars/jdo-api-3.0.1.jar}}
{{734a779,783}}
{{> /jars/jline-2.14.6.jar}}
{{> /jars/joda-time-2.10.10.jar}}
{{> /jars/jodd-core-3.5.2.jar}}
{{> /jars/jpam-1.1.jar}}
{{> /jars/json-1.8.jar}}
{{739a789}}
{{> /jars/jta-1.1.jar}}
{{765a816,818}}
{{> /jars/libfb303-0.9.3.jar}}
{{> /jars/libthrift-0.12.0.jar}}
{{> /jars/log4j-1.2.17.jar}}
{{792a846}}
{{> /jars/protobuf-java-2.5.0.jar}}
{{804a859,860}}
{{> /jars/slf4j-api-1.7.30.jar}}
{{> /jars/slf4j-log4j12-1.7.30.jar}}
{{809a866,867}}
{{> /jars/spark-hive_2.12-3.2.0.jar}}
{{> /jars/spark-hive-thriftserver_2.12-3.2.0.jar}}
{{829a888,889}}
{{> /jars/ST4-4.0.4.jar}}
{{> /jars/stax-api-1.0.1.jar}}
{{830a891}}
{{> /jars/super-csv-2.2.0.jar}}
{{832a894}}
{{> /jars/transaction-api-1.1.jar}}
{{833a896}}
{{> /jars/velocity-1.5.jar}}
{{836a900,901}}
{{> /jars/zookeeper-3.6.2.jar}}
{{> /jars/zookeeper-jute-3.6.2.jar}}
{{919a985}}
{{> /python/dist/}}
{{1015a1082,1087}}
{{> /python/pyspark.egg-info/}}
{{> /python/pyspark.egg-info/dependency_links.txt}}
{{> /python/pyspark.egg-info/PKG-INFO}}
{{> /python/pyspark.egg-info/requires.txt}}
{{> /python/pyspark.egg-info/SOURCES.txt}}
{{> /python/pyspark.egg-info/top_level.txt}}
{{1269a1342,1346}}
{{> /python/pyspark/__pycache__/}}
{{> /python/pyspark/__pycache__/install.cpython-38.pyc}}
{{> /python/pyspark/python/}}
{{> /python/pyspark/python/pyspark/}}
{{> /python/pyspark/python/pyspark/shell.py}}
{{1497a1575,1579}}
{{> /R/lib/SparkR/doc/}}
{{> /R/lib/SparkR/doc/index.html}}
{{> /R/lib/SparkR/doc/sparkr-vignettes.html}}
{{> /R/lib/SparkR/doc/sparkr-vignettes.R}}
{{> /R/lib/SparkR/doc/sparkr-vignettes.Rmd}}
{{1514a1597}}
{{> /R/lib/SparkR/Meta/vignette.rds}};;;, 19/Aug/22 04:02;victor.tso;I have the same question. We want to have Hadoop provided by the environment, but at minimum spark-hive should be there, as the environment couldn't possibly provide that.;;;
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Support ignoreCorruptRecord flag to ensure querying broken sequence file table smoothly
Issue key: SPARK-38639
Issue id: 13435426
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: tonydoen
Creator: tonydoen
Created: 23/Mar/22 19:28
Updated: 29/Jul/22 08:28
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.1
Fix Version/s: 3.2.1
Component/s: SQL
Due Date: 23/Mar/22 00:00
Votes: 0
Labels: 
Description: There's an existing flag "spark.sql.files.ignoreCorruptFiles" and "spark.sql.files.ignoreMissingFiles" that will quietly ignore attempted reads from files that have been corrupted, but it still allows the query to fail on sequence files.

 

Being able to ignore corrupt record is useful in the scenarios that users want to query successfully in dirty data(mixed schema in one table).

 

We would like to add a "spark.sql.hive.ignoreCorruptRecord"  to fill out the functionality.
Environment: 
Original Estimate: 172800.0
Remaining Estimate: 172800.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 172800.0
Σ Remaining Estimate: 172800.0
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jul 29 08:28:00 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10rfs:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 23/Mar/22 20:34;apachespark;User 'TonyDoen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35954;;;, 23/Mar/22 20:49;tonydoen;related pr : [https://github.com/apache/spark/pull/35954];;;, 24/Mar/22 13:21;apachespark;User 'TonyDoen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35962;;;, 24/Mar/22 13:22;apachespark;User 'TonyDoen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35962;;;, 24/Mar/22 14:04;apachespark;User 'TonyDoen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35963;;;, 24/Mar/22 14:05;apachespark;User 'TonyDoen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35963;;;, 28/Mar/22 11:42;apachespark;User 'TonyDoen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35990;;;, 28/Mar/22 11:43;apachespark;User 'TonyDoen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35990;;;, 29/Jul/22 08:27;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/37341;;;, 29/Jul/22 08:28;apachespark;User 'caican00' has created a pull request for this issue:
https://github.com/apache/spark/pull/37341;;;
Affects Version/s.1: 3.2.1
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0

Summary: Parquet written by spark in yarn mode can not be read by spark in local[2+] mode
Issue key: SPARK-39239
Issue id: 13445998
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: kondziolka9ld
Creator: kondziolka9ld
Created: 20/May/22 08:17
Updated: 20/May/22 08:20
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Hi,
I came across a strange issue, namely data written by spark in yarn mode can not be read by spark in local[2+] mode. By saying can not be read I mean that read operations hangs forever. Strangely enough, local[1] is able to read these parquet data. Additionally, repartition of data before writing is some kind of workaround as well. I attached thread dump and in fact, thread waits on latch. I am not sure if it is a bug or some kind of misconfiguration or misunderstanding.
----
h4. Reproduction scenario:
h4. Writer console log:
{code:java}
user@host [] /tmp $ spark-shell --master yarn
[...]
scala> (1 to 1000).toDF.write.parquet("hdfs:///tmp/sample_1")
scala> (1 to 1000).toDF.repartition(42).write.parquet("hdfs:///tmp/sample_2"){code}
h4. Reader console log:
{code:java}
user@host [] /tmp $ spark-shell --master local[2]
[...]
scala> spark.read.parquet("hdfs:///tmp/sample_2").count # data were repartitioned before write
res2: Long = 1000
scala> spark.read.parquet("hdfs:///tmp/sample_1").count # # it will hang forever
 [Stage 5:=============================>                             (1 + 0) / 2]

user@host [] /tmp $ spark-shell --master local[1]
[...]
scala> spark.read.parquet("hdfs:///tmp/sample_1").count
res0: Long = 1000                                                           
     {code}
----
h4. Thread dump of locked thread
{code:java}
"main" #1 prio=5 os_prio=0 tid=0x00007f93b8054000 nid=0x6dce waiting on condition [0x00007f93c0658000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000000eb65eab8> (a scala.concurrent.impl.Promise$CompletionLatch)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)
        at org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:859)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)
        at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
        at org.apache.spark.rdd.RDD$$Lambda$2193/1084000875.apply(Unknown Source)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)
        at org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3006)
        at org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3005)
        at org.apache.spark.sql.Dataset$$Lambda$2847/937335652.apply(Unknown Source)
        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)
        at org.apache.spark.sql.Dataset$$Lambda$2848/1831604445.apply(Unknown Source)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
        at org.apache.spark.sql.execution.SQLExecution$$$Lambda$2853/2038636888.apply(Unknown Source)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
        at org.apache.spark.sql.execution.SQLExecution$$$Lambda$2849/1622269832.apply(Unknown Source)
        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)
        at org.apache.spark.sql.Dataset.count(Dataset.scala:3005)
        at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:24)
        at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:28)
        at $line19.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:30)
        at $line19.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:32)
        at $line19.$read$$iw$$iw$$iw$$iw.<init>(<console>:34)
        at $line19.$read$$iw$$iw$$iw.<init>(<console>:36)
        at $line19.$read$$iw$$iw.<init>(<console>:38)
        at $line19.$read$$iw.<init>(<console>:40)
        at $line19.$read.<init>(<console>:42)
        at $line19.$read$.<init>(<console>:46)
        at $line19.$read$.<clinit>(<console>)
        at $line19.$eval$.$print$lzycompute(<console>:7)
        - locked <0x00000000f67afed8> (a $line19.$eval$)
        at $line19.$eval$.$print(<console>:6)
        at $line19.$eval.$print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
        at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
        at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
        at scala.tools.nsc.interpreter.IMain$$Lambda$1216/588503940.apply(Unknown Source)
        at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
        at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
        at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
        at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
        at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
        at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
        at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:894)
        at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:762)
        at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:464)
        at scala.tools.nsc.interpreter.ILoop.loop(ILoop.scala:485)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:239)
        at org.apache.spark.repl.Main$.doMain(Main.scala:78)
        at org.apache.spark.repl.Main$.main(Main.scala:58)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
 {code}
Dumps for all thread are available as attachment.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 20/May/22 08:18;kondziolka9ld;threaddump_spark_shell;https://issues.apache.org/jira/secure/attachment/13043958/threaddump_spark_shell
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-05-20 08:17:40.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z12jhk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Remove an expired indexFilePath from the ESS shuffleIndexCache or the PBS indexCache to save memory.
Issue key: SPARK-38805
Issue id: 13438163
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: weixiuli
Creator: weixiuli
Created: 06/Apr/22 11:55
Updated: 16/Apr/22 20:02
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.0, 3.1.1, 3.1.2, 3.2.0, 3.2.1
Fix Version/s: 
Component/s: Shuffle
Due Date: 
Votes: 0
Labels: 
Description: Support to automatically remove an expired indexFilePath from the ESS shuffleIndexCache or the PBS indexCache to save memory.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Apr 06 12:26:03 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z117s8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Apr/22 12:25;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36088;;;, 06/Apr/22 12:26;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36088;;;
Affects Version/s.1: 3.1.1
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.7.0, Unknown

Summary: GroupState.getCurrentWatermarkMs() on static Dataframe throws exception but should return -1
Issue key: SPARK-38861
Issue id: 13438975
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: tchristman
Creator: tchristman
Created: 11/Apr/22 14:17
Updated: 16/Apr/22 19:12
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: As per documentation GroupState.getCurrentWatermarkMs() returns -1 when operating on a static Dataframe, as found here: [https://spark.apache.org/docs/3.1.3/api/scala/org/apache/spark/sql/streaming/GroupState.html#getCurrentWatermarkMs():Long]

 

In reality this function throws UnsupportedOperationException as documented when operating on a DataFrame without a watermark set.  This happens even if the static DataFrame had a (useless) watermark configured.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sat Apr 16 19:12:39 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z11crk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 16/Apr/22 19:12;srowen;Can you show the trace, where the UOE comes from?;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: spark-sql can not enable isolatedClientLoader to extend dsv2 catalog when using builtin hiveMetastoreJar
Issue key: SPARK-38642
Issue id: 13435510
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: suheng.cloud
Creator: suheng.cloud
Created: 24/Mar/22 08:20
Updated: 24/Mar/22 08:20
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Hi, all:

I make use of IsolatedClientLoader to enable datasource v2 catalog on hive, It works well on api/spark-shell, while failed on spark-sql cmd.

After dig into source, I found that the SparkSQLCLIDriver(spark-sql) initialize differently by using CliSessionState which will be reused through cli lifecycle.

Thus the IsolatedClientLoader creator in HiveUtils will determine to off isolate because encoutering special global SessionState by that type.In my case, namespaces/tables will not recognized from another hive catalog since a CliSessionState in sparkSession will always be used to connected with.

I notice [SPARK-21428|https://issues.apache.org/jira/browse/SPARK-21428] but think that since the datasource v2 api should be more popular, SparkSQLCLIDriver should also adjust that?

my env:

spark-3.1.2
hadoop-cdh5.13.0
hive-2.3.6
for each v2 catalog set spark.sql.hive.metastore.jars=builtin(we have no auth to deploy jars on target clusters)

Now, for workaround this, we have to deploy jars on hdfs and use 'path' way which cause a significant delay on catalog initialize.

Any help is appreciate, thanks.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-03-24 08:20:36.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z10ry8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.1
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0

Summary: csv parser exception when quote and escape are both double-quote and a value is just "," and column pruning enabled
Issue key: SPARK-38331
Issue id: 13430727
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: causton
Creator: causton
Created: 25/Feb/22 16:01
Updated: 28/Feb/22 13:57
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.1
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Workaround: disable column pruning.

Example pyspark code (from Databricks):
{noformat}
import pyspark
print(pyspark.version.__version__)

# enable column pruning (reset default value)
spark.conf.set('spark.sql.csv.parser.columnPruning.enabled', 'true')

dbutils.fs.put(file='/tmp/example.csv', contents='''"col1","b4_comma","comma","col4"
"","",",","x"
''', overwrite=True)

df = spark.read.csv(
    path='/tmp/example.csv'
    ,inferSchema=True
    ,header=True
    ,escape='"'
    ,multiLine=True
    ,unescapedQuoteHandling='RAISE_ERROR'
    ,mode='FAILFAST'
    )
ex = None
try:
    df.select(df.col1,df.comma).take(1)
except Exception as e:
    ex = e
    
if ex:
    print('[pruning] Exception is raised if b4_comma is NOT selected')
    
df.select(df.b4_comma, df.comma).take(1)
print('[pruning] No exception if b4_comma is selected')

ex = None
try:
    df.count()
except Exception as e:
    ex = e
    
if ex:
    print('[pruning] Exception raised by count')

print('\ndisabling pruning\n')
    
    
# disable column pruning
spark.conf.set('spark.sql.csv.parser.columnPruning.enabled', 'false')
df.select(df.col1,df.comma).take(1)
print('[no prune] No exception if b4_comma is NOT selected') {noformat}
 

Output:
{noformat}
3.1.2
Wrote 47 bytes.
[pruning] Exception is raised if b4_comma is NOT selected
[pruning] No exception if b4_comma is selected
[pruning] Exception raised by count

disabling pruning

[no prune] No exception if b4_comma is NOT selected {noformat}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 28/Feb/22 13:56;causton;example.csv;https://issues.apache.org/jira/secure/attachment/13040530/example.csv
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Feb 28 13:57:45 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zyk0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Feb/22 11:47;LuciferYang;Can you upload a example.csv

 ;;;, 28/Feb/22 13:57;causton;[~LuciferYang] , file uploaded.;;;
Affects Version/s.1: 3.2.1
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.7.0

Summary: Add bucketed scan behavior change to migration guide
Issue key: SPARK-38207
Issue id: 13428318
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: mauzhang
Creator: mauzhang
Created: 14/Feb/22 12:04
Updated: 15/Feb/22 03:55
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Documentation
Due Date: 
Votes: 0
Labels: 
Description: Default behavior of bucketed scan is changed in https://issues.apache.org/jira/browse/SPARK-32859 but it's not mentioned in SQL migration guide.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Feb 15 03:55:47 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0zjsg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 15/Feb/22 03:55;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35514;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: [K8S] Support write container stdout/stderr to file 
Issue key: SPARK-36793
Issue id: 13401905
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: warrenzhu25
Creator: warrenzhu25
Created: 17/Sep/21 17:56
Updated: 13/Feb/22 03:41
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 1
Labels: 
Description: Currently, executor and driver pod only redirect stdout/stderr. If users want to sidecar logging agent to send stdout/stderr to external log storage,  only way is to change entrypoint.sh, which might break compatibility with community version.

We should support this feature, and this feature could be enabled by spark config. Related spark configs are:
|Key|Default|Desc|
|Spark.kubernetes.logToFile.enabled|false|Whether to write executor/driver stdout/stderr as log file|
|Spark.kubernetes.logToFile.path|/var/log/spark|The path to write executor/driver stdout/stderr as log file|
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Feb 13 03:41:04 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v1qw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 17/Sep/21 19:49;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/34035;;;, 13/Feb/22 03:41;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/35501;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Executor pods keep existing if driver container was restarted
Issue key: SPARK-37856
Issue id: 13421791
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: dnskrv
Creator: dnskrv
Created: 10/Jan/22 19:06
Updated: 10/Jan/22 19:08
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: I run Spark Thrift Server on Kubernetes cluster, so the driver pod runs continuously and it creates and manages executor pods. From time to time OOM issue occurs on a driver pod or executor pods.

When it happens on
 * executor - the executor pod is getting deleted and the driver creates a new executor pod instead. It works as expected.
 * driver     - Kubernetes restarts the driver container and the driver creates new executor pods. All previous executors stop, but still exist with *Error* state for Spark 3.1.2 or with *Completed* state for Spark 3.2.0

The behavior can be reproduced by restarting a pod container with the command
{code:java}
kubectl exec POD_NAME -c CONTAINER_NAME -- /sbin/killall5{code}
Property _spark.kubernetes.executor.deleteOnTermination_ is set to *true* by default.

If I delete driver pod all executor pods (in any state) are also deleted completely.

+Pod list+
{code:java}
NAME                                           READY   STATUS      RESTARTS   AGE
spark-thrift-server-85cf5d689b-vvrwd           1/1     Running     1          3d15h
spark-thrift-server-198cc57e3f9a7400-exec-10   1/1     Running     0          86m
spark-thrift-server-198cc57e3f9a7400-exec-6    1/1     Running     0          12h
spark-thrift-server-198cc57e3f9a7400-exec-8    1/1     Running     0          9h
spark-thrift-server-198cc57e3f9a7400-exec-9    1/1     Running     0          3h12m
spark-thrift-server-1a9aee7e31f36eea-exec-17   0/1     Completed   0          38h
spark-thrift-server-1a9aee7e31f36eea-exec-18   0/1     Completed   0          38h
spark-thrift-server-1a9aee7e31f36eea-exec-19   0/1     Completed   0          36h
spark-thrift-server-1a9aee7e31f36eea-exec-21   0/1     Completed   0          24h
 {code}
+Driver pod+
{code:java}
apiVersion: v1
kind: Pod
metadata:
  name: spark-thrift-server-85cf5d689b-vvrwd
  uid: b69a7c68-a767-4e3b-939c-061347b1c25e
spec:
  ...
status:
  containerStatuses:
  - containerID: containerd://7206acf424aa30b6f8533c0e32c99ebfdc5ee80648e76289f6bd2f87460ddcd3
    image: xxx/spark:3.2.0
    lastState:
      terminated:
        containerID: containerd://fe3cacb8e6470ac37dcd50d525ae3d54c8b6bfef3558325bc22e7b40daab1703
        exitCode: 143
        finishedAt: "2022-01-09T16:09:50Z"
        reason: OOMKilled
        startedAt: "2022-01-07T00:32:21Z"
    name: spark-thrift-server
    ready: true
    restartCount: 1
    started: true
    state:
      running:
        startedAt: "2022-01-09T16:09:51Z" {code}
Executor pod
{code:java}
apiVersion: v1
kind: Pod
metadata:
  name: spark-thrift-server-1a9aee7e31f36eea-exec-17
  ownerReferences:
  - apiVersion: v1
    controller: true
    kind: Pod
    name: spark-thrift-server-85cf5d689b-vvrwd
    uid: b69a7c68-a767-4e3b-939c-061347b1c25e
spec:
  ...
status:
  containerStatuses:
  - containerID: containerd://75c68190147ba980f4b9014eef3989ddc2ee30de321fd1119957b6684a995c19
    image: xxx/spark:3.2.0
    lastState: {}
    name: spark-kubernetes-executor
    ready: false
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: containerd://75c68190147ba980f4b9014eef3989ddc2ee30de321fd1119957b6684a995c19
        exitCode: 0
        finishedAt: "2022-01-09T16:08:57Z"
        reason: Completed
        startedAt: "2022-01-09T01:39:15Z" {code}
Environment: Kubernetes 1.20 | Spark 3.1.2 | Hadoop 3.2.0 | Java 11 | Scala 2.12

Kubernetes 1.20 | Spark 3.2.0 | Hadoop 3.3.1 | Java 11 | Scala 2.12
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-01-10 19:06:13.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0yfpc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.2.0
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Broadcast Join throws HintErrorLogger for joins with multiple tables
Issue key: SPARK-37811
Issue id: 13420662
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: tsarangi
Creator: tsarangi
Created: 04/Jan/22 20:29
Updated: 04/Jan/22 21:01
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: broadcast, HINTS, JOIN, SQL
Description: Following query throws HintErrorLogger Warnings in v3.1.2. 

 
{code:java}
// code placeholder
val Query = " SELECT /*+ BROADCASTJOIN(L1, L2, L3) */ " +
              " L1.v1 AS L1V1 " +
              " L4.* " +
              " FROM L1 " +
              " INNER JOIN L2 ON L2.id = L1.id " +
              " INNER JOIN L3 ON L3.id = L1.id " +
              " LEFT JOIN L4 ON L4.id = L1.id AND L4.idx = L2.idx AND L4.time BETWEEN L3.time1 AND L3.time2 "
            {code}
 

Following is the warning it thorws during runtime:
{code:java}
WARN HintErrorLogger: Count not find relation 'L1' specified in hint 'BROADCASTJOIN(L1,L2,L3)'
WARN HintErrorLogger: Count not find relation 'L2' specified in hint 'BROADCASTJOIN(L1,L2,L3)'
WARN HintErrorLogger: Count not find relation 'L3' specified in hint 'BROADCASTJOIN(L1,L2,L3)'{code}
 

The same query didn't have any warnings in v2.4.7. I am not entirely sure if this is inherently not broadcasting the three small tables ({{{}L1, L2, L3{}}}) when doing a Left Join with a bigger table (L4).

I have set \{{autoBroadcastJoinThreshold = 4G }} which is way bigger than L1+L2+L3.

 

Let me know if you need more info.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2022-01-04 20:29:38.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0y8qw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Pandas groupby UDFs would benefit from automatically redistributing data on the groupby key in order to prevent network issues running udf
Issue key: SPARK-37100
Issue id: 13408005
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: richard.williamson
Creator: richard.williamson
Created: 22/Oct/21 20:28
Updated: 23/Oct/21 02:12
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: when running high cardinality pandas udf groupby steps (100,000s+ of unique groups) - jobs will either fail or have high amount of task failures due to network errors on larger clusters 100+ nodes - this was not the specific code causing issues but should be close to representative:


from pyspark.sql.functions import pandas_udf, PandasUDFType
from pyspark.sql.functions import rand
from fancyimpute import IterativeSVD
import numpy as np
import pandas as pd
​
df = spark.range(0, 100000).withColumn('v', rand())
@pandas_udf(df.schema, PandasUDFType.GROUPED_MAP)
def solver(pdf):
pd.DataFrame(data=IterativeSVD(verbose=False).fit_transform(pdf.to_numpy()))
return pdf
​
df.groupby('id').apply(solver).count()
 
df.repartition('id') – this is required to fix it - can we make this automatically happen without any adverse impacts?
Environment: 
Original Estimate: 604800.0
Remaining Estimate: 604800.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 604800.0
Σ Remaining Estimate: 604800.0
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-10-22 20:28:33.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0w3co:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: guava version mismatch with hadoop-aws
Issue key: SPARK-36864
Issue id: 13403627
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: warrenzhu25
Creator: warrenzhu25
Created: 27/Sep/21 17:33
Updated: 27/Sep/21 17:45
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Build
Due Date: 
Votes: 0
Labels: 
Description: When use hadoop-aws 3.2 with spark 3.0, got below error. This is caused by guava version mismatch as hadoop used guava 27.0-jre while spark used 14.0.1.

Exception in thread "main" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;Ljava/lang/Object;)V
 at org.apache.hadoop.fs.s3a.S3AUtils.lookupPassword(S3AUtils.java:742)
 at org.apache.hadoop.fs.s3a.S3AUtils.getAWSAccessKeys(S3AUtils.java:712)
 at org.apache.hadoop.fs.s3a.S3AUtils.createAWSCredentialProviderSet(S3AUtils.java:559)
 at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:52)
 at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:264)
 at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3303)
 at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)
 at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)
 at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)
 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
 at org.apache.spark.util.Utils$.getHadoopFileSystem(Utils.scala:1853)
 at org.apache.spark.deploy.history.EventLogFileWriter.<init>(EventLogFileWriters.scala:60)
 at org.apache.spark.deploy.history.SingleEventLogFileWriter.<init>(EventLogFileWriters.scala:213)
 at org.apache.spark.deploy.history.EventLogFileWriter$.apply(EventLogFileWriters.scala:181)
 at org.apache.spark.scheduler.EventLoggingListener.<init>(EventLoggingListener.scala:66)
 at org.apache.spark.SparkContext.<init>(SparkContext.scala:584)
 at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2588)
 at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:937)
 at scala.Option.getOrElse(Option.scala:189)
 at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:931)
 at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:30)
 at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
 at java.lang.reflect.Method.invoke(Method.java:498)
 at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
 at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:944)
 at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
 at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
 at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
 at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1023)
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1032)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Sep 27 17:45:38 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vcd4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Sep/21 17:45;apachespark;User 'warrenzhu25' has created a pull request for this issue:
https://github.com/apache/spark/pull/34117;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: DPP: Don't insert redundant filters in case static partition pruning can be done
Issue key: SPARK-36819
Issue id: 13402566
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: mannswinky
Creator: mannswinky
Created: 21/Sep/21 20:02
Updated: 21/Sep/21 20:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Don't insert dynamic partition pruning filters in case the filters already referred statically. In case the filtering predicate on dimension table is in joinKey, no need to insert DPP filter in that case.

DPP is not required in this Sample query:
{code:java}
SELECT f.date_id, f.pid, f.sid FROM
 (select date_id, product_id as pid, store_id as sid from fact_stats) as f
 JOIN dim_stats s
 ON f.sid = s.store_id WHERE s.store_id = 3{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Tue Sep 21 20:04:39 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v5ts:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Sep/21 20:04;mannswinky;https://github.com/apache/spark/pull/34062;;;, 21/Sep/21 20:04;apachespark;User 'Swinky' has created a pull request for this issue:
https://github.com/apache/spark/pull/34062;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Spark uncaught exception handler is using logError
Issue key: SPARK-36756
Issue id: 13401046
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: cchantepie
Creator: cchantepie
Created: 14/Sep/21 15:14
Updated: 15/Sep/21 01:19
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.0.4, 3.1.0, 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: Spark is [setting up an handler|https://github.com/apache/spark/blob/v3.0.1/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L915] to catch any uncaught exception.

This [handler itself catch any subsequent exception|https://github.com/apache/spark/blob/v3.0.1/core/src/main/scala/org/apache/spark/util/SparkUncaughtExceptionHandler.scala#L64] that can happen while reporting the initially uncaught exception.

Issue is that if the subsequent exception is due to a log4j issue, as the {{catch}} there is also using {{logError}}, it will loop, not display any exception in the logs, and exiting with code 51.

e.g. We got an log4j issue when using logstash JSON logging :

{noformat}
at net.logstash.log4j.JSONEventLayoutV1.format(JSONEventLayoutV1.java:137)
at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310)
at org.apache.log4j.WriterAppender.append(WriterAppender.java:162)
at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)
at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)
at org.apache.log4j.Category.callAppenders(Category.java:206)
at org.apache.log4j.Category.forcedLog(Category.java:391)
at org.apache.log4j.Category.log(Category.java:856)
at org.slf4j.impl.Log4jLoggerAdapter.error(Log4jLoggerAdapter.java:576)
at org.apache.spark.internal.Logging.logError(Logging.scala:94)
at org.apache.spark.internal.Logging.logError$(Logging.scala:93)
at org.apache.spark.util.SparkUncaughtExceptionHandler.logError(SparkUncaughtExceptionHandler.scala:28)
at org.apache.spark.util.SparkUncaughtExceptionHandler.uncaughtException(SparkUncaughtExceptionHandler.scala:37)
at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1057)
at java.lang.ThreadGroup.uncaughtException(ThreadGroup.java:1052)
at java.lang.Thread.dispatchUncaughtException(Thread.java:1959)
{noformat}

*Suggested fix:*

Directly using {{println}} and {{printStackTrace}} as safe fallback in this {{catch}}.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): java
Custom field (Last public comment date): 2021-09-14 15:14:37.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uwg8:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.0.1
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Spark UI-Stages DAG visualization is empty in IE11
Issue key: SPARK-35823
Issue id: 13384638
Parent id: 13384636.0
Issue Type: Sub-task
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jobitmathew
Creator: jobitmathew
Created: 19/Jun/21 08:22
Updated: 10/Sep/21 06:55
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.3, 3.1.2, 3.2.0, 3.3.0
Fix Version/s: 
Component/s: Web UI
Due Date: 
Votes: 0
Labels: 
Description: 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 08 16:55:14 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s3ag:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 08/Sep/21 16:55;dc-heros;I'm working on this.;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, EMR-6.8.0, EMR-6.8.1, EMR-6.9.0, EMR-6.9.1, Unknown

Summary: spark.history.kerberos.principal doesn't take value _HOST
Issue key: SPARK-36622
Issue id: 13398413
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: pralabhkumar
Creator: pralabhkumar
Created: 31/Aug/21 09:19
Updated: 08/Sep/21 10:59
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Deploy, Security, Spark Core
Due Date: 
Votes: 0
Labels: 
Description: spark.history.kerberos.principal doesn't understand value _HOST. 

It says failure to login for principal : spark/_HOST@realm . 

It will be helpful to take _HOST value via config file and change it with current hostname(similar to what Hive does) . This will also help to run SHS on multiple machines without hardcoding principal hostname.  .spark.history.kerberos.principal

 

It require minor change in HistoryServer.scala in initSecurity  method . 

 

Please let me know , if this request make sense , I'll create the PR . 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Sep 08 10:59:42 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ug7c:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 01/Sep/21 19:25;thejdeep;Is it possible to just use bash eval of `hostname` for this use-case ?;;;, 02/Sep/21 13:15;tgraves;Supported _HOST for SHS likely makes sense since its a server.;;;, 03/Sep/21 17:07;pralabhkumar;[~thejdeep] 

Its better to have _HOST , its been common practice for  hiveserver and similar  projects. 

 

[~tgraves]

Agreed

 

Please let me know  , if you are ok . I can create the PR . 

 ;;;, 06/Sep/21 02:59;angerszhuuu;+1 on this;;;, 06/Sep/21 07:38;apachespark;User 'pralabhkumar' has created a pull request for this issue:
https://github.com/apache/spark/pull/33917;;;, 06/Sep/21 12:37;pralabhkumar;[~angerszhuuu]

[~tgraves]

[~hyukjin.kwon]

 

Have created the PR . Please review 

 

 ;;;, 08/Sep/21 10:59;pralabhkumar;[~angerszhuuu] [~tgraves]

 

Please review the PR ;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, EMR-6.6.0, Unknown

Summary: Expose executionId to QueryExecutionListener
Issue key: SPARK-36658
Issue id: 13399055
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: ivoson
Creator: ivoson
Created: 03/Sep/21 01:57
Updated: 03/Sep/21 04:07
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Now in [QueryExecutionListener|https://github.com/apache/spark/blob/v3.2.0-rc2/sql/core/src/main/scala/org/apache/spark/sql/util/QueryExecutionListener.scala#L38] we have exposed API to get the query execution information:

def onSuccess(funcName: String, qe: QueryExecution, durationNs: Long): Unit

def onFailure(funcName: String, qe: QueryExecution, exception: Exception): Unit

 

But we can not get a clear information that which query is this. In Spark SQL, I think that executionId is the direct identifier of a query execution. So I think it make sense to expose executionId to the QueryExecutionListener, so that people can easily find the exact query in UI or history server to track more information of the query execution. And there is no easy way we can find the relevant executionId from a QueryExecution object. 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Sep 03 04:06:58 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uk60:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 03/Sep/21 02:01;ivoson;cc [~cloud_fan] could you share thoughts about this?;;;, 03/Sep/21 02:28;ivoson;Will create a RP for this.;;;, 03/Sep/21 04:06;apachespark;User 'ivoson' has created a pull request for this issue:
https://github.com/apache/spark/pull/33905;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Volcano resource manager for Spark on Kubernetes
Issue key: SPARK-35623
Issue id: 13381844
Parent id: 
Issue Type: Brainstorming
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: dipanjanK
Creator: dipanjanK
Created: 03/Jun/21 07:15
Updated: 02/Sep/21 11:56
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 2
Labels: kubernetes, resourcemanager
Description: Dear Spark Developers, 
  
 Hello from the Netherlands! Posting this here as I still haven't gotten accepted to post in the spark dev mailing list.
  
 My team is planning to use spark with Kubernetes support on our shared (multi-tenant) on premise Kubernetes cluster. However we would like to have certain scheduling features like fair-share and preemption which as we understand are not built into the current spark-kubernetes resource manager yet. We have been working on and are close to a first successful prototype integration with Volcano ([https://volcano.sh/en/docs/]). Briefly this means a new resource manager component with lots in common with existing spark-kubernetes resource manager, but instead of pods it launches Volcano jobs which delegate the driver and executor pod creation and lifecycle management to Volcano. We are interested in contributing this to open source, either directly in spark or as a separate project.
  
 So, two questions: 
  
 1. Do the spark maintainers see this as a valuable contribution to the mainline spark codebase? If so, can we have some guidance on how to publish the changes? 
  
 2. Are any other developers / organizations interested to contribute to this effort? If so, please get in touch.
  
 Best,
 Dipanjan
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Sep 02 11:27:40 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rm2g:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): holden
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 11/Jun/21 03:42;pingsutw;[~dipanjanK] Thanks for proposing this feature.

I'm interested to contribute this feature.;;;, 21/Jun/21 11:48;dipanjanK;Hi [~pingsutw], thank you for expressing your interest! We are in the process of publishing a first draft. In the meantime, how can we contact you, to maybe give you a more detailed overview? Do you have a preferred email address?;;;, 21/Jun/21 23:48;k82cn;That's interesting, I'd like to help on that :);;;, 23/Jun/21 22:13;holden;I'm also interested in this. I sent a message to the dev list back on Jun 17th about this (or more generally adding support for batch schedulers in general).

 

I know some groups inside of Spark have had a working group format where they sync periodically and write reports back to the mailing list. Since it seems like there are a few folks  interested maybe we could try that?;;;, 24/Jun/21 02:11;pingsutw;[~dipanjanK] Here is my email address. pingsutw@gmail.com;;;, 24/Jun/21 12:15;dipanjanK;I added our WIP code on Github: [https://github.com/spark-volcano-wip/spark-3-volcano. |https://github.com/spark-volcano-wip/spark-3-volcano/blob/main/README.md#how-it-works]

Please feel free to take a look and comment.

High level overview in [https://github.com/spark-volcano-wip/spark-3-volcano/blob/main/README.md#how-it-works]

Please let me know if you'd like to be added as a collaborator. 

[~holden] - sounds like a great idea. Certainly me and my colleagues on working on this project would be interested in it.  What format of collaboration are you thinking? More face to face such as Slack or Google Meets, or something more async like Google Groups? 

 

 ;;;, 24/Jun/21 12:26;dipanjanK;[~pingsutw] - I sent you an invite as a collaborator.;;;, 02/Sep/21 11:27;senthh;[~dipanjanK] Include me too pls.

mail id: senthissenthh@gmail.com;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0

Summary: KinesisInitialPosition interface should be public
Issue key: SPARK-36520
Issue id: 13395428
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: noamtm
Creator: noamtm
Created: 16/Aug/21 08:29
Updated: 16/Aug/21 08:37
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: DStreams
Due Date: 
Votes: 0
Labels: kinesis
Description: In `org.apache.spark.streaming.kinesis` there's a Java interface `KinesisInitialPosition` - for some reason it's not public, so can't be used.

The reason it needs to be used is simple: it is the parent of `KinesisInitialPositions.Latest` and `KinesisInitialPositions.TrimHorizon`. Without using it, it's impossible to pass instances of `Latest` and `TrimHorizon` around without calling them by name.

The workaround is to use the enum `InitialPositionInStream` instead, and a factory method `fromKinesisInitialPosition(..)` - but it doesn't seem like the correct pattern, because it belongs to Amazon Kinesis Client library and not Spark (it's currently the only import in my class which is outside of `org.apache.spark`).

The change is trivial, so it's mainly a matter of why *not* to do it.

KinesisInitialPositions: https://github.com/apache/spark/blob/master/external/kinesis-asl/src/main/java/org/apache/spark/streaming/kinesis/KinesisInitialPositions.java
KinesisInitialPosition: https://github.com/awslabs/amazon-kinesis-client/blob/master/amazon-kinesis-client/src/main/java/software/amazon/kinesis/common/InitialPositionInStream.java
Environment: 
Original Estimate: 3600.0
Remaining Estimate: 3600.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 3600.0
Σ Remaining Estimate: 3600.0
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Java
Custom field (Last public comment date): 2021-08-16 08:29:17.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0txsg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Unable to Write non-Delta Formats to Azure Data Lake Gen2 Due to Owner of the Parent Folder's Permissions
Issue key: SPARK-36411
Issue id: 13393433
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: ossuleim
Creator: ossuleim
Created: 04/Aug/21 10:03
Updated: 04/Aug/21 10:04
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Input/Output
Due Date: 
Votes: 0
Labels: 
Description: When writing a Spark DataFrame to Azure Data Lake Gen2 storage using any format other than "Delta", a folder is created on the filesystem as expected but the permissions of the owner of that folder are the same as the permissions of the parent folder owner. If the parent folder owner does not have any access permissions, the write command will create the folder on the filesystem with the same permissions of the parent folder owner for our new owner. This is an expected behavior as per engineering team of ADLS Gen2. 

The write will fail with the following error:

Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4) (10.10.10.10 executor 0): Operation failed: "This request is not authorized to perform this operation using this permission.", 403, PUT, [https://storageName.dfs.core.windows.net/path/to/folder/_started_2238238532712736832?resource=file&timeout=90|https://storagename.dfs.core.windows.net/path/to/folder/_started_2238238532712736832?resource=file&timeout=90], AuthorizationPermissionMismatch, "This request is not authorized to perform this operation using this permission. RequestId:xxxx Time:2021-08-03T08:35:26.1753745Z" at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsRestOperation.execute(AbfsRestOperation.java:237) at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsClient.createPath(AbfsClient.java:311) at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystemStore.createFile(AzureBlobFileSystemStore.java:501) at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem.create(AzureBlobFileSystem.java:208) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892) at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789) at com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFileAbsPath(DirectoryAtomicCommitProtocol.scala:123) at com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol.newTaskTempFile(DirectoryAtomicCommitProtocol.scala:104) at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:121) at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:111) at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:327) at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:266) at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75) at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110) at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75) at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55) at org.apache.spark.scheduler.Task.doRunTask(Task.scala:150) at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119) at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110) at org.apache.spark.scheduler.Task.run(Task.scala:91) at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:789) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1643)
 
Suggested solution:

 Instead of letting the storage create the folder implicitly when writing the files, if we create the folder on the storage first using fs.mkdirs() (same as we do for Delta to create the delta log folder), this should fix the issue:

Delta writer:

// first create _delta_log directory

import org.apache.hadoop.fs.Path

val logPath = new Path(deltaPath, "_delta_log")
 val fs = logPath.getFileSystem(spark.sessionState.newHadoopConf)
 fs.mkdirs(logPath)

Parquet writer:

val finalPath = new Path(parquetPath, fileName)
 val fs = finalPath.getFileSystem(spark.sessionState.newHadoopConf)
 val txnId = math.abs(scala.util.Random.nextLong).toString
 val startMarker = new Path(finalPath.getParent, new Path(s"_started_$txnId"))
 fs.create(startMarker, false).close()

Instead for the Parquet/CSV/JSON ... etc. writers, we can add an "fs.mkdirs(parquetPath)" to create the folder as a first step before writing the files when it is writing to ADLS Gen1 and ADLS Gen2.
Environment: Azure Databricks 8.4, Spark 3.1.2, [https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/8.4#system-environment.]

Azure Data Lake Gen2 used as the location for the writers.
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-04 10:03:01.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tlh4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Google Kubernetes Engine authentication fails
Issue key: SPARK-36366
Issue id: 13392830
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: tiagovrtr
Creator: tiagovrtr
Created: 30/Jul/21 22:53
Updated: 30/Jul/21 22:53
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: google, kubernetes, kubernetesexecutor, newbie
Description: When connecting to a Google Kubernetes Engine, a command {{gcloud container clusters get-credentials}} is used that generates a {{~/.kube/config}} file. The distinctive trait in this config file is that it uses an {{auth-provider}} relying on {{gcloud}} to inject the keys {{expiry}} and {{access-token}} from the general Google SDK auth config, as seen here:
{code:json}
users:
- name: gke_my-project_my-region_my-cluster
  user:
    auth-provider:
      config:
        cmd-args: config config-helper --format=json
        cmd-path: /Users/user/google-cloud-sdk/bin/gcloud
        expiry-key: '{.credential.token_expiry}'
        token-key: '{.credential.access_token}'
{code}
{{kubectl}}, because it uses {{client-go}}, supports the auth-provider and fetches the token and expiry from the json returne by config-helper. As Spark is using the fabric8 client, this is yet to be supported, breaking when running spark-submit:
{code:java}
Exception in thread "main" io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://my-endpoint/api/v1/namespaces/my-namespace/pods. Message: Forbidden! User gke_my-project_my-region_my-cluster doesn't have permission. pods is forbidden: User "system:anonymous" cannot create resource "pods" in API group "" in the namespace "my-namespace".
{code}
Environment: 
{code}
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.15", GitCommit:"73dd5c840662bb066a146d0871216333181f4b64", GitTreeState:"clean", BuildDate:"2021-01-13T13:22:41Z", GoVersion:"go1.13.15", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"18+", GitVersion:"v1.18.19-gke.1701", GitCommit:"d7cecefb99b58e8968f59b59d76448eb1e6ea403", GitTreeState:"clean", BuildDate:"2021-06-23T21:51:59Z", GoVersion:"go1.13.15b4", Compiler:"gc", Platform:"linux/amd64"}

$ spark-submit --version
version 3.1.2
Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 11.0.10
{code}
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-30 22:53:06.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0thrc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): liyinan926
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Document local-cluster mode option in developper tools doc
Issue key: SPARK-36335
Issue id: 13392439
Parent id: 
Issue Type: Documentation
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: YActs
Creator: YActs
Created: 29/Jul/21 02:05
Updated: 29/Jul/21 07:11
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Documentation
Due Date: 
Votes: 0
Labels: 
Description: Local-cluster mode is often used by developers but is not documented on any testing related docs. I think It should be documented on developer tools doc (https://spark.apache.org/developer-tools.html).

Also this mode is slightly mentioned on a user reference doc though it is only for unit tests, so we should remove it to avoid confusing people.

 

Related links:

https://issues.apache.org/jira/browse/SPARK-595

[https://github.com/apache/spark/pull/33537]

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jul 29 07:11:34 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0tfco:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 29/Jul/21 05:31;apachespark;User 'yutoacts' has created a pull request for this issue:
https://github.com/apache/spark/pull/33568;;;, 29/Jul/21 07:11;YActs;documented local-cluster mode on developer tools doc.

https://github.com/apache/spark-website/pull/350;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Consider mapper location and shuffle block size in OptimizeLocalShuffleReader
Issue key: SPARK-36234
Issue id: 13391009
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: michaelzhang-db
Creator: michaelzhang-db
Created: 20/Jul/21 22:30
Updated: 20/Jul/21 22:30
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: This is a follow-up to SPARK-36105 (OptimizeLocalShuffleReader support reading data of multiple mappers in one task). We should consider using the mapper locations along with shuffle block size when coalescing mappers (specifically in events where there are more mappers than there is parallelism.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-07-20 22:30:48.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0t6iw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Incorrect pyspark.sql.types.Row __new__ and __init__ type annotations
Issue key: SPARK-36220
Issue id: 13390812
Parent id: 
Issue Type: Bug
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: tobiasedwards
Creator: tobiasedwards
Created: 20/Jul/21 03:26
Updated: 20/Jul/21 03:56
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: This bug involves incorrect type annotations for {{pyspark.sql.types.Row}}'s {{\_\_new\_\_}} and {{\_\_init\_\_}} methods when invoked without keyword arguments (_i.e._, {{\*args}} rather than {{\*\*kwargs}}).

When creating a [Row|https://hyukjin-spark.readthedocs.io/en/latest/reference/api/pyspark.sql.types.Row.html] with unnamed fields which are not of type {{str}} (_e.g._, {{row1 = Row("Alice", 11)}} appears in the {{Row}} documentation) type checkers produce an error.

The implementation doesn't assume the arguments are of type {{str}}, and in fact the documentation includes an example where non-{{str}} types are provided in this way (see [the final example here|https://hyukjin-spark.readthedocs.io/en/latest/reference/api/pyspark.sql.types.Row.html]).

An example of the type error produced by [pyright|https://github.com/microsoft/pyright] is

{code}
error: No overloads for "__init__" match the provided arguments
    Argument types: (Literal['Alice'], Literal[11]) (reportGeneralTypeIssues)
{code}

Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python
Custom field (Last public comment date): Tue Jul 20 03:56:50 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0t5b4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 20/Jul/21 03:56;apachespark;User 'tobiasedwards' has created a pull request for this issue:
https://github.com/apache/spark/pull/33428;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Support pathlib.PurePath-like objects in DataFrameReader / DataFrameWriter
Issue key: SPARK-35919
Issue id: 13386358
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: ei-grad
Creator: ei-grad
Created: 28/Jun/21 22:51
Updated: 28/Jun/21 22:51
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.8, 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: 
Description: It would be nice to support Path objects in `spark.\{read,write}.\{parquet,orc,csv,...etc}` methods.

Without pyspark source code changes it currently seems possible only by the ugly monkeypatching hacks - https://stackoverflow.com/q/68170685/2649222.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-06-28 22:51:52.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0sdwg:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: DPP: Update exprId for IN subquery
Issue key: SPARK-35911
Issue id: 13386126
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: mannswinky
Creator: mannswinky
Created: 27/Jun/21 18:45
Updated: 27/Jun/21 18:52
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Change exprId for IN subquery for DPP in executed plan; to have same expr Id as DynamicPruning filter in optimized plan.
This minor change shall make debugging easier in complex queries.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Jun 27 18:52:49 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0scgw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Jun/21 18:52;apachespark;User 'Swinky' has created a pull request for this issue:
https://github.com/apache/spark/pull/33110;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: CatalogFileIndex only used if Partition Columns Nonempty
Issue key: SPARK-35864
Issue id: 13385464
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: jdcasale
Creator: jdcasale
Created: 23/Jun/21 16:48
Updated: 23/Jun/21 16:48
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Currently, when deciding whether to use a CatalogFileIndex, we gate on catalogTable.get.partitionColumnNames.nonEmpty ([see here|[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L398])] I believe what we actually want is to check that the catalogTable.get.dataSchema.nonEmpty, as I don't think it's actually necessary that a table have partition columns in order to be read from a CatalogFileIndex. This isn't a correctness issue, just a missed optimization any time there are no partition columns for a table.

 

Palantir [fixed this in our fork|https://github.com/palantir/spark/commit/e040ff5bf4d1b2d37264ad19468e0892c63b9798] a long time ago, but I don't think anyone ever remembered to push it upstream.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-06-23 16:48:20.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0s8ds:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Support Joint eviction strategies for cached RDD partitions
Issue key: SPARK-35751
Issue id: 13383662
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: qfoxzjd
Creator: qfoxzjd
Created: 14/Jun/21 06:01
Updated: 14/Jun/21 06:32
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Block Manager
Due Date: 
Votes: 0
Labels: 
Description: As of now, there's as it were ousting procedure for cached RDD segment in Spark. The default RDD removal methodology is LRU .When memory space not adequate for RDD caching, a few allotments will be ousted, on the off chance that these segments are utilized once more latterly, they will be replicated by the Ancestry data and cached in memory once more. The replicate stage will bring in extra taken a toll. Be that as it may, LRU has no ensure for the most reduced duplicate cost. The to begin with RDD that required to be cached is ordinarily created by perusing from HDFS and doing a few changes. The perusing operation ordinarily fetched longer time than other Start transformations. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 14 06:32:46 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0rxa0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Jun/21 06:32;apachespark;User 'qfoxzjd' has created a pull request for this issue:
https://github.com/apache/spark/pull/32900;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: SaveMode.Overwrite not usable when using s3a root paths 
Issue key: SPARK-34298
Issue id: 13355661
Parent id: 
Issue Type: Bug
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: cornelcreanga
Creator: cornelcreanga
Created: 30/Jan/21 16:32
Updated: 03/Jun/21 17:54
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Spark Core
Due Date: 
Votes: 0
Labels: 
Description: SaveMode.Overwrite does not work when using paths containing just the root eg "s3a://peakhour-report". To reproduce the issue (an s3 bucket + credentials are needed):

{color:#0033b3}val {color}{color:#000000}out {color}= {color:#067d17}"s3a://peakhour-report"{color}

{color:#0033b3}val {color}{color:#000000}sparkContext{color}: {color:#000000}SparkContext {color}= {color:#000000}SparkContext{color}.getOrCreate()
{color:#0033b3}val {color}{color:#000000}someData {color}= {color:#871094}Seq{color}(Row({color:#1750eb}24{color}, {color:#067d17}"mouse"{color}))
{color:#0033b3}val {color}{color:#000000}someSchema {color}= {color:#871094}List{color}(StructField({color:#067d17}"age"{color}, {color:#000000}IntegerType{color}, {color:#0033b3}true{color}),StructField({color:#067d17}"word"{color}, {color:#000000}StringType{color},{color:#0033b3}true{color}))

{color:#0033b3}val {color}{color:#000000}someDF {color}= {color:#871094}spark{color}.createDataFrame(
 {color:#871094}spark{color}.sparkContext.parallelize({color:#000000}someData{color}),StructType({color:#000000}someSchema{color}))
{color:#000000}sparkContext{color}.hadoopConfiguration.set({color:#067d17}"fs.s3a.access.key"{color}, accessK{color:#000000}ey{color}))
{color:#000000}sparkContext{color}.hadoopConfiguration.set({color:#067d17}"fs.s3a.secret.key"{color}, {color:#000000}secretKey{color}))
{color:#000000}sparkContext{color}.hadoopConfiguration.set({color:#067d17}"fs.s3a.aws.credentials.provider"{color}, {color:#067d17}"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"{color})
{color:#000000}sparkContext{color}.hadoopConfiguration.set({color:#067d17}"fs.s3a.impl"{color}, {color:#067d17}"org.apache.hadoop.fs.s3a.S3AFileSystem"{color})
{color:#000000}someDF{color}.write.format({color:#067d17}"parquet"{color}).partitionBy({color:#067d17}"age"{color}).mode({color:#000000}SaveMode{color}.{color:#871094}Overwrite{color})
 .save({color:#000000}out{color})

 

Error stacktrace:

Exception in thread "main" java.lang.IllegalArgumentException: Can not create a Path from an empty string
 at org.apache.hadoop.fs.Path.checkPathArg(Path.java:168)[....]
at org.apache.hadoop.fs.Path.suffix(Path.java:446)
 at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions(InsertIntoHadoopFsRelationCommand.scala:240)

 

If you change out from {color:#0033b3}val {color}{color:#000000}out {color}= {color:#067d17}"s3a://peakhour-report"{color} to {color:#0033b3}val {color}{color:#000000}out {color}= {color:#067d17}"s3a://peakhour-report/folder" {color:#172b4d}the code works.{color}{color}

{color:#067d17}{color:#172b4d}There are two problems in the actual code from InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions: {color}{color}

{color:#067d17}{color:#172b4d}a) it uses org.apache.hadoop.fs.Path.suffix method that doesn't work on root paths
{color}{color}

{color:#067d17}{color:#172b4d}b) it tries to delete the root folder directly (in our case the s3 bucket name) and this is prohibited (in the S3AFileSystem class){color}{color}

{color:#067d17}{color:#172b4d}I think that there are two choices:{color}{color}

{color:#067d17}{color:#172b4d}a) throw an explicit error when using overwrite mode for root folders {color}{color}

{color:#067d17}{color:#172b4d}b)fix the actual issue. don't use the Path.suffix method and change the clean up code from InsertIntoHadoopFsRelationCommand.deleteMatchingPartitions to list the root folder content and delete the entries one by one.{color}{color}

I can provide a patch for both choices, assuming that they make sense.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): HADOOP-17744
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Jun 03 17:54:17 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0n65k:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 04/Feb/21 13:59;stevel@apache.org;root dirs are special in that they always exist. Normally apps like spark and hive don't notice this as nobody ever runs jobs which write to the base of file:// or hdfs:// ; object stores are special there.

You might the commit algorithms get a bit confused too. In which case: 

* fixes for the s3a committers welcome;
* there is a serialized test phase in hadoop-aws where we do stuff against the root dir; 
* and a PR for real integration tests can go in https://github.com/hortonworks-spark/cloud-integration
* anything related to the classic MR committer will be rejected out of fear of going near it; not safe for s3 anyway

Otherwise: workaround is to write into a subdir. Sorry;;;, 04/Feb/21 17:48;cornelcreanga;Thanks for the answer. In this wouldn't be better to implement the option a) - throw an explicit error with a meaningful message(eg root dirs are not supported in overwrite mode etc) when trying to use a root dir? Right now one will get an java.lang.IndexOutOfBoundsException and will have to dig into the Spark code in order to understand what's the problem (as it happened to me).;;;, 05/Feb/21 16:51;stevel@apache.org;well, I'm sure a PR with tests will get reviewed...;;;, 03/Jun/21 17:54;stevel@apache.org;Actually, you've just found a bug in Path.suffix, HADOOP-17744. Patches welcome there.

Whether that will fix your problem: who knows. But it will get you further;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: add hostNetwork feature to executor pod
Issue key: SPARK-35572
Issue id: 13381212
Parent id: 
Issue Type: New Feature
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Minor
Resolution: 
Assignee: 
Reporter: zWangSheng
Creator: zWangSheng
Created: 31/May/21 06:00
Updated: 31/May/21 06:08
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: features
Description: Add new feature hostNetwork to executor pod 

Allow spark users to perceive use k8s hostNetwork

In the process of the company's business promotion, the function of k8s hostNetwork is used. But it is found that spark does not support enable hostNetwork in the configuration.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon May 31 06:08:53 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ri80:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 31/May/21 06:08;apachespark;User 'zwangsheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/32708;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Switch the datasource example due to the depreciation of the dataset
Issue key: SPARK-36024
Issue id: 13387833
Parent id: 
Issue Type: Documentation
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: yoda-mon
Creator: yoda-mon
Created: 06/Jul/21 05:01
Updated: 31/Jan/24 15:59
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Documentation
Due Date: 
Votes: 0
Labels: 
Description: The S3 bucket that used for an example in "Integration with Cloud Infrastructures" document will be deleted on Jul 1, 2021 [https://registry.opendata.aws/landsat-8/ |https://registry.opendata.aws/landsat-8/]

The dataset will move to another bucket but it requires `--request-payer requester` option so users have to pay S3 cost. [https://registry.opendata.aws/usgs-landsat/]

 

So I think it's better to change the datasource like this.

[https://github.com/yoda-mon/spark/commit/cdb24acdbb57a429e5bf1729502653b91a600022]

 

I chose [NYC Taxi data| [https://registry.opendata.aws/nyc-tlc-trip-records-pds/|https://registry.opendata.aws/nyc-tlc-trip-records-pds/),]] here for an example. 
 Unlike landat data it's not compressed, but it is just an example and there are several tutorials using Spark  (e.g. [https://github.com/aws-samples/amazon-eks-apache-spark-etl-sample)]

 

Reed test result
{code:java}
scala> sc.textFile("s3a://nyc-tlc/misc/taxi _zone_lookup.csv").take(10).foreach(println) "LocationID","Borough","Zone","service_zone" 1,"EWR","Newark Airport","EWR" 2,"Queens","Jamaica Bay","Boro Zone" 3,"Bronx","Allerton/Pelham Gardens","Boro Zone" 4,"Manhattan","Alphabet City","Yellow Zone" 5,"Staten Island","Arden Heights","Boro Zone" 6,"Staten Island","Arrochar/Fort Wadsworth","Boro Zone" 7,"Queens","Astoria","Boro Zone" 8,"Queens","Astoria Park","Boro Zone" 9,"Queens","Auburndale","Boro Zone"
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): HADOOP-17784, HADOOP-19057
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Wed Jan 31 15:59:15 UTC 2024
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0smyw:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 06/Jul/21 08:57;gurwls223;cc [~stevel@apache.org] FYI.;;;, 06/Jul/21 09:32;stevel@apache.org;similar to HADOOP-17784

I'm "in discussions" with them. Maybe I can persuade them to leave the index file up

And I'd like to move on to a dataset where (a) it's stable (b) got real ORC/Parquet data alongside the CSV

Finally: need to make sure that this time, not matter how "stable" the source is, whoever runs it knows we need it.

Where in the docs is this?

(oh, obviously it'll be something I wrote, won't it...);;;, 07/Jul/21 03:05;yoda-mon;Sorry, I forgot to refer to [HADOOP-17784|https://issues.apache.org/jira/browse/HADOOP-17784].

I think about Hadoop case, the dataset feature above is important to keep test quority. About Spark case, it is document so relatively light issue. If it is important to keep using same bucket/file between Hadoop test and Spark document, my example above is not so good.;;;, 28/Jul/21 19:02;stevel@apache.org;yes, you can change the example. For hadoop we're trying to keep the landsat file around, because removing it breaks regression testing all old releases.;;;, 08/Oct/21 12:49;stevel@apache.org;Amazon are being very nice here and keeping the landsat index file for us: )

even so, it'd be good to move onto something more structured anyway, just to discourage people using .csv.gz as a persistence format;;;, 31/Jan/24 15:59;stevel@apache.org;update, aws bundle is gone. I'm going to go with the nyc data, but pick a parquet file for a change, so we could think about some testing there in future;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Misleading Spark Streaming source documentation
Issue key: SPARK-36984
Issue id: 13406102
Parent id: 
Issue Type: Documentation
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: Novotný
Creator: Novotný
Created: 12/Oct/21 08:16
Updated: 12/Dec/22 18:11
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.1, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Documentation, PySpark, Structured Streaming
Due Date: 
Votes: 0
Labels: 
Description: The documentation at [https://spark.apache.org/docs/latest/streaming-programming-guide.html#advanced-sources] clearly states that *Kafka* (and Kinesis) are available in the Python API v 3.1.2 in *Spark Streaming (DStreams)*. (see attachments for highlight)

However, there is no way to create DStream from Kafka in PySpark >= 3.0.0, as the `kafka.py` file is missing in [https://github.com/apache/spark/tree/master/python/pyspark/streaming]. I'm coming from PySpark 2.4.4 where this was possible. _Should Kafka be excluded as advanced source for spark streaming in Python API in the docs?_

 

Note that I'm aware of this Kafka integration guide [https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html] but I'm not interested in Structured Streaming as it doesn't support arbitrary stateful operations in Python. DStreams support this functionality with `updateStateByKey`.
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 12/Oct/21 08:17;Novotný;docs_highlight.png;https://issues.apache.org/jira/secure/attachment/13034825/docs_highlight.png
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Thu Oct 14 02:14:07 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vrmo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 14/Oct/21 02:14;gurwls223;Yeah we should fix the docs. feel free to edit -  we don't have the support anymore in Dstream. Users should better migrate to Structured Streaming.;;;
Affects Version/s.1: 3.0.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Optimise speed and memory with Pyspark when create DataFrame (with patch)
Issue key: SPARK-36600
Issue id: 13397533
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: pprados
Creator: pprados
Created: 26/Aug/21 11:27
Updated: 12/Dec/22 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: PySpark
Due Date: 
Votes: 0
Labels: easyfix
Description: The Python method {{SparkSession._createFromLocal()}} start to the data, and create a list if it's not an instance of list. But it is necessary only if the scheme is not present.
{quote}# make sure data could consumed multiple times
 if not isinstance(data, list):
  data = list(data)
{quote}
If you use {{createDataFrame(data=_a_generator_,...)}}, all the datas were save in memory in a list, then convert to a row in memory, then convert to buffer in pickle format, etc.

Two lists were present at the same time in memory. The list created by _createFromLocal() and the list created later with
{quote}# convert python objects to sql data
data = [schema.toInternal(row) for row in data]
{quote}
The purpose of using a generator is to reduce the memory footprint when the data are dynamically build.
{quote}def _createFromLocal(self, data, schema):
  """
  Create an RDD for DataFrame from a list or pandas.DataFrame, returns
  the RDD and schema.
  """

  if schema is None or isinstance(schema, (list, tuple)):
    *# make sure data could consumed multiple times*
    *if inspect.isgeneratorfunction(data):* 
      *data = list(data)*
    struct = self._inferSchemaFromList(data, names=schema)
    converter = _create_converter(struct)
    data = map(converter, data)
    if isinstance(schema, (list, tuple)):
      for i, name in enumerate(schema):
        struct.fields[i].name = name
        struct.names[i] = name
      schema = struct

    elif not isinstance(schema, StructType):
      raise TypeError("schema should be StructType or list or None, but got: %s" % schema)

  # convert python objects to sql data
  data = [schema.toInternal(row) for row in data]
  return self._sc.parallelize(data), schema{quote}
Then, it is interesting to use a generator.

 
{quote}The patch:

diff --git a/python/pyspark/sql/session.py b/python/pyspark/sql/session.py
index 57c680fd04..0dba590451 100644
--- a/python/pyspark/sql/session.py
+++ b/python/pyspark/sql/session.py
@@ -15,6 +15,7 @@
 # limitations under the License.
 #
 
+import inspect
 import sys
 import warnings
 from functools import reduce
@@ -504,11 +505,11 @@ class SparkSession(SparkConversionMixin):
 Create an RDD for DataFrame from a list or pandas.DataFrame, returns
 the RDD and schema.
 """
- # make sure data could consumed multiple times
- if not isinstance(data, list):
- data = list(data)
 
 if schema is None or isinstance(schema, (list, tuple)):
+ # make sure data could consumed multiple times
+ if inspect.isgeneratorfunction(data): # PPR
+ data = list(data)
 struct = self._inferSchemaFromList(data, names=schema)
 converter = _create_converter(struct)
 data = map(converter, data)
{quote}
Environment: 
Original Estimate: 7200.0
Remaining Estimate: 7200.0
Time Spent: 
Work Ratio: 0%
Σ Original Estimate: 7200.0
Σ Remaining Estimate: 7200.0
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 26/Aug/21 13:21;pprados;optimize_memory_pyspark.patch;https://issues.apache.org/jira/secure/attachment/13032550/optimize_memory_pyspark.patch
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): Patch
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): Python
Custom field (Last public comment date): Tue Oct 05 00:59:41 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0uas0:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 21/Sep/21 11:38;aholod;Hello! I would like to help with the task. Please assign it to me, if possible ;;;, 05/Oct/21 00:59;gurwls223;You can just create a PR right away. Once that's fixed, it will be assigned to you.;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Decommissioning executors get killed before transferring their data because of the hardcoded timeout of 60 secs
Issue key: SPARK-36872
Issue id: 13403690
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: shkhrgpt
Creator: shkhrgpt
Created: 28/Sep/21 02:41
Updated: 28/Jan/22 03:27
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.1, 3.1.2, 3.2.0
Fix Version/s: 
Component/s: Kubernetes
Due Date: 
Votes: 0
Labels: 
Description: During the graceful decommissioning phase, executors need to transfer all of their shuffle and cache data to the peer executors. However, they get killed before transferring all the data because of the hardcoded timeout value of 60 secs in the decommissioning script. As a result of executors dying prematurely, the spark tasks on other executors fail which causes application failures, and it is hard to debug those failures. To fix the issue, we ended up writing a custom script with a different timeout and rebuilt the spark image but we would prefer an easier solution that does not require rebuilding the image. 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Jan 28 03:27:20 UTC 2022
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vcr4:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 27/Jan/22 03:58;abhisrao;[~shkhrgpt], could you please share more details on which script you are referring to? We're facing similar issues and we're looking for options to fix this.;;;, 27/Jan/22 17:18;shkhrgpt;[~abhisrao] I am referring to the following script:

 

[https://github.com/apache/spark/blob/master/resource-managers/kubernetes/docker/src/main/dockerfiles/spark/decom.sh]

 

 ;;;, 28/Jan/22 03:27;abhisrao;Thanks. We'll have a look at this.;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, EMR-6.6.0

Summary: Missing spark.redaction.string.regex property
Issue key: SPARK-36510
Issue id: 13395217
Parent id: 
Issue Type: Documentation
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: dnskrv
Creator: dnskrv
Created: 13/Aug/21 22:30
Updated: 15/Nov/21 18:26
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: Documentation
Due Date: 
Votes: 0
Labels: 
Description: The property *spark.redaction.string.regex* is missing in [Runtime Environment|https://spark.apache.org/docs/3.1.2/configuration.html#runtime-environment] properties table but referred by *spark.sql.redaction.string.regex* description as its default value
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Fri Aug 13 23:07:45 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0twhk:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 13/Aug/21 23:07;apachespark;User 'dnskr' has created a pull request for this issue:
https://github.com/apache/spark/pull/33740;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Fix behavior inconsistent in Hive table when ‘path’ is provided in SERDEPROPERTIES
Issue key: SPARK-37027
Issue id: 13406921
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: yuzhousun
Creator: yuzhousun
Created: 17/Oct/21 03:34
Updated: 18/Oct/21 18:10
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 2.4.5, 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: If a Hive table is created with both {{WITH SERDEPROPERTIES ('path'='<tableLocation>')}} and {{LOCATION <tableLocation>}}, Spark can return doubled rows when reading the table. This issue seems to be an extension of SPARK-30507.


 Reproduce steps:
 # Create table and insert records via Hive (Spark doesn't allow to insert into table like this)
{code:sql}
CREATE TABLE `test_table`(
  `c1` LONG,
  `c2` STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
WITH SERDEPROPERTIES ('path'='<tableLocationPath>'" )
STORED AS
  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
LOCATION '<tableLocationPath>';

INSERT INTO TABLE `test_table`
VALUES (0, '0');

SELECT * FROM `test_table`;
-- will return
-- 0 0
{code}

 # Read above table from Spark
{code:sql}
SELECT * FROM `test_table`;
-- will return
-- 0 0
-- 0 0
{code}

But if we set {{spark.sql.hive.convertMetastoreParquet=false}}, Spark will return same result as Hive (i.e. single row)

A similar case is that, if a Hive table is created with both {{WITH SERDEPROPERTIES ('path'='<anotherPath>')}} and {{LOCATION <tableLocation>}}, Spark will read both rows under {{anotherPath}} and rows under {{tableLocation}}, regardless of {{spark.sql.hive.convertMetastoreParquet}} ‘s value. However, actually Hive seems to return only rows under {{tableLocation}}

Another similar case is that, if {{path}} is provided in {{TBLPROPERTIES}}, Spark won’t double the rows when {{'path'='<tableLocation>'}}. If {{'path'='<anotherPath>'}}, Spark will read both rows under {{anotherPath}} and rows under {{tableLocation}}, Hive seems to keep ignoring the {{path}} in {{TBLPROPERTIES}}

Code examples for the above cases (diff patch wrote in {{HiveParquetMetastoreSuite.scala}}) can be found in Attachments
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): SPARK-28266
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 17/Oct/21 03:37;yuzhousun;SPARK-37027-test-example.patch;https://issues.apache.org/jira/secure/attachment/13035016/SPARK-37027-test-example.patch
Custom field (Affects version (Component)): 
Custom field (Attachment count): 1.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Oct 18 18:10:16 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0vwns:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 18/Oct/21 18:10;xkrogen;[~yuzhousun] actually this is already fixed by SPARK-28266 in 3.1.3 and 3.2.0. Can you try compiling from latest {{master}} or using the 3.2.0 binaries (not yet officially released but [can be found on Maven Central|https://search.maven.org/search?q=g:org.apache.spark%20AND%20v:3.2.0]).;;;
Affects Version/s.1: 3.1.2
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Document change for Spark sql jdbc
Issue key: SPARK-36801
Issue id: 13402069
Parent id: 
Issue Type: Documentation
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: senthh
Creator: senthh
Created: 19/Sep/21 18:19
Updated: 19/Sep/21 18:59
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.0.0, 3.0.2, 3.0.3, 3.1.0, 3.1.1, 3.1.2
Fix Version/s: 
Component/s: Documentation
Due Date: 
Votes: 0
Labels: 
Description: Reading using Spark SQL jdbc DataSource  does not maintain nullable type and changes "non nullable" columns to "nullable".

 

For example:

mysql> CREATE TABLE Persons(Id int NOT NULL, FirstName varchar(255), LastName varchar(255), Age int);
Query OK, 0 rows affected (0.04 sec)

mysql> show tables;
+-------------------+
| Tables_in_test_db |
+-------------------+
| Persons |
+-------------------+
1 row in set (0.00 sec)

mysql> desc Persons;
+-----------+--------------+------+-----+---------+-------+
| Field | Type | Null | Key | Default | Extra |
+-----------+--------------+------+-----+---------+-------+
| Id | int | NO | | NULL | |
| FirstName | varchar(255) | YES | | NULL | |
| LastName | varchar(255) | YES | | NULL | |
| Age | int | YES | | NULL | |
+-----------+--------------+------+-----+---------+-------+

 

 

{color:#cc7832}val {color}df = spark.read.format({color:#6a8759}"jdbc"{color})
 .option({color:#6a8759}"database"{color}{color:#cc7832},{color}{color:#6a8759}"Test_DB"{color})
 .option({color:#6a8759}"user"{color}{color:#cc7832}, {color}{color:#6a8759}"root"{color})
 .option({color:#6a8759}"password"{color}{color:#cc7832}, {color}{color:#6a8759}""{color})
 .option({color:#6a8759}"driver"{color}{color:#cc7832}, {color}{color:#6a8759}"com.mysql.cj.jdbc.Driver"{color})
 .option({color:#6a8759}"url"{color}{color:#cc7832}, {color}{color:#6a8759}"jdbc:mysql://localhost:3306/Test_DB"{color})
 .option({color:#6a8759}"query"{color}{color:#cc7832}, {color}{color:#6a8759}"(select * from Persons)"{color})
 .load()
df.printSchema()

 

*output:*

 

root
 |-- Id: integer (nullable = true)
 |-- FirstName: string (nullable = true)
 |-- LastName: string (nullable = true)
 |-- Age: integer (nullable = true)

 

 

So we need to add a note, in Documentation[1], "All columns are automatically converted to be nullable for compatibility reasons."

 Ref:

[1 ][https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#jdbc-to-other-databases]

 

 
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Sun Sep 19 18:59:52 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0v2rc:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 19/Sep/21 18:59;apachespark;User 'senthh' has created a pull request for this issue:
https://github.com/apache/spark/pull/34042;;;, 19/Sep/21 18:59;apachespark;User 'senthh' has created a pull request for this issue:
https://github.com/apache/spark/pull/34042;;;
Affects Version/s.1: 3.0.2
Attachment.1: 
EMR Versions: EMR-6.3.0, EMR-6.3.1, EMR-6.4.0, EMR-6.5.0, Unknown

Summary: Improve SQL syntax for MERGE
Issue key: SPARK-36472
Issue id: 13394491
Parent id: 
Issue Type: Improvement
Status: Open
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: dnskrv
Creator: dnskrv
Created: 10/Aug/21 18:42
Updated: 11/Aug/21 10:08
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Existing SQL syntax for *MEGRE* (see Delta Lake examples [here|https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge] and [here|https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/delta-merge-into]) could be improved by adding an alternative for {{<merge_condition>}}

*Main assumption*
 In common cases target and source tables have the same column names used in {{<merge_condition>}} as merge keys, for example:
{code:sql}
ON target.key1 = source.key1 AND target.key2 = source.key2{code}
It would be more convenient to use a syntax similar to:
{code:sql}
ON COLUMNS (key1, key2)
-- or
ON MATCHING (key1, key2)
{code}
The same approach is used for [JOIN|https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html] where {{join_criteria}} syntax is
{code:sql}
ON boolean_expression | USING ( column_name [ , ... ] )
{code}
*Improvement proposal*
 Syntax
{code:sql}
MERGE INTO target_table_identifier [AS target_alias]
USING source_table_identifier [<time_travel_version>] [AS source_alias]
ON { <merge_condition> | COLUMNS ( column_name [ , ... ] ) }
[ WHEN MATCHED [ AND <condition> ] THEN <matched_action> ]
[ WHEN MATCHED [ AND <condition> ] THEN <matched_action> ]
[ WHEN NOT MATCHED [ AND <condition> ]  THEN <not_matched_action> ]
{code}
Example
{code:sql}
MERGE INTO target
USING source
ON COLUMNS (key1, key2)
WHEN MATCHED THEN
    UPDATE SET *
WHEN NOT MATCHED THEN
    INSERT *
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): 2021-08-10 18:42:30.0
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0ts08:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

Summary: Create hive permanent function with owner name
Issue key: SPARK-35913
Issue id: 13386152
Parent id: 
Issue Type: Improvement
Status: In Progress
Project key: SPARK
Project name: Spark
Project type: software
Project lead: matei
Project description: <div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html">Spark SQL</a> for SQL and structured data processing, 
<a href="http://spark.apache.org/docs/latest/mllib-guide.html">MLLib</a> for machine learning, 
<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX</a> for graph processing, and 
<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">Spark Streaming</a>.


<table style="border: 0px; width: 100%;">
<tr>
<td>
For more information, see:
<br><a href="http://spark.apache.org/">The Spark Homepage</a>
<br>
<a href="https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage">The Spark Wiki</a> and <a href="https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark">How to Contribute to Spark</a>
<br>
<a href="https://github.com/apache/spark">Spark's Github Repository</a>
</td>
<td style="text-align: center;">
<a href="http://spark.apache.org/"><img src="http://spark.apache.org/images/spark-logo.png" width="180px"></a>
</td>

</tr>
</table>

</div>
Project url: http://spark.apache.org
Priority: Trivial
Resolution: 
Assignee: 
Reporter: dzcxzl
Creator: dzcxzl
Created: 28/Jun/21 04:54
Updated: 28/Jun/21 05:02
Last Viewed: 26/Jul/24 00:18
Resolved: 
Affects Version/s: 3.1.2
Fix Version/s: 
Component/s: SQL
Due Date: 
Votes: 0
Labels: 
Description: Now create a hive permanent function, no owner name, null value

 
{code:java}
private def toHiveFunction(f: CatalogFunction, db: String): HiveFunction = {
  val resourceUris = f.resources.map { resource =>
    new ResourceUri(ResourceType.valueOf(
      resource.resourceType.resourceType.toUpperCase(Locale.ROOT)), resource.uri)
  }
  new HiveFunction(
    f.identifier.funcName,
    db,
    f.className,
    null,
    PrincipalType.USER,
    (System.currentTimeMillis / 1000).toInt,
    FunctionType.JAVA,
    resourceUris.asJava)
}
{code}
Environment: 
Original Estimate: 
Remaining Estimate: 
Time Spent: 
Work Ratio: 
Σ Original Estimate: 
Σ Remaining Estimate: 
Σ Time Spent: 
Security Level: 
Outward issue link (Blocked): 
Inward issue link (Child-Issue): 
Inward issue link (Duplicate): 
Outward issue link (Duplicate): 
Inward issue link (Problem/Incident): 
Inward issue link (Reference): 
Outward issue link (Reference): 
Outward issue link (dependent): 
Attachment: 
Custom field (Affects version (Component)): 
Custom field (Attachment count): 0.0
Custom field (Blog - New Blog Administrators): 
Custom field (Blog - New Blog PMC): 
Custom field (Blog - Write access): 
Custom field (Blog Administrator?): 
Custom field (Blogs - Admin for blog): 
Custom field (Blogs - Email Address): 
Custom field (Blogs - Existing Blog Access Level): 
Custom field (Blogs - Existing Blog Name): 
Custom field (Blogs - New Blog Write Access): 
Custom field (Blogs - Username): 
Custom field (Bug Category): 
Custom field (Bugzilla - Email Notification Address): 
Custom field (Bugzilla - List of usernames): 
Custom field (Bugzilla - PMC Name): 
Custom field (Bugzilla - Project Name): 
Custom field (Bugzilla Id): 
Custom field (Bugzilla Id).1: 
Custom field (Change Category): 
Custom field (Complexity): 
Custom field (Discovered By): 
Custom field (Docs Text): 
Custom field (Enable Automatic Patch Review): False
Custom field (Epic Colour): 
Custom field (Epic Link): 
Custom field (Epic Name): 
Custom field (Epic Status): 
Custom field (Estimated Complexity): 
Custom field (Evidence Of Open Source Adoption): 
Custom field (Evidence Of Registration): 
Custom field (Evidence Of Use On World Wide Web): 
Custom field (Existing GitBox Approval): 
Custom field (External issue ID): 
Custom field (External issue URL): 
Custom field (Fix version (Component)): 
Custom field (Flags): 
Custom field (Git Notification Mailing List): 
Custom field (Git Repository Import Path): 
Custom field (Git Repository Name): 
Custom field (Git Repository Type): 
Custom field (GitHub Options): 
Custom field (Github Integration): 
Custom field (Github Integrations - Other): 
Custom field (Global Rank): 9223372036854775807
Custom field (INFRA - Subversion Repository Path): 
Custom field (Initial Confluence Contributors): 
Custom field (Language): 
Custom field (Last public comment date): Mon Jun 28 05:01:58 UTC 2021
Custom field (Level of effort): 
Custom field (Machine Readable Info): 
Custom field (Mentor): 
Custom field (New-TLP-TLPName): 
Custom field (Original story points): 
Custom field (Parent Link): 
Custom field (Priority): 
Custom field (Project): 
Custom field (Protected Branch): 
Custom field (Rank): 0|z0scmo:
Custom field (Rank (Obsolete)): 9223372036854775807
Custom field (Review Date): 
Custom field (Reviewer): 
Custom field (Severity): 
Custom field (Severity).1: 
Custom field (Shepherd): 
Custom field (Skill Level): 
Custom field (Source Control Link): 
Custom field (Space Description): 
Custom field (Space Key): 
Custom field (Space Name): 
Custom field (Start Date): 
Custom field (Tags): 
Custom field (Target Version/s): 
Custom field (Target end): 
Custom field (Target start): 
Custom field (Team): 
Custom field (Test and Documentation Plan): 
Custom field (Testcase included): 
Custom field (Tester): 
Custom field (Workaround): 
Comment: 28/Jun/21 05:01;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/33114;;;
Affects Version/s.1: 
Attachment.1: 
EMR Versions: EMR-6.4.0, EMR-6.5.0

